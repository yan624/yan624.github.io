<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yan624.github.io","root":"/","scheme":"Pisces","version":"8.0.0-rc.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"scrollpercent":true,"enable":true,"sidebar":false},"bookmark":{"enable":true,"save":"manual","color":"#222"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="大纲         序号 课程 内容     2~8 吴恩达深度学习 one hot编码、RNN包括双向和深层、GRU、LSTM   9~14 李宏毅机器学习 RNN包括双向和深层、LSTM、RNN反向传播、seq2seq   15~20 李宏毅深度学习 计算图、语言模型中的深度学习、几个有用的网络架构。到原视频的 p12 结束，由于后续部">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达李宏毅综合学习笔记：RNN入门">
<meta property="og:url" content="https://yan624.github.io/posts/5e27260b.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="大纲         序号 课程 内容     2~8 吴恩达深度学习 one hot编码、RNN包括双向和深层、GRU、LSTM   9~14 李宏毅机器学习 RNN包括双向和深层、LSTM、RNN反向传播、seq2seq   15~20 李宏毅深度学习 计算图、语言模型中的深度学习、几个有用的网络架构。到原视频的 p12 结束，由于后续部">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/ont%20hot%E4%BE%8B%E5%AD%90.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84RNN%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84RNN%E5%AE%9E%E4%BE%8B.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/1%20of%20N%20encoding.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/beyond%201-of-N%20encoding.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Feedforward%20Network%E6%97%A0%E6%B3%95%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E4%B8%80%E4%B8%AARNN%E7%9A%84%E5%B0%8F%E5%9E%8B%E4%BE%8B%E5%AD%90.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/deep%20RNN.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Elman%20Network%E5%92%8Cordan%20Network.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Bidirectional%20RNN.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/LSTM%E7%A4%BA%E4%BE%8B.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/LSTM%E4%BE%8B%E5%AD%90%E4%BB%8B%E7%BB%8D.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/LSTM%E4%BE%8B%E5%AD%90%E8%AE%A1%E7%AE%97.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF2.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/RNN%20training%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E5%85%B6%E4%BB%96%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8A%9E%E6%B3%95.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Many%20to%20One.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Many%20to%20Many%EF%BC%88No%20Limitation%EF%BC%89.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Computational%20Graph%E4%BE%8B%E5%AD%90.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Computational%20Graph%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99%E7%A4%BA%E4%BE%8B.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E6%AD%A3%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E5%8F%8D%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Language%20Modeling.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/N-gram.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/NN-based%20LM.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9A%84%E5%87%A0%E7%8E%87.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/RNN-based%20LM.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Challenge%20of%20N-gram.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Feedforward%20NN%E5%92%8CRNN%E7%9A%84%E5%AF%B9%E6%AF%94.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Recursive%20Network%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E6%A0%B9%E6%8D%AE%E5%8F%A5%E5%AD%90%E8%AF%AD%E6%B3%95%E7%BB%93%E6%9E%84%E8%AE%AD%E7%BB%831.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E6%A0%B9%E6%8D%AE%E5%8F%A5%E5%AD%90%E8%AF%AD%E6%B3%95%E7%BB%93%E6%9E%84%E8%AE%AD%E7%BB%832.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Recursive%20Neural%20Tensor%20Network.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84Generation.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Generation%E6%B1%87%E6%80%BB.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Image%20Caption%20Generation.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Attention%E6%9C%BA%E5%88%B6.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Attention%E6%9C%BA%E5%88%B6%E8%AE%A1%E7%AE%97score.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Attention%E6%9C%BA%E5%88%B6%E8%AE%A1%E7%AE%97%E5%AE%8C%E6%AF%95%E5%90%8E%E8%BE%93%E5%85%A5%E5%88%B0decoder.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Attention%20for%20Speach%20Recognition.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%9A%84Memory%20Network.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E6%9B%B4%E5%A4%8D%E6%9D%82%E7%9A%84memory%20network.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E5%AF%B9Generation%E7%9A%84%E5%BB%BA%E8%AE%AE1.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E5%8A%A9%E4%BA%8E%E7%90%86%E8%A7%A3Pointer%20Network%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Pointer%20Network%20for%20Summarization.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/Pointer%20Network%E7%9A%84%E5%81%9A%E6%B3%95.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7%EF%BC%88tricks%EF%BC%89/%E8%AE%AD%E7%BB%83%E5%8D%A1%E4%BD%8F%E4%BA%86%E6%98%AF%E5%9B%A0%E4%B8%BA%EF%BC%9F.jpg">
<meta property="article:published_time" content="2019-04-23T08:46:12.000Z">
<meta property="article:modified_time" content="2020-11-11T14:15:52.604Z">
<meta property="article:author" content="朱冲䶮">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="rnn">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/ont%20hot%E4%BE%8B%E5%AD%90.jpg">

<link rel="canonical" href="https://yan624.github.io/posts/5e27260b.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<!--阿里云矢量库样式-->
<link rel="stylesheet" href="//at.alicdn.com/t/font_1717154_g2j7b1y4jgc.css" />


  <title>吴恩达李宏毅综合学习笔记：RNN入门 | 博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">末流炼丹师</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-常用链接">

    <a href="/%E5%B8%B8%E7%94%A8%E9%93%BE%E6%8E%A5" rel="section"><i class="fas fa-bookmark fa-fw"></i>常用链接</a>

  </li>
        <li class="menu-item menu-item-时间线">

    <a href="/categories/assorted/timeline/" rel="section"><i class="iconfont icon-timeline fa-fw"></i>时间线</a>

  </li>
        
            
  <li class="menu-item menu-item-博客分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>博客分类</a>

  </li>


      
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">183</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    
    	
        <!-- 弹窗插件 -->
    		<link rel="stylesheet" type="text/css" href="/lib/spop/spop.min.css">
        <script type="text/javascript" src="/lib/spop/spop.min.js"></script>
        <!--判断此文是否为特殊的文章-->
        <script>
          var templateSentence = '这是条不可能出现的弹窗提示。';
          if('学习笔记' == '学习笔记')
            templateSentence = '<h4 class="spop-title">注意</h4>此文仅为博主的学习笔记。';
          else if('学习笔记' == '4me')
            templateSentence = '<h4 class="spop-title">注意</h4>此文仅供个人查阅，对于他人没什么太大的价值。';
          spop({
            template: templateSentence,
            group: 'tips',
            position  : 'bottom-center',
            style: 'success',
            autoclose: 5500,
            onOpen: function () {
              //这里设置灰色背景色
            },
            onClose: function() {
              //这里可以取消背景色
              /*spop({
                template: 'ε = = (づ′▽`)づ',
                group: 'tips',
                position  : 'bottom-center',
                style: 'success',
                autoclose: 1500
              });*/
            }
          });
        </script>
    	
    
    	
    

    <link itemprop="mainEntityOfPage" href="https://yan624.github.io/posts/5e27260b.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A6%99%E8%9B%99%E7%A7%8D%E5%AD%90.webp">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达李宏毅综合学习笔记：RNN入门
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-23 16:46:12" itemprop="dateCreated datePublished" datetime="2019-04-23T16:46:12+08:00">2019-04-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-11 22:15:52" itemprop="dateModified" datetime="2020-11-11T22:15:52+08:00">2020-11-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/coding/" itemprop="url" rel="index"><span itemprop="name">coding</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/coding/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="大纲">大纲</h1>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>序号</th>
<th>课程</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2~8</td>
<td>吴恩达深度学习</td>
<td>one hot编码、RNN包括双向和深层、GRU、LSTM</td>
</tr>
<tr class="even">
<td>9~14</td>
<td>李宏毅机器学习</td>
<td>RNN包括双向和深层、LSTM、RNN反向传播、seq2seq</td>
</tr>
<tr class="odd">
<td>15~20</td>
<td>李宏毅深度学习</td>
<td>计算图、语言模型中的深度学习、几个有用的网络架构。到原视频的 p12 结束，由于后续部分涉及到了 GAN 等其他模型，所以不在此处做笔记，详见<a href="https://yan624.github.io/posts/b803ed7e.html">对神经网络整体的理解</a>博文中靠后的几节</td>
</tr>
<tr class="even">
<td>21</td>
<td>李宏毅机器学习/深度学习剩余的一些知识点</td>
<td>这些知识点记录在其他的博客中，此处只提供链接</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<h1 id="字母表示">字母表示</h1>
<p>假设：</p>
<p>x: Harry Potter and Hermione Granger invented a new spell.</p>
<p>y: 1 1 0 1 1 0 0 0 0</p>
<p>其中1代表人名地名之类的单词。这句话一共有九个单词，则x可以表示为：<span class="math inline">\(x^{&lt;1&gt;} x^{&lt;2&gt;} \cdots x^{&lt;t&gt;} \cdots x^{&lt;9&gt;}\)</span>。</p>
<p>则y可以表示为：<span class="math inline">\(y^{&lt;1&gt;} y^{&lt;2&gt;} \cdots y^{&lt;t&gt;} \cdots y^{&lt;9&gt;}\)</span></p>
<p>输入的长度表示为<span class="math inline">\(T_x\)</span>，则<span class="math inline">\(T_x = 9\)</span>。</p>
<p>输出的长度表示为<span class="math inline">\(T_y\)</span>，则<span class="math inline">\(T_y = 9\)</span>。</p>
<p>之前在神经网络中<span class="math inline">\(X^i\)</span>或<span class="math inline">\(X^(i)\)</span>代表第i个训练样本。现在在序列模型中，<span class="math inline">\(X^{(i)&lt;t&gt;}\)</span>代表代表第i个训练样本的第t个元素。对应地，<span class="math inline">\(T^i_x\)</span>就代表第i个样本的输入长度。</p>
<h1 id="one-hot编码">one hot编码</h1>
<p>在我们做自然语言处理时，一件需要事先决定的是，怎么表示一个序列里的单词。</p>
<p>第一件事就是做一张词表（Vocabulary）有时也叫字典（Dictionary），然后将表示方法中要使用的单词列出一列。最后将一个单词用一个稀疏向量表示，如Harry表示为<span class="math inline">\(\begin{pmatrix}0&amp;0&amp;0&amp;\cdots&amp;1&amp;0&amp;\cdots&amp;0\end{pmatrix}\)</span>。1所在位置就是Harry这个单词在词表中的所在位置。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/ont%20hot%E4%BE%8B%E5%AD%90.jpg" alt="ont hot例子" /></p>
<h1 id="循环神经网络rnn">循环神经网络——RNN</h1>
<p>与Simple Neural Network不同的是，循环神经网络的每一层都要有输入x和输出y。</p>
<p>第一步与Simple Neural Network类似，<span class="math inline">\(a_1 = w_{ax} * x^{&lt;1&gt;} + b_a\)</span>，这样就获得了激活值a，但是这时需要使用sigmoid函数或者其他函数直接算出y，另外与Simple Neural Network不同的是，它在计算激活值时需要附带加上前一层的激活值乘上一个权重，此权重与其他的权重类似，也是NN自己训练的。所以第二个序列的计算公式是<span class="math inline">\(a_2 = w_{aa} * a_1 + w_{ax} * x^{&lt;2&gt;} + b_a\)</span>。后面的序列就跟第二个序列一样。<strong>注意一点，RNN中平行方向是时间序列，并不是隐藏层，并且此例中为了方便起见，垂直方向只有一个隐藏层。那几个圆圈是神经元</strong>。看下图。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84RNN%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="吴恩达深度学习中的RNN示意图" /> 1. 由于为了一般化，第一层需要修改成跟后面的计算类似，所以引入一个零向量<span class="math inline">\(a_0\)</span>来计算<span class="math inline">\(a_1\)</span>。 所以RNN的计算公式为： <span class="math display">\[
\left\{ 
    \begin{array}{c}
        a^{&lt;1&gt;} = g_1(w_{aa} * a^{&lt;0&gt;} + w_{ax} * x^{&lt;1&gt;} + b_a)\\
        \hat{y}^{&lt;1&gt;} = g_2(w_{ya} * a^{&lt;1&gt;} + b_y)\\
        a^{&lt;2&gt;} = g_1(w_{aa} * a^{&lt;1&gt;} + w_{ax} * x^{&lt;2&gt;} + b_a)\\
        \hat{y}^{&lt;2&gt;} = g_2(w_{ya} * a^{&lt;2&gt;} + b_y)\\
        \vdots\\
        a^{&lt;t&gt;} = g_1(w_{aa} * a^{&lt;t-1&gt;} + w_{ax} * x^{&lt;t&gt;} + b_a)\\
        \hat{y}^{&lt;t&gt;} = g_2(w_{ya} * a^{&lt;t&gt;} + b_y)\\
    \end{array}
\right. 
\]</span> 注意上式中的<span class="math inline">\(w_{aa}\)</span>、<span class="math inline">\(w_{ax}\)</span>、<span class="math inline">\(w_{ya}\)</span>、<span class="math inline">\(b_{a}\)</span>和<span class="math inline">\(b_{y}\)</span>并没有上标或者下标，所以意味着每一层同一个符号的权重值和偏差值都是一样的。另外对于激活函数也是用户自行选择，在<a href="https://yan624.github.io/posts/b803ed7e.html">对神经网络整体的理解</a>一文中已经解释的很清楚了，为了区分输入与输出的激活函数不同，我特意使用了不同的下标，这个下标仅代表这个意思。</p>
<ol start="2" type="1">
<li>为了进一步地一般化，我们将<span class="math inline">\(w_{aa}\)</span>、<span class="math inline">\(w_{ax}\)</span>合并成为<span class="math inline">\(w_{a}\)</span>，如果表示为矩阵形式就是<span class="math inline">\(w_{a} = \begin{pmatrix}w_{aa} | w_{ax}\end{pmatrix}\)</span>，然后将1中的最后两行表达式一般化为： <span class="math display">\[
a^{&lt;t&gt;} = g_1(w_{a} * [a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_a)\\
\hat{y}^{&lt;t&gt;} = g_2(w_{y} * a^{&lt;t&gt;} + b_y)\\
\]</span> 表达式<span class="math inline">\([a^{&lt;t-1&gt;}, x^{&lt;t&gt;}]\)</span>的意思是将两个向量堆起来，如果表示为矩阵形式就是<span class="math inline">\(\begin{pmatrix} a^{&lt;t-1&gt;}\\ x^{&lt;t&gt;}\\ \end{pmatrix}\)</span>，上式为了排版问题就不写成矩阵形式了。</li>
</ol>
<h2 id="rnn的反向传播">RNN的反向传播</h2>
<p>跟Simple Neural Network类似，也要先定义一个cost function，可以选择crossentropy。由于RNN每一层都有输出值y，所以需要对每一层都求出代价，最后将这些代价值加起来</p>
<div class="note primary">
<pre><code>&lt;p&gt;吴恩达老师在讲反向传播的实现时并没有讲计算过程，所以有点糊里糊涂的。从代价函数到激活值反向传播还可以理解，但是从后一层到前一层的反向传播理解不了。另外由于权重值一样，那么权重值到底该怎么更新？&lt;/p&gt;</code></pre>
</div>
<h2 id="不同类型的rnn">不同类型的RNN</h2>
<p>上面讲到的都是<span class="math inline">\(T_x = T_y\)</span>，但是有时候输入和输出的长度并不相同。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84RNN%E5%AE%9E%E4%BE%8B.jpg" alt="不同类型的RNN实例" /></p>
<p>多对多（many to many）、多对一（many to one）、一对一（one to one）、一对多（one to many）架构 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg" alt="不同类型的RNN结构" /></p>
<h2 id="语言模型和序列生成">语言模型和序列生成</h2>
<h2 id="对新序列采样">对新序列采样</h2>
<h2 id="长期依赖梯度消失">长期依赖，梯度消失</h2>
<p>观察两个句子：</p>
<ul>
<li>The cat, which already ate..., was full.</li>
<li>The cats, which already ate..., were full.</li>
</ul>
<p>这两个句子只有复数形式上的不同，但是开头的名词影响到了最后面的be动词。但是我们目前见到的最基本的RNN不擅长捕获这种长期依赖效应。</p>
<p>用梯度消失解释一下为什么，其实原理相同的，这里引用之前的文章 <a href="https://yan624.github.io//%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8">梯度消失和梯度爆炸</a></p>
<h1 id="gru单元gate-recurrent-unit">GRU单元——Gate Recurrent Unit</h1>
<p>中文名为门控循环单元。它也解决了梯度消失的问题。</p>
<h2 id="符号表示">符号表示</h2>
<p>c = memonry cell，使用<span class="math inline">\(c^{&lt;t&gt;}\)</span>符号表示输出，其中<span class="math inline">\(c^{&lt;t&gt;} = a^{&lt;t&gt;}\)</span>，由于后面的LSTM的c和a代表意思不同，所以这里直接使用c来表示输出值。所以本小章下的c你都看作是a即可。</p>
<h2 id="gru工作流程">GRU工作流程</h2>
<p>由于通过<span class="math inline">\(c^{&lt;t-1&gt;}\)</span>来更新<span class="math inline">\(c^{&lt;t&gt;}\)</span>的值，但是现在我们使用GRU，GRU就是来控制是否更新<span class="math inline">\(c^{&lt;t&gt;}\)</span>的值的，这里使用“更新”的名词可能有点怪，因为<span class="math inline">\(c^{&lt;t&gt;}\)</span>实际上是通过<span class="math inline">\(c^{&lt;t-1&gt;}\)</span><strong>计算</strong>出来的。那么公式<span class="math inline">\(a^{&lt;t&gt;} = g_1(w_{a} * [a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_a)\)</span>变为<span class="math inline">\(\tilde{c}^{&lt;t&gt;} = tanh(w_{c} * [c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c)\)</span>，这里的<span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span>是一个候选值——candidate value，类似于中间变量，而激活函数我们选择tanh。</p>
<p>GRU的核心是有一个Gate，就是上面说的是否更新值的功能，它的公式为<span class="math inline">\(\Gamma_u = \sigma(w_{u} * [c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u)\)</span>，<span class="math inline">\(\Gamma_u\)</span>的u的意思是update，sigmoid函数的输出范围在0-1之间，所以就完成了类似更新的功能。如果是0就代表不让你更新，如果是1就代表让你更新，这里听起来还有点绕，没关系看下面的表达式。</p>
<p>这时开始执行更新步骤：<span class="math inline">\(c^{&lt;t&gt;} = \Gamma_u * \tilde{c}^{&lt;t&gt;} + (1 - \Gamma_u) * c^{&lt;t-1&gt;}\)</span>，这一步可以看出如果<span class="math inline">\(\Gamma_u\)</span>等于1就将<span class="math inline">\(c^{&lt;t&gt;}\)</span>更新为<span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span>，如果等于0就相当于不让你更新，结果还是上一个的c，即<span class="math inline">\(c^{&lt;t-1&gt;}\)</span>。</p>
<p>将公式写在一起，GRU的工作流程就是： <span class="math display">\[
\begin{cases}
    \tilde{c}^{&lt;t&gt;} = tanh(w_{c} * [c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u)\\
    c^{&lt;t&gt;} = \Gamma_u * \tilde{c}^{&lt;t&gt;} + (1 - \Gamma_u) * c^{&lt;t-1&gt;}\\
\end{cases}
\]</span></p>
<h2 id="gru完整版">GRU完整版</h2>
<p>可以看到下式中就多了一个<span class="math inline">\(\Gamma_r\)</span>，但是为什么不用上面的简化版呢？那是因为经研究者多年的尝试，发现下面的版本是很实用的，也算是一个标准版，你可以自己开发不同的版本。 <span class="math display">\[
\begin{cases}
    \tilde{c}^{&lt;t&gt;} = tanh(w_{c} * [\Gamma_r * c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_u)\\
    \Gamma_r = \sigma(w_{r} * [c^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_r)\\
    c^{&lt;t&gt;} = \Gamma_u * \tilde{c}^{&lt;t&gt;} + (1 - \Gamma_u) * c^{&lt;t-1&gt;}\\
\end{cases}
\]</span></p>
<h1 id="lstm">LSTM</h1>
<p>吴恩达老师讲得感觉理解起来有点费劲，因为他觉得图片比文字更难理解，所以写了一大堆公式，只是再后面补充了图片。所以我建议看李宏毅老师的深度学习视频来理解LSTM。李宏毅老师的视频用了一张图片很好的解释了LSTM，并且他还举了一个例子，更加生动形象。</p>
<p>可能是东西方的差异，我感觉是图片好理解点，所以我选择看李宏毅老师的视频。这里就不写了，因为我在<strong>下面写了</strong>李宏毅老师课程的<strong>笔记</strong>。</p>
<h1 id="双向神经网络">双向神经网络</h1>
<h1 id="深层神经网络">深层神经网络</h1>
<hr />
<p>李宏毅机器学习 **********************************</p>
<h1 id="字母表示-1">字母表示</h1>
<p>跟吴恩达老师讲的类似，李宏毅老师也讲了文字如何表示，与吴恩达老师不同的是，李宏毅老师多讲了几个。</p>
<p>最简单的方法利用向量来表示文字，就是上面说过的one-hot：</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/1%20of%20N%20encoding.jpg" alt="1 of N encoding" /> 因为会出现某些单词没见到过，所以需要使用other这一维来表示。并且在右边的图中还可以使用字母来表示。然后理想上只要将词向量放入神经网络就会出现结果。但是Feedforward Network其实没办法解决这问题。</p>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/beyond%201-of-N%20encoding.jpg" alt="beyond 1-of-N encoding" /><figcaption>beyond 1-of-N encoding</figcaption>
</figure>
<p>可以看到下图，由于Feedforward Network没有记忆，所以两个句子对它来说是一个意思，但是对人来说可以很明显判断出第一句话台北是目的地，第二句话台北是出发地。Feedforward Network它只能训练当前的词，前一个词是什么它并不知道。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward%20Network%E6%97%A0%E6%B3%95%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="Feedforward Network无法解决的问题" /></p>
<h1 id="rnn">RNN</h1>
<p>上面讲到Feedforward Network由于没有记忆，无法记住前一个或者前几个词，所以就诞生了RNN。RNN其实也没那么神秘，就是每次输入并交给激活函数计算完毕后，将计算结果存入缓存中，并且在下一次计算时，将缓存取出来一起计算（这里一起计算的意思是将 memory 也当做 input，也就是说<strong>下图的 RNN 有 4 个输入</strong>）。就是下图的蓝色方框，由于是第一次计算，其中初始化为0。下图第一遍已经在计算了，实际上已经准备更新蓝色方框中的值了。RNN在上面的章节中其实已经写过了，都是类似的。</p>
<p><strong>注意一点，下图代表一个 RNN，那几个圆圈是一个神经细胞，而不代表一个 RNN</strong>。一个神经细胞中有一个权重向量，对比 simple NN 就能理解了。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%80%E4%B8%AARNN%E7%9A%84%E5%B0%8F%E5%9E%8B%E4%BE%8B%E5%AD%90.jpg" alt="一个RNN的小型例子" /></p>
<p>经过上面的例子发现，当前的输入已经在依赖前一个的缓存了，所以当顺序有所变化，或者前一个数据有所变化时，RNN可以察觉到，输出的结果也自然不同。</p>
<h2 id="deep-rnn">deep RNN</h2>
<p>我一共写了两个RNN的笔记，无论是吴恩达老师的还是李宏毅老师的到目前为止，RNN其实都不是deep的，之前也在疑惑，RNN横轴有很多层，但是实际上那些层只是不同时间的输入，根本不算deep。今天继续看下去，发现这个问题终于有解了，RNN也可以是deep的。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/deep%20RNN.jpg" alt="deep RNN" /></p>
<h2 id="elman-network-jordan-network">Elman Network &amp; Jordan Network</h2>
<p>上面讲的RNN都被称为Elman Network。还有另一种辩题叫做Jordan Network，它将输出值缓存起来。传说之中Jordan Network可以有更好的性能。</p>
<div class="note primary"><p>为什么有更好的性能</p>
</div>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Elman%20Network%E5%92%8Cordan%20Network.jpg" alt="Elman Network和ordan Network" /><figcaption>Elman Network和ordan Network</figcaption>
</figure>
<h2 id="双向rnnbidirectional-rnn">双向RNN——Bidirectional RNN</h2>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Bidirectional%20RNN.jpg" alt="RNN——Bidirectional RNN" /><figcaption>RNN——Bidirectional RNN</figcaption>
</figure>
<h1 id="长短期记忆long-short-term-memorylstm">长短期记忆——Long Short-term Memory(LSTM)</h1>
<div class="note primary"><p>LSTM的神经元个数不同有什么区别？其他的NN架构也有同样的疑问</p>
</div>
<p>上面讲的memory实际上是最简单的，LSTM才是现在最常用的Memory。Menory在RNN中实际只是一个神经元而已，它负责输入和输出。它们之间的关联是：RNN依旧是RNN，只不过把RNN中的神经元换成了LSTM。我们知道神经元的逻辑其实很简单，只有输入——计算——输入到激活函数——输出激活值，而LSTM只不过麻烦一点罢了。</p>
<p>下图就是一个LSTM。Input Gate中如果f(z)是1就代表Gate打开，也就是f(z)*g(z) = 1 * g(z) = g(z)，就相当于可以让外界输入。如果f(z)=0，Gate被关闭，那么 f(z)*g(z)=0，是不是就像不允许外界输入一样？因为你输入多少都被置为0。而Forget Gate也类似，当f(z)=1时，即Forget Gate被打开，这里与直觉有点相反，因为Gate打开，有点感觉像遗忘。但是其实c*f(z) = 1，所以Forget Gate为1其实是记住原本的c的意思。</p>
<p>另外图中也写到了，Gate的激活函数一般选sigmoid，里面的值就代表Gate的打开程度。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E7%A4%BA%E4%BE%8B.jpg" alt="LSTM示例" /></p>
<h2 id="lstm的例子">LSTM的例子</h2>
<p>例子介绍：只有一个LSTM，输入有3维，输出有1维。<span class="math inline">\(x_2 = 1\)</span>则<span class="math inline">\(x_1\)</span>的值就会被存到Memory中，<span class="math inline">\(x_2 = -1\)</span>则重置Memory，<span class="math inline">\(x_3 = 1\)</span>则输出。</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E4%BE%8B%E5%AD%90%E4%BB%8B%E7%BB%8D.jpg" alt="LSTM例子介绍" /> 注：下图中的蓝色数字和灰色数字是权重值。</p>
<div class="note primary"><p>Q：权重值是初始化的？还是固定的？还是初始化后自己可以训练的？其实就是LSTM的反向传播算法要弄懂。</p>
<p>A：是初始化后自己可以训练的。<em>2019 年 11 月 11 日回答</em>。</p>
</div>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E4%BE%8B%E5%AD%90%E8%AE%A1%E7%AE%97.jpg" alt="LSTM例子计算" /><figcaption>LSTM例子计算</figcaption>
</figure>
<ol type="1">
<li>Input Gate： 将偏差设为-10是因为我们通过x2来对Input Gate控制。平常x2=0，计算x*w+b=-10，那么通过sigmoid function就会得到一个接近于0的值，所以就实现了将Input Gate关闭的功能。而如果x2=1，那么x2*100=100，通过sigmoid function就会得到一个接近于1的值，Input Gate就实现了打开的功能。</li>
<li>Forget Gate: 这里的功能跟Input Gate类似。</li>
<li>Output Gate: 如果Output Gate被关闭，那么输出0.</li>
</ol>
<h2 id="多个lstm工作场景">多个LSTM工作场景</h2>
<p>里面的<span class="math inline">\(x^t\)</span>就是对应于NN中的一个向量，它分别乘上4个参数矩阵得到4个不同的向量，以此操控LSTM，而LSTM实际上就等于神经元，说白了就是一个类似激活函数的功能。</p>
<div class="group-picture"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF.jpg" alt="LSTM实际工作场景" /></div><div class="group-picture-column" style="width: 50%;"><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF2.jpg" alt="LSTM实际工作场景2" /></div></div></div>
<p>多个LSTM连起来工作就是像下面一样，红线和红线旁边的那个黑色曲线链接的值之前没有讲过，但是下图的这样才是LSTM实际的长相，所以之前讲的那么复杂实际上还是LSTM的简化版。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="LSTM实际工作流程" /></p>
<h1 id="rnn反向传播">RNN反向传播</h1>
<div class="note info"><p>此处的笔记来源于 <a href="https://www.bilibili.com/video/av10590361?p=37" target="_blank" rel="noopener">26: Recurrent Neural Network (Part II)</a> 从 0 分开始。</p>
</div>
<p>BPTT——backpropagation through time，与 NN 的 backpropagation 类似，李宏毅老师也没讲原理直接跳过了。</p>
<p>然而不幸的是，RNN 的 training 是很困难的。下面蓝色的线是希望的结果，但是实际上是绿色的线，会出现剧烈地抖动，最后在某个点出现NAN。这就是类似梯度消失问题。可以使用一些办法解决，但是现在用得最多的方法是LSTM。</p>
<p><strong>视频中花了很长的时间去讲解梯度爆炸和梯度消失的问题，但是我没有将它记录在这，详情可访问下面两个链接。</strong> <div class="note primary"><p>Q：如何防止出现如下剧烈抖动的 loss 曲线？</p>
<p>A：处理梯度爆炸的问题。</p>
</div> <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/RNN%20training%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="RNN trWaining碰到的问题" /></p>
<h1 id="其他解决梯度消失的办法">其他解决梯度消失的办法</h1>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%85%B6%E4%BB%96%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8A%9E%E6%B3%95.jpg" alt="其他解决梯度消失的办法" /><figcaption>其他解决梯度消失的办法</figcaption>
</figure>
<h1 id="seq2seq">seq2seq</h1>
<h2 id="many-to-one">Many to one</h2>
<p>输入一个向量sequence，只输出一个向量。</p>
<ol type="1">
<li>语义分析。比如分析电影评论是好是坏。</li>
<li>key term extraction。对文档提取关键词。</li>
</ol>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many%20to%20One.jpg" alt="Many to One" /><figcaption>Many to One</figcaption>
</figure>
<h2 id="many-to-manyoutput-is-shorter">Many to many(Output is shorter)</h2>
<p>输入和输出都是向量sequence，但是输出要短。</p>
<ol type="1">
<li>Speech Recognition 。语音辨识。</li>
</ol>
<h2 id="many-to-manyno-limitation">Many to many(No limitation)</h2>
<p>输入和输出都是序列且长短不一。被称为 <strong>Sequence to sequence learning</strong> 。</p>
<ol type="1">
<li>Machine Translation. 机器翻译。</li>
</ol>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many%20to%20Many（No%20Limitation）.jpg" alt="Many to Many(No Limitation)" /><figcaption>Many to Many(No Limitation)</figcaption>
</figure>
<h2 id="beyond-sequence">Beyond Sequence</h2>
<ol type="1">
<li>Syntactic parsing</li>
</ol>
<h2 id="autoencoder">autoencoder</h2>
<p><a href="https://www.bilibili.com/video/BV13x411v7US?p=37" target="_blank" rel="noopener">26: Recurrent Neural Network (Part II)</a>，45.43 开始。</p>
<hr />
<p>李宏毅深度学习</p>
<hr />
<h1 id="注意事项">注意事项</h1>
<p>此系列视频还有两个Review视频，分别为第一个视频：Basic Structures for Deep Learning Models(Part 1)， 第二个视频：Basic Structures for Deep Learning Models(Part 2)。</p>
<p>个人认为Review视频不需要看，而且这两个视频时间贼长，加起来得有两个多小时。没必要浪费时间，即使你根本没学过Review中的知识点也不用去看。他的Review里不会讲很深，基本上就过过场，就算有很深的东西也完全不影响继续往下学。1P时长80分钟，说实话如果自己属于小白阶段，去看那么长的视频是挺打击人的兴趣的，如果是大佬或者已经入门的人当然看得津津有味了。 <a href="https://yan624.github.io/posts/5e27260b.html#字母表示-1">此文</a>记录了李宏毅机器学习视频中讲解的RNN的笔记。</p>
<h1 id="computational-graph-backpropagation">Computational Graph &amp; Backpropagation</h1>
<div class="note danger"><p>2019年6月7号更新：关于计算图这章，现在才发现原来很重要，因为这是完成<strong>自动求导</strong>的关键。学了 pytorch 之后才发现的。</p>
</div>
<h2 id="什么是computational-graph">什么是Computational Graph</h2>
<p>这实际上跟要学的深度学习没什么关系，只是名字好听点，无视就好，如下图就是一个Computational Graph。主要用来在计算神经网络一些输出时，便于理解。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E4%BE%8B%E5%AD%90.jpg" alt="Computational Graph例子" /></p>
<p>在看一个比较贴近实际的例子，顺便复习一下链式求导法则。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99%E7%A4%BA%E4%BE%8B.jpg" alt="Computational Graph链式求导法则示例" /></p>
<h2 id="通过链式求导的例子理解反向传播backpropagation算法">通过链式求导的例子理解反向传播（Backpropagation）算法</h2>
<p>首先进行正向链式求导，如下图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E6%AD%A3%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="正向链式求导" /></p>
<p>图中要求计算e对a求偏导，首先给出a=3, b=2。其中c=a+b, d=b+1。</p>
<p>按照李宏毅老师使用链式求导法则，先要计算c对a求导得到1。e再对c求导得到b+1，带入b=2，得到3。所以3对a求偏导等于1*3=3。</p>
<p>上面这种链式求导法则有点乱，如果没仔细学过<em>微积分</em>可能难以理解。其实对于方程e = (a+b) * (b+1)，e对a求偏导，直接看出来都可以。利用考研时的口诀“左导右不导，左不导右导”（也就是<a href="https://baike.baidu.com/item/%E8%8E%B1%E5%B8%83%E5%B0%BC%E8%8C%A8%E5%85%AC%E5%BC%8F/8779293?fr=aladdin" target="_blank" rel="noopener">莱布尼茨公式</a>），直接得到结果<span class="math inline">\(\frac{\partial e}{\partial a} = b+1\)</span>。</p>
<p>然后将b=2带入b+1得到结果还是3。</p>
<p>接着进行反向模式，如下图: <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%8F%8D%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="反向链式求导" /></p>
<p>现在图中要求计算<span class="math inline">\(\frac{\partial e}{\partial a}\)</span>以及<span class="math inline">\(\frac{\partial e}{\partial b}\)</span>，当然你可以分别进行两次链式求导，得到结果。但是如果从e出发，也就是反向，那么就可以同时得到<span class="math inline">\(\frac{\partial e}{\partial a}\)</span>以及<span class="math inline">\(\frac{\partial e}{\partial b}\)</span>的结果。</p>
<p>不要在意e为什么等于1，只不过一个输入而已。 此外，如果阅读过《deep learning and neural network》一书，看过吴恩达机器学习视频或者其它资料的应该已经能反应出来。连接线上的求偏导实际上就跟神经网络上的权重一个意思，然后也是一层一层地反向传播。</p>
<p>这个输入e实际上就是神经网络中的反向传播算法中的输入。就是最后一层神经元的误差<span class="math inline">\(\delta^l = h-y\)</span>。这里吴恩达老师和《deep learning and neural network》作者的最后一层误差公式不一样，<strong>目前不明</strong>，暂时不做解释，这里的公式是吴恩达老师的。</p>
<p>然后就是误差*权重+偏差得到前一层的误差，具体不展开。</p>
<h2 id="反向传播的好处">反向传播的好处</h2>
<p>如果你的root只有一个，那么这个Computational Graph中的所有偏微分就都可以一次性算出。对应于神经网络，我们就是要这样的效果。</p>
<h2 id="参数共享parameter-sharing">参数共享（Parameter sharing）</h2>
<p>略，看了一眼貌似挺简单。16:20</p>
<h2 id="computational-graph-for-feedforword-net">Computational Graph for Feedforword Net</h2>
<p>李宏毅深度学习p3从21:16到52:48讲解梯度下降算法、前馈神经网络以及反向传播算法的具体数学原理 一直没看懂原理，以后再看。</p>
<h2 id="computational-graph-for-recurrent-network">Computational Graph for Recurrent Network</h2>
<h1 id="deep-learning-for-language-modeling">※ Deep Learning for Language Modeling</h1>
<p>语言模型就是预测一个word sequence出现的几率有多大。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Language%20Modeling.jpg" alt="Language Modeling" /></p>
<h2 id="n-gram">N-gram</h2>
<p>N-gram是自然语言处理中的算法。2-gram读作bi-gram。</p>
<h3 id="传统做法">传统做法</h3>
<ul>
<li>怎么预测一句话出现的几率</li>
<li>收集大量文本作为训练数据
<ul>
<li>然后计算<span class="math inline">\(w_1\cdots w_n\)</span>这句话在训练数据中出现的概率</li>
</ul></li>
<li>N-gram语言模型：
<ul>
<li>如何计算一小部分的概率？例如下图的p(beach|nice)出现的概率。就是将nice beach出现的次数除以nice出现的次数。</li>
</ul></li>
</ul>
<p>前两条是理想的处理办法，但是麻烦的是要预测的句子在语料库——corpus中八成一次都没出现过。于是就需要使用N-gram模型。它的处理办法就是将句子拆成比较小的部分——component，再把每个小部分的概率乘起来就是句子出现的几率。像下图这种只考虑前一个单词的模型叫做2-gram model。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/N-gram.jpg" alt="N-gram" /></p>
<h3 id="nn-based-lm">NN-based LM</h3>
<p>怎么做基于NN的N-gram？做法如下：</p>
<ol type="1">
<li>搜集training数据</li>
<li>learn一个Neural Network，通过两个词predict下一个词，如下图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/NN-based%20LM.jpg" alt="NN-based LM" /></li>
<li>使用cross entropy minimize</li>
<li>有了Neural Network后算一个句子的几率，如下图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9A%84%E5%87%A0%E7%8E%87.jpg" alt="计算句子的几率" /> 其中STRAT是一个token，代表句子的起始。</li>
</ol>
<h3 id="rnn-based-lm">RNN-based LM</h3>
<p>往上翻<strong>循环神经网络——RNN</strong>，原理就是这个。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/RNN-based%20LM.jpg" alt="RNN-based LM" /></p>
<h3 id="challenge-of-n-gram">Challenge of N-gram</h3>
<h4 id="nn-based-model">NN-based model</h4>
<p>为什么要使用NN-based model。相较于传统方法有什么好处。就是概率估不准，因为永远没有足够的数据。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Challenge%20of%20N-gram.jpg" alt="Challenge of N-gram" /> <div class="note info"><p>视频13:20~27:01仔细讲解了为什么要使用NN，而且把我困惑了快一个月的问题解决了，就是将文字转为数字之后进行训练的意义。</p>
</div></p>
<h4 id="rnn-based-model">RNN-based model</h4>
<p>为什么要使用RNN-based model。相较于传统方法有什么好处。</p>
<h1 id="几个有用的network架构">几个有用的network架构</h1>
<h2 id="spatial-transformer-layer">Spatial Transformer Layer</h2>
<p><a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" target="_blank" rel="noopener">论文地址</a>，中文可以叫<strong>空间变换层</strong>。 此神经网络架构的出现的原因：CNN 对图片的缩放以及旋转无所谓（CNN is invariant to scaling androtation）。比如说在图片的局部地区中，一个人移动一点点距离，对 CNN 来说其实没什么多大区别。不过距离有点远的话，还是有点影响的。</p>
<h2 id="highway-network">Highway Network</h2>
<p>先对前馈神经网络和 RNN 进行一下对比。</p>
<ol type="1">
<li>Feedforward NN 不是每一步都有输入。</li>
<li>Feedforward NN 每一层都有不同的参数。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward%20NN和RNN的对比.jpg" alt="Feedforward NN和RNN的对比" /></li>
</ol>
<p><a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Network 论文地址</a>；<a href="https://arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="noopener">Highway Network 实战论文地址</a> Highway Network 的想法就是把 RNN <strong>立</strong>起来，把它当做前馈神经网络来用。 <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">Highway Network 的改进版论文地址</a>，这个就是<strong>残差神经网络</strong>。</p>
<h2 id="grid-lstm">Grid LSTM</h2>
<p><a href="https://arxiv.org/pdf/1507.01526.pdf" target="_blank" rel="noopener">论文地址</a> 太复杂了，估计以后也很难用到。。。</p>
<h2 id="recusive-network">Recusive Network</h2>
<p>Recursive Network 是 Recurrent Network 更 Generalize 的版本。Recurrent Network 是 Recursive Network 的一个特殊的例子，如果翻译成中文的话，实际上名字都一样。所以可以称之为递归式网络。以下是 RNN 和 Recursive Network 的对比图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Recursive%20Network示意图.jpg" alt="Recursive Network示意图" /></p>
<p>在做 Recursive Network 之前，需要考虑输入的序列的结构。图中将 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 一同输入进一个 function，但是其实可以不这么做，具体要怎么输入，取决于输入数据的结构。<strong>而由于 f 与 f 前后相接，所以在写代码时需要预先做好设计</strong>。</p>
<p>举个具体的例子，要判断“not very good”包含什么情绪，可以先使用语法解析，将句子结构化，然后根据句子的语法结构来使用 Recursive Network 进行训练，如下图：</p>
<p>“very”的词向量和“good”的词向量一同放入 f 中训练，我们可以将得到的向量看做是“very good”的意思。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/根据句子语法结构训练1.jpg" title="根据句子语法结构训练1" alt="根据句子语法结构训练1" /></p>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/根据句子语法结构训练2.jpg" title="根据句子语法结构训练2" alt="根据句子语法结构训练2" /><figcaption>根据句子语法结构训练2</figcaption>
</figure>
<p>当然两个词向量不能是简单的相加，具体做法可以自行选择。最简单的做法可以参考下图的上半部分，而下图的下半部分被称为 <strong>Recursive Neural Tensor Network</strong>，总而言之就是一个很复杂的做法来解决两个词向量不仅仅是进行简单的拼接或者相加。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Recursive%20Neural%20Tensor%20Network.jpg" alt="Recursive Neural Tensor Network" /></p>
<p>对于 f 还有其他的做法，如 Matrix-Vector Recursive Network，<a href="https://arxiv.org/pdf/1503.00075.pdf" target="_blank" rel="noopener">Tree LSTM 2015</a> 等。具体就不记了，以后可以查 Recursive Network 相关论文。</p>
<h1 id="conditional-generation-by-rnn-attention">Conditional Generation by RNN &amp; Attention</h1>
<p>注意本文讲的是 RNN <strong>入门</strong>，而下面的部分也只是讲普通的 RNN Generation，甚至连 decoder 部分都没用。下图是生成文字，其实也可以生成图片、音频等，我就不一一截图了，第二张图将这些<strong>想法</strong>及其<strong>论文</strong>都汇总了。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/一个简单的Generation.jpg" alt="一个简单的Generation" /></p>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Generation汇总.jpg" alt="Generation汇总" /><figcaption>Generation汇总</figcaption>
</figure>
<h2 id="conditional-generation">Conditional Generation</h2>
<p>但是在真实的场景中，我们不仅仅是希望只生成随机的句子，我们更偏向于生成一些基于某些条件的句子，比如：当看见一张一个人正在跳舞的图片，我们希望电脑生成“A young girl is dancing”；当给予一个条件“Hello”时，我们希望电脑生成“Hello, nice to see you.”。</p>
<p>一个实际的例子，我们可以将一张图片输入进 CNN，从而产生一个向量，再把该向量输入进 decoder 部分，最后生成句子。如下图所示，其他类型的<strong>条件生成</strong>也类似。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Image%20Caption%20Generation.jpg" title="Image Caption Generation" alt="Image Caption Generation" /></p>
<h2 id="attention">Attention</h2>
<p>将 <span class="math inline">\(z_0\)</span> 与 <span class="math inline">\(h_1 h_2 h_3 h_4\)</span> 分别做一次 match，至于 match 怎么计算可以看下图右边。</p>
<p>计算步骤可以参考下列公式： <span class="math display">\[
\begin{align}
    h &amp; = [h^1, h^2, h^3, h^4] \\
    s &amp; = h^T z \\
    c^0 &amp; = h^t s \\
\end{align}
\]</span></p>
<p>attention score 的计算公式可以由自己设计，下图使用的是 <span class="math inline">\(h^T W z\)</span>，有兴趣的话，<a href="https://yan624.github.io/·学习笔记/AI/nlp/CS224n学习笔记.html#attention">这里</a>介绍了三个。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制.jpg" alt="Attention机制" /></p>
<p>然后获得 <span class="math inline">\(a^1_0 a^2_0 a^3_0 a^4_0\)</span>，之后将它们输入 softmax 层（有实验发现其实不经过 softmax 层也可以，甚至效果更好），最后将所有 a 分别乘上它们对应的向量并且相加，得到一个向量 <span class="math inline">\(c^0\)</span>。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制计算score.jpg" alt="Attention机制计算score" /></p>
<p>使用 Attention 机制计算完毕后，将向量 <span class="math inline">\(c^0\)</span> 输入进 decoder 即可，接下来的计算都是以此类推。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制计算完毕后输入到decoder.jpg" alt="Attention机制计算完毕后输入到decoder" /></p>
<h3 id="attention应用到speach-recognition">Attention应用到Speach Recognition</h3>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention%20for%20Speach%20Recognition.jpg" alt="Attention for Speach Recognition" /><figcaption>Attention for Speach Recognition</figcaption>
</figure>
<h2 id="memory-network">Memory Network</h2>
<p><a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">论文地址</a>，<a href="https://www.bilibili.com/video/av9770302/?p=8" target="_blank" rel="noopener">视频地址</a>43:00开始。 Memory Network 最先被用在 Reading Comprehension，说白了就是一个 Attention 机制。下图就是一个简易的 <strong>Memory Network</strong>。</p>
<ol type="1">
<li>首先将 document 由多个句子组成，句子由 vector x 表示。具体如何表示的问题，可以由自定义解决，如 bag of word 或者由词向量表示；</li>
<li>query 就是问题，也由 vector q 表示；</li>
<li>使用 q 对每个句子做 attention 得到 match score <span class="math inline">\(\alpha\)</span>，然后使用 <span class="math inline">\(\alpha\)</span> 和 x 做 weighted sum；</li>
<li>最后将 weighted sum 后的 vector 和 vector q 都丢到 DNN 中，得到答案。</li>
</ol>
<p>注：这是在做阅读理解。document -&gt; vector 等于 input(I) 和 generalization(G)，attention 等于 output(O)，生成答案等于 response(R)。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/一个简易的Memory%20Network.jpg" title="一个简易的Memory Network" alt="一个简易的Memory Network" /></p>
<p>Memory Network 还有更复杂的版本，即 attention 的 vector 和抽取信息的 vector 并不需要是同一个，如下图所示。</p>
<ol type="1">
<li>将 document 表示为句子时，使用两组向量。一组用于计算 match score，一组用于 weighted sum。</li>
<li>其他的步骤都差不多，但是有一个地方不一样。在 weighted sum 得到一个 vector 之后，可以和 q 加在一起，得到一个新的 q，再重复 步骤 1。而且这个步骤可以做很多次，做完之后再输入进 DNN 获取答案。这个步骤被称之为 Hopping，注意从 document 抽取的两组 vector 在 hopping 的时候，可以是不一样的。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/更复杂的memory%20network.jpg" alt="更复杂的memory network" /></li>
</ol>
<h2 id="neural-turing-machine">Neural Turing Machine</h2>
<h2 id="tips-for-generation">Tips for Generation</h2>
<p>这里听不太懂，跳过了。有 Beam Search 之类的。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/对Generation的建议1.jpg" alt="对Generation的建议1" /></p>
<h1 id="pointer-network">Pointer Network</h1>
<p><a href="https://pdfs.semanticscholar.org/eb5c/1ce6818333560d0d3247c0c74985ef295d9d.pdf" target="_blank" rel="noopener">论文地址</a></p>
<p>举一个简单的例子助于理解 Pointer Network。在二维坐标系中任意给出 4 个点，我们的目标是找到几个点，将它们连起来形成一个封闭圈，剩下的那几个点要正好在这个封闭圈之中，如下图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/助于理解Pointer%20Network的一个例子.jpg" alt="助于理解Pointer Network的一个例子" /></p>
<p>当然，这肯定已经有一些算法可以解了，比如在坐标系中计算距离。但是今天我们使用硬 train 一发的方法，即不管三七二十一将它输入到神经网络里面训练。首先制造一些训练数据，然后给 encoder-deocder 训练。<strong>具体的训练步骤为</strong>：输入点的坐标，输出 one-hot 表示 <code>{1,2,3,4,END}</code> 的五维向量，碰到 END 则代表解码完毕。</p>
<p>但结果是网络训练不起来，因为在上述的例子中我们只输入了 4 个点，我们的目的是得到 1-4 个点。但是如果我们的测试数据是输入 400 个点呢？那么我们也只会得到 1-4 个点，因为 <code>{1,2,3,4,END}</code> 是预先定义好的。你可能会想那就多定义一点啊，但是下次我要是输入 4000 个点呢？要是 40000 个点呢？总有你无法预先定义的时候。</p>
<p>所以我们需要 <strong>Pointer Network</strong> 来<strong>动态的改变类别</strong>（具体做法详见下一小节），注意我这里直接说成类别了，我们可以把 decoder 部分看作是多元分类的工作，如输出 4000 个点，就是 4000 元分类。</p>
<p><strong>上面的例子其实是 Pointer Netwoek 论文中的一个例子，但是对于这个例子来说，使用 Pointer Network 其实没多大意义，因为问题本身有更简单的解法，下面说一下有意义的用途。</strong></p>
<p>Pointer Network 应用于 <strong>Summarization</strong>，<a href="https://www.aclweb.org/anthology/P17-1099" target="_blank" rel="noopener">论文地址</a>。给定一篇文档，让机器做出总结。对于此类问题，我们会碰到很多<strong>生僻的地名、人名</strong>等等字词。我们可以使用 Pointer Network 来解决这个问题。</p>
<p>下图就是做法，整张图的意思就是在做文本摘要的工作，输入一个句子，输出摘要。先不看中间的黄色圆圈 <span class="math inline">\(p_{gen}\)</span>，看看其他部分（红黄两部分）就是很普通的 encoder-decoder。但是对于这个 encoder-decoder 来说，词表中并没有 <em>Aregentina</em> 这个单词。那么我们就可以使用 Pointer Network，这个 <span class="math inline">\(p_{gen}\)</span> 就是概率（具体描述见下一节）。最后结果就是我们将注意力关注到 <em>Aregentina</em> 这个单词。当然对于 encoder-decoder 这部分的工作也是要做的，我们可以将两个结果加起来，从而判断出最终要产生哪个单词，做法详见原论文。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Pointer%20Network%20for%20Summarization.jpg" alt="Pointer Network for Summarization" /></p>
<p>还可用于 <strong>machine learning</strong>、<strong>chatbot</strong> 等。</p>
<h2 id="具体做法">具体做法</h2>
<p>具体的实现就是像下图一样，首先在输入的序列之前加入一个 END 序列，然后将 decoder 删掉。我们还是使用 Attention 机制计算每个序列的 attention score，但是这次的 score 不再乘上它对应的向量，而是直接当做向量输出，意思就是把所以的 score 做一次 max，最大的就输出 1。而<strong>停止条件就是 END 这个序列的 score 是最大的，即为 1 就停止训练。</strong></p>
<p>这样的做法乍一看好像无法理解，我解释一下。由于 encoder 是对序列的长度不敏感的，也就是说如果预先定义的类别是 40 维，而我输入 400 个点，那么对于 encoder 来说，它可以增加神经元的数量从而使得 400 个点<strong>正好</strong>全部输入进 encoder。但是对于 decoder 来说，它输出只能是 40 维。<strong>那么 Pointer Network 的做法是将 decoder 删除，把输出的工作也交给 encoder 去做。所以我输入 400 个点，自然也就可以输出 400 维的类别</strong>（这里应该是 401 维，因为还有一个 END 序列）。看下图的 encoder，<span class="math inline">\(h^4\)</span> 的分数是 0.7，所以我们的输出就是 4，当然对于向量来说就是 <span class="math inline">\((0, 0, 0, 0, 1)\)</span>。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Pointer%20Network的做法.jpg" alt="Pointer Network的做法" /></p>
<h1 id="剩余的一些与nlp关系不大的视频">剩余的一些与NLP关系不大的视频</h1>
<ol type="1">
<li>调参的技巧</li>
<li>训练的时候的 loss 曲线一般如下所示，过去人们常常以为这是卡在了 local minima（右上图），但是后来发现可能是卡在了 saddle point（右下）。有一个对为什么卡在 saddle point 而不是 local minima 的直观解释：local minima 要求你的所有 dimension 都必须往下凹的，可以看右上的图，二维的图展示了要想处于 local minima，必须两个维度在一个区间都是向下的才行。假设你有 1000 个维度，那么其实出现这种几率是很低的。 假设多数点的微分值是零的情况，通常这些点都是有些维度趋于向上，有些维度趋于向下，类似右下图。现在多数人比较相信我们训练数据时，其实是卡在了鞍点上。 但是 Ian GoodFellow（花书作者）曾经说过，可能这种说法也是不准确的。因为即使是鞍点，该位置的梯度也应该很低才对，但是他给出了一张图，证明训练至 loss 卡住之后，该处的梯度很大。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/训练卡住了是因为？.jpg" alt="训练卡住了是因为？" /></li>
<li>Brute-force Memorization：<a href="https://arxiv.org/pdf/1706.05394" target="_blank" rel="noopener">神经网络其实可能只是在暴力记忆？</a></li>
<li>知识蒸馏</li>
</ol>
<script>
window.onload = function () {
    $('colgroup').remove()
    }
</script>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    赞赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/alipay.gif" alt="朱冲䶮 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>朱冲䶮
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yan624.github.io/posts/5e27260b.html" title="吴恩达李宏毅综合学习笔记：RNN入门">https://yan624.github.io/posts/5e27260b.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag"># 学习笔记</a>
              <a href="/tags/rnn/" rel="tag"># rnn</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/a72e4bb6.html" rel="prev" title="git学习记录">
      <i class="fa fa-chevron-left"></i> git学习记录
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/8de7f8fd.html" rel="next" title="吴恩达深度学习学习笔记：自然语言处理与词嵌入">
      吴恩达深度学习学习笔记：自然语言处理与词嵌入 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#大纲"><span class="nav-number">1.</span> <span class="nav-text">大纲</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#字母表示"><span class="nav-number">2.</span> <span class="nav-text">字母表示</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#one-hot编码"><span class="nav-number">3.</span> <span class="nav-text">one hot编码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#循环神经网络rnn"><span class="nav-number">4.</span> <span class="nav-text">循环神经网络——RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn的反向传播"><span class="nav-number">4.1.</span> <span class="nav-text">RNN的反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#不同类型的rnn"><span class="nav-number">4.2.</span> <span class="nav-text">不同类型的RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语言模型和序列生成"><span class="nav-number">4.3.</span> <span class="nav-text">语言模型和序列生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对新序列采样"><span class="nav-number">4.4.</span> <span class="nav-text">对新序列采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#长期依赖梯度消失"><span class="nav-number">4.5.</span> <span class="nav-text">长期依赖，梯度消失</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gru单元gate-recurrent-unit"><span class="nav-number">5.</span> <span class="nav-text">GRU单元——Gate Recurrent Unit</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#符号表示"><span class="nav-number">5.1.</span> <span class="nav-text">符号表示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gru工作流程"><span class="nav-number">5.2.</span> <span class="nav-text">GRU工作流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gru完整版"><span class="nav-number">5.3.</span> <span class="nav-text">GRU完整版</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lstm"><span class="nav-number">6.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#双向神经网络"><span class="nav-number">7.</span> <span class="nav-text">双向神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深层神经网络"><span class="nav-number">8.</span> <span class="nav-text">深层神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#字母表示-1"><span class="nav-number">9.</span> <span class="nav-text">字母表示</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rnn"><span class="nav-number">10.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-rnn"><span class="nav-number">10.1.</span> <span class="nav-text">deep RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#elman-network-jordan-network"><span class="nav-number">10.2.</span> <span class="nav-text">Elman Network &amp; Jordan Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#双向rnnbidirectional-rnn"><span class="nav-number">10.3.</span> <span class="nav-text">双向RNN——Bidirectional RNN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#长短期记忆long-short-term-memorylstm"><span class="nav-number">11.</span> <span class="nav-text">长短期记忆——Long Short-term Memory(LSTM)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#lstm的例子"><span class="nav-number">11.1.</span> <span class="nav-text">LSTM的例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多个lstm工作场景"><span class="nav-number">11.2.</span> <span class="nav-text">多个LSTM工作场景</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rnn反向传播"><span class="nav-number">12.</span> <span class="nav-text">RNN反向传播</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#其他解决梯度消失的办法"><span class="nav-number">13.</span> <span class="nav-text">其他解决梯度消失的办法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#seq2seq"><span class="nav-number">14.</span> <span class="nav-text">seq2seq</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#many-to-one"><span class="nav-number">14.1.</span> <span class="nav-text">Many to one</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#many-to-manyoutput-is-shorter"><span class="nav-number">14.2.</span> <span class="nav-text">Many to many(Output is shorter)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#many-to-manyno-limitation"><span class="nav-number">14.3.</span> <span class="nav-text">Many to many(No limitation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beyond-sequence"><span class="nav-number">14.4.</span> <span class="nav-text">Beyond Sequence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#autoencoder"><span class="nav-number">14.5.</span> <span class="nav-text">autoencoder</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#注意事项"><span class="nav-number">15.</span> <span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#computational-graph-backpropagation"><span class="nav-number">16.</span> <span class="nav-text">Computational Graph &amp; Backpropagation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是computational-graph"><span class="nav-number">16.1.</span> <span class="nav-text">什么是Computational Graph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#通过链式求导的例子理解反向传播backpropagation算法"><span class="nav-number">16.2.</span> <span class="nav-text">通过链式求导的例子理解反向传播（Backpropagation）算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播的好处"><span class="nav-number">16.3.</span> <span class="nav-text">反向传播的好处</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数共享parameter-sharing"><span class="nav-number">16.4.</span> <span class="nav-text">参数共享（Parameter sharing）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#computational-graph-for-feedforword-net"><span class="nav-number">16.5.</span> <span class="nav-text">Computational Graph for Feedforword Net</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#computational-graph-for-recurrent-network"><span class="nav-number">16.6.</span> <span class="nav-text">Computational Graph for Recurrent Network</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#deep-learning-for-language-modeling"><span class="nav-number">17.</span> <span class="nav-text">※ Deep Learning for Language Modeling</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#n-gram"><span class="nav-number">17.1.</span> <span class="nav-text">N-gram</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#传统做法"><span class="nav-number">17.1.1.</span> <span class="nav-text">传统做法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nn-based-lm"><span class="nav-number">17.1.2.</span> <span class="nav-text">NN-based LM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn-based-lm"><span class="nav-number">17.1.3.</span> <span class="nav-text">RNN-based LM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#challenge-of-n-gram"><span class="nav-number">17.1.4.</span> <span class="nav-text">Challenge of N-gram</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#nn-based-model"><span class="nav-number">17.1.4.1.</span> <span class="nav-text">NN-based model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rnn-based-model"><span class="nav-number">17.1.4.2.</span> <span class="nav-text">RNN-based model</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#几个有用的network架构"><span class="nav-number">18.</span> <span class="nav-text">几个有用的network架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#spatial-transformer-layer"><span class="nav-number">18.1.</span> <span class="nav-text">Spatial Transformer Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#highway-network"><span class="nav-number">18.2.</span> <span class="nav-text">Highway Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#grid-lstm"><span class="nav-number">18.3.</span> <span class="nav-text">Grid LSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#recusive-network"><span class="nav-number">18.4.</span> <span class="nav-text">Recusive Network</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#conditional-generation-by-rnn-attention"><span class="nav-number">19.</span> <span class="nav-text">Conditional Generation by RNN &amp; Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#conditional-generation"><span class="nav-number">19.1.</span> <span class="nav-text">Conditional Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention"><span class="nav-number">19.2.</span> <span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#attention应用到speach-recognition"><span class="nav-number">19.2.1.</span> <span class="nav-text">Attention应用到Speach Recognition</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#memory-network"><span class="nav-number">19.3.</span> <span class="nav-text">Memory Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-turing-machine"><span class="nav-number">19.4.</span> <span class="nav-text">Neural Turing Machine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tips-for-generation"><span class="nav-number">19.5.</span> <span class="nav-text">Tips for Generation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pointer-network"><span class="nav-number">20.</span> <span class="nav-text">Pointer Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#具体做法"><span class="nav-number">20.1.</span> <span class="nav-text">具体做法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#剩余的一些与nlp关系不大的视频"><span class="nav-number">21.</span> <span class="nav-text">剩余的一些与NLP关系不大的视频</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="朱冲䶮"
      src="/images/%E5%A6%99%E8%9B%99%E7%A7%8D%E5%AD%90.webp">
  <p class="site-author-name" itemprop="name">朱冲䶮</p>
  <div class="site-description" itemprop="description">记录</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">183</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">117</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
      <!-- 不蒜子/busuanzi -->
      <div class="site-state-item site-state-posts">
      	<span class="site-state-item-count">255.6k</span>
      	<span class="site-state-item-name">总字数</span>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/zhu-yu-er-85" title="zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;zhu-yu-er-85" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:897538633@qq.com" title="E-Mail → mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/yan624" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yan624" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/noval" title="神奇的按钮 → noval"><i class="fa fa-book fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      友链
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://huaguoguo.gitee.io/" title="http:&#x2F;&#x2F;huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://lzh0928.gitee.io/" title="https:&#x2F;&#x2F;lzh0928.gitee.io&#x2F;" rel="noopener" target="_blank">Mr.Liu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://smallwhitezzz.gitee.io/blog" title="https:&#x2F;&#x2F;smallwhitezzz.gitee.io&#x2F;blog" rel="noopener" target="_blank">凯子</a>
        </li>
    </ul>
  </div>
<!-- CloudCalendar -->
<div class="widget-wrap" style="width: 90%;margin-left: auto;margin-right: auto; opacity: 0.97;">
	<div class="widget" id="CloudCalendar"></div>
</div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div><!--
樱花特效 
最初在某人的博客中看到这个特效，于是在网上搜了一圈，发现还有其他人也用它。它使用起来特别简单，只需要一行代码。
然后在 github 上搜了一下，发现有个 jquery-sakura，但是这个插件用起来很麻烦，经过测试，我的博客上无法使用。
后来发现是两个不同的插件，只是刚好特效一样。
于是我又搜了一下，貌似发现了源头，好像是一个博主随手写的，并没有发到 github 上。
原地址为：https://cangshui.net/2372.html
-->
<script>
	var pathname = window.location.pathname;
	// pathname == '/' || pathname == '/index.html'
	if(pathname == '/categories/assorted/timeline/'){
		document.write("<script src='/lib/sakura/sakura-flying.js'><\/script>");
	}
</script>
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
<script src="/lib/my-utils.js"></script>
<script src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- 背景插件 -->
<script src="https://cdn.bootcss.com/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<!--
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(10), {
		duration:90000,//1 min 半一换
		fade: 1500
	});
</script>
-->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://yan624.github.io/posts/5e27260b.html',]
      });
      });
  </script>
<!-- calendar widget -->


</body>
</html>
