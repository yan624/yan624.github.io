<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yan624.github.io","root":"/","scheme":"Pisces","version":"8.0.0-rc.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"scrollpercent":true,"enable":true,"sidebar":false},"bookmark":{"enable":true,"save":"manual","color":"#222"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="论文概要 论文地址，发表于 2016 年。  语义分析的目的是将自然语言映射到机器可解释的有意义表示。传统的方法依赖于高质量的词汇、人工构建的模板以及特定领域或特定表示的语言特征，本文提出了一种注意力增强的 encoder-decoder 通用模型。将输入的话表示为向量形式，并通过调节输出序列或者树生成逻辑形式（总结来说，就是将话语转为逻辑形式，详情请看图 1）。 下图将一句话转为了逻辑形式">
<meta property="og:type" content="article">
<meta property="og:title" content="Language to Logical Form with Neural Attention">
<meta property="og:url" content="https://yan624.github.io/posts/521b57a9.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="论文概要 论文地址，发表于 2016 年。  语义分析的目的是将自然语言映射到机器可解释的有意义表示。传统的方法依赖于高质量的词汇、人工构建的模板以及特定领域或特定表示的语言特征，本文提出了一种注意力增强的 encoder-decoder 通用模型。将输入的话表示为向量形式，并通过调节输出序列或者树生成逻辑形式（总结来说，就是将话语转为逻辑形式，详情请看图 1）。 下图将一句话转为了逻辑形式">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALanguage%20to%20Logical%20Form%20with%20Neural%20Attention/%E8%AF%AD%E5%8F%A5%E8%BD%AC%E4%B8%BA%E9%80%BB%E8%BE%91%E5%BD%A2%E5%BC%8F.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALanguage%20to%20Logical%20Form%20with%20Neural%20Attention/Seq2Seq.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALanguage%20to%20Logical%20Form%20with%20Neural%20Attention/Seq2Tree%E6%A8%A1%E5%9E%8B.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALanguage%20to%20Logical%20Form%20with%20Neural%20Attention/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84Seq2Tree%E7%9A%84%E4%BE%8B%E5%AD%90.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALanguage%20to%20Logical%20Form%20with%20Neural%20Attention/%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D.jpg">
<meta property="article:published_time" content="2019-07-08T10:33:15.000Z">
<meta property="article:modified_time" content="2020-07-17T15:13:04.996Z">
<meta property="article:author" content="朱冲䶮">
<meta property="article:tag" content="KBQA">
<meta property="article:tag" content="语义解析KBQA">
<meta property="article:tag" content="seq2tree">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALanguage%20to%20Logical%20Form%20with%20Neural%20Attention/%E8%AF%AD%E5%8F%A5%E8%BD%AC%E4%B8%BA%E9%80%BB%E8%BE%91%E5%BD%A2%E5%BC%8F.jpg">

<link rel="canonical" href="https://yan624.github.io/posts/521b57a9.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<!--阿里云矢量库样式-->
<link rel="stylesheet" href="//at.alicdn.com/t/font_1717154_g2j7b1y4jgc.css" />


  <title>Language to Logical Form with Neural Attention | 博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">末流炼丹师</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-常用链接">

    <a href="/%E5%B8%B8%E7%94%A8%E9%93%BE%E6%8E%A5" rel="section"><i class="fas fa-bookmark fa-fw"></i>常用链接</a>

  </li>
        <li class="menu-item menu-item-时间线">

    <a href="/categories/assorted/timeline/" rel="section"><i class="iconfont icon-timeline fa-fw"></i>时间线</a>

  </li>
        
            
  <li class="menu-item menu-item-博客分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>博客分类</a>

  </li>


      
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">181</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    
    	
    
    	
    
    	
    

    <link itemprop="mainEntityOfPage" href="https://yan624.github.io/posts/521b57a9.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A6%99%E8%9B%99%E7%A7%8D%E5%AD%90.webp">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Language to Logical Form with Neural Attention
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-07-08 18:33:15" itemprop="dateCreated datePublished" datetime="2019-07-08T18:33:15+08:00">2019-07-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-17 23:13:04" itemprop="dateModified" datetime="2020-07-17T23:13:04+08:00">2020-07-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="论文概要">论文概要</h1>
<div class="note info"><p><a href="https://arxiv.org/pdf/1601.01280.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2016 年。</p>
</div>
<p>语义分析的目的是将自然语言映射到机器可解释的有意义表示。传统的方法依赖于高质量的词汇、人工构建的模板以及特定领域或特定表示的语言特征，本文提出了一种注意力增强的 encoder-decoder 通用模型。将输入的话表示为向量形式，并通过调节输出序列或者树生成逻辑形式（总结来说，就是<strong>将话语转为逻辑形式</strong>，详情请看图 1）。 下图将一句话转为了逻辑形式，不同于以前的方法，它是通过神经网络生成的，而以前的方法依赖于手写的规则。图片取自 <a href="https://www.aclweb.org/anthology/W00-1317" target="_blank" rel="noopener" title="Automated construction ofdatabase interfaces: Intergrating statistical and rela-tional learning for semantic parsing">Tang and Mooney200</a>。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language%20to%20Logical%20Form%20with%20Neural%20Attention/语句转为逻辑形式.jpg" alt="语句转为逻辑形式" /></p>
<p>基于 RNN 的 encoder-decoder 已成功应用于各种 NLP 任务，图 1 中使用了 LSTM，我们的做法是提出了两个变体模型。<strong>第一个模型</strong>将语义解析视为普通的序列转换任务，<strong>第二个模型</strong>配备了层次树解码器，该解码器明确地捕获逻辑形式的组合结构。我们还引入了<strong>注意力机制</strong>，并提出一个识别步骤来<strong>识别很少提到的实体</strong>和<strong>数字</strong>。 对<strong>四个数据集</strong>的实验结果表明，我们的方法在不使用人工设计特征的情况下具有竞争力，并且易于迁移。 我们的工作综合了两种标准研究，即<strong>语义分析</strong>和 <strong>encoder-decoder 架构的神经网络</strong>。 <a id="more"></a></p>
<h1 id="相关工作">相关工作</h1>
<p>学习语义解析器的问题引起了广泛的关注，可以追溯到 Woods（1973年）。。。。</p>
<h1 id="任务定义">任务定义</h1>
<p>我们的目标是学习一个模型，将<u><strong>自然语言输入 <span class="math inline">\(q = x_1 \dots x_{|q|}\)</span></strong></u> 映射为其含义的<u><strong>逻辑形式（logical form）表示 <span class="math inline">\(a = y_1 \dots y_{|a|}\)</span></strong></u>。条件概率被分解为： <span class="math display">\[
\begin{align}
    p(a|q) &amp; = \prod^{|a|}_{t=1} p(y_t|y_{&lt;t},q) \tag 1\\
    y_{&lt;t} &amp; = y_1 \dots y_{t-1}
\end{align}
\]</span> 我们的模型包含一个编码器和一个解码器，编码器负责将输入的自然语言 q 编码成向量，解码器负责生成 <span class="math inline">\(y_1 \dots y_{|a|}\)</span>。下面将仔细描述。</p>
<h2 id="seq2seq">Seq2Seq</h2>
<p>对于普通的 Seq2Seq 任务，使用 LSTM 来计算，如下图所示。<span class="math inline">\(h^l_t\)</span> 代表第 l 层的第 t 个时间步的隐藏层，公式为： <span class="math display">\[
\begin{align}
    h^l_t = \text{LSTM}(h^l_{t-1},h^{l-1}_t) \tag 2
\end{align}
\]</span> <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language%20to%20Logical%20Form%20with%20Neural%20Attention/Seq2Seq.jpg" alt="Seq2Seq" /> 在实验中，遵循 <a href="https://arxiv.org/pdf/1409.2329.pdf" target="_blank" rel="noopener" title="RECURRENT NEURAL NETWORK REGULARIZATION">Zaremba et al. 2015</a> 提出的架构。不过，使用其他类型的门控激活函数也是可以的（例如<a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener" title="Learning phrase representations using RNN encoder-decoder for statistical machine translation">Cho et al. 2014</a>）。<strong>对于 encoder</strong>，<span class="math inline">\(h^0_t = W_qe(x_t)\)</span>（注：此公式是第 0 层的运算步骤，即输入层）是 RNN 中输入的词向量，<span class="math inline">\(W_q \in \mathbb{R}^{n \times |V_q|}\)</span> 代表输入层的权重值矩阵，e(·) 代表对应 token 的索引。<strong>对于 decoder</strong>，<span class="math inline">\(h^0_t = W_ae(y_{t-1})\)</span> 代表前一个预测词的词向量，其中 <span class="math inline">\(W_a \in \mathbb{R}^{n \times |V_a|}\)</span>。接下来，最后的 LSTM <span class="math inline">\(h^L_t\)</span> 被用于预测 <span class="math inline">\(t\)</span>-th 输出 token，计算公式为： <span class="math display">\[
\begin{align}
    p(y_t|y_t,q) = softmax(W_oh^L_t)^T e(y_t) \tag 3
\end{align}
\]</span> <strong>该公式用于预测每一个 token</strong>。另外补充一点，增加了 “start-of-sequence” <code>&lt;s&gt;</code> 和 “end-of-sequence” <code>&lt;/s&gt;</code>。 该模型总的来说，就是 LSTM 的计算方法，也没什么好说的。</p>
<h2 id="seq2tree">Seq2Tree</h2>
<p>Seq2Seq 模型有一个<strong>缺点</strong>就是它<strong>忽略了逻辑形式的层次结构</strong>。所以，要改良的话，它需要记住各种辅助信息（比如括号对），以此生成格式良好的输出。如下图 3 所示，是一个层次树 decoder： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language%20to%20Logical%20Form%20with%20Neural%20Attention/Seq2Tree模型.jpg" alt="Seq2Tree模型" /></p>
<p>Seq2Tree 与 Seq2Seq 的编码器类似，不同的是解码器。Seq2Tree 以自上而下的方式生成逻辑，为了定义树结构，我们定义了一个表示子树的 “nonterminal” <code>&lt;n&gt;</code> 标记。如图 3 所示，将<strong><em>逻辑形式 “lambda $0 e (and (&gt;(departure_time $0) 1600:ti) (from $0 dallas:ci))”</em></strong>预处理为树，方法是<strong>用 nonterminal 替换括号对之间的标记</strong>（token）。<strong>特殊记号 <code>&lt;s&gt;</code> 和 <code>&lt;(&gt;</code> 分别表示序列和 nonterminal 序列的开头</strong>（由于缺少空间，图3中省略了），<strong>记号 <code>&lt;/s&gt;</code> 代表序列结束</strong>。具体步骤是： 1. 编码输入值 q； 2. 层次树解码器使用 RNN 在逻辑形式 a（在<strong>任务定义</strong>中已经说明了 q 和 a 的含义）的对应部分的子树中生成 tokens（注意这里的 token 带了 s）； 3. 如果预测的 token 为 <code>&lt;n&gt;</code>，则通过调节 nonterminal 的隐藏向量来解码序列。（博主注：举个例子理解一下：看图 3 的第一层，先是使用 encoder 进行编码，接着开始对逻辑形式进行解码，逻辑形式就是上面的斜体部分。接下来预测到了 token<code>&lt;n&gt;</code> 于是调用 nonterminal 的隐藏向量来进行解码，即生成一棵子树。以此类推，碰到 toekn <code>&lt;n&gt;</code> 就开始解码） 4. 与 Seq2Seq 解码器不同，当前的隐藏状态不仅仅取决于上一个时间步，为了更好地利用 parent nonterminal 的信息，我们引入了一个 parent-feeding 的连接，其中 parent nonterminal 的隐藏向量与输入连接（concatenated）并喂入 LSTM。</p>
<p>再举个例子帮助理解一下，如图 4 所示。逻辑形式为 <strong><em>A B (C)</em></strong>，其中 <span class="math inline">\(y_1 \dots y_6\)</span> 代表不同的时间步，<strong><em>(C)</em></strong> 对应子树。解码一共有<strong>两个步骤</strong>：一旦输入值 q 被编码，首先在深度为 1 处生成 <span class="math inline">\(y_1 \dots y_4\)</span>，直到 token <code>&lt;/s&gt;</code> 被预测到；接下来通过调节 nonterminal <span class="math inline">\(t_3\)</span> 的隐藏向量来生成 <span class="math inline">\(y_5, y_6\)</span>，<span class="math inline">\(p(a|q)\)</span> 的概率是这<strong>两个序列解码步骤</strong>的乘积： <span class="math display">\[
p(a|q) = p(y_1 y_2 y_3 y_4 | q) p(y_5 y_6 | y_{\leq 3},q)
\]</span> <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language%20to%20Logical%20Form%20with%20Neural%20Attention/一个简单的Seq2Tree的例子.jpg" alt="一个简单的Seq2Tree的例子" /></p>
<h2 id="attention-机制">Attention 机制</h2>
<p><strong><em>Attention 的原理</em></strong>。</p>
<h2 id="训练模型">训练模型</h2>
<p>我们的目标是<strong>最大化</strong>由自然语言语句作为输入时产生的逻辑形式的可能性，所以目标函数为： <span class="math display">\[
\text{minimize} - \sum_{(q,a) \in D} logp(a|q)
\]</span> 其中 <span class="math inline">\(D\)</span> 是所有自然语言逻辑形式训练对的集合，<span class="math inline">\(p(a|q)\)</span> 按式（1）计算。采用 <strong>RMSProp</strong> 算法解决了这一非凸优化问题。此外，使用 <strong>Dropout</strong> 进行正则化。</p>
<h2 id="推论">推论</h2>
<p>暂时略。</p>
<h2 id="参数识别">参数识别</h2>
<p>大多数的语义分析数据集都是为问答开发的。在经典的系统中，问题被映射乘逻辑形式，并在知识库中获取答案。由于问答任务的性质，许多自然语言的语句都包含实体或数字，它们通常被解析为逻辑形式的参数。其中不可避免地会有一些罕见或者根本不会出现在数据集中的实体或数字（对于小规模数据集尤其如此）。传统的序列编码器只是简单地用一个特殊的位置单词符号替换稀有单词（<a href="https://arxiv.org/pdf/1410.8206.pdf" target="_blank" rel="noopener" title="Addressing the Rare Word Problem in Neural Machine Translation">Luong et al. 2015a</a>; <a href="https://arxiv.org/pdf/1412.2007.pdf" target="_blank" rel="noopener" title="On Using Very Large Target Vocabulary for Neural Machine Translation">Jean et al. 2015</a>），这对语义分析是有害的。 为此开发了一个简单的参数识别程序。具体来说就是在输入的问题中标识实体和数字，并用它们的<strong>类型</strong>和<strong>唯一 id</strong> 替换它们。例如，将训练样本“<em>jobs with a salary of 40000</em>”及其逻辑形式“job(ANS), salary_greater_than(ANS,40000, year)”预处理为“jobs with a salary of <em><span class="math inline">\(num_0\)</span></em>”和“job(ANS), salary_greater_than(<em>ANS</em>,<em><span class="math inline">\(num_0\)</span></em>,<em>year</em>)”。一旦解码完毕，后处理步骤就会将所有标记 <span class="math inline">\(type_i\)</span> 恢复到它们以前的实体或数字。</p>
<h1 id="实验">实验</h1>
<p>我们将我们的方法在四个数据集上分别与以前的多个系统进行比较，下面将描述这些数据集。代码可在此处获得<a href="https://github.com/donglixp/lang2logic" target="_blank" rel="noopener" class="uri">https://github.com/donglixp/lang2logic</a>（lua 版，官方），<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch" target="_blank" rel="noopener" class="uri">https://github.com/Alex-Fabbri/lang2logic-PyTorch</a>（python 版，非官方）。</p>
<h2 id="数据集">数据集</h2>
<p><strong>JOBS</strong> 工作 <strong>GEO</strong> Geoquery data <strong>ATIS</strong> Airline Travel Information System（航空旅行信息系统） <strong>IFTTT</strong> if this then that（<a href="https://ifttt.com/" target="_blank" rel="noopener">地址</a> <a href="https://baike.baidu.com/item/ifttt/8378533" target="_blank" rel="noopener">百度百科介绍</a>），<a href="https://www.aclweb.org/anthology/P15-1085" target="_blank" rel="noopener">Quirk et al.2015</a> 从 IFTTT 网站提取大量的 if-this-then-that 来创建此数据库 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language%20to%20Logical%20Form%20with%20Neural%20Attention/数据集介绍.jpg" alt="数据集介绍" /></p>
<h2 id="设置">设置</h2>
<p>自然语言语句是小写的，并且使用基于维基百科的常见拼写错误列表来纠正拼写错误。使用 NLTK 来限制词汇（[Bird et al.2009] Natural Language Processing with Python. O’Reilly Media.），对于 IFTTT 过滤了在训练集中出现少于五次的 token，channels 和 functions。对于其他数据集，过滤了在训练集中至少两次没有出现的输入词，但保留了逻辑形式中的所有 token。并且使用了<strong>参数识别</strong>，当然也可以使用更复杂的办法。 超参数在 JOBS 和 GEO 上使用了交叉验证，使用了 ATIS 和 IFTTT 作为标准开发集（就是验证集，不同的叫法而已 development/validation）。 - <strong>RMSProp</strong>：batch size = 20；parameter = 0.95； - <strong>梯度修剪</strong>为 5 以缓解梯度爆炸（<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">Pascanu et al.2013</a>）； - <strong>参数</strong>从均匀分布 <span class="math inline">\(U(-0.08, 0.08)\)</span> 中随机初始化； - 两层 <strong>LSTM</strong> 用于 IFTTT，单层 LSTM 用于其他数据集； - <strong>dropout</strong> <span class="math inline">\(\in\)</span> {0.2,0.3,0.4,0.5}； - 隐藏向量和词嵌入<strong>维度</strong>从 {150, 200, 250} 选择； - <strong>early stopping</strong>； - 输入句子在进入编码器之前被反转（<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever et al.2014</a>）； - <strong>贪婪搜索</strong>生成逻辑形式； - <strong>softmax</strong> 用于分类。</p>
<h2 id="结果">结果</h2>
<p>Attention 机制可以提高性能，对于小数据集<strong>参数识别</strong>至关重要。</p>
<h2 id="错误分析">错误分析</h2>
<h1 id="数据格式">数据格式</h1>
<p><a href="http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf" target="_blank" rel="noopener" title="Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars">论文</a>使用 Logical Form 对基准数据集做实验，数据集包括 Geo880 和 Jobs640，论文中使用的是 Logical Form 的其中一种表示——<strong>PCCG</strong>，它是 CCG 的改进版。他们将数据集分割为训练集和测试集，Language to Logical Form with Neural Attention 沿用了此分割方式（比如说将 GEO 分割为 680 个训练样本，200 个测试样本），并且采用此论文的思想，即：将自然语言映射为 Logical Form。 虽然 PCCG 的 Logical Form 效果不错，但是作者没有使用他，而是使用了 <a href="https://www.aclweb.org/anthology/D11-1140" target="_blank" rel="noopener">lambda-caculus</a>。<strong>作者将 Geo880 等数据集改写为了 lambda-calculus 的形式</strong>。<em>在<a href="https://github.com/yuxuan1995liu/Semantic-Parsing-Data-Pre-Processing" target="_blank" rel="noopener">此处</a>可找到全部数据，但是这里面的格式不是 lambda-calculus。我有点搞不懂他提供的数据到底是什么意思</em>。<strong>19.09.16 补充</strong>：经过多方查找，终于找到了 geo880 最初的<a href="https://link_springer.gg363.site/content/pdf/10.1007/3-540-44795-4_40.pdf" target="_blank" rel="noopener" title="Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing">论文</a>，在<a href="https://www.cs.utexas.edu/~ml/publications/year/2001" target="_blank" rel="noopener">此处</a>找到的。GEO880 最初版本并不是 lambda calculus。 作者将数据改写为 lambda-calcullus 形式是我估计的。因为全文找不到数据的来源，格式转换的说明也找不到。只是在 Section 4.1 Datasets 中说到： &gt; <strong>GEO 有 880 个示例，将其分割为 680 个训练样本以及 200 个测试样本（Zettlemoyer and Collins, 2005）， 我们使用了基于 lambda-calculus 的具有相同含义的表示</strong>。</p>
<p>所以我推测作者应该是将原本的 PCCG 表示的 GEO 改成了 lambda-calculus 表示。 <a href="https://arxiv.org/pdf/1805.04793" target="_blank" rel="noopener" title="Coarse-to-Fine Decoding for Neural Semantic Parsing">论文</a>是作者对 Language to Logical Form with Neural Attention 的改进版。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    赞赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/alipay.gif" alt="朱冲䶮 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>朱冲䶮
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yan624.github.io/posts/521b57a9.html" title="Language to Logical Form with Neural Attention">https://yan624.github.io/posts/521b57a9.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/KBQA/" rel="tag"># KBQA</a>
              <a href="/tags/%E8%AF%AD%E4%B9%89%E8%A7%A3%E6%9E%90KBQA/" rel="tag"># 语义解析KBQA</a>
              <a href="/tags/seq2tree/" rel="tag"># seq2tree</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/d87c2736.html" rel="prev" title="Large-scale Simple Question Answering with Memory Network">
      <i class="fa fa-chevron-left"></i> Large-scale Simple Question Answering with Memory Network
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/ecdf3755.html" rel="next" title="Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing">
      Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#论文概要"><span class="nav-number">1.</span> <span class="nav-text">论文概要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#相关工作"><span class="nav-number">2.</span> <span class="nav-text">相关工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#任务定义"><span class="nav-number">3.</span> <span class="nav-text">任务定义</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#seq2seq"><span class="nav-number">3.1.</span> <span class="nav-text">Seq2Seq</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#seq2tree"><span class="nav-number">3.2.</span> <span class="nav-text">Seq2Tree</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention-机制"><span class="nav-number">3.3.</span> <span class="nav-text">Attention 机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练模型"><span class="nav-number">3.4.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#推论"><span class="nav-number">3.5.</span> <span class="nav-text">推论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数识别"><span class="nav-number">3.6.</span> <span class="nav-text">参数识别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实验"><span class="nav-number">4.</span> <span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据集"><span class="nav-number">4.1.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#设置"><span class="nav-number">4.2.</span> <span class="nav-text">设置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结果"><span class="nav-number">4.3.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#错误分析"><span class="nav-number">4.4.</span> <span class="nav-text">错误分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据格式"><span class="nav-number">5.</span> <span class="nav-text">数据格式</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="朱冲䶮"
      src="/images/%E5%A6%99%E8%9B%99%E7%A7%8D%E5%AD%90.webp">
  <p class="site-author-name" itemprop="name">朱冲䶮</p>
  <div class="site-description" itemprop="description">记录</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">181</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">105</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
      <!-- 不蒜子/busuanzi -->
      <div class="site-state-item site-state-posts">
      	<span class="site-state-item-count">247.1k</span>
      	<span class="site-state-item-name">总字数</span>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/zhu-yu-er-85" title="zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;zhu-yu-er-85" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:897538633@qq.com" title="E-Mail → mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/yan624" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yan624" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/noval" title="神奇的按钮 → noval"><i class="fa fa-book fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      友链
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://huaguoguo.gitee.io/" title="http:&#x2F;&#x2F;huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://lzh0928.gitee.io/" title="https:&#x2F;&#x2F;lzh0928.gitee.io&#x2F;" rel="noopener" target="_blank">Mr.Liu</a>
        </li>
    </ul>
  </div>
<!-- CloudCalendar -->
<div class="widget-wrap" style="width: 90%;margin-left: auto;margin-right: auto; opacity: 0.97;">
	<div class="widget" id="CloudCalendar"></div>
</div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div><!--
樱花特效 
最初在某人的博客中看到这个特效，于是在网上搜了一圈，发现还有其他人也用它。它使用起来特别简单，只需要一行代码。
然后在 github 上搜了一下，发现有个 jquery-sakura，但是这个插件用起来很麻烦，经过测试，我的博客上无法使用。
后来发现是两个不同的插件，只是刚好特效一样。
于是我又搜了一下，貌似发现了源头，好像是一个博主随手写的，并没有发到 github 上。
原地址为：https://cangshui.net/2372.html
-->
<script>
	var pathname = window.location.pathname;
	if(pathname == '/' || pathname == '/index.html'){
		document.write("<script src='/lib/sakura/sakura-flying.js'><\/script>");
	}
</script>
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
<script src="/lib/my-utils.js"></script>
<script src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- 背景插件 -->
<script src="https://cdn.bootcss.com/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<!--
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(10), {
		duration:90000,//1 min 半一换
		fade: 1500
	});
</script>
-->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://yan624.github.io/posts/521b57a9.html',]
      });
      });
  </script>
<!-- calendar widget -->


</body>
</html>
