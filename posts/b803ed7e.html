<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yan624.github.io","root":"/","scheme":"Pisces","version":"8.0.0-rc.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"scrollpercent":true,"enable":true,"sidebar":false},"bookmark":{"enable":true,"save":"manual","color":"#222"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文虽然理了一遍神经网络的知识点，但还是有些地方不明白，文中对此进行了提问。  大纲    序号 描述的内容     2~4 神经网络和深度学习的发展史。   5 从二元分类开始。   6~10 浅层神经网络的介绍。如神经网络中一些参数代表的意思、激活函数、梯度下降、随机初始化。   11~15 深层神经网络的介绍。正向传播和反向传播中向量化后的计">
<meta property="og:type" content="article">
<meta property="og:title" content="对神经网络整体的理解">
<meta property="og:url" content="https://yan624.github.io/posts/b803ed7e.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="本文虽然理了一遍神经网络的知识点，但还是有些地方不明白，文中对此进行了提问。  大纲    序号 描述的内容     2~4 神经网络和深度学习的发展史。   5 从二元分类开始。   6~10 浅层神经网络的介绍。如神经网络中一些参数代表的意思、激活函数、梯度下降、随机初始化。   11~15 深层神经网络的介绍。正向传播和反向传播中向量化后的计">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%8E%9F%E5%9B%BE.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/sigmoid%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/softmax%E4%BE%8B%E5%AD%90.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%AE%80%E5%8C%96%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%9B%B4%E8%A7%82%E6%98%A0%E5%83%8F.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E5%BC%80%E5%8F%91%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%86%E5%B8%83%E4%B8%8D%E5%8C%B9%E9%85%8D.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%86%B3%E7%AD%96%E7%95%8C%E9%99%90.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E5%90%8C%E6%97%B6%E9%AB%98%E6%96%B9%E5%B7%AE%E5%92%8C%E9%AB%98%E5%81%8F%E5%B7%AE%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/dropout%E5%BE%85%E5%88%A0%E9%99%A4%E7%BB%93%E7%82%B9.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8dropout%E5%90%8E%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/early%20stopping.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BB%8E%E7%B2%97%E7%B3%99%E5%88%B0%E7%B2%BE%E7%BB%86%E7%9A%84%E5%8F%96%E5%80%BC%E7%AD%96%E7%95%A5.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E9%80%89%E6%8B%A9.jpg">
<meta property="article:published_time" content="2019-04-12T12:12:28.000Z">
<meta property="article:modified_time" content="2020-08-18T04:03:53.279Z">
<meta property="article:author" content="朱冲䶮">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="simple NN">
<meta property="article:tag" content="back propagation">
<meta property="article:tag" content="gradient descent">
<meta property="article:tag" content="bias">
<meta property="article:tag" content="variance">
<meta property="article:tag" content="regularization">
<meta property="article:tag" content="dropout">
<meta property="article:tag" content="early stopping">
<meta property="article:tag" content="激活函数">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%8E%9F%E5%9B%BE.jpg">

<link rel="canonical" href="https://yan624.github.io/posts/b803ed7e.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<!--阿里云矢量库样式-->
<link rel="stylesheet" href="//at.alicdn.com/t/font_1717154_dea9txmf0dl.css" />
<!-- 百度统计 -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?92e095b76795f9a4c661cb408e43ae3f";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  <title>对神经网络整体的理解 | 博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">末流炼丹师</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-常用链接">

    <a href="/%E5%B8%B8%E7%94%A8%E9%93%BE%E6%8E%A5" rel="section"><i class="fas fa-bookmark fa-fw"></i>常用链接</a>

  </li>
        <li class="menu-item menu-item-时间线">

    <a href="/categories/assorted/timeline/" rel="section"><i class="iconfont icon-timeline fa-fw"></i>时间线</a>

  </li>
        
            
  <li class="menu-item menu-item-博客分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>博客分类</a>

  </li>


      
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">174</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    
    	
        <!-- 弹窗插件 -->
    		<link rel="stylesheet" type="text/css" href="/lib/spop/spop.min.css">
        <script type="text/javascript" src="/lib/spop/spop.min.js"></script>
        <!--判断此文是否为特殊的文章-->
        <script>
          var templateSentence = '这是条不可能出现的弹窗提示。';
          if('学习笔记' == '学习笔记')
            templateSentence = '<h4 class="spop-title">注意</h4>此文仅为博主的学习笔记。';
          else if('学习笔记' == '4me')
            templateSentence = '<h4 class="spop-title">注意</h4>此文仅供个人查阅，对于他人没什么太大的价值。';
          spop({
            template: templateSentence,
            group: 'tips',
            position  : 'bottom-center',
            style: 'success',
            autoclose: 5500,
            onOpen: function () {
              //这里设置灰色背景色
            },
            onClose: function() {
              //这里可以取消背景色
              /*spop({
                template: 'ε = = (づ′▽`)づ',
                group: 'tips',
                position  : 'bottom-center',
                style: 'success',
                autoclose: 1500
              });*/
            }
          });
        </script>
    	
    
    	
    
    	
    
    	
    
    	
    
    	
    
    	
    
    	
    
    	
    
    	
    

    <link itemprop="mainEntityOfPage" href="https://yan624.github.io/posts/b803ed7e.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A6%99%E8%9B%99%E7%A7%8D%E5%AD%90.webp">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          对神经网络整体的理解
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-12 20:12:28" itemprop="dateCreated datePublished" datetime="2019-04-12T20:12:28+08:00">2019-04-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-18 12:03:53" itemprop="dateModified" datetime="2020-08-18T12:03:53+08:00">2020-08-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <div class="note info"><p>本文虽然理了一遍神经网络的知识点，但还是有些地方不明白，文中对此进行了提问。</p>
</div>
<h1 id="大纲">大纲</h1>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">序号</th>
<th>描述的内容</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2~4</td>
<td>神经网络和深度学习的发展史。</td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td>从二元分类开始。</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6~10</td>
<td>浅层神经网络的介绍。如神经网络中一些参数代表的意思、激活函数、梯度下降、随机初始化。</td>
</tr>
<tr class="even">
<td style="text-align: center;">11~15</td>
<td>深层神经网络的介绍。正向传播和反向传播中向量化后的计算、参数和超参数、神经网络和大脑的关系。</td>
</tr>
<tr class="odd">
<td style="text-align: center;">16</td>
<td>一个Simple NN的例子。</td>
</tr>
<tr class="even">
<td style="text-align: center;">17~22</td>
<td>深度学习的实用性层面。数据切分、偏差与方差、正则化、dropout、其他正则化方法、均值归一化、梯度消失和梯度爆炸、梯度检验。</td>
</tr>
<tr class="odd">
<td style="text-align: center;">23~25</td>
<td>一些优化算法。Mini-batch、指数加权平均、Momentum、RMSprop、Adam、Adagrad。</td>
</tr>
<tr class="even">
<td style="text-align: center;">26~29</td>
<td>超参数调试、Batch正则化、激活函数以及一些深度学习框架。</td>
</tr>
<tr class="odd">
<td style="text-align: center;">30~end</td>
<td>本文略长，后序的文章请看对应章节的链接。</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<h1 id="神经网络和深度学习的发展">神经网络和深度学习的发展</h1>
<p>TODO</p>
<h1 id="神经网络和深度学习的关系">神经网络和深度学习的关系</h1>
<p>TODO</p>
<h1 id="为什么要深度学习">为什么要深度学习</h1>
<p>TODO</p>
<h1 id="从二元分类开始">从二元分类开始</h1>
<p>暂时省略，因为这里已经会了。</p>
<h1 id="神经网络的表示">神经网络的表示</h1>
<p>规定如下，l：第几层；w：权重值；b：偏差；z：输出值；a：激活值；i，j：都代表第几个神经元，如<span class="math inline">\(w^l_i\)</span>代表第l层的第i个权重值；W：向量化后的权重值；Z：向量化后的输出值；A：向量化后的激活值；<span class="math inline">\(\alpha\)</span>：学习速率；<span class="math inline">\(\lambda\)</span>：正则化项；</p>
<p>如果输出值z和激活值a无法理解或者区分，没关系，继续往下看就知道了。 如下图所示，一般规定input layer为第0层，不算入神经网络的层数中，所以下图是一个三层神经网络架构。 1. input layer的输入值被称为x，下图一共有三个输入所以分别被称为<span class="math inline">\(x_1\ x_2\ x_3\)</span>。为了方便起见，可以将input layer的值x以<span class="math inline">\(a^0\)</span>来代替，下面解释a代表什么。 2. hidden layer中的值被称为a——<strong>激活值</strong>（activations），图中有四个神经元，所以分别被称为<span class="math inline">\(a^1_1\ a^1_2\ a^1_3\)</span>，上标代表着所在神经网络中的第几层，下标代表着所在层中的第几个神经元。如果表示成向量形式就是 <span class="math display">\[
\begin{pmatrix}
    x_1\\
    x_2\\
    x_3\\
\end{pmatrix} = 
\begin{pmatrix}
    a^0_1\\
    a^0_2\\
    a^0_3\\
\end{pmatrix} 和
\begin{pmatrix}
    a^1_1\\
    a^1_2\\
    a^1_3\\
    a^1_4\\
\end{pmatrix} 和
\begin{pmatrix}
    a^2_1\\
    a^2_2\\
    a^2_3\\
    a^2_4\\
\end{pmatrix} 和
\begin{pmatrix}
    a^3_1\\
\end{pmatrix}
\]</span></p>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%8E%9F%E5%9B%BE.jpg" alt="神经网络架构原图" /><figcaption>神经网络架构原图</figcaption>
</figure>
<h2 id="神经网络中神经元的一些参数的含义特别解释w的含义">神经网络中神经元的一些参数的含义，特别解释w的含义</h2>
<p>hidden layer和output layer的每个神经元都有几个参数。分别为<span class="math inline">\(w^l\ b^l\)</span>，对照上图，这里的<span class="math inline">\(w^l\)</span>是一个(4,3)的矩阵，<span class="math inline">\(b^l\)</span>是一个(4,1)的向量。解释如下： <span class="math display">\[
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4\\
\end{cases}
\]</span> 可以看到一个公式中有三个w和一个b，一共有四个公式。<span class="math inline">\(w^l_{ij}\)</span>代表第l-1层的第j个神经元到第l层的第i个神经元上的w。如<span class="math inline">\(w^1_{12}\)</span>代表第0层的第2个神经元到第1层的第1个神经元上的w。注意这里的i和j实际上是与直觉相反的，也就是说按直觉来看应该是<span class="math inline">\(w^l_{ji}\)</span>才正常。如果对w的表示有疑惑的，可以看<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%ADweight%E7%9A%84%E8%A1%A8%E7%A4%BA%E9%97%AE%E9%A2%98.html">这篇</a>。 注意下这里的z是<strong>输出值</strong>，之前一直在说hidden layer中的值是a——激活值，其实a就是将z放到一个<strong>激活函数</strong>（activation function）中得到的一个值，这个激活函数是随用户挑选的，如果不能理解激活函数是什么，就暂时理解为激活函数自己想设成什么就设成什么。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg" alt="神经网络架构图" /></p>
<h1 id="神经网络中的输出是怎么计算的">神经网络中的输出是怎么计算的</h1>
<h2 id="以一个样本为例">以一个样本为例</h2>
<p>第0层是输入层，所以是不需要计算的，x我本来就有，我还计算什么？对吧。从hidden layer1开始到output layer每一层都需要计算一连串的值，下面给出第一层的计算公式： <span class="math display">\[
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1，a^1_1 = \sigma(z^1_1)\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2，a^1_2 = \sigma(z^1_2)\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3，a^1_3 = \sigma(z^1_3)\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4，a^1_3 = \sigma(z^1_4)\\
\end{cases}
\]</span> 这里的<span class="math inline">\(\sigma(z)\)</span>函数其实就是上面说的<strong>激活函数</strong>，一般来讲<span class="math inline">\(\sigma\)</span>这个符号特指sigmoid function: <span class="math inline">\(\frac{1}{1+e^{-z}}\)</span>。 这4行公式其实在上面已经给出部分，每一行包含两个公式，也就是说一个神经元中实际上先得到了z，然后再通过激活函数将z转为a。这里可能会有疑惑，已经得到z了为什么还要用一个函数将z转为a呢？这样不是毫无意义？下面有一部分会具体解释，也可以看下面几篇的解释： <a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">神经网络激励函数的作用是什么？有没有形象的解释？</a> <a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a> 现在回到本文，正如我上面所说，我一共写了四个公式（激活函数现在暂时不看），所以我要分别计算四个公式，也就是说要计算四次。那么有没有办法只计算一次就得到所有结果呢？答案是<strong>向量化</strong>（vectorization），现在开始用向量化来解决这个问题。 <span class="math display">\[
\begin{pmatrix}
    z^1_1\\
    z^1_2\\
    z^1_3\\
    z^1_4\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&amp;w^1_{12}&amp;w^1_{13}\\
    w^1_{21}&amp;w^1_{22}&amp;w^1_{23}\\
    w^1_{31}&amp;w^1_{32}&amp;w^1_{33}\\
    w^1_{41}&amp;w^1_{42}&amp;w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^0_1\\
    a^0_2\\
    a^0_3\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix} 
\]</span> <span class="math inline">\(===&gt;\ z^1 = w^1 * a^0 + b^1\)</span> 以下是整个神经网络的计算过程，也就是说只需要下面6行就可以代替上文占据几个屏幕的内容。 <span class="math display">\[
\begin{array}{c|}
    z^1 = w^1 * a^0 + b^1\\
    a^1 = \sigma(z^1)\\
    z^2 = w^2 * a^1 + b^2\\ 
    a^2 = \sigma(z^2)\\
    z^3 = w^3 * a^2 + b^3\\
    a^3 = \sigma(z^3)\\ 
\end{array} =&gt;记为P
\]</span> 最后一个a就是整个神经网络的输出值，也就是预测值（prediction），也可以用<span class="math inline">\(\hat{y}\)</span>表示，自然<span class="math inline">\(\hat{y} = a^3\)</span>。</p>
<h2 id="向量化计算多个样本">向量化计算多个样本</h2>
<p>上面我没有特意地说明其实我们只使用了一个样本，我们一直在使用<span class="math inline">\(a^0_1\ a^0_2\ a^0_3\)</span>，但是<span class="math inline">\(a^0_1\ a^0_2\ a^0_3\)</span>实际上只是<strong>一个</strong>样本。<span class="math inline">\(a^0\)</span>代表的是一个样本，<span class="math inline">\(a^0_1\)</span>代表的是样本中的第一个特征，如果不明白我可以举个例子：<span class="math inline">\(a^0_1\)</span>代表天气样本中的第一个特征——温度，<span class="math inline">\(a^0_2\)</span>代表湿度，<span class="math inline">\(a^0_3\)</span>代表PM2.5，<span class="math inline">\(a^0\)</span>代表整一个天气样本。 那么如果有成千上万个样本，总不能使用P计算成千上万次吧。这里再次使用向量化进行计算。 <span class="math display">\[
\begin{pmatrix}
    z^{11}_1&amp;z^{12}_1&amp;\cdots\\
    z^{11}_2&amp;z^{12}_2&amp;\cdots\\
    z^{11}_3&amp;z^{12}_3&amp;\cdots\\
    z^{11}_4&amp;z^{12}_4&amp;\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&amp;w^1_{12}&amp;w^1_{13}\\
    w^1_{21}&amp;w^1_{22}&amp;w^1_{23}\\
    w^1_{31}&amp;w^1_{32}&amp;w^1_{33}\\
    w^1_{41}&amp;w^1_{42}&amp;w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{01}_1&amp;a^{01}_1&amp;\cdots\\
    a^{01}_2&amp;a^{02}_1&amp;\cdots\\
    a^{01}_3&amp;a^{03}_1&amp;\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}
\]</span> <span class="math inline">\(===&gt;\ z^1 = w^1 * a^0 + b^1\)</span> <span class="math display">\[
\begin{pmatrix}
    z^{21}_1&amp;z^{22}_1&amp;\cdots\\
    z^{21}_2&amp;z^{22}_2&amp;\cdots\\
    z^{21}_3&amp;z^{22}_3&amp;\cdots\\
    z^{21}_4&amp;z^{22}_4&amp;\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^2_{11}&amp;w^2_{12}&amp;w^2_{13}&amp;w^2_{14}\\
    w^2_{21}&amp;w^2_{22}&amp;w^2_{23}&amp;w^2_{24}\\
    w^2_{31}&amp;w^2_{32}&amp;w^2_{33}&amp;w^2_{34}\\
    w^2_{41}&amp;w^2_{42}&amp;w^2_{43}&amp;w^2_{44}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{11}_1&amp;a^{11}_1&amp;\cdots\\
    a^{11}_2&amp;a^{12}_1&amp;\cdots\\
    a^{11}_3&amp;a^{13}_1&amp;\cdots\\
    a^{11}_3&amp;a^{14}_1&amp;\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^2_1\\
    b^2_2\\
    b^2_3\\
    b^2_4\\
\end{pmatrix}
\]</span> <span class="math inline">\(===&gt;\ z^2 = w^2 * a^1 + b^2\)</span> 省略号代表后面有无数个样本，同理矩阵相乘也可以只用一个字母表示。上标的第二个数字代表是第几个样本，第一个数字依旧是代表所属第几层。</p>
<h1 id="激活函数">※ 激活函数</h1>
<p><a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a></p>
<table>
<colgroup>
<col style="width: 57%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="header">
<th>激活函数名称</th>
<th>如何选择</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sigmoid</td>
<td><strong>输出层</strong>为<strong>二元分类</strong>时选用。对于隐藏层来说，基本不会用 sigmoid 函数，因为现在已经有更好的激活函数<br /> <strong>缺点</strong>：1）会产生梯度消失/弥散（<strong>注：sigmoid 不会导致梯度爆炸</strong>），详见下面的 Sigmoid 章节；2）不是原点对称；3）计算 exp 较耗时。</td>
</tr>
<tr class="even">
<td>tanh</td>
<td><strong>优点</strong>：1）原点对称；2）比 sigmoid 快。<br /> <strong>缺点</strong>：1）还是有梯度消失</td>
</tr>
<tr class="odd">
<td><strong>ReLU</strong></td>
<td>首选 ReLU，如果 ReLU 不行，再换其他形式的 ReLU。<a href="https://github.com/llSourcell/Which-Activation-Function-Should-I-Use" target="_blank" rel="noopener">观点来源</a><br /> <strong>优点</strong>：1）解决了部分梯度消失问题；2）收敛速度更快。<br /> <strong>缺点</strong>：1）梯度消失的问题没有完全解决，在激活函数（-）部分相当于让神经元死亡，且无法复活。</td>
</tr>
<tr class="even">
<td>Leaky ReLU</td>
<td></td>
</tr>
<tr class="odd">
<td>Parametric ReLU</td>
<td></td>
</tr>
<tr class="even">
<td>Randomized ReLU</td>
<td></td>
</tr>
<tr class="odd">
<td>ELU</td>
<td></td>
</tr>
<tr class="even">
<td>SELU</td>
<td></td>
</tr>
<tr class="odd">
<td>GELU</td>
<td></td>
</tr>
<tr class="even">
<td>Swish</td>
<td></td>
</tr>
<tr class="odd">
<td>softmax</td>
<td><strong>输出层</strong>为<strong>多元分类</strong>时选用。只适合于输出层</td>
</tr>
<tr class="even">
<td>log_softmax</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="sigmoid">Sigmoid</h2>
<p>上文中我们一直假设使用 sigmoid function 作为激活函数。但是事实上还有很多其他选择，甚至其他的激活函数比sigmoid funtion效果要更好。 上面讲过<span class="math inline">\(\sigma(z)\)</span>特指 sigmoid function，现在我们将表达式改为：<span class="math inline">\(a = g(z)\)</span>，用g来表示激活函数，它可以是线性的，也可以是非线性的。 引用吴恩达在深度学习视频中的话： &gt; 有一个函数总是比 sigmoid function 表现得更好，就是tanh函数或者叫双曲正切函数，公式为：<span class="math inline">\(\frac{e^z-e^{-z}}{e^z+e^{-z}}, \, x\in(-1,1)\)</span>，在数学上实际是<span class="math inline">\(\sigma\)</span>函数平移后的版本。 &gt; <strong>事实证明，如果将<span class="math inline">\(g(z)\)</span>选为 tanh 函数，效果几乎总比 <span class="math inline">\(\sigma(z)\)</span> 函数要好。</strong></p>
<p>有一个例外是 output layer，它还是使用 sigmoid funtion，因为 output layer 跟普通的分类问题没什么区别，它要得到0~1之间的一个概率。 sigmoid function 的值总是位于 0~1 之间，tanh function 的值总是位于 -1~1 之间。 <div class="note warning"><p>在 CNN 中 ReLu 激活函数可能是首选，但是对于 RNN 来说，首选是 tanh，而不是 relu。</p>
</div></p>
<h3 id="缺陷">缺陷</h3>
<p>Sigmoid 函数将导致梯度消失。梯度的问题，在下面的章节（21 章左右）中会有讲到，但是此部分只讲 sigmoid 函数的梯度消失问题。 首先我们脑中大概有个 sigmoid 函数的图像，这应该很简单。注意函数的两边，我们发现函数曲线在 <span class="math inline">\(-\infty\)</span> 方向越来越接近 0，在 <span class="math inline">\(\infty\)</span> 方向越来越接近 1。以正方向为例，我们可以得知输入 sigmoid 的值越大，sigmoid 的输出值越接近 1。 做一个小小的测试，当输入值为 3 时，输出值为 0.9526，当输入值为 9 时，输出值为 0.9999。可以观察发现，输入值相差巨大的情况下，输出值居然相差无几。当然如果举一个更极端的例子，比如输入 20 和 2000，就会发现输出值都非常接近 1，详见下图。也就是说，输入值相差巨大，但是经过 sigmoid 之后，输出值居然相差无几。<strong>换句话说就是一个值在经过 sigmoid 之后被衰减了</strong>。通俗来讲，我管你是 2000 还是 20000000，只要经过我 sigmoid，你输出就只能是一个接近 1 的数。这样就是被衰减了。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/sigmoid梯度消失.jpg" alt="sigmoid梯度消失" /></p>
<p>其次我们又知道对于一个神经网络而言，它一般会叠的很深，四五层都很常见。有了以上两个基础，下面举个具体的例子。 我们知道梯度下降算法的公式是 <span class="math inline">\(W = W - \alpha \Delta W\)</span>，在进行一次梯度下降后，对于 W 来说，<strong>变化</strong>就是 <span class="math inline">\(\alpha \Delta W\)</span>，不严格的说其实只有 <span class="math inline">\(\Delta W\)</span>。然后将新的 W 传入 sigmoid 函数，我们就会发现 <span class="math inline">\(\Delta W\)</span> 被衰减了（为什么会衰减上面已经说过了）。而 <span class="math inline">\(\Delta W\)</span> 其实是<strong>梯度</strong>，也就是说梯度被衰减了，然后再经过多层神经网络之后，梯度被一减再减。 综上所述，梯度在第一层可能很大，在经过几层 sigmoid 函数之后，可能就<strong>减</strong>没了。 <strong>不过，由于 sigmoid 导数的取值范围是 (0, 0.25)，所以梯度也不会很大，但是这仍然架不住多层的神经网络</strong>。</p>
<h2 id="relu">ReLU</h2>
<p>但是不管是<span class="math inline">\(\sigma\)</span>或者tanh函数都一个缺点，那就是当z非常大或者非常小时，函数的斜率（导数的梯度）很小。这样会拖慢梯度下降。在机器学习中还有一个函数，即ReLU函数——Rectified Linear Unit，表达式为<span class="math inline">\(max(0, z)\)</span>。 所以在选择激活函数时有一些经验法则： 1. 如果你的输出值是0或1，那么<span class="math inline">\(\sigma\)</span>函数很适合做output layer的激活函数，非二元分类的情况下使用tanh函数几乎都比<span class="math inline">\(\sigma\)</span>优越。藏层单元全用ReLU函数，现在ReLU函数已经是隐藏层的默认激活函数了，大多数人都这么做。 2. 还有个叫Leaky ReLU的函数比ReLU稍微好点，但是目前暂时不是很多人用。</p>
<h2 id="softmax回归">Softmax回归</h2>
<p>Sogmoid函数<span class="math inline">\(\sigma = \frac{1}{1 + e^{-z}}\)</span>适用于二元分类，那么碰到多元分类怎么么办呢？Softmax函数就可以解决这个问题。 Softmax函数计算步骤如下，假设是n元分类： <span class="math display">\[
Z^L = W^L * A^{L-1} + b^L\\
t = e^{Z^L}\\
A^L = \frac{e^{Z^L}}{\sum^n_{i=1}t_i},\quad A^L_i = \frac{t_i}{\sum^n_{i=1}t_i}\\
\]</span> 多元分类中每一个神经元代表对应标签的概率是多少，并且将概率相加等于1。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/softmax%E4%BE%8B%E5%AD%90.jpg" alt="softmax例子" /></p>
<h2 id="为什么需要非线性激活函数">为什么需要非线性激活函数</h2>
<div class="note info"><p>为什么<span class="math inline">\(w*x+b=z\)</span>之后，需要使用一个激活函数<span class="math inline">\(a = \sigma(z)\)</span>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
</div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。 那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。 <span class="math display">\[
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w&#39; * a^0 + b&#39; \qquad由于w和b只是一堆常数，所以将w&#39;代替w^2 * w^1，b&#39;同理
    \end{array}
\right. 
\]</span> 通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<span class="math inline">\(a^2 = w&#39; * a^0 + b&#39;\)</span>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。 只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降反向传播算法backpropagation解析">梯度下降，反向传播算法——backpropagation解析</h1>
<div class="note primary"><p>Q：首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？ A：（2020.2.21） 1. <a href="https://yan624.github.io/·zcy/AI/ml/梯度下降算法的推导.html">梯度下降算法的推导</a> 2. <a href="https://yan624.github.io/·zcy/AI/深度学习500问笔记.html#词向量乘上权重以及做梯度下降有什么意义">深度学习500问笔记#词向量乘上权重以及做梯度下降有什么意义</a></p>
</div>
<p>本节的示例均建立在一个样本的情况下，如果是多个样本经过神经网络，可能略微不同。我看了吴恩达老师的深度学习课程，发现多个样本与一个样本的区别，可能只在偏差b那里会有点不同。 下图以一个三层神经网络为例，说明正向与反向传播过程。由于神经元之间的链接太多会导致混乱，所以下图只链接了第一个神经元。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="使用导数解释反向传播算法" /> 下图略微简化一下反向传播算法中的导数项，并且完成了最后的权重值优化。值得注意的是：如果cost function不同，下面求导结果会略微不同，本文统一使用<span class="math inline">\(cost = \frac{1}{m} * \sum{(\hat{y} - y)^2}\)</span>，但是神经网络一般是使用<strong>交叉熵</strong>——crossentropy，其公式为：<span class="math inline">\(cost = -\frac{1}{m} * (y * log(\hat{y}) + (1 - y) * log(1 - \hat{y}))\)</span>。使用前者，<span class="math inline">\(\frac{\partial{J}}{\partial{z^{(3)}}} = (a^{(3)} - y) * g&#39;(z^{(3)})\)</span>；如果使用后者，<span class="math inline">\(\frac{\partial{J}}{\partial{z^{(3)}}} = a^{(3)} - y\)</span>。可以看到使用两个不同的代价函数，会有不同的结果，这是因为两个函数求导的结果不一样。而两者对表达式<span class="math inline">\(\frac{\partial{J}}{\partial{z^{(3)}}}\)</span>的结果只差了一个<span class="math inline">\(g&#39;(z^{(3)})\)</span>，这完全是巧合罢了。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%AE%80%E5%8C%96%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="简化反向传播算法" /> <strong>另外再提醒一下自己，这里全是以一个样本为例。但是仅仅这样权重已经是一个二维矩阵了，要是如果传入多个样本，权重岂不是是一个三维矩阵？然而不管传入几个样本权重实际上对于不同的样本是没有变化的，所以还是二维矩阵。</strong></p>
<h1 id="随机初始化">※ 随机初始化</h1>
<p>对于逻辑回归可以将<strong>权重</strong>（weight）全部初始化为0，但是对于神经网络来说，将个权重初始化为0，再使用梯度下降会完全无效。实际上将偏差b初始化为0是可以的，但是权重不行。 解释起来太麻烦，详情看吴恩达深度学习——01神经网络和深度学习第三周浅层神经网络，3.11随机初始化。吴恩达老师解释地还是很清楚的。 可以像以下这样设置weight：<span class="math inline">\(w^l = np.random.randn((2, 2)) * 0.01\)</span>//这可以产生参数为(2, 2)的高斯分布随机变量，后面再成一个很小的数，比如0.01。而对于b，之前说了初始化为0也可以。 对于上式的0.01可能会感到很疑惑，为什么要乘这么一个值。因为我们一般将weight初始化为很小的值，如果weight值很大，最终导致z也很大，那么会落在sigmoid function或者tanh function的平缓部分，会使梯度的写了很小，意味着梯度下降算法会非常慢，所以学习得很慢。</p>
<h2 id="初始化补充">初始化补充</h2>
<p>经在作业中做的测试得出如下结论：</p>
<table>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>Train accuracy</strong></th>
<th><strong>Problem/Comment</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3-layer NN with <strong>zeros initialization</strong></td>
<td>50%</td>
<td>fails to break symmetry</td>
</tr>
<tr class="even">
<td>3-layer NN with large <strong>random initialization</strong></td>
<td>83%</td>
<td>too large weights</td>
</tr>
<tr class="odd">
<td>3-layer NN with <strong>He initialization</strong></td>
<td>99%</td>
<td><strong>recommended method</strong></td>
</tr>
</tbody>
</table>
<p>其中&quot;He initialization&quot;最近（论文是2015年的）新搞出来得初始化算法，现在推荐使用此算法进行初始化。</p>
<h1 id="核对矩阵维数">核对矩阵维数</h1>
<p>w的维数应该与dw的维数相同。b和db的维数相同</p>
<h1 id="为什么使用深度表示why-deep-representations">为什么使用深度表示——Why deep representations</h1>
<p>引用在2017course深度学习课程上吴恩达老师的话 &gt;深度神经网络能解决很多问题，其实并不需要很大的神经网络，但是得有深度。得有比较多的隐藏层。</p>
<p>为什么深度神经网络会很好用？ 1. 深度神经网络到底在计算什么？假设现在在做一个人脸识别系统。那么神经网络的第一层会去找照片里的边缘部分；第二层会去识别人类的特征，比如耳朵，鼻子，嘴巴；第三层会去识别不同的人脸。如下图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%9B%B4%E8%A7%82%E6%98%A0%E5%83%8F.jpg" alt="深度表示的直观映像" /> 这种识别模式可能难以理解，但是会在卷积神经网络——Convolutional Neural Network中详细解释。 这视频的这一章节有点难以总结，可以看看<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>，总共也就10分钟。</p>
<h1 id="深层神经网络块">深层神经网络块</h1>
<p>此视频中画出了深度神经网络的<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701023" target="_blank" rel="noopener">代码流程</a>。</p>
<h1 id="参数vs超参数">参数VS超参数</h1>
<p>有如下超参数（hyperparameters）：W, b, lerning rate <span class="math inline">\(\alpha\)</span>, iterations, hidden layer L, hidden units, choice of activatation function.这些超参数都需要自己设置。 上面这些都是基础的，实际上还有其他的超参数，稍后会涉及到。</p>
<h1 id="神经网络和大脑有什么关系">神经网络和大脑有什么关系？</h1>
<p>计算机视觉、其他深度学习领域或者其他学科在早期可能都受过人类大脑的启发，但是近年来人类将神经网络类比为大脑的次数越来越少，也就是说近年来大家都不怎么认为这二者有关联。</p>
<h1 id="一个simple-nn的例子">一个Simple NN的例子</h1>
<p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">Simple Neural Network</a>例子</p>
<hr />
<p>本节以下开始利用算法改善深层神经网络 *********************************************************************************************************************************************</p>
<h1 id="训练开发测试集">训练/开发/测试集</h1>
<p>训练集——training set 开发集/交叉验证集/验证集——dev set/cross validation set/validation set 测试集——test set</p>
<p>以前数据量小的时候，比如100个样本、10000个样本。一般将数据按三七分，七份训练集，三份测试集。验证集（以下均称验证集）在训练集中再细分，比如二八分，八份训练集。 但是现在进入大数据时代，验证集和测试集已经没有必要占大量比例了。比如现在有100万的样本，那么验证集和测试集只需要各抽取大约10000的样本即可。也就是98/1/1的比例，甚至验证集和测试集可以再降低占比。</p>
<h2 id="训练集和验证集测试集分布不匹配">训练集和验证集/测试集分布不匹配</h2>
<p>如下图，吴恩达老师建议最好让<strong>验证集</strong>和<strong>测试集</strong>匹配，即来自同一源，要都来自网络高清图，要么都来自手机低像素拍摄。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E5%BC%80%E5%8F%91%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%86%E5%B8%83%E4%B8%8D%E5%8C%B9%E9%85%8D.jpg" alt="训练集和验证集测试集分布不匹配" /> 如果直接不设置测试集也是可以的。</p>
<h1 id="偏差方差">※ 偏差/方差</h1>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%86%B3%E7%AD%96%E7%95%8C%E9%99%90.jpg" alt="欠拟合和过拟合的决策界限" /><figcaption>欠拟合和过拟合的决策界限</figcaption>
</figure>
<p>下图讲述了什么是<strong>过拟合</strong>，什么是<strong>欠拟合</strong>，如图所示，该神经网络用于判断一张图片是猫还是狗。 左边。训练样本中的误差为1%，这个值已经很小了，但是在验证集上的误差有11%。这就代表了过拟合，试想一下，在训练集上误差很小是因为你的决策界限划分的很好，在上图中的最后一个例子，整条决策界限画的十分完美，但是我们要知道在验证集中，这样一条完美的线肯定不能再拟合的很好。因为训练集和验证集即使来源于同一份数据，他们之间的分布也是不一样的，你训练出一条完美的曲线，在另一份数据集上肯定是过于完美了。所以导致了下图中验证集上的误差有11%。我们称这种情况为<strong>高方差</strong>——high variance。 中间。训练样本中的误差为15%，这已经不需要再看验证集上的误差了。因为训练集上的误差那么大，肯定是没有拟合好，所以这就是欠拟合，我们称为<strong>高偏差</strong>——high bais。 右边。如果训练集中的误差很高，验证集上的误差更高，那么可以判断为同时具有高方差和高偏差。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/欠拟合和过拟合.jpg" alt="欠拟合和过拟合" /></p>
<p>如果训练集上的误差为0.5%，验证集上的误差为1%。那这就是低方差和低偏差，这是很好结果。 最后一点，以上均建立在人眼判断的误差为0%上以及训练集和验证集来自相同分布。如果人眼判断的误差也高达15%，那么中间的例子也算是可以的结果一般来说<strong>最优误差</strong>也被称为<strong>贝叶斯误差</strong>。（不知道语义解析领域如何定义最优误差？） 关于上图同时高方差和高方差，就如同下图紫色线条的决策界限一般。过渡拟合了数据，但是拟合的数据其实狗屁不通。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E5%90%8C%E6%97%B6%E9%AB%98%E6%96%B9%E5%B7%AE%E5%92%8C%E9%AB%98%E5%81%8F%E5%B7%AE%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84.jpg" alt="同时高方差和高偏差是怎么样的" /></p>
<h2 id="非理想状态下的偏差方差">非理想状态下的偏差/方差</h2>
<p>那么当上图的理想状态被打破时（比如一张图片很模糊，连人眼也无法分辨），该如何分辨偏差/方差呢？ 可以通过对比训练集的 error 和 验证集的 error 来确定是否高方差。比如训练集的 error 为 1%，验证集的为 15%，那就是高方差。实际上跟上面的判断方法一样。</p>
<h2 id="机器学习遇到偏差或方差的解决办法">机器学习遇到偏差或方差的解决办法</h2>
<div class="note info">
<pre><code>&lt;p&gt;笔记中都记了，懒得再写一遍了。补充一点，遇到偏差或方差都可以更换神经网络架构，比如换成CNN或者RNN，如果是高偏差还可以使用更大的神经网络。&lt;/p&gt;</code></pre>
</div>
<p>可以看这个6分半中的小视频，<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702115" target="_blank" rel="noopener">机器学习基础</a>。</p>
<h1 id="正则化">正则化</h1>
<p>如果出现了过拟合，即高方差的情况，第一件想到的事是<strong>正则化</strong>——regularization。当然也可以增加数据，不过有时候数据不是那么容易获取的。可以对W使用<a href="https://www.baidu.com/s?wd=L2%E8%8C%83%E6%95%B0" target="_blank" rel="noopener">L2范数</a>进行正则化，当然对b也可以进行L2范数正则化，不过一般不加。L2范数的公式为<span class="math inline">\(||w||^2_2 = \sum_{j=1}^n w^2_j = W^T * W\)</span> 因此代价函数修改为<span class="math inline">\(cost = \frac{1}{m} \sum^m_{i=1} g(\hat{y}^i, y^i) + \frac{\lambda}{2m}||w||^2_2\)</span>，<span class="math inline">\(\lambda\)</span>是正则化的超参数。这里的w实际上是一个二维矩阵，所以L2范数需要把里面的每一个值的平方都加起来。 如果加入了正则化项，那么在计算dW时有点变化。将会变为：<span class="math inline">\(dW = dZ * A\_prev + \frac{\lambda}{m} w ^ l\)</span></p>
<h2 id="为什么正则化可以防止过拟合">为什么正则化可以防止过拟合</h2>
<p>略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702116" target="_blank" rel="noopener">1.5 为什么正则化可以减少过拟合？</a></p>
<h2 id="dropout">dropout</h2>
{% note primary %}
Q：那么问题又来了，dropout背后的原理是什么？
A：[深度学习500问笔记#Dropout-系列问题](https://yan624.github.io/·zcy/AI/深度学习500问笔记.html#Dropout-系列问题)（2020.2.21）
{% endnote %}
<p>dropout，中文翻译为<strong>随机失活</strong>。 先将神经网络复制一遍，然后dropout会遍历神经网络的每一层，并设置消除神经网络中结点的概率，比如设置0.5。下图的带X的结点就是准备消除的。另外每一层的概率都可以是不同的，如果在某一层不担心会过拟合可以将概率设为1.0，比如输出层。如果觉得某些层比其他层更容易过拟合，可以把那些层的keep-prob设置的更低。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/dropout%E5%BE%85%E5%88%A0%E9%99%A4%E7%BB%93%E7%82%B9.jpg" alt="dropout待删除结点" /> 下图则是消除后的神经网络。将结点的进出的链接全部删除。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8dropout%E5%90%8E%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg" alt="使用dropout后的神经网络" /> dropout使用之后，就让一个样本进入神经网络进行训练。而对于其他样本也如法炮制，需要再进行复制一遍神经网络，并进行dropout。 以上均是逻辑上的做法，接下来讲实际编码该怎么做。</p>
<ol type="1">
<li>设置一个结点保留的概率——keep-prob，假设为0.8。<span class="math inline">\(d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) &lt; keep-prob\)</span>，这样会得到一个True和False的数组，但是python中Ture等于1，False等于0。</li>
<li>让<span class="math inline">\(a^3\)</span>乘上这个向量。<span class="math inline">\(a^3 = np.multiply(a^3, d^3)\)</span>。由于False等于0，所以变相地将<span class="math inline">\(a^3\)</span>中的值失活了。</li>
<li>最后一步看起来有点奇怪，<span class="math inline">\(a^3 /= keep-prob\)</span>。 完整代码如下： <span class="math display">\[
\begin{cases}
 d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) &lt; keep-prob\\
 a^3 = np.multiply(a^3, d^3)\\
 a^3 /= keep-prob\\
\end{cases}
\]</span></li>
</ol>
<p>对于最后一步，由于<span class="math inline">\(Z^4 = W^4 * A^3 + b^4\)</span>，由于<span class="math inline">\(A^3\)</span>被dropout减少0.2，为了使得<span class="math inline">\(Z^4\)</span>不受影响，所以对<span class="math inline">\(A^3\)</span>除0.8，来保证<span class="math inline">\(A^3\)</span>的值不变。由于早期的版本没有除于keep-prob，使得测试阶段，平均值越来越复杂。 最后，从技术上来讲，输入值也可以使用dropout，但是基本不这么做，直接把keep-prob设为1.0即可，当然0.9也可以。不过太低的值一般不会去设置。 以上的步骤被称为<strong>Inverted dropout</strong>——<strong>反向随机失活</strong>。 dropout在计算机视觉中用的非常多，甚至成了标配。但要记住一点，dropout是一种正则化方法，为了预防过拟合。所以除非算法过拟合，不然不会使用dropout。由于计算机视觉的特殊性，他们才经常用dropout。 dropout的缺点是使我们失去了代价函数这一调试功能。我们经常使用代价函数得到误差，从而画出曲线图。但是使用dropout之后，这样的曲线图就不再准确了。</p>
<h3 id="测试">测试</h3>
<p>在测试阶段不再使用dropout，因为我们不希望输出结果是随机的，如果使用dropout预测会受到干扰。</p>
<h3 id="理解dropout">理解dropout</h3>
{% note info %}
略。有点晦涩。
{% endnote %}
<p>看<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a>。。</p>
<h2 id="其他正则化方法">其他正则化方法</h2>
<ol type="1">
<li>Data augment——数据增强。如果拟合猫咪图片分类器，可以对原图片做一些处理，来增加数据，比如翻转、旋转、随机裁剪等。</li>
<li>Early stopping。在训练时画出代价的曲线图，x轴为迭代次数，再绘制验证时的误差。然后选择验证误差曲线图中最低点的迭代次数，下次训练时就改用这个迭代次数，或者也可以在程序中写一个条件判断。如下图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/early%20stopping.jpg" alt="early stopping" /></li>
</ol>
<h1 id="均值归一化输入">均值归一化输入</h1>
<div class="note info">
<pre><code>略。其实很简单。</code></pre>
</div>
<h1 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h1>
<p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702118&amp;cid=2001699114" target="_blank" rel="noopener">视频</a> <a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另外一个参考视频</a>，08:37开始。 <a href="https://www.bilibili.com/video/av10590361/?p=37" target="_blank" rel="noopener">另一个</a>13:50~18左右</p>
<h2 id="解决办法">解决办法</h2>
<p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701047" target="_blank" rel="noopener">视频</a></p>
<h1 id="梯度检验">梯度检验</h1>
<p>Gradient checking(Grad check). <a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701048" target="_blank" rel="noopener">原理视频</a> <a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702119" target="_blank" rel="noopener">实战视频</a> 梯度检验可以帮助我们发现神经网络中的一些bug。具体原理是，通过数学上导数的定义来确认反向传播算法是否正确。如果学过高数就会知道，使用导数的定义求解和直接使用公式求解，两者结果十分接近或者一模一样。如果二者不一样说明肯定是求错了。 对应于神经网络，那就肯定是代码写错了。具体操作可在视频中看见，每个视频都不超过10分钟。</p>
<h2 id="注意事项">注意事项</h2>
<ol type="1">
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果梯度检验确实发现问题，要检查每一项，看看是哪个i的w和b有问题。</li>
<li>记得正则化项，它也被包含在w的梯度中。</li>
<li>梯度检验不能和dropout一起用。</li>
<li><del>在随机初始化时就运行一遍梯度检验；或许在训练一会后可以再运行一遍梯度检验。当W和b接近于0时，梯度下降正确执行在现实中几乎不太可能。</del>吴恩达老师说这条他在现实中几乎不会这么做，并且第五条的翻译，个人感觉翻得有问题，然后看了英文原文后，感觉原文表达得也不是很好，我看不太懂，所以这条就不算进注意事项了。</li>
</ol>
<h1 id="mini-batch梯度下降">Mini-batch梯度下降</h1>
<p>移至《<a href="https://yan624.github.io/·zcy/AI/dl/机器学习中的各种优化算法.html">机器学习中的各种优化算法</a>》。</p>
<h1 id="指数加权平均">指数加权平均</h1>
<p>移至《<a href="https://yan624.github.io/·zcy/AI/dl/机器学习中的各种优化算法.html">机器学习中的各种优化算法</a>》。</p>
<h1 id="learning-rate-decay">※ Learning rate decay</h1>
<p>移至《<a href="https://yan624.github.io/·zcy/AI/dl/机器学习中的各种优化算法.html">机器学习中的各种优化算法</a>》。</p>
<h1 id="如何为超参数选择范围">如何为超参数选择范围</h1>
<p>上面说了那么多算法，其中包括了许多超参数，那么应该怎么为超参数选择值呢？</p>
<h2 id="超参数的重要程度">超参数的重要程度</h2>
<p>按照吴恩达老师的排序，超参数的重要程度如下： 1. learning rate<span class="math inline">\(\alpha\)</span> 2. Momentum的<span class="math inline">\(\beta\)</span>, hidden layer units, mini-batch size 3. layer的数量，learning rate decay 4. Adam中的<span class="math inline">\(\beta_1\quad \beta_2\quad \epsilon\)</span>不是很重要，一般按<span class="math inline">\(0.9\quad 0.99\quad 10^{-8}\)</span>设置</p>
<h2 id="超参数的取值">超参数的取值</h2>
<ol type="1">
<li>随机取值</li>
<li>从粗糙到精细的策略。首先进行随机取值，发现某个点的效果很好，并且附近的点也很好，然后放大这块区域，进行更密集地取值。下图被圈出来的蓝点就是效果不错的，然后被方框画出一大块区域进行密集地取值或者也可以在这块区域随机取值。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BB%8E%E7%B2%97%E7%B3%99%E5%88%B0%E7%B2%BE%E7%BB%86%E7%9A%84%E5%8F%96%E5%80%BC%E7%AD%96%E7%95%A5.jpg" alt="从粗糙到精细的取值策略" /></li>
</ol>
<h2 id="选择合适的范围">选择合适的范围</h2>
<p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701053" target="_blank" rel="noopener">视频1</a> <a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701054" target="_blank" rel="noopener">视频2</a></p>
<h2 id="补充用神经网络训练另一个神经网络的超参数">补充——用神经网络训练另一个神经网络的超参数</h2>
<p>看完李宏毅深度学习后的补充，他在教学视频中也讲述了如何调整超参数，有个说的挺有创意的，就是<strong>用神经网络来训练超参数如何取值</strong>。典型的例子就是 <strong>Swish</strong>，它可以用神经网络训练出最好的几个激活函数。</p>
<h1 id="batch-normalization对激活值均值归一化">batch normalization——对激活值均值归一化</h1>
<p><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">论文地址</a> 这个原来不是一个算法，它就是让我们对神经网络的每一层都做一次normalization，从而提供性能，而算法是在batch中做的，所以叫这名。 <a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036#/learn/content?type=detail&amp;id=2001701055&amp;cid=2001693088" target="_blank" rel="noopener">视频地址</a>，第25章写了均值归一化，它对输入值进行了均值归一，更易于算法优化。而batch normalization对激活值进行了均值归一化，说白了是一个东西。</p>
<h2 id="代价函数">代价函数</h2>
<p>代价函数为<span class="math inline">\(cost(\hat{y}, y) = - \sum^n_{j=1} y_j * log(\hat{y_j})\)</span>。</p>
<div class="note primary"><p>但是这里可能会有点奇怪。因为二元分类的代价函数是<span class="math inline">\(cost(\hat{y}, y) = - \sum^n_{j=1} (y_j * log(\hat{y_j}) + (1 - y_j) * log(\hat{1-y_j}))\)</span>。怎么多元分类的表达式那么短？</p>
<p>2020.8.18 更新：不管是二元还是多元，其实公式都是一样的，最后都会缩减到只有一项，因为 y 是一个 one hot 向量。</p>
</div>
<h1 id="选择深度学习框架">选择深度学习框架</h1>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E9%80%89%E6%8B%A9.jpg" alt="深度学习框架选择" /><figcaption>深度学习框架选择</figcaption>
</figure>
<h1 id="序列模型rnn">序列模型——RNN</h1>
<p><a href="https://yan624.github.io/posts/5e27260b.html">传送门</a></p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    赞赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/alipay.gif" alt="朱冲䶮 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>朱冲䶮
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yan624.github.io/posts/b803ed7e.html" title="对神经网络整体的理解">https://yan624.github.io/posts/b803ed7e.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag"># 学习笔记</a>
              <a href="/tags/simple-NN/" rel="tag"># simple NN</a>
              <a href="/tags/back-propagation/" rel="tag"># back propagation</a>
              <a href="/tags/gradient-descent/" rel="tag"># gradient descent</a>
              <a href="/tags/bias/" rel="tag"># bias</a>
              <a href="/tags/variance/" rel="tag"># variance</a>
              <a href="/tags/regularization/" rel="tag"># regularization</a>
              <a href="/tags/dropout/" rel="tag"># dropout</a>
              <a href="/tags/early-stopping/" rel="tag"># early stopping</a>
              <a href="/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" rel="tag"># 激活函数</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/fe37c4.html" rel="prev" title="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题">
      <i class="fa fa-chevron-left"></i> 《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/f076a4cb.html" rel="next" title="Android开发，使用腾讯云的API请求对象存储中的资源始终失败">
      Android开发，使用腾讯云的API请求对象存储中的资源始终失败 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#大纲"><span class="nav-number">1.</span> <span class="nav-text">大纲</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络和深度学习的发展"><span class="nav-number">2.</span> <span class="nav-text">神经网络和深度学习的发展</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络和深度学习的关系"><span class="nav-number">3.</span> <span class="nav-text">神经网络和深度学习的关系</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#为什么要深度学习"><span class="nav-number">4.</span> <span class="nav-text">为什么要深度学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#从二元分类开始"><span class="nav-number">5.</span> <span class="nav-text">从二元分类开始</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络的表示"><span class="nav-number">6.</span> <span class="nav-text">神经网络的表示</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络中神经元的一些参数的含义特别解释w的含义"><span class="nav-number">6.1.</span> <span class="nav-text">神经网络中神经元的一些参数的含义，特别解释w的含义</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络中的输出是怎么计算的"><span class="nav-number">7.</span> <span class="nav-text">神经网络中的输出是怎么计算的</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#以一个样本为例"><span class="nav-number">7.1.</span> <span class="nav-text">以一个样本为例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#向量化计算多个样本"><span class="nav-number">7.2.</span> <span class="nav-text">向量化计算多个样本</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#激活函数"><span class="nav-number">8.</span> <span class="nav-text">※ 激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoid"><span class="nav-number">8.1.</span> <span class="nav-text">Sigmoid</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#缺陷"><span class="nav-number">8.1.1.</span> <span class="nav-text">缺陷</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#relu"><span class="nav-number">8.2.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax回归"><span class="nav-number">8.3.</span> <span class="nav-text">Softmax回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么需要非线性激活函数"><span class="nav-number">8.4.</span> <span class="nav-text">为什么需要非线性激活函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度下降反向传播算法backpropagation解析"><span class="nav-number">9.</span> <span class="nav-text">梯度下降，反向传播算法——backpropagation解析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#随机初始化"><span class="nav-number">10.</span> <span class="nav-text">※ 随机初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#初始化补充"><span class="nav-number">10.1.</span> <span class="nav-text">初始化补充</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#核对矩阵维数"><span class="nav-number">11.</span> <span class="nav-text">核对矩阵维数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#为什么使用深度表示why-deep-representations"><span class="nav-number">12.</span> <span class="nav-text">为什么使用深度表示——Why deep representations</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深层神经网络块"><span class="nav-number">13.</span> <span class="nav-text">深层神经网络块</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参数vs超参数"><span class="nav-number">14.</span> <span class="nav-text">参数VS超参数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络和大脑有什么关系"><span class="nav-number">15.</span> <span class="nav-text">神经网络和大脑有什么关系？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#一个simple-nn的例子"><span class="nav-number">16.</span> <span class="nav-text">一个Simple NN的例子</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#训练开发测试集"><span class="nav-number">17.</span> <span class="nav-text">训练&#x2F;开发&#x2F;测试集</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#训练集和验证集测试集分布不匹配"><span class="nav-number">17.1.</span> <span class="nav-text">训练集和验证集&#x2F;测试集分布不匹配</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#偏差方差"><span class="nav-number">18.</span> <span class="nav-text">※ 偏差&#x2F;方差</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#非理想状态下的偏差方差"><span class="nav-number">18.1.</span> <span class="nav-text">非理想状态下的偏差&#x2F;方差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习遇到偏差或方差的解决办法"><span class="nav-number">18.2.</span> <span class="nav-text">机器学习遇到偏差或方差的解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#正则化"><span class="nav-number">19.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么正则化可以防止过拟合"><span class="nav-number">19.1.</span> <span class="nav-text">为什么正则化可以防止过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout"><span class="nav-number">19.2.</span> <span class="nav-text">dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#测试"><span class="nav-number">19.2.1.</span> <span class="nav-text">测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#理解dropout"><span class="nav-number">19.2.2.</span> <span class="nav-text">理解dropout</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他正则化方法"><span class="nav-number">19.3.</span> <span class="nav-text">其他正则化方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#均值归一化输入"><span class="nav-number">20.</span> <span class="nav-text">均值归一化输入</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度消失和梯度爆炸"><span class="nav-number">21.</span> <span class="nav-text">梯度消失和梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#解决办法"><span class="nav-number">21.1.</span> <span class="nav-text">解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度检验"><span class="nav-number">22.</span> <span class="nav-text">梯度检验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#注意事项"><span class="nav-number">22.1.</span> <span class="nav-text">注意事项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mini-batch梯度下降"><span class="nav-number">23.</span> <span class="nav-text">Mini-batch梯度下降</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#指数加权平均"><span class="nav-number">24.</span> <span class="nav-text">指数加权平均</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#learning-rate-decay"><span class="nav-number">25.</span> <span class="nav-text">※ Learning rate decay</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#如何为超参数选择范围"><span class="nav-number">26.</span> <span class="nav-text">如何为超参数选择范围</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数的重要程度"><span class="nav-number">26.1.</span> <span class="nav-text">超参数的重要程度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数的取值"><span class="nav-number">26.2.</span> <span class="nav-text">超参数的取值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#选择合适的范围"><span class="nav-number">26.3.</span> <span class="nav-text">选择合适的范围</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#补充用神经网络训练另一个神经网络的超参数"><span class="nav-number">26.4.</span> <span class="nav-text">补充——用神经网络训练另一个神经网络的超参数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#batch-normalization对激活值均值归一化"><span class="nav-number">27.</span> <span class="nav-text">batch normalization——对激活值均值归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#代价函数"><span class="nav-number">27.1.</span> <span class="nav-text">代价函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#选择深度学习框架"><span class="nav-number">28.</span> <span class="nav-text">选择深度学习框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#序列模型rnn"><span class="nav-number">29.</span> <span class="nav-text">序列模型——RNN</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="朱冲䶮"
      src="/images/%E5%A6%99%E8%9B%99%E7%A7%8D%E5%AD%90.webp">
  <p class="site-author-name" itemprop="name">朱冲䶮</p>
  <div class="site-description" itemprop="description">记录</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">174</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">104</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
      <!-- 不蒜子/busuanzi -->
      <div class="site-state-item site-state-posts">
      	<span class="site-state-item-count">230k</span>
      	<span class="site-state-item-name">总字数</span>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/zhu-yu-er-85" title="zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;zhu-yu-er-85" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:897538633@qq.com" title="E-Mail → mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/yan624" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yan624" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/noval" title="神奇的按钮 → noval"><i class="fa fa-book fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      友链
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://huaguoguo.gitee.io/" title="http:&#x2F;&#x2F;huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://lzh0928.gitee.io/" title="https:&#x2F;&#x2F;lzh0928.gitee.io&#x2F;" rel="noopener" target="_blank">Mr.Liu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://smallwhitezzz.gitee.io/blog" title="https:&#x2F;&#x2F;smallwhitezzz.gitee.io&#x2F;blog" rel="noopener" target="_blank">凯子</a>
        </li>
    </ul>
  </div>
<!-- CloudCalendar -->
<div class="widget-wrap" style="width: 90%;margin-left: auto;margin-right: auto; opacity: 0.97;">
	<div class="widget" id="CloudCalendar"></div>
</div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div><!--
樱花特效 
最初在某人的博客中看到这个特效，于是在网上搜了一圈，发现还有其他人也用它。它使用起来特别简单，只需要一行代码。
然后在 github 上搜了一下，发现有个 jquery-sakura，但是这个插件用起来很麻烦，经过测试，我的博客上无法使用。
后来发现是两个不同的插件，只是刚好特效一样。
于是我又搜了一下，貌似发现了源头，好像是一个博主随手写的，并没有发到 github 上。
原地址为：https://cangshui.net/2372.html
-->
<script>
	var pathname = window.location.pathname;
	// pathname == '/' || pathname == '/index.html'
	if(pathname == '/categories/assorted/timeline/'){
		document.write("<script src='/lib/sakura/sakura-flying.js'><\/script>");
	}
</script>
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
<script src="/lib/my-utils.js"></script>
<script src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- 背景插件 -->
<script src="https://cdn.bootcss.com/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<!--
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(10), {
		duration:90000,//1 min 半一换
		fade: 1500
	});
</script>
-->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://yan624.github.io/posts/b803ed7e.html',]
      });
      });
  </script>
<!-- calendar widget -->


</body>
</html>
