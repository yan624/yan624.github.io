<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yan624.github.io","root":"/","scheme":"Pisces","version":"8.0.0-rc.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本笔记记录的内容来源于 b站——CS224n 斯坦福深度自然语言处理课。   之后我补充了18 章之后的内容，其来源为b站——(2019)斯坦福CS224n深度学习自然语言处理课程 by Chris Manning  开场白   略 词向量表示：word2vec   课程计划如下：    神经网络词嵌入学习的通用做法：定义一个模型，根据中心词 \(w_t\) 去预测上下文单词。给定">
<meta name="keywords" content="学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="CS224n学习笔记">
<meta property="og:url" content="https://yan624.github.io/posts/d9a134a.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="本笔记记录的内容来源于 b站——CS224n 斯坦福深度自然语言处理课。   之后我补充了18 章之后的内容，其来源为b站——(2019)斯坦福CS224n深度学习自然语言处理课程 by Chris Manning  开场白   略 词向量表示：word2vec   课程计划如下：    神经网络词嵌入学习的通用做法：定义一个模型，根据中心词 \(w_t\) 去预测上下文单词。给定">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/课程计划.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/以前的低维词向量表示方法.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/skip-grams模型.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/softmax.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/scientistis%20study%20whales%20from%20space的句法分析.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/example%20of%20treebanks.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/attention机制.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/加入attention机制后的性能.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/more%20attention（coverage）.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/Recursive%20vs.%20recurrent%20neural%20network.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive%20neural%20network%20for%20training.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive%20neural%20network%20details.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing%20a%20sentence%20with%20rnn%201.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing%20a%20sentence%20with%20rnn%202.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing%20a%20sentence%20with%20rnn%203.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing%20a%20sentence%20with%20rnn%204.jpg">
<meta property="og:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/related%20work%20for%20parsing.jpg">
<meta property="og:updated_time" content="2020-06-14T10:53:52.159Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS224n学习笔记">
<meta name="twitter:description" content="本笔记记录的内容来源于 b站——CS224n 斯坦福深度自然语言处理课。   之后我补充了18 章之后的内容，其来源为b站——(2019)斯坦福CS224n深度学习自然语言处理课程 by Chris Manning  开场白   略 词向量表示：word2vec   课程计划如下：    神经网络词嵌入学习的通用做法：定义一个模型，根据中心词 \(w_t\) 去预测上下文单词。给定">
<meta name="twitter:image" content="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/课程计划.jpg">

<link rel="canonical" href="https://yan624.github.io/posts/d9a134a.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<!--图片缩放插件样式-->
<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css" />
<!--阿里云矢量库样式-->
<link rel="stylesheet" href="//at.alicdn.com/t/font_1717154_621sfmh583s.css" />

        

  <title>CS224n学习笔记 | 博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">低阶炼金术士</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-常用链接">

    <a href="/常用链接" rel="section"><i class="fas fa-bookmark fa-fw"></i>常用链接</a>

  </li>
        <li class="menu-item menu-item-时间线">

    <a href="/categories/timeline/" rel="section"><i class="iconfont icon-timeline fa-fw"></i>时间线</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">94</span></a>

  </li>
        
            
  <li class="menu-item menu-item-博客分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>博客分类</a>

  </li>


      
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">163</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    
    	
        <!-- 弹窗插件 -->
    		<link rel="stylesheet" type="text/css" href="/lib/spop/spop.min.css">
        <script type="text/javascript" src="/lib/spop/spop.min.js"></script>
        <!--判断此文是否为特殊的文章-->
        <script>
          var templateSentence = '这是条不可能出现的弹窗提示。';
          if('学习笔记' == '学习笔记')
            templateSentence = '<h4 class="spop-title">注意</h4>此文仅为博主的学习笔记，并非教学，其中可能含有理论错误。';
          else if('学习笔记' == '4me')
            templateSentence = '<h4 class="spop-title">注意</h4>此文仅供个人查阅，对于他人没什么太大的价值。';
          spop({
            template: templateSentence,
            group: 'tips',
            position  : 'bottom-center',
            style: 'success',
            autoclose: 5500,
            onOpen: function () {
              //这里设置灰色背景色
            },
            onClose: function() {
              //这里可以取消背景色
              spop({
                template: 'ε = = (づ′▽`)づ',
                group: 'tips',
                position  : 'bottom-center',
                style: 'success',
                autoclose: 1500
              });
            }
          });
        </script>
    	
    

    <link itemprop="mainEntityOfPage" href="https://yan624.github.io/posts/d9a134a.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS224n学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-05-20 10:51:09" itemprop="dateCreated datePublished" datetime="2019-05-20T10:51:09+08:00">2019-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-14 18:53:52" itemprop="dateModified" datetime="2020-06-14T18:53:52+08:00">2020-06-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <div class="note info"><p>  本笔记记录的内容来源于 <a href="https://www.bilibili.com/video/av41393758" target="_blank" rel="noopener">b站——CS224n 斯坦福深度自然语言处理课</a>。   之后我补充了18 章之后的内容，其来源为<a href="https://www.bilibili.com/video/av46216519" target="_blank" rel="noopener">b站——(2019)斯坦福CS224n深度学习自然语言处理课程 by Chris Manning</a></p>
</div>
<h1 id="开场白">开场白</h1>
<p>  略</p>
<h1 id="词向量表示word2vec">词向量表示：word2vec</h1>
<p>  课程计划如下： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/课程计划.jpg" alt="课程计划"></p>
<p>  <strong>神经网络词嵌入学习的通用做法</strong>：定义一个模型，根据中心词 <span class="math inline">\(w_t\)</span> 去预测上下文单词。给定 <span class="math inline">\(w_t\)</span> 的条件下 context 的概率。 <span class="math display">\[
p(context|w_t) = \dots
\]</span>   然后用损失函数判断预测的准确性，例如： <span class="math display">\[
J = 1 - p(w_{-t}|w_t), \quad  \text{-t 代表 t 周围的单词}
\]</span>   如果可以精准地根据 t 预测到这些单词，那么概率就为 1，于是损失就没有了。但通常情况下，做不到这点。<strong>所以我们应该调整词汇表示，从而使损失最小化</strong>。 <a id="more"></a>   下图是以前的低维词向量表示方法，2003 年 Bengio 发表的这篇现在属于开创性的论文其实并没有太多人关注，因为那时候深度学习并没有很流行。但是当这篇论文开始流行的时候，就开始大行其道了。于是 2008 年 Collobert 和 Weston 开启了一个新方向，<strong>他们觉得如果我们只想要得到好的单词表示，我们甚至不需要构建一个具有预测功能的概率语言模型（probabilistic language model），我们只需要找到一种学习单词表示的方法即可</strong>。于是 2013 年有了 word2vec 模型。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/以前的低维词向量表示方法.jpg" alt="以前的低维词向量表示方法"></p>
<p>  word2vec 是一个软件，实际上，它里面包含很多东西。有两个用于生成词汇向量的算法（Hierarchical softamx，negative sampling），还有两套效率中等的训练方法（Skip-grams，CBOW）。<em>这里的软件应该指的不是那种可以运行 exe 文件</em>。本节只讲 skip-grams 算法，并且不会讲那两个高效的词向量生成算法，而是将一个效率极低的算法（因为比较简单且包含了基本概念）。   skip-grams 模型的概念是：在每一个估算步中，都取一个词为中心词汇，然后尝试预测它<strong>一定范围内</strong>的上下文的词汇。这个模型将定义一个概率分布：<strong>给定一个中心词汇预测某个单词在它上下文中出现的概率</strong>。如下图所示： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/skip-grams模型.jpg" alt="skip-grams模型"></p>
<p>  我们将会选取词汇的向量表示，以让概率分布值最大化。</p>
<h2 id="优化目标">优化目标</h2>
<p>  我们需要做的是定义一个半径 m，然后从中心词汇开始到距离为 m 的位置来预测周围的词汇。这句话比较抽象，因为到这为止，你还是构建不出一个模型（优化目标）。下面先给出模型的公式，注意一撇不是求导： <span class="math display">\[
J&#39;(\theta) = \prod^T_{t=1} \prod_{-m \leq m, j \neq 0} p(w_{t+j}|w_t;\theta)
\]</span>   其中定义一句话有 T 个单词，word t = 1 <span class="math inline">\(\dots\)</span> T。上式中 m 为半径窗口，j 为整个窗口之中的索引。先不看第一个累乘符号，当 t = 1 时，也就是当中心词的索引为 1 时，以 m 为半径，预测该中心词汇的上下文单词出现的概率，并将所有的概率累乘。即公式： <span class="math inline">\(\prod_{-m \leq m, j \neq 0} p(w_{t+j}|w_t;\theta)\)</span>。而第一个累乘符号指的是，将句子中每一个字都当做一次中心词汇，然后将概率再累乘起来。当然当中心词的索引比较靠前时，可能窗口会超出句子的前部，比如 中心词汇所以为 1，而 m = 5，则需要预测 -4，-3... 的位置，这显然不可能，所以需要自己做一下处理。   公式中的 <span class="math inline">\(\theta\)</span> 是模型唯一的参数，让上下文所有词汇出现的概率都尽可能的高，其实 <span class="math inline">\(\theta\)</span> 就是词向量，而模型的输入就是 one-hot 表示。但是，处理概率问题是一件很不爽的事，我们要做最大化操作，实际上就是解决对数分布的问题。这样求积就会变成求和，如下所示： <span class="math display">\[
J(\theta) = -\frac{1}{T} \sum^T_{t=1} \sum_{-m \leq m, j \neq 0} log \, p(w_{t+j}|w_t;\theta)
\]</span>   这样我们就得到了<strong>负的对数似然</strong>，上述公式就是最终版。但是这里还有一小点就是 m 其实也算是模型的参数，但是确是<strong>超参数</strong>，需要自己手动改的。所以上面说“<em>公式中的 <span class="math inline">\(\theta\)</span> 是模型唯一的参数</em>”也没错。事实上这个模型还有很多其他的超参数，但是现在暂且视为常数。   公式前面有个负号，是因为我们要求最小化问题，而原式只能取最大值，所以取了个负号。</p>
<h3 id="确定相应的概率分布">确定相应的概率分布</h3>
<p>  那么我们具体应该怎么通过中心词汇来预测周围单词出现的概率呢？也就是说公式中的函数 p 应该是什么。其实 p 就是 softmax 函数。具体来说就是用由词向量构成的中心词汇去预测周围词汇的概率分布。下图就是 softmax 函数。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/softmax.jpg" alt="softmax"></p>
<h2 id="损失函数">损失函数</h2>
<p>  最前面讲到需要有一个损失函数来判断预测的准确性。我们使用 cross-entropy loss。</p>
<h2 id="计算">计算</h2>
<p><span class="math display">\[
\begin{align}
     &amp; \frac{\partial}{\partial v_c} log \frac{exp(u^T_o v_c)}{\sum^v_{w=1} exp(u^T_w v_c))} \\
    = &amp; \frac{\partial}{\partial v_c} (\underbrace{log \, exp(u^T_o v_c)}_{1} - \underbrace{log \, \sum^v_{w=1} exp(u^T_w v_c)}_2) \\
     &amp; \frac{\partial}{\partial v_c} log \, exp(u^T_o v_c) &amp; \text{1} \\
    = &amp; \frac{\partial}{\partial v_c} u^T_o v_c = u_o  \\
     &amp; \frac{\partial}{\partial v_c} log \, \sum^v_{w=1} exp(u^T_w v_c) &amp; \text{2} \\
    = &amp; \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \frac{\partial}{\partial v_c} \sum^v_{x=1} exp(u^T_x v_c) \\
    = &amp; \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \sum^v_{x=1} \frac{\partial}{\partial v_c} exp(u^T_x v_c) \\
    = &amp; \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \sum^v_{x=1} exp(u^T_x v_c) \, u_x \\
    = &amp; \sum^v_{x=1} \frac{exp(u^T_x v_c)}{\sum^v_{w=1} exp(u^T_w v_c)} \, u_x \\
    = &amp; \sum^v_{x=1} p(x|c) u_x \\
     &amp; u_o - \sum^v_{x=1} p(x|c) u_x &amp; \text{合并}\\
\end{align}
\]</span></p>
<h1 id="高级词向量表示">高级词向量表示</h1>
<p>  略。说了一些 word2vec 算法以及 GloVe 等算法。</p>
<h1 id="word-window分类与神经网络">Word Window分类与神经网络</h1>
<p>  略。讲 Word Window 分类和简单的神经网络。</p>
<h1 id="反向传播和项目建议">反向传播和项目建议</h1>
<p>  讲反向传播，略。</p>
<h1 id="依存分析">※ 依存分析</h1>
<p>  6分38秒开始进入正题，之前都在说学校里的事。</p>
<h2 id="语言结构的两种观点">语言结构的两种观点</h2>
<p>  Constituency=phrase structure grammar=context-free grammars(CFGs)。上下文无关文法。   Dependency。依存句法分析。   传统上讲，语言学家和自然语言处理器想做的是描述人类语言结构。过去有两个工具可以做到这点，1. 上下文无关文法（计算机科学中）/短语结构文法（语言学家）；2. 依存句法结构。   依存句法分析做的是<strong>通过找到句子中每一个词所依赖的部分来描述句子的结构</strong>。如果一个词修饰另一个词或者是另一些词的论证，那么它就是那个词的依赖。例：“barking dog”，barking 是 dong 的依赖，因为 barking 修饰 dog。“dog by the door”，by the door 也是 dog 的依赖。我们可以<strong>在词之间添加依存关系，通常用箭头表示它们之间的依存关系</strong>，可以参考 <a href="http://hanlp.com/" target="_blank" rel="noopener">这里</a> 加以理解。   对于语义含糊的例子，都可以考虑使用依存分析，例如下图。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/scientistis%20study%20whales%20from%20space的句法分析.jpg" alt="scientistis study whales from space的句法分析"></p>
<p>  一个重要的概念：<strong>人类的语言确实有歧义，我们希望可以通过这些依存关系来描述人类语言</strong>。可以发现我们分析出了两组关系。   另一个重要的概念是：<strong>完整的语言学以树库（treebanks）的形式标注数据</strong>。1990年开始，将网络上的句子的句法结构描述为依存关系图，如下图所示，这是来自雅虎问答上的句子，我们将这些称为树库。1990年，我们投入了大量资源来建立这种标注型树库。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/example%20of%20treebanks.jpg" alt="example of treebanks"></p>
<p>  此处没有讲上下文语法，在过去的 10 年间（视频中的时间是 2017 年），nlp 中，<strong>依存句法分析</strong>已经取代了<strong>上下文无关文法</strong>。人们发现<strong>依存分析文法</strong>是一种（依存句法分析应该是依存分析文法的一种实现）仅仅构建语义表征就能轻松得到语言理解的合适框架。</p>
<h2 id="dependency-grammar-and-dependency-structure">Dependency Grammar and Dependency Structure</h2>
<p>  开始时间 22.19。   上面了解了什么是<strong>依存分析语法</strong>（从此节开始称之为语法，我感觉“文法”翻译得怪怪的），接下来讲解具体应该怎么做。   句法分析的思想是<strong>一个句法模型就是我们有一个词法项之间的关系或者词之间的关系</strong>。也就是说我们在词法项之间画箭头，这些箭头就是依存。通常我们做依存分析时，要做的工作要比这多。通常我们会根据一些语法关系来给这些依存关系分类并命名，比如主语、谓语、辅助修饰词等。</p>
<h2 id="dependency-parsing">Dependency parsing</h2>
<p>  依存分析有多种方式，视频中采用 Greedy transition-based parsing。视频开始于 52.05。   </p>
<h1 id="tensorflow-入门">Tensorflow 入门</h1>
<p>  略，不学 tensorflow。</p>
<h1 id="rnn和语言模式">RNN和语言模式</h1>
<p>  讲了传统语言模型，例如马尔科夫模型，n-gram 模型等。讲了 simple RNN ，bi-RNN and deep bi-RNN。提到了梯度消失，梯度爆炸，grad clipping 等。大部分都会，主要记录一些不会的内容。</p>
<h2 id="梯度消失">梯度消失</h2>
<p>  19.50 - 49.05</p>
<h2 id="梯度爆炸">梯度爆炸</h2>
<p>  49.06 - 62.38</p>
<h2 id="序列模型用于其他任务">序列模型用于其他任务</h2>
<p>  66:00 - NER - Entity level sentiment in context - opinionated expressions</p>
<h1 id="机器翻译和高级循环神经网络">机器翻译和高级循环神经网络</h1>
<p>  花了二三十分钟讲机器翻译，然后讲解各类 RNN，包括 GRU, LSTM, <strong>Pointer-Sentinel Model</strong>。</p>
<h1 id="神经机器翻译和注意力模型">神经机器翻译和注意力模型</h1>
<p>  先将机器翻译，后讲 attention。</p>
<h2 id="神经机器翻译">神经机器翻译</h2>
<h2 id="attention">attention</h2>
<p>  下图是 attention 的工作原理。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/attention机制.jpg" alt="attention机制"> 0. 图中的 a 代表 score，<span class="math inline">\(\bar{h}_s\)</span> 代表 encoder 中每个 time step 生成的隐藏状态向量，<span class="math inline">\(c_t\)</span> 代表 attention 之后的向量； 1. 首先将开始标志输入到一个 decoder，代表开始进行翻译，输出一个单词后，将该 decoder 的<strong>隐藏状态</strong>（注意是隐藏状态而不是输出值，此节课视频中有明确指出）与 encoder 中的<strong>隐藏状态</strong>进行计算得到一个 score。打分的公式为 <span class="math inline">\(score(h_{t - 1}, \bar{h}_s)\)</span>，score 具体是什么公式可以自己定义，最简单就是向量内积，下面会细说； 2. 关于 score 函数，它有多种选择，<strong>注意一点</strong>下面的 score 函数只是对<strong>一个</strong>时间步上的隐藏状态打分，<span class="math inline">\(\bar{h}_s\)</span> 也可以是个矩阵，即一步计算所有时间步的 attention score（这做法是最好的）。以下罗列几种做法，被广泛采用（2017 年的说法，现不知）的是第二个表达式，第三个表达式的 <span class="math inline">\(v_a\)</span> 也是一个向量参数。另外对于第三个表达式 <span class="math inline">\(v_a tanh(W_a [h_t;\bar{h}_s])\)</span>，它不是 score function，而是 Bahdanau，不知道为什么把它放到 score function 这。 <span class="math display">\[
score(h_t, \bar{h}_s) = 
\begin{cases}
h^T_t \bar{h}_s \\
h^T_t W_a \bar{h}_s \\
v_a tanh(W_a [h_t;\bar{h}_s])
\end{cases}
\]</span> 3. 将 score 送入 softmax 得到概率； 4. 通过公式 <span class="math inline">\(c_t = \sum_s a_t(s)\bar{h}_s\)</span>，将所有的向量乘上注意力分数加起来； 5. 将此新向量当做下一个 decoder 的输入；</p>
<p>  下图是加 attention 机制和不加的区别。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/加入attention机制后的性能.jpg" alt="加入attention机制后的性能"></p>
<h2 id="coverage">coverage</h2>
<p>  coverage = more attention，想法源于计算机视觉，请看下图。神经网络读入一张图片，要求输出一段话。但是我们知道一段话不仅要描写图中的鸟，还要描写鸟旁边的事物，所以就引出了多次注意，即神经网络需要注意图中更多的地方。将这一想法引入 NLP 中，其实就是多做几次 attention，<em>注：这一想法貌似就是后来 transformer 的 multi-head attention</em>。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/more%20attention（coverage）.jpg" alt="more attention(coverage)"></p>
<h2 id="search">※ search</h2>
<p>  </p>
<h1 id="gru及nmt的其他议题">※ GRU及NMT的其他议题</h1>
<h2 id="gruslstms">GRUs/LSTMs</h2>
<p>  gated unit 是如何解决 BPTT 的。</p>
<h2 id="nmt-evaluation">NMT evaluation</h2>
<h1 id="语音处理的端对端模型">语音处理的端对端模型</h1>
<p>  略，不做语音。</p>
<h1 id="卷积神经网络">卷积神经网络</h1>
<p>  略，不学 CNN。</p>
<h1 id="树rnn和短语句分析">※ 树RNN和短语句分析</h1>
<p>  人类语言具有嵌套结构（训练结构、树结构），如：[The man from [the company that you spoke with about [the project] yesterday]]。   那么如何使用向量来表示这些句子的语义呢？可以使用 tree RNN，如下图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/Recursive%20vs.%20recurrent%20neural%20network.jpg" alt="Recursive vs. recurrent neural network"> &gt;   <strong>Tree recursive neural network 的问题在于你需要得到一个树形结构</strong>，这是一个比较大的问题。树形网络并没有火遍全球，在语言方面确实有原因喜欢这类的模型（原因后面会有讲到），但是如果你在 arxiv 里面找，人们在语言神经网络研究中所使用的的方法时，你会发现人们并不多使用树形结构模型。LSTMs 的比例几乎是其十倍之多。 &gt;   这里面比较大的原因是树形递归神经网络的使用者必须构建一个树形结构。<strong>在你构建完成后，使用反向传播学习模型会是一个问题</strong>。 &gt;   <a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a> 25分开始。</p>
<h2 id="simple-tree-rnn">simple tree RNN</h2>
<h3 id="树rnn的计算">树RNN的计算</h3>
<p>  那么具体如何使用树形递归神经网络计算呢？比如下图中使用向量 [3 3] 和 [8 5] 计算，输入进神经网络之后，就会输出一个向量 [8 3] 和一个分数 1.3，这个分数代表输出的向量 [8 3] 是否合理（即结构是否合理。如果不太理解什么是结构是否合理，请看下两张图以及博客内容即可理解）。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive%20neural%20network%20for%20training.jpg" alt="recursive neural network for training"></p>
<p>  具体的做法如下图所示。应该很好理解，就不详细说明了，其中对于计算 score 的 U，我猜测可能是一个 trainable 的参数，视频中并没有详细的说明。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive%20neural%20network%20details.jpg" alt="recursive neural network details"></p>
<p>  那么到了真正的实战阶段应该怎么做呢？训练一个贪心的解析器，对于单词两两组合，然后发现最前的两个单词 "The cat" 组成的短语训练之后的分数最高，然后我们将 "The cat" 看作一个成分并且尤其对应的语义 [5 2]。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing%20a%20sentence%20with%20rnn%201.jpg" alt="parsing a sentence with rnn"></p>
<p>  接下来继续重复做，请注意现在的 "The cat" 是一个成分（可看作单词），而不是两个单词。又做一遍解析之后发现 "the mat" 的分数最高。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing%20a%20sentence%20with%20rnn%202.jpg" alt="parsing a sentence with rnn——2"></p>
<p>  再将 "the mat" 看作一个成分，并拥有对应的语义。 <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing%20a%20sentence%20with%20rnn%203.jpg" alt="parsing a sentence with rnn 3"></p>
<p>  以此类推，我们发现 "on the mat" 的分数最高，然后发现 "sat on the mat" 的分数最高，最后就得到 "The cat sat on the mat" 的分数最高。如下图： <img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing%20a%20sentence%20with%20rnn%204.jpg" alt="parsing a sentence with rnn 4"></p>
<p>  这是一棵解析树（parse tree），我们会得到这棵解析树的分数，它的分数由每个节点的分数加和得到。<strong>我们要做的就是找到由这堆节点所能组成的分数最高的解析树</strong>。   还需要一个优化目标，similar to max-margin parsing(Taskar et al. 2004), a supervised max-margin objective: <span class="math inline">\(J = \sum_i s(x_i, y_i) - \max_{y \in A(x_i)} (s(x_i, y) - \Delta(y, y_i))\)</span>   最后我们还需要反向传播算法进行计算，这一工作早在 20 世纪 90 年代就由几个德国人做过了。Goller 和 Kuchler 提出了这个算法，并命名为 <strong>back propagation through structure</strong>.</p>
<h2 id="syntactically-untied-rnn">Syntactically-Untied RNN</h2>
<p>  语义解绑树形递归神经网络，这被证明是构建高质量解析器的一个成功的方法。   <a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a>，50 分开始。</p>
<h2 id="compositionality-through-recursive-matrix-vector-spaces">Compositionality Through Recursive Matrix-Vector Spaces</h2>
<p>  <a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a>，65 分开始。</p>
<h2 id="related-work-for-parsing">related work for parsing</h2>
<figure>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/related%20work%20for%20parsing.jpg" alt><figcaption>related work for parsing</figcaption>
</figure>
<h1 id="共指解析">※ 共指解析</h1>
<p>  <span class="math inline">\(B^3\)</span>(B-CUBED) 算法用于评估。其他的一些算法： - MUC Score (Vilain et al., 1995) - BEAF (Luo 2005); entity based - BLANC (Recasens and Hovy 2011) Cluster RAND-index</p>
<p>  在于语言学中，人们常区分两种关系。其中之一是共指，即两个词指代同一个实体，这和文本结构无关；另一种关系是首语重复，它指的是文本中某一项，一个照应语（或一个指代，anaphor）指代的事物由另一项决定，即先行词。   一些共指消解的做法： - Mention Pair models - Mention Ranking models - Entity-Mention models</p>
<p>  神经共指模型，人们通过深度学习和共指做的内容。 - Wisemean, Rush, Shieber, and Weston (ACL 2015) + Mention-pair model. Only partially neural network system over conventional, categorical coreference features - Wiseman, Rush and Shieber (NAACL 2016) + Use RNNs to learn global representations of entity clusters from mentions - Clark and Manning (ACL 2016) + An entity-mention model based around clustering using distributed representations of mentions and entity clusters - Clark and Manning (EMNLP 2016) + Expolores deep reinforcement learning to improve a metion-pair model</p>
<div class="note info"><p>  此节视频讲了很多共指的理论，我没有记下来。实际内容比这里记的还要多一点。</p>
</div>
<h1 id="用于回答问题的动态神经网络">用于回答问题的动态神经网络</h1>
<p>  略，这节听不懂。</p>
<h1 id="nlp的问题和可能性架构">※ NLP的问题和可能性架构</h1>
<p>  tree-RNN、pointer model、sub-word and character-based model 等。</p>
<h1 id="应对深度-nlp-的局限性">※ 应对深度 NLP 的局限性</h1>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/学习笔记/" rel="tag"># 学习笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/44544db3.html" rel="prev" title="深度学习算法（一）：simple NN（前馈神经网络的正反向推导）">
      <i class="fa fa-chevron-left"></i> 深度学习算法（一）：simple NN（前馈神经网络的正反向推导）
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/b2bd11c2.html" rel="next" title="深度学习算法（二）：simple RNN 推导与理解">
      深度学习算法（二）：simple RNN 推导与理解 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#开场白"><span class="nav-number">1.</span> <span class="nav-text">开场白</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#词向量表示word2vec"><span class="nav-number">2.</span> <span class="nav-text">词向量表示：word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#优化目标"><span class="nav-number">2.1.</span> <span class="nav-text">优化目标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#确定相应的概率分布"><span class="nav-number">2.1.1.</span> <span class="nav-text">确定相应的概率分布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数"><span class="nav-number">2.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算"><span class="nav-number">2.3.</span> <span class="nav-text">计算</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#高级词向量表示"><span class="nav-number">3.</span> <span class="nav-text">高级词向量表示</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#word-window分类与神经网络"><span class="nav-number">4.</span> <span class="nav-text">Word Window分类与神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#反向传播和项目建议"><span class="nav-number">5.</span> <span class="nav-text">反向传播和项目建议</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#依存分析"><span class="nav-number">6.</span> <span class="nav-text">※ 依存分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#语言结构的两种观点"><span class="nav-number">6.1.</span> <span class="nav-text">语言结构的两种观点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dependency-grammar-and-dependency-structure"><span class="nav-number">6.2.</span> <span class="nav-text">Dependency Grammar and Dependency Structure</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dependency-parsing"><span class="nav-number">6.3.</span> <span class="nav-text">Dependency parsing</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensorflow-入门"><span class="nav-number">7.</span> <span class="nav-text">Tensorflow 入门</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rnn和语言模式"><span class="nav-number">8.</span> <span class="nav-text">RNN和语言模式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度消失"><span class="nav-number">8.1.</span> <span class="nav-text">梯度消失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度爆炸"><span class="nav-number">8.2.</span> <span class="nav-text">梯度爆炸</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#序列模型用于其他任务"><span class="nav-number">8.3.</span> <span class="nav-text">序列模型用于其他任务</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#机器翻译和高级循环神经网络"><span class="nav-number">9.</span> <span class="nav-text">机器翻译和高级循环神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经机器翻译和注意力模型"><span class="nav-number">10.</span> <span class="nav-text">神经机器翻译和注意力模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#神经机器翻译"><span class="nav-number">10.1.</span> <span class="nav-text">神经机器翻译</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention"><span class="nav-number">10.2.</span> <span class="nav-text">attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coverage"><span class="nav-number">10.3.</span> <span class="nav-text">coverage</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#search"><span class="nav-number">10.4.</span> <span class="nav-text">※ search</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gru及nmt的其他议题"><span class="nav-number">11.</span> <span class="nav-text">※ GRU及NMT的其他议题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#gruslstms"><span class="nav-number">11.1.</span> <span class="nav-text">GRUs/LSTMs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nmt-evaluation"><span class="nav-number">11.2.</span> <span class="nav-text">NMT evaluation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#语音处理的端对端模型"><span class="nav-number">12.</span> <span class="nav-text">语音处理的端对端模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络"><span class="nav-number">13.</span> <span class="nav-text">卷积神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#树rnn和短语句分析"><span class="nav-number">14.</span> <span class="nav-text">※ 树RNN和短语句分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#simple-tree-rnn"><span class="nav-number">14.1.</span> <span class="nav-text">simple tree RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#树rnn的计算"><span class="nav-number">14.1.1.</span> <span class="nav-text">树RNN的计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#syntactically-untied-rnn"><span class="nav-number">14.2.</span> <span class="nav-text">Syntactically-Untied RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#compositionality-through-recursive-matrix-vector-spaces"><span class="nav-number">14.3.</span> <span class="nav-text">Compositionality Through Recursive Matrix-Vector Spaces</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#related-work-for-parsing"><span class="nav-number">14.4.</span> <span class="nav-text">related work for parsing</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#共指解析"><span class="nav-number">15.</span> <span class="nav-text">※ 共指解析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#用于回答问题的动态神经网络"><span class="nav-number">16.</span> <span class="nav-text">用于回答问题的动态神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#nlp的问题和可能性架构"><span class="nav-number">17.</span> <span class="nav-text">※ NLP的问题和可能性架构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#应对深度-nlp-的局限性"><span class="nav-number">18.</span> <span class="nav-text">※ 应对深度 NLP 的局限性</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="朱冲䶮"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">朱冲䶮</p>
  <div class="site-description" itemprop="description">记录学习问题，积累做的 leetcode 题目</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">163</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">94</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
      <!-- 不蒜子/busuanzi -->
      <div class="site-state-item site-state-posts">
      	<span class="site-state-item-count">203.6k</span>
      	<span class="site-state-item-name">总字数</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/zhu-yu-er-85" title="zhihu → https://www.zhihu.com/people/zhu-yu-er-85" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>zhihu</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:897538633@qq.com" title="E-Mail → mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/yan624" title="GitHub → https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div><script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
<script src="/lib/my-utils.js"></script>
<!--图片缩放插件-->
<script src="/lib/zoomify/zoomify.min.js"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- 背景插件 -->
<script src="https://cdn.bootcss.com/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(11), {
		duration:90000,//1min半一换
		fade: 1500
	});
</script>
<!-- 图片缩放 -->
<script>
$('.content img').zoomify({duration: 500, });
$('.content img').on('zoom-in.zoomify', function () {
	$('.sidebar').css('display', 'none');
});
$('.content img').on('zoom-out-complete.zoomify', function () {
	$('.sidebar').css('display', '');
});
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>

  




  <script src="/js/local-search.js"></script>












  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://yan624.github.io/posts/d9a134a.html',]
      });
      });
  </script>
<!-- calendar widget -->


</body>
</html>
