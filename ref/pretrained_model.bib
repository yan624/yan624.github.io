// ========== LSTM ==========
// ELMo
@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}
// ********** LSTM **********
// ========== Transformer ==========
@article{wu2020tod,
  title={Tod-bert: Pre-trained natural language understanding for task-oriented dialogues},
  author={Wu, Chien-Sheng and Hoi, Steven and Socher, Richard and Xiong, Caiming},
  journal={arXiv preprint arXiv:2004.06871},
  year={2020}
}
// ********** Transformer **********
// ========== 预训练架构（方法） ==========
@article{mehri2019pretraining,
  title={Pretraining methods for dialog context representation learning},
  author={Mehri, Shikib and Razumovskaia, Evgeniia and Zhao, Tiancheng and Eskenazi, Maxine},
  journal={arXiv preprint arXiv:1906.00414},
  year={2019}
}
// ********** 预训练架构（方法） **********
@article{mehri2019multi,
  title={Multi-Granularity Representations of Dialog},
  author={Mehri, Shikib and Eskenazi, Maxine},
  journal={arXiv preprint arXiv:1908.09890},
  year={2019}
}
