<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2">























  

<link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.10.2/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


<!--图片缩放插件样式-->
<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">

  <meta name="description" content="记录学习问题，积累做的 leetcode 题目">
<meta name="keywords" content="博客，java，javaWeb，NLP，python，机器学习，深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/page/4/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="记录学习问题，积累做的 leetcode 题目">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="记录学习问题，积累做的 leetcode 题目">






  <link rel="canonical" href="http://yan624.github.io/page/4/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
	<!--加载flower canvas-->
<script>
var pathname = window.location.pathname;
if(pathname == '/flower.html'){
	var body =  document.getElementsByTagName('body')[0];
	var canvas = document.createElement("canvas")
	canvas.setAttribute('id', 'sakura')
	// '<canvas id="sakura"></canvas>'
	body.appendChild(canvas)
}
</script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">低阶炼金术士</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">19</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">23</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">124</span></a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/27、Sequence-to-Action：End-to-End Semantic Graph Generation for Semantic Parsing.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/27、Sequence-to-Action：End-to-End Semantic Graph Generation for Semantic Parsing.html" class="post-title-link" itemprop="url">论文笔记：Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-08 22:33:46" itemprop="dateCreated datePublished" datetime="2019-07-08T22:33:46+08:00">2019-07-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-31 21:22:57" itemprop="dateModified" datetime="2019-08-31T21:22:57+08:00">2019-08-31</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://mp.weixin.qq.com/s?__biz=MzI2NjkyNDQ3Mw==&amp;mid=2247486979&amp;idx=2&amp;sn=2d95556630820c853f2ca9b2855dd60a&amp;chksm=ea87f6d5ddf07fc3cc8477d3a0cd5142e9191d91ff3161847524c37539b372b306a8f9b700a8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">某篇解析</a>；<a href="http://tongtianta.site/paper/11795" target="_blank" rel="noopener">某篇解析</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1809.00773.pdf" target="_blank" rel="noopener">论文地址</a>。<br>&emsp;&emsp;本文提出一种神经语义分析方法——Sequence-to-Action，将语义分析当做一个端到端的<strong>语义图生成</strong>的过程。我们同时使用了最近语义分析两个有前途的方向，<strong>首先</strong>我们的模型使用了一个语义图来表示一个句子的含义，该语义图与知识库紧密相关（博主注：<strong>即可以将语义图看作是知识库的一个子图</strong>）。<strong>其次</strong>，利用神经网络强大的表示学习和预测能力，提出一种 RNN 模型，能够有效的将句子映射到动作序列，从而生成语义图（博主注：<strong>此动作序列就是指生成语义图的动作，将这些动作看作是一个序列</strong>）。实验表明该方法在 OVERNIGHT 数据集上展现了一流的性能，在 GEO 以及 ATIS 数据集上得到了有一定竞争力的性能。<br>&emsp;&emsp;语义分析旨在<strong>将自然语言句子映射为逻辑形式</strong>（Zelle andMooney, 1996; Zettlemoyer and Collins, 2005;Wong and Mooney, 2007; Lu et al., 2008;Kwiatkowski et al., 2013）。例如“Which states border Texas?”将会被映射为 <em>answer (A, (state (A),nextto (A, stateid ( texas ))))</em>。<br>&emsp;&emsp;语义分析器需要两个函数，一个处理结构预测，另一个处理语义基础。传统的语义解析器通常基于复合语法，如 CCG（Zettlemoyer and Collins, <a href="https://arxiv.org/pdf/1207.1420" target="_blank" rel="noopener">2005</a>, <a href="https://www.aclweb.org/anthology/D07-1071" target="_blank" rel="noopener">2007</a>），DCS（<a href="https://www.aclweb.org/anthology/P11-1060" target="_blank" rel="noopener">Liang et al., 2011</a>）等。不幸的是，设计语法和学习精确的词汇仍是一个挑战，特别是在开放域。而且设计有效的特性往往很困难，它的学习过程也不是端到端的。为了解决上述问题，本文提出了两种有前途的研究方向：<strong>基于语义图</strong>的方法和<strong>基于 seq2seq</strong> 方法。<br>&emsp;&emsp;基于语义图的方法(Reddy et al.,2014, 2016; Bast and Haussmann, 2015; Yih et al.,2015)将句子的含义表示为语义图（即知识库的子图，参考图 1 中的例子）并<strong>将语义分析视为语义图匹配/生成过程</strong>。<strong>与逻辑形式相比，语义图与知识库有着紧密的关系</strong>(Yih et al., 2015), ，与句法结构有许多共性（Reddy et al.,2014）。基于语义图的句法分析的主要挑战是如何有效地构造句子的语义图，目前语义图是通过与模式匹配（Bast and Haussmann, 2015），从依赖树转换（Reddy et al., 2014, 2016），或者通过 staged heuristic search algorithm（Yih et al.,2015）构建的。这些方法都是基于人工设计的构造过程，它们很难处理开放/复杂的情况。<br>&emsp;&emsp;近年来，得益于 RNN 模型有较强的表示能力和预测能力，其在 Seq2Seq 模型上取得了成功，比如机器翻译。许多 Seq2Seq 模型也用于语义分析（Xiaoet al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016），不需要高质量的词典、人工构建的语法和特性。这些模型通过端到端的训练，利用注意力机制（Bahdanauet al., 2014; Luong et al., 2015）学习句子和逻辑形式之间的软对齐。<br>&emsp;&emsp;本文提出了一种新的神经语义分析框架——Sequence-to-Action。它可以同时利用语义图表示的优点和 seq2seq 模型强大的预测能力。具体来说，我们将语义分析建模为一个端到端的语义图生成过程。例如，在图 1 中，我们的模型将通过生成一系列变量[add variable:a，addtype:state，…]来解析“which states border Texas”这句话。为了实现上述目标，我们首先设计了一个动作集，对语义图的生成过程进行编码（包括节点动作：add variable,add entity,add type，边动作：add edg 以及操作动作：argmin,argmax,count,sum 等）然后我们设计了一个 RNN 模型，该模型可以生成一个动作序列来构造句子的语义图。最后，我们在解码过程中合并结构和语义约束来进一步增强解析。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/27、Sequence-to-Action：End-to-End Semantic Graph Generation for Semantic Parsing.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/26、Language to Logical Form with Neural Attention.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/26、Language to Logical Form with Neural Attention.html" class="post-title-link" itemprop="url">论文笔记：Language to Logical Form with Neural Attention</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-08 18:33:15" itemprop="dateCreated datePublished" datetime="2019-07-08T18:33:15+08:00">2019-07-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-16 21:54:16" itemprop="dateModified" datetime="2019-09-16T21:54:16+08:00">2019-09-16</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1601.01280.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2016 年。</p>
          </div>
<p>&emsp;&emsp;语义分析的目的是将自然语言映射到机器可解释的有意义表示。传统的方法依赖于高质量的词汇、人工构建的模板以及特定领域或表示的语言特性，本文提出了一种注意力增强的 encoder-decoder 通用模型。将输入的话表示为向量形式，并通过调节输出序列或者树生成逻辑形式（总结来说，就是<strong>将话语转为逻辑形式</strong>，详情请看图 1）。<br>&emsp;&emsp;下图将一句话转为了逻辑形式，不同于以前的方法，它是通过神经网络生成的，而以前的方法依赖于手写的规则。图片取自 <a href="https://www.aclweb.org/anthology/W00-1317" title="Automated construction ofdatabase interfaces: Intergrating statistical and rela-tional learning for semantic parsing" target="_blank" rel="noopener">Tang and Mooney200</a>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/语句转为逻辑形式.jpg" alt="语句转为逻辑形式"></p>
<p>&emsp;&emsp;基于 RNN 的 encoder-decoder 已成功应用于各种 NLP 任务，图 1 中使用了 LSTM，我们的做法是提出了两个变体模型。<strong>第一个模型</strong>将语义解析视为普通的序列转换任务，<strong>第二个模型</strong>配备了层次树解码器，该解码器明确地捕获逻辑形式的组合结构。我们还引入了<strong>注意力机制</strong>，并提出一个识别步骤来<strong>识别很少提到的实体</strong>和<strong>数字</strong>。<br>&emsp;&emsp;对<strong>四个数据集</strong>的实验结果表明，我们的方法在不使用人工设计特征的情况下具有竞争力，并且易于迁移。<br>&emsp;&emsp;我们的工作综合了两种标准研究，即<strong>语义分析</strong>和 <strong>encoder-decoder 架构的神经网络</strong>。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;学习语义解析器的问题引起了广泛的关注，可以追溯到 Woods（1973年）。。。。</p>
<h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>&emsp;&emsp;我们的目标是学习一个模型，将<u><strong>自然语言输入 <script type="math/tex">q = x_1 \dots x_{|q|}</script></strong></u> 映射为其含义的<u><strong>逻辑形式（logical form）表示 <script type="math/tex">a = y_1 \dots y_{|a|}</script></strong></u>。条件概率被分解为：</p>
<script type="math/tex; mode=display">
\begin{align}
    p(a|q) & = \prod^{|a|}_{t=1} p(y_t|y_{<t},q) \tag 1\\
    y_{<t} & = y_1 \dots y_{t-1}
\end{align}</script><p>&emsp;&emsp;我们的模型包含一个编码器和一个解码器，编码器负责将输入的自然语言 q 编码成向量，解码器负责生成 <script type="math/tex">y_1 \dots y_{|a|}</script>。下面将仔细描述。</p>
<h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>&emsp;&emsp;对于普通的 Seq2Seq 任务，使用 LSTM 来计算，如下图所示。<script type="math/tex">h^l_t</script> 代表第 l 层的第 t 个时间步的隐藏层，公式为：</p>
<script type="math/tex; mode=display">
\begin{align}
    h^l_t = \text{LSTM}(h^l_{t-1},h^{l-1}_t) \tag 2
\end{align}</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/Seq2Seq.jpg" alt="Seq2Seq"><br>&emsp;&emsp;在实验中，遵循 <a href="https://arxiv.org/pdf/1409.2329.pdf" title="RECURRENT NEURAL NETWORK REGULARIZATION" target="_blank" rel="noopener">Zaremba et al. 2015</a> 提出的架构。不过，使用其他类型的门控激活函数也是可以的（例如<a href="https://arxiv.org/pdf/1406.1078.pdf" title="Learning phrase representations using RNN encoder-decoder for statistical machine translation" target="_blank" rel="noopener">Cho et al. 2014</a>）。<strong>对于 encoder</strong>，<script type="math/tex">h^0_t = W_qe(x_t)</script>（注：此公式是第 0 层的运算步骤，即输入层）是 RNN 中输入的词向量，<script type="math/tex">W_q \in \mathbb{R}^{n \times |V_q|}</script> 代表输入层的权重值矩阵，e(·) 代表对应 token 的索引。<strong>对于 decoder</strong>，<script type="math/tex">h^0_t = W_ae(y_{t-1})</script> 代表前一个预测词的词向量，其中 <script type="math/tex">W_a \in \mathbb{R}^{n \times |V_a|}</script>。接下来，最后的 LSTM <script type="math/tex">h^L_t</script> 被用于预测 <script type="math/tex">t</script>-th 输出 token，计算公式为：</p>
<script type="math/tex; mode=display">
\begin{align}
    p(y_t|y_t,q) = softmax(W_oh^L_t)^T e(y_t) \tag 3
\end{align}</script><p>&emsp;&emsp;<strong>该公式用于预测每一个 token</strong>。另外补充一点，增加了 “start-of-sequence” <code>&lt;s&gt;</code> 和 “end-of-sequence” <code>&lt;/s&gt;</code>。<br>&emsp;&emsp;该模型总的来说，就是 LSTM 的计算方法，也没什么好说的。</p>
<h2 id="Seq2Tree"><a href="#Seq2Tree" class="headerlink" title="Seq2Tree"></a>Seq2Tree</h2><p>&emsp;&emsp;Seq2Seq 模型有一个<strong>缺点</strong>就是它<strong>忽略了逻辑形式的层次结构</strong>。所以，要改良的话，它需要记住各种辅助信息（比如括号对），以此生成格式良好的输出。如下图 3 所示，是一个层次树 decoder：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/Seq2Tree模型.jpg" alt="Seq2Tree模型"></p>
<p>&emsp;&emsp;Seq2Tree 与 Seq2Seq 的编码器类似，不同的是解码器。Seq2Tree 以自上而下的方式生成逻辑，为了定义树结构，我们定义了一个表示子树的 “nonterminal” <code>&lt;n&gt;</code> 标记。如图 3 所示，将<strong><em>逻辑形式 “lambda $0 e (and (&gt;(departure_time $0) 1600:ti) (from $0 dallas:ci))”</em></strong>预处理为树，方法是<strong>用 nonterminal 替换括号对之间的标记</strong>（token）。<strong>特殊记号 <code>&lt;s&gt;</code> 和 <code>&lt;(&gt;</code> 分别表示序列和 nonterminal 序列的开头</strong>（由于缺少空间，图3中省略了），<strong>记号 <code>&lt;/s&gt;</code> 代表序列结束</strong>。具体步骤是：</p>
<ol>
<li>编码输入值 q；</li>
<li>层次树解码器使用 RNN 在逻辑形式 a（在<strong>任务定义</strong>中已经说明了 q 和 a 的含义）的对应部分的子树中生成 tokens（注意这里的 token 带了 s）；</li>
<li>如果预测的 token 为 <code>&lt;n&gt;</code>，则通过调节 nonterminal 的隐藏向量来解码序列。（博主注：举个例子理解一下：看图 3 的第一层，先是使用 encoder 进行编码，接着开始对逻辑形式进行解码，逻辑形式就是上面的斜体部分。接下来预测到了 token<code>&lt;n&gt;</code> 于是调用 nonterminal 的隐藏向量来进行解码，即生成一棵子树。以此类推，碰到 toekn <code>&lt;n&gt;</code> 就开始解码）</li>
<li>与 Seq2Seq 解码器不同，当前的隐藏状态不仅仅取决于上一个时间步，为了更好地利用 parent nonterminal 的信息，我们引入了一个 parent-feeding 的连接，其中 parent nonterminal 的隐藏向量与输入连接（concatenated）并喂入 LSTM。</li>
</ol>
<p>&emsp;&emsp;再举个例子帮助理解一下，如图 4 所示。逻辑形式为 <strong><em>A B (C)</em></strong>，其中 <script type="math/tex">y_1 \dots y_6</script> 代表不同的时间步，<strong><em>(C)</em></strong> 对应子树。解码一共有<strong>两个步骤</strong>：一旦输入值 q 被编码，首先在深度为 1 处生成 <script type="math/tex">y_1 \dots y_4</script>，直到 token <code>&lt;/s&gt;</code> 被预测到；接下来通过调节 nonterminal <script type="math/tex">t_3</script> 的隐藏向量来生成 <script type="math/tex">y_5, y_6</script>，<script type="math/tex">p(a|q)</script> 的概率是这<strong>两个序列解码步骤</strong>的乘积：</p>
<script type="math/tex; mode=display">
p(a|q) = p(y_1 y_2 y_3 y_4 | q) p(y_5 y_6 | y_{\leq 3},q)</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/一个简单的Seq2Tree的例子.jpg" alt="一个简单的Seq2Tree的例子"></p>
<h2 id="Attention-机制"><a href="#Attention-机制" class="headerlink" title="Attention 机制"></a>Attention 机制</h2><p>&emsp;&emsp;<strong><em>Attention 的原理</em></strong>。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>&emsp;&emsp;我们的目标是<strong>最大化</strong>由自然语言语句作为输入时产生的逻辑形式的可能性，所以目标函数为：</p>
<script type="math/tex; mode=display">
\text{minimize} - \sum_{(q,a) \in D} logp(a|q)</script><p>&emsp;&emsp;其中 <script type="math/tex">D</script> 是所有自然语言逻辑形式训练对的集合，<script type="math/tex">p(a|q)</script> 按式（1）计算。采用 <strong>RMSProp</strong> 算法解决了这一非凸优化问题。此外，使用 <strong>Dropout</strong> 进行正则化。</p>
<h2 id="推论"><a href="#推论" class="headerlink" title="推论"></a>推论</h2><p>&emsp;&emsp;暂时略。</p>
<h2 id="参数识别"><a href="#参数识别" class="headerlink" title="参数识别"></a>参数识别</h2><p>&emsp;&emsp;大多数的语义分析数据集都是为问答开发的。在经典的系统中，问题被映射乘逻辑形式，并在知识库中获取答案。由于问答任务的性质，许多自然语言的语句都包含实体或数字，它们通常被解析为逻辑形式的参数。其中不可避免地会有一些罕见或者根本不会出现在数据集中的实体或数字（对于小规模数据集尤其如此）。传统的序列编码器只是简单地用一个特殊的位置单词符号替换稀有单词（<a href="https://arxiv.org/pdf/1410.8206.pdf" title="Addressing the Rare Word Problem in Neural Machine Translation" target="_blank" rel="noopener">Luong et al. 2015a</a>; <a href="https://arxiv.org/pdf/1412.2007.pdf" title="On Using Very Large Target Vocabulary for Neural Machine Translation" target="_blank" rel="noopener">Jean et al. 2015</a>），这对语义分析是有害的。<br>&emsp;&emsp;为此开发了一个简单的参数识别程序。具体来说就是在输入的问题中标识实体和数字，并用它们的<strong>类型</strong>和<strong>唯一 id</strong> 替换它们。例如，将训练样本“<em>jobs with a salary of 40000</em>”及其逻辑形式“job(ANS), salary_greater_than(ANS,40000, year)”预处理为“jobs with a salary of <em><script type="math/tex">num_0</script></em>”和“job(ANS), salary_greater_than(<em>ANS</em>,<em><script type="math/tex">num_0</script></em>,<em>year</em>)”。一旦解码完毕，后处理步骤就会将所有标记 <script type="math/tex">type_i</script> 恢复到它们以前的实体或数字。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;我们将我们的方法在四个数据集上分别与以前的多个系统进行比较，下面将描述这些数据集。代码可在此处获得<a href="https://github.com/donglixp/lang2logic" target="_blank" rel="noopener">https://github.com/donglixp/lang2logic</a>（lua 版，官方），<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch" target="_blank" rel="noopener">https://github.com/Alex-Fabbri/lang2logic-PyTorch</a>（python 版，非官方）。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>&emsp;&emsp;<strong>JOBS</strong>    工作<br>&emsp;&emsp;<strong>GEO</strong>        Gene Expression Omnibus（基因表达综合）<br>&emsp;&emsp;<strong>ATIS</strong>    Airline Travel Information System（航空旅行信息系统）<br>&emsp;&emsp;<strong>IFTTT</strong>    if this then that（<a href="https://ifttt.com/" target="_blank" rel="noopener">地址</a> <a href="https://baike.baidu.com/item/ifttt/8378533" target="_blank" rel="noopener">百度百科介绍</a>），<a href="https://www.aclweb.org/anthology/P15-1085" target="_blank" rel="noopener">Quirk et al.2015</a> 从 IFTTT 网站提取大量的 if-this-then-that 来创建此数据库<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/数据集介绍.jpg" alt="数据集介绍"></p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>&emsp;&emsp;自然语言语句是小写的，并且使用基于维基百科的常见拼写错误列表来纠正拼写错误。使用 NLTK 来限制词汇（[Bird et al.2009] Natural Language Processing with Python. O’Reilly Media.），对于 IFTTT 过滤了在训练集中出现少于五次的 token，channels 和 functions。对于其他数据集，过滤了在训练集中至少两次没有出现的输入词，但保留了逻辑形式中的所有 token。并且使用了<strong>参数识别</strong>，当然也可以使用更复杂的办法。<br>&emsp;&emsp;超参数在 JOBS 和 GEO 上使用了交叉验证，使用了 ATIS 和 IFTTT 作为标准开发集（就是验证集，不同的叫法而已 development/validation）。</p>
<ul>
<li><strong>RMSProp</strong>：batch size = 20；parameter = 0.95；</li>
<li><strong>梯度修剪</strong>为 5 以缓解梯度爆炸（<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">Pascanu et al.2013</a>）；</li>
<li><strong>参数</strong>从均匀分布 <script type="math/tex">U(-0.08, 0.08)</script> 中随机初始化；</li>
<li>两层 <strong>LSTM</strong> 用于 IFTTT，单层 LSTM 用于其他数据集；</li>
<li><strong>dropout</strong> <script type="math/tex">\in</script> {0.2,0.3,0.4,0.5}；</li>
<li>隐藏向量和词嵌入<strong>维度</strong>从 {150, 200, 250} 选择；</li>
<li><strong>early stopping</strong>；</li>
<li>输入句子在进入编码器之前被反转（<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever et al.2014</a>）；</li>
<li><strong>贪婪搜索</strong>生成逻辑形式；</li>
<li><strong>softmax</strong> 用于分类。</li>
</ul>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>&emsp;&emsp;Attention 机制可以提高性能，对于小数据集<strong>参数识别</strong>至关重要。</p>
<h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><h1 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h1><p>&emsp;&emsp;<a href="http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf" title="Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars" target="_blank" rel="noopener">论文</a>使用 Logical Form 对基准数据集做实验，数据集包括 Geo880 和 Jobs640，论文中使用的是 Logical Form 的其中一种表示——<strong>PCCG</strong>，它是 CCG 的改进版。他们将数据集分割为训练集和测试集，Language to Logical Form with Neural Attention 沿用了此分割方式（比如说将 GEO 分割为 680 个训练样本，200 个测试样本），并且采用此论文的思想，即：将自然语言映射为 Logical Form。<br>&emsp;&emsp;虽然 PCCG 的 Logical Form 效果不错，但是作者没有使用他，而是使用了 <a href="https://www.aclweb.org/anthology/D11-1140" target="_blank" rel="noopener">lambda-caculus</a>。<strong>作者将 Geo880 等数据集改写为了 lambda-calculus 的形式</strong>。<em>在<a href="https://github.com/yuxuan1995liu/Semantic-Parsing-Data-Pre-Processing" target="_blank" rel="noopener">此处</a>可找到全部数据，但是这里面的格式不是 lambda-calculus。我有点搞不懂他提供的数据到底是什么意思</em>。<strong>19.09.16 补充</strong>：经过多方查找，终于找到了 geo880 最初的<a href="https://link_springer.gg363.site/content/pdf/10.1007/3-540-44795-4_40.pdf" title="Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing" target="_blank" rel="noopener">论文</a>，在<a href="https://www.cs.utexas.edu/~ml/publications/year/2001" target="_blank" rel="noopener">此处</a>找到的。GEO880 最初版本并不是 lambda calculus。<br>&emsp;&emsp;作者将数据改写为 lambda-calcullus 形式是我估计的。因为全文找不到数据的来源，格式转换的说明也找不到。只是在 Section 4.1 Datasets 中说到：</p>
<blockquote>
<p>&emsp;&emsp;<strong>GEO 有 880 个示例，将其分割为 680 个训练样本以及 200 个测试样本（Zettlemoyer and Collins, 2005）， 我们使用了基于 lambda-calculus 的具有相同含义的表示</strong>。</p>
</blockquote>
<p>&emsp;&emsp;所以我推测作者应该是将原本的 PCCG 表示的 GEO 改成了 lambda-calculus 表示。<br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1805.04793" title="Coarse-to-Fine Decoding for Neural Semantic Parsing" target="_blank" rel="noopener">论文</a>是作者对 Language to Logical Form with Neural Attention 的改进版。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/26、Language to Logical Form with Neural Attention.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/5、Large scale Simple Question Answering with Memory Network.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/5、Large scale Simple Question Answering with Memory Network.html" class="post-title-link" itemprop="url">论文笔记：Large-scale Simple Question Answering with Memory Network</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-07 18:49:45" itemprop="dateCreated datePublished" datetime="2019-07-07T18:49:45+08:00">2019-07-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-23 17:19:03" itemprop="dateModified" datetime="2019-10-23T17:19:03+08:00">2019-10-23</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1506.02075v1.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2015 年。<br>&emsp;&emsp;开放域问答系统的目的是在不受域限制的情况下，为用自然语言表达的问题提供准确的答案。问答系统有很长的历史，它们搜索文本文档或在网络上提取答案（see e.g.(Voorhees and Tice, 2000; Dumais et al., 2002)）。最近在公开的大型知识库（KBS）方面也取得了进展，如 Freebase 知识库。然而，尽管最近大家都在关注设计一个具有推理能力的系统，它可以检索并使用 KB 中的<strong>多重事实</strong>进行问答。但是其实只涉及 <strong>KB 中单个事实的简单问答</strong>都还没被解决，本论文中将其称为 Simple Question Answering。<br>&emsp;&emsp;KBQA 现存的方法：1）将 question 转为结构化的 KB 查询语句（Berant et al. 2013）；或者 2）学习将 question 以及 facts 嵌入到低维向量空间中，然后在这些向量中通过计算相似度检索答案（<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a</a>）。<br>&emsp;&emsp;本文贡献有二：</p>
<ul>
<li>其一，为了<strong>研究现有系统</strong>以及<strong>通过多任务学习在不同数据源上同时训练</strong>成为可能，我们收集了第一个基于知识库的大规模的问答数据集，称为 SimpleQuestions。包含了人类编写和 Freebase facts 相关的超过 10 万个问题，另外现有的基准数据集 WebQuestions 包含的问题少于 6 千个，这些问题是使用 google suggest api 自动创建的。</li>
<li>其二，提出了一种基于词嵌入的问答系统，在 Memory Networks (MemNNs)（<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Weston et al., 2015</a>;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">Sukhbaatar et al., 2015</a>） 框架下开发而成。</li>
</ul>
<p>&emsp;&emsp;虽然我们的模型与之前的 QA 嵌入模型（<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a</a>;<a href="https://arxiv.org/pdf/1404.4326.pdf" target="_blank" rel="noopener">Bordes et al., 2014b</a>）相似，但使用 MemNNs 的框架为未来工作中更复杂的推理方案提供了思路，因为 MemNNs 在复杂的推理问答任务上表现了很好的性能（<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Weston et al., 2015</a>）。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><ol>
<li>Sections 3, 4：介绍了基于词嵌入的问答系统；</li>
<li>Section 5：相关工作；</li>
<li>Section 6：实验结果。</li>
</ol>
<h1 id="Memory-Network-for-Simple-QA"><a href="#Memory-Network-for-Simple-QA" class="headerlink" title="Memory Network for Simple QA"></a>Memory Network for Simple QA</h1><p>&emsp;&emsp;Memory network 由一个 memory（一个索引对象数组）和一个神经网络组成。神经网络由 Input map(I), Generalization(G), Output map(O) and Response(R) 构成。其工作流如下所示：</p>
<ol>
<li>Storing Freebase：第一阶段。解析 Freebase（可以是 FB2M 或 FB5M，取决于配置）并且将它存进 memory。它使用 Input module 去预处理数据；</li>
<li>Training：第二阶段。训练 MemNN 去回答问题。此处使用 Input, Output and Response modules，训练主要关注核心 Output module 嵌入模型的参数；</li>
<li>Connecting Reverb：第三阶段。将来自 Reverb 的 new facts 添加到 memory 中。这是在训练完毕后进行的，为了测试 MemNNs 在不需要重新训练的情况下处理 new facts 的能力。它使用 Input module 去预处理 Reverb facts 并且使用 Generalization module 将它们和已经被存储的 facts 连接。</li>
</ol>
<h2 id="Input-module"><a href="#Input-module" class="headerlink" title="Input module"></a>Input module</h2><p>&emsp;&emsp;此组件预处理 3 种类型的数据，它们会被输入进神经网络：</p>
<ol>
<li>Freebase facts：用于填充 memory；</li>
<li>questions：系统需要回答的问题；</li>
<li>Reverb facts：在 workflow 第二阶段中，我们用它扩展 memory。</li>
</ol>
<h3 id="preprocessing-Freebase"><a href="#preprocessing-Freebase" class="headerlink" title="preprocessing Freebase"></a>preprocessing Freebase</h3><p>&emsp;&emsp;Freebase 数据最初存储原子 facts，包括将单个实体作为主语或者宾语，再在它们之间加上一个联系（即谓语）。<strong>但是这样的存储需要从两个方面与 QA 任务适应</strong>。</p>
<ol>
<li>为了回答不止有一个答案的问题，我们将 fact 重新定义为一个三元组，其包含 subject，relationship 以及通过 relationship 连接至 subject 的一组 objetcs 。这个分组过程将 atomic facts 转为 grouped facts，以下将其简单的称为 facts。Table 2 显示了这样分组可以减少 facts 的数量。</li>
<li></li>
</ol>
<h3 id="Preprocessing-Freebase-facts"><a href="#Preprocessing-Freebase-facts" class="headerlink" title="Preprocessing Freebase facts"></a>Preprocessing Freebase facts</h3><p>&emsp;&emsp;</p>
<h3 id="Preprocessing-questions"><a href="#Preprocessing-questions" class="headerlink" title="Preprocessing questions"></a>Preprocessing questions</h3><h3 id="Preprocessing-Reverb-facts"><a href="#Preprocessing-Reverb-facts" class="headerlink" title="Preprocessing Reverb facts"></a>Preprocessing Reverb facts</h3><h2 id="Generalization-module"><a href="#Generalization-module" class="headerlink" title="Generalization module"></a>Generalization module</h2><p>&emsp;&emsp;此模块负责将新的元素增加到 memory 中。在我们的例子中，memory 具有一个 multigraph 结构，其中每个节点都是 Freebase 的一个实体，multigraph 中被标记的 arcs 是 Freebase 中的 relationships：预处理之后，所有 Freebase 的 facts 都使用此结构存储。<br>&emsp;&emsp;<br>&emsp;&emsp;为了将 Reverb 的 subject 和 object 链接到 Freebase 实体，我们使用 precomputed entity links (<a href="https://www.aclweb.org/anthology/W12-3016.pdf" target="_blank" rel="noopener">Lin et al., 2012</a>)。。。。</p>
<h2 id="Output-module"><a href="#Output-module" class="headerlink" title="Output module"></a>Output module</h2><p>&emsp;&emsp;Output 模块通过给定 input ，在 memory 中执行查表（lookup）操作，返回该问题的 supporting facts。在我们的 simple QA 例子中，此模块只返回一个 supporting fact。为了避免为所有存储的 facts 评分（即为了避免时间代价太大），我们先执行一步<em>近似实体链接</em>（proximate entity linking），以生成一个小的候选 facts 集合。最后， supporting fact 指的是与嵌入模型中的问题最相似的候选 fact。</p>
<h3 id="Candidate-generation"><a href="#Candidate-generation" class="headerlink" title="Candidate generation"></a>Candidate generation</h3><p>&emsp;&emsp;略。</p>
<h3 id="Scoring"><a href="#Scoring" class="headerlink" title="Scoring"></a>Scoring</h3><p>&emsp;&emsp;略。</p>
<h2 id="Response-module"><a href="#Response-module" class="headerlink" title="Response module"></a>Response module</h2><p>&emsp;&emsp;在 memory network 中，Response 模块对 Output 模块的结果进行后处理操作，以计算预期的答案。在我们的例子中，它返回被挑选出来的 supporting fact 的对象集（博主注：这个对象集我猜测是 KG 中的三元组）。<br>&emsp;&emsp;注：不必纠结 Response 模块的具体功能，可以自己定制，必然在<a href="https://zhuanlan.zhihu.com/p/29590286" target="_blank" rel="noopener">记忆网络之Memory Networks</a>中写到最初的 memory network 的 response 模块只是简单地将向量转成单词。</p>
<h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/liuchonge/article/details/78128238" target="_blank" rel="noopener">记忆网络之open-domain QA 应用</a>，csdn 的一篇博客，也对此论文的训练方法做了总结。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/5、Large scale Simple Question Answering with Memory Network.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/24、An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/24、An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge.html" class="post-title-link" itemprop="url">论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-06 16:52:07" itemprop="dateCreated datePublished" datetime="2019-07-06T16:52:07+08:00">2019-07-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-30 22:53:27" itemprop="dateModified" datetime="2019-08-30T22:53:27+08:00">2019-08-30</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P17-1021" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;随着知识库数量的增加，人们越来越希望寻找到一些有效的方法来获取这些资源。现在有几种专门为<strong>查询 KBs</strong> 设计的<strong>语言</strong>：SPARQL（<a href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/" target="_blank" rel="noopener">rudhommeaux and Seaborne, 2008</a>）。但要使用这些语言，用户不仅需要熟悉它们，还要了解 KBs 的体系结构。相比之下，<strong>以自然语言为查询语言</strong>的 KB-QA 是一种更友好的方案，近年来已成为研究热点。这项任务<strong>以前</strong>有两个主流的研究方向：</p>
<ol>
<li>基于语义解析（semantic parsing-base, SP-based）</li>
<li>基于信息检索（information  retrieval-based, IR-based）</li>
</ol>
<p>&emsp;&emsp;<strong>现在</strong>随着神经网络方法的发展，基于神经网络的 KB-QA 已经取得了令人瞩目的成果。其中至关重要的步骤就是计算<strong>问题和候选答案</strong>之间的相似性分数，这一步骤的<strong>关键一点</strong>就是学习它们的表示。然而以往的研究更注重<strong>答案的学习表示</strong>。例如，<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al. 2014a</a> 考虑候选答案子图的重要性，<a href="https://www.aclweb.org/anthology/P15-1026" target="_blank" rel="noopener">Dong et al. 2015</a>利用上下文和答案的类型。无论如何，<strong>问题的表示</strong>终究还是表达不全。现有的方法 <a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a,</a> <a href="https://arxiv.org/pdf/1404.4326.pdf" target="_blank" rel="noopener">b</a> 使用 bag-of-word 模型将问题表示为一个向量，但是这样<strong>问题与答案的关联性</strong>还是被忽视了。我们认为一个问题应该根据回答时不同的侧重面来表示（注：<em>其实就是想用注意力机制</em>，回答的侧重面可以是答案实体本身、答案类型、答案上下文等）。<br>&emsp;&emsp;因此本文提出了一个端到端的神经网络模型，通过 <strong>cross-attention</strong> 机制，根据不同的候选答案动态地表示问题及对应的分数。此外还利用了 KB 中的全部知识，旨在将 KB 中丰富的知识集成到答案中，以此缓解 out-of-vocabulary(<strong>OOV</strong>) 的问题，从而帮助 cross-attention 更精确地表示问题。最后实验结果表明了该方法确实有效。<br>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P15-1026" title="Question Answering over Freebase wit hMulti-Column Convolutional Neural Networks" target="_blank" rel="noopener">论文</a>（<a href="https://yan624.github.io/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html">论文笔记地址</a>）中的方法很有启发性，但是由于简单地选择三个独立的 CNN ，因此过于机械化。所以我们使用了基于 cross-attention 的神经网络模型。<br>&emsp;&emsp;模型架构如下，步骤与之前的论文的步骤类似。<strong>1)</strong>先找到问题的主题（main entity/topic entity）；<strong>2)</strong>然后在知识库中找到主题相连的节点作为候选答案，<strong>3)</strong>最后送入 score layer 进行评分，排序分数选出分数最高的候选答案作为正确答案。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge/MCCNN总览.jpg" alt="MCCNN总览"></p>
<p>&emsp;&emsp;为了方便描述，我们将任何一种基本元素称为资源（resource），无论是实体还是关系。比如 (/m/0f8l9c,location.country.capital,/m/05qtj) 的描述是法国的首都是巴黎，其中的 <em>/m/0f8l9c</em> 和 <em>/m/05qtj</em> 分别代表法国和巴黎，<em>location.country.capital</em> 是一种关系。</p>
<h1 id="我们的方法"><a href="#我们的方法" class="headerlink" title="我们的方法"></a>我们的方法</h1><h2 id="候选者生成"><a href="#候选者生成" class="headerlink" title="候选者生成"></a>候选者生成</h2><p>&emsp;&emsp;略，我已经写过无数遍了。使用 Freebase API 构建的。</p>
<h2 id="The-Neural-Cross-Attention-Model"><a href="#The-Neural-Cross-Attention-Model" class="headerlink" title="The Neural Cross-Attention Model"></a>The Neural Cross-Attention Model</h2><p>&emsp;&emsp;下图是模型的架构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge/MCCNN架构.jpg" alt="MCCNN架构"></p>
<ul>
<li>问题表示（图 2 中左侧部分显示了处理步骤）<ol>
<li>使用向量表示问题中的每个单词，这跟其他 NLP 任务差不多，不过它是随机初始化的词嵌入矩阵 <script type="math/tex">E_w \in \mathbb{R}^{d \text{x} v_w}</script>，然后取出对应单词的词向量。d 代表词向量的维度，<script type="math/tex">v_w</script> 代表词表的大小。</li>
<li>将词向量送入 LSTM，值得注意的是我们没有使用单向 LSTM，因为这样一个单词表示只会捕获到之前的单词的信息而不会包含之后的单词。为此我们使用了双向 LSTM 外加 Bahdanau（<a href="https://arxiv.org/pdf/1409.0473.pdf" title="Neural machine translation by jointly learning to align and translate" target="_blank" rel="noopener">Bahdanau, 2014</a>） attention 的处理；</li>
<li>这样就会获得两个表示 <script type="math/tex">(\overrightarrow{h_1}, \overrightarrow{h_2}, \dots, \overrightarrow{h_n})</script> 以及 <script type="math/tex">(\overleftarrow{h_1}, \overleftarrow{h_2}, \dots, \overleftarrow{h_n})</script>，然后将两个表示拼接起来组成 [<script type="math/tex">\overrightarrow{h_i};\overleftarrow{h_i}</script>]，正反向 LSTM 单元的大小都是 <script type="math/tex">\frac{d}{2}</script>。</li>
</ol>
</li>
<li>回答的不同侧面表示（图 2 中右侧下方部分）<ol>
<li>直接使用 KB 的嵌入矩阵 <script type="math/tex">E_k \in \mathbb{R}^{d \text{x} v_k}</script>，其中 <script type="math/tex">v_k</script> 代表知识库中资源的大小，该嵌入矩阵随机初始化并在训练时学习表示，使用全局信息对表示的进一步提高将在 3.3 节 Combining Global Knowledge（原论文）描述。具体来说我们使用回答的四个方面：问答实体 <script type="math/tex">a_e</script>，回答关系 <script type="math/tex">a_r</script>，回答类型 <script type="math/tex">a_t</script>，回答上下文 <script type="math/tex">a_c</script>。它们的嵌入被分别表示为 <script type="math/tex">e_e</script>, <script type="math/tex">e_r</script>, <script type="math/tex">e_t</script>, <script type="math/tex">e_c</script>；</li>
<li>值得注意的是问答上下文由多个 KB 资源组成，我们将它们定义为 (<script type="math/tex">c_1, c_2, \dots, c_m</script>)，首先获得它们的嵌入 (<script type="math/tex">e_{c_1}, e_{c_2}, \dots, e_{c_m}</script>)，然后计算它们的平均值 <script type="math/tex">e_c = \frac{1}{m} \sum^m_{i=1} e_{c_i}</script></li>
</ol>
</li>
<li>Cross-Attention model（图 2 中右侧上方部分以及最上方部分），详见 3.2.3 Cross-Attention model</li>
</ul>
<h2 id="Combining-Global-Knowledge"><a href="#Combining-Global-Knowledge" class="headerlink" title="Combining Global Knowledge"></a>Combining Global Knowledge</h2><p>&emsp;&emsp;Combining Global Knowledg，利用TransE得到knowledge embedding。</p>
<h1 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h1><ol>
<li>使用了 Bahdanau Attention 处理；</li>
<li>使用了双向 LSTM，会得到两个向量，最后将这两个向量拼接在一起，就是 BiLSTM 这层的最终向量。另外正反的 LSTM 的长度都是 <script type="math/tex">\frac{d}{2}</script>；</li>
<li>回答通过问答实体 <script type="math/tex">a_e</script>，回答关系 <script type="math/tex">a_r</script>，回答类型 <script type="math/tex">a_t</script>，回答上下文 <script type="math/tex">a_c</script> 四个方面来表示，其中 ac 是所有词向量的平均值。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li><a href="https://arxiv.org/pdf/1404.4326" title="Open question answering with weakly supervised embedding models" target="_blank" rel="noopener">Antoine Bordes 等 2014b</a>；</li>
<li><a href="https://arxiv.org/pdf/1406.3676" title="Question Answering with Subgraph Embeddings" target="_blank" rel="noopener">Antoine Bordes 等 2014a</a>；</li>
<li><a href="https://www.aclweb.org/anthology/P14-2105" title="Semantic Parsing for Single-Relation Question Answering" target="_blank" rel="noopener">Yih W 等 2014</a>，实际上是基于语义解析的，但是用了词向量；</li>
<li><a href="https://www.aclweb.org/anthology/D14-1071" title="Joint relational embeddings for knowledge-based question answering" target="_blank" rel="noopener">Min-Chul Yang 等 2014</a>，实际上是基于语义解析的但是用了词向量；</li>
<li><a href="https://www.aclweb.org/anthology/P15-1026" title="Question Answering over Freebase with Multi-Column Convolutional Neural Networks" target="_blank" rel="noopener">Dong 等 2015</a>，这篇是跟我们的文章最相近的（使用了 CNN 而非 RNN + Attention）；</li>
<li><a href="https://www.aclweb.org/anthology/C16-1226" title="Hybrid Question Answering over Knowledge Base and Free Text" target="_blank" rel="noopener">Kun Xu 等 2016b</a>；<a href="https://arxiv.org/pdf/1603.00957.pdf" title="Question Answering on Freebase via Relation Extraction and Textual Evidence" target="_blank" rel="noopener">Xu K 等 2016a</a>。</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/24、An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html" class="post-title-link" itemprop="url">论文笔记：Question Answering over Freebase with Multi-Column Convolutional Neural Networks</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-06 14:48:24" itemprop="dateCreated datePublished" datetime="2019-07-06T14:48:24+08:00">2019-07-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-06 10:24:20" itemprop="dateModified" datetime="2019-08-06T10:24:20+08:00">2019-08-06</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P15-1026" target="_blank" rel="noopener">论文地址</a>，发表于 2015 年。<br>&emsp;&emsp;大多数现有的系统通常依靠人工制作的特性和规则来进行<em>问题理解</em>以及<em>答案排序</em>。此外，一些方法（<a href="https://arxiv.org/pdf/1406.3676.pdf" title="Question Answering with Subgraph Embeddings" target="_blank" rel="noopener">Bordes et al., 2014a</a>; <a href="https://arxiv.org/pdf/1404.4326.pdf" title="Open Question Answering with Weakly Supervised Embedding Models" target="_blank" rel="noopener">Bordeset al., 2014b</a>）使用问题的词嵌入的总和来表示问题，但是这忽略了<strong>词序信息</strong>，无法处理复杂问题，例如 who killed A 和 who A killed 两个问题的表示是一样的。本文介绍了 multi-column convolutional neural networks (MCCNNs)，从三个方面（<strong>回答路径（Answer Type），回答上下文（Answer Context），回答类型（Answer Path）</strong>）理解问题。使用 Freebase 作为知识库，在 WebQuestions 数据集上进行了广泛的实验。最终表明，此方法拥有更好的性能。<br>&emsp;&emsp;神经网络训练步骤：</p>
<ol>
<li>MCCNNs 从输入的问题中使用不同 column networks 去提取<strong>回答路径，回答上下文，回答类型</strong>。跟 Bordes 的论文一样，该论文知识库（本文就是 FreeBase）中的实体和关系也由向量表示。</li>
<li>然后评分层（score layer）根据问题和候选答案的表示进行排序（点积）。</li>
</ol>
<h1 id="处理步骤"><a href="#处理步骤" class="headerlink" title="处理步骤"></a>处理步骤</h1><p>&emsp;&emsp;给定一个自然语言问题 <script type="math/tex">q = w_1 \dots w_n</script>，从 FreeBase 中检索相应的实体和属性，然后将它们作为候选答案 <script type="math/tex">C_q</script>。比如，问题 <em>when did Avatar release in UK</em> （阿凡达在英国的发行时间）的答案是 <em>2009-12-17</em>。需要注意的是对于该问题也许有一系列的正确答案。以下数据将被使用到：<strong>WebQuestions</strong>，<strong>FreeBase</strong>，<strong>WikiAnswers</strong>。<br>&emsp;&emsp;MCCNN 概览如图 1 所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Question Answering over Freebase with Multi-Column Convolutional Neural Networks/MCCNN概览.jpg" alt="MCCNN概览"></p>
<p>&emsp;&emsp;比如说，对于问题 whendid Avatar release in UK，从 FreeBase 中查询 <strong>Avatar</strong>（可以称为 <strong>main entity</strong> 或者 <strong>topic entity</strong>） 的<strong>相连节点</strong>（related nodes），这些相连节点被认为是候选答案（<script type="math/tex">C_q</script>）。然后对于每个候选答案 a，模型将会预测一个分数 S(q,a) 以判断 a 是否为正确答案。<br>&emsp;&emsp;对于问题的三个侧面的向量表示分别以 <script type="math/tex">f_1(q)</script> <script type="math/tex">f_2(q)</script> <script type="math/tex">f_3(q)</script> 表示，同理答案的三个侧面分别以 <script type="math/tex">g_1(a)</script> <script type="math/tex">g_2(a)</script> <script type="math/tex">g_3(a)</script> 表示。<script type="math/tex">f_i(q)</script> 和 <script type="math/tex">g_i(a)</script>拥有相同的维度。使用这些问答的表示，我们可以计算问答对 (q,a) 的分数。具体来说，评分函数 S(q,a) 定义为（如图 1 所示，评分层计算分数并将其加起来）：</p>
<script type="math/tex; mode=display">
S(q,a) = \underbrace{f_1(q)^Tg_1(a)}_{\text{answer path}} + \underbrace{f_2(q)^Tg_2(a)}_{\text{answer context}} + \underbrace{f_3(q)^Tg_3(a)}_{\text{answer type}}</script><h2 id="候选者生成"><a href="#候选者生成" class="headerlink" title="候选者生成"></a>候选者生成</h2><p>&emsp;&emsp;训练神经网络的<strong>第一步是</strong>从 FreeBase 中为问题检索候选答案。用户提出的问题应该包含一个<strong>可识别</strong>的实体，该实体与知识库相连。我们使用 <strong>Freebase Search API</strong>（<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf" title="Freebase: a collaboratively created graph database for structuringhuman knowledge" target="_blank" rel="noopener">Bollacker et al., 2008)</a>） 查询问题中的命名体。如果没有任何命名体，则查询名词短语，我们使用调用 API 返回的列表中的第一个实体。这个实体解决办法也被 <a href="https://www.aclweb.org/anthology/P14-1090" title="Information Extraction over Structured Data: Question Answering with Freebase" target="_blank" rel="noopener">Yao and Van Durme, 2014)</a> 使用，还可以研发更好的办法，但不是本论文的关注点。<strong>最后关联实体的所有 2-hops（应该是周围的意思，我没有查到是什么意思，但是在<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html#论文总结">博客笔记</a>中有所总结） 节点被认为是候选答案</strong>。并把问题 q 的候选答案集合称为 <script type="math/tex">C_q</script>。</p>
<h2 id="MCCNNs-for-Question-Understanding"><a href="#MCCNNs-for-Question-Understanding" class="headerlink" title="MCCNNs for Question Understanding"></a>MCCNNs for Question Understanding</h2><p>&emsp;&emsp;MCCNNs 使用多列（<strong>列</strong>指的是图 1 中左侧那三片）卷积网络从字嵌入中学习不同方面。使用 <a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf" title="Natural Language Processing (Almost) from Scratch" target="_blank" rel="noopener">Collobert R 等 2011</a> 的方法解决语言长度不一的问题。具体的做法可参考原论文 <strong>4.2 MCCNNs for Question Understanding</strong>。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&emsp;&emsp;未来的探索方向：</p>
<ol>
<li>整合更多的外部知识源，如clueweb；</li>
<li>以多任务学习方式训练MCCNN；</li>
<li>由于我们的模型能够检测到问题中最重要的单词，因此使用结果挖掘有效的问题模式将是非常有趣的。</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/22、Joint Relational Embeddings for Knowledge based Question Answering.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/22、Joint Relational Embeddings for Knowledge based Question Answering.html" class="post-title-link" itemprop="url">论文笔记：Joint Relational Embeddings for Knowledge-based Question Answering</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-06 13:37:44" itemprop="dateCreated datePublished" datetime="2019-07-06T13:37:44+08:00">2019-07-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-17 14:13:19" itemprop="dateModified" datetime="2019-07-17T14:13:19+08:00">2019-07-17</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/D14-1071" target="_blank" rel="noopener">论文地址</a>，发表于 2014 年。<br>&emsp;&emsp;将自然语言（natural language，NL）问题转换为对应的逻辑形式（logical form，LF）是基于知识库问答（KB-QA）任务的核心任务，转换问题也被称作语义分析。<del>在 KB-QA 任务领域，与以往（Mooney, 2007; Liang et al., 2011;Cai and Yates, 2013; Fader et al., 2013; Berant etal., 2013; Bao et al., 2014）<u>基于词汇化短语（lexicalized phrases）和逻辑谓语（logical predicates）之间的映射作为词汇触发器（lexical trigger）来执行语义分析中的转换任务</u>不同（其中 Fader 2013 提出的论文在<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html">论文笔记1</a>和<a href="https://yan624.github.io/论文/21、Question Answering with Subgraph Embeddings.html">论文笔记2</a>中具有提及，ctrl f 之后搜索 <em>Paraphrase-Driven Learning for Open Question Answering</em> 或者 <em>Fader</em> 即可找到对应位置）</del>，本论文进一步提出了一种<strong>将 NL 问题映射到 LFs 中</strong>的新的<strong>嵌入式</strong>方法，其利用<strong>词汇表达</strong>与 <strong>KB 中的属性</strong>在隐含空间中的语义关联来实现。实验表明，在两个公开的 QA 数据集上，该方法优于其他三种 KB-QA 的基线方法。<br>&emsp;&emsp;先前工作必须处理以下两种限制：</p>
<ol>
<li>由于逻辑谓语的含义通常具有不同的自然语言表达（natural language expression，NLE）形式，因此从谓语提取的词汇触发器可能有时会受到大小限制；</li>
<li>由于命名体识别（named entity recognition，NER）组件检测到的实体将用于与逻辑谓语一起组成逻辑形式，因此它们的类型也应该与谓语一致。然而，现有的 KB-QA 系统使用的 NER 组件大都独立于 NLE 到谓语的映射步骤。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;一如既往地（我为什么要说一如既往？因为前两篇论文笔记都记录了）说明<strong>语义分析</strong>有多糟糕，需要使用大量的人力，继而只能被限制在特定的领域（以后关于这些劣势都不写了）。<br>&emsp;&emsp;一如既往地描述了 FreeBase。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/22、Joint Relational Embeddings for Knowledge based Question Answering.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/21、Question Answering with Subgraph Embeddings.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/21、Question Answering with Subgraph Embeddings.html" class="post-title-link" itemprop="url">论文笔记：Question Answering with Subgraph Embeddings</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-05 15:58:26" itemprop="dateCreated datePublished" datetime="2019-07-05T15:58:26+08:00">2019-07-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-06 10:41:31" itemprop="dateModified" datetime="2019-08-06T10:41:31+08:00">2019-08-06</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1406.3676" target="_blank" rel="noopener">论文地址</a>，发表于 2014 年。<br>&emsp;&emsp;本文的作者在同年发表了另一篇论文，将上一篇论文称为 A，此论文称为 B，对于 A 论文我也做了<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html">论文笔记</a>，本论文是上一篇论文的改进版。A 只是对简单问题进行研究，B 研究如何改进模型并回答更复杂的问题。<br>&emsp;&emsp;开放域问答中的一流技术大致可以分为两大类：1)基于信息检索；2)基于语义解析。<strong>信息检索</strong>系统首先通过 KBs 的搜索 API（转换方式估计是手写模版，论文中未细说） <strong>将问题转换为有效的查询语句</strong>（比如 neo4j 数据库的 CQL）以此检索到大量的候选答案，然后再仔细地识别准确的答案（<a href="https://www.sciencedirect.com/science/article/pii/S0020025511003860" title="A survey on question answering technology from an information retrieval perspective" target="_blank" rel="noopener">Kolomiyets O 等 2011</a>，<a href="https://www2012.universite-lyon.fr/proceedings/proceedings/p639.pdf" title="Template-based Question Answering over RDF Data" target="_blank" rel="noopener">Unger C 等 2012</a>，<a href="https://www.aclweb.org/anthology/P14-1090" title="Information Extraction over Structured Data: Question Answering with Freebase" target="_blank" rel="noopener">Yao X 等 2014</a>）。<strong>语义解析</strong>旨在通过语义分析系统正确<strong>解释</strong>问题的含义，<strong>解释步骤</strong>的做法是把问题转换为数据库查询语句（这里的查询语句应该是逻辑形式，比如<strong>组合范畴法</strong>），以此查询到正确的答案。尽管这两种方法有能力去处理大规模知识库，但是需要专家手动的创建词汇、语法以及 KB 协议才能有所成效。且<strong>没有通用性</strong>，<strong>无法方便地扩展到</strong>具有其他模式、更广泛词汇或英语以外语言的<strong>新数据库</strong>。<br>&emsp;&emsp;相反，<a href="https://www.aclweb.org/anthology/P13-1158" target="_blank" rel="noopener">Paraphrase-Driven Learning for Open Question Answering</a> 提出了一个几乎不需要人工注释的开放域 QA 框架，虽然这是一种有趣的方法，但是它被其他方法超越了。即第二段提到的论文 A。<br>&emsp;&emsp;相比于论文 A，作者作出了以下几点<strong>改进</strong>：1）对于候选答案，考虑更多更长的路径（之前只考虑了 main entity 周围的节点）；2）对候选答案进行更有意义的表示：答案的表示包含问答路径以及周围的子图。</p>
<h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>&emsp;&emsp;假设所有潜在的答案都是 KB 中的实体，当 KB 中不存在该实体时，可以使用一些方法解决（论文中具体没说，只是说了一种极简单的方式：<em>When this entity is not given, plain string matching is used to perform entity resolution</em>）。<br>&emsp;&emsp;此外 N 代表词典的大小，其中 <script type="math/tex">N = N_W + N_S</script>，<script type="math/tex">N_W</script> 代表词嵌入的大小，<script type="math/tex">N_S</script> 代表实体和关系的数量。</p>
<h1 id="改进：考虑多维度的信息"><a href="#改进：考虑多维度的信息" class="headerlink" title="改进：考虑多维度的信息"></a>改进：考虑多维度的信息</h1><p>&emsp;&emsp;以下描述一个候选答案的特征表示，论文将以三个角度进行表示：</p>
<ol>
<li>Single Entity：此表示方式与上一篇论文一样，没什么讲究。就是 Freebase 中的一个实体，<script type="math/tex">\psi(a)</script> 代表答案的 1-of-<script type="math/tex">N_S</script>（one hot）表示；</li>
<li>Path Representation：答案被认为是一条 path，该 path 从<strong>问题中被提及的实体</strong>到<strong>答案实体</strong>。此实验中，考虑 1-hop 或者 2-hops 级别的 path。比如，(barack obama, people.person.place of birth, honolulu) 是 1-hop path，(barack obama, people.person.place of birth, location. location.containedby, hawaii) 是 2-hop path。这导致了 <script type="math/tex">\psi(a)</script> 代表 3-of-<script type="math/tex">N_S</script> 或者 4-of-<script type="math/tex">N_S</script> 的向量，至于为什么是 *-of-<script type="math/tex">N_S</script>，显而易见。</li>
<li>Subgraph Representation：我们将 2 中的 <strong>Path</strong> 和连接候选答案的整个<strong>子图</strong>进行编码。<em>具体看论文，写的有点看不懂</em>。</li>
</ol>
<p>&emsp;&emsp;我们的假想是将所有的信息都编码进表示以提高结果，但是这不大可能。所以还是采用将子图编码进表示的方法。下图即为实验的模型，右下角显示了编码方式。<br><img src="https://img-blog.csdn.net/20171101002818501?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTEFXXzEzMDYyNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="架构图" title="架构图"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;与论文 A 差不多，多了一个多任务训练，其他的细枝末节没仔细看。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/21、Question Answering with Subgraph Embeddings.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/20、Open Question Answering with Weakly supervised Embedding Models.html" class="post-title-link" itemprop="url">论文笔记：Open Question Answering with Weakly supervised Embedding Models</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-05 10:54:15" itemprop="dateCreated datePublished" datetime="2019-07-05T10:54:15+08:00">2019-07-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-29 23:16:09" itemprop="dateModified" datetime="2019-08-29T23:16:09+08:00">2019-08-29</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1404.4326" target="_blank" rel="noopener">论文地址</a>，论文发表于 2014 年。<br>&emsp;&emsp;建立一个能够回答任何问题的计算机是人工智能的一个长期目标。这一领域一个重要的发展时大规模知识库的建立，如 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Freebase</a> 和 <a href="https://content.iospress.com/download/semantic-web/sw134?id=semantic-web%2Fsw134" target="_blank" rel="noopener">DBPedia</a>，它们存储了大量的通用信息。它们由三元组的形式构成一个数据库，通过各种关系和格式连接成实体对。那么回答问题被定义为<strong>给定一个用自然语言表达的查询语句</strong>（一个查询语句的例子：中国的首都在哪？）<strong>从知识库中检索正确的实体或实体集的任务</strong>。<br>&emsp;&emsp;最近，通过将问题映射为<strong>逻辑形式</strong>或者类似<strong>数据库查询</strong>的方法取得了富有希望的进展。虽然这种方法可能有效，但是缺点是要采用大量的人为标记的数据或者需要工作人员定义词汇表和语法。<br>&emsp;&emsp;本文采用一种激进的学习方式，将问题映射为向量（无法人为解释）的特征表示。并且将重点放在回答一些基于比较宽泛的主题的简单事实性问题。这项任务的难点来自词汇的多样性，而不是句法的复杂性。<br>&emsp;&emsp;该方法采用随机梯度下降，然后使用 fine-tuning 进行训练。经验表明该模型能够捕获一些有意义的信号，且这是唯一一种能够在弱标记数据上训练的方法。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><ol>
<li>Section 2：讨论了之前的工作；</li>
<li>Section 3：介绍了开放域问答的问题；</li>
<li>Section 4：给出了模型；</li>
<li>Section 5：实验结果。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li>大规模的问答历史悠久，主要由 TREC tracks（<a href="https://arxiv.org/pdf/cs/0110053.pdf" target="_blank" rel="noopener">Voorhees 2000</a>） 发起，这是第一个成功地<strong>将问题转换为查询</strong>的问答系统。将问题转换为查询之后，又<strong>将查询提供给 web 搜索引擎</strong>，然后<strong>从返回的页面或片段中取出答案</strong>（<a href="http://aiweb.cs.washington.edu/research/projects/ai3/mulder/mulder-www10.pdf" target="_blank" rel="noopener">Kwok 2001</a>, <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-06/SS02-06-002.pdf" target="_blank" rel="noopener">Banko 2002</a>）。这种方法需要大量的人工操作来处理查询，然后解析和搜索结果。</li>
<li>大型 KBs 的出现，如 FreeBase 和 DBPedia（论文地址已在第一章给出），改变了上述状况，但是也带来巨大的挑战。语言的多样性以及 KBs 规模的庞大，使得需要通过监督学习来处理大量的<strong>带标签</strong>的数据。最早的方法是基于手写模板的 KBs 开放问答，然而对于日新月异 KBs（增加/删除三元组和实体） 还不够成熟。之后开始尝试使用较少的监督情况下<strong>学习 KBs 和自然语言之间的联系</strong>，但是这项工作实际上在解决<strong>信息提取</strong>的问题（<a href="https://www.aclweb.org/anthology/P09-1113" title="Distant supervision for relation extraction without labeled data" target="_blank" rel="noopener">Mintz M 等 2009</a>，<a href="https://www.aclweb.org/anthology/P11-1055" title="Knowledge-Based Weak Supervision for Information Extractionof Overlapping Relation" target="_blank" rel="noopener">Hoffmann R 等 2011</a>，<a href="https://www.aclweb.org/anthology/D12-1093" title="Reading The Web with Learned Syntactic-Semantic Inference Rules" target="_blank" rel="noopener">Lao N 等 2012</a>，<a href="https://www.aclweb.org/anthology/N13-1008" title="Relation Extraction with Matrix Factorization and Universal Schemas" target="_blank" rel="noopener">Riedel S 等 2013</a>）。以上以及本文未提及到的这些通过直接或者间接的监督机器学习来获得更多表现力的解决办法实际上是为了避开标签数据过多的问题。</li>
<li>近年来，有一种基于语义解析器（<a href="https://www.aclweb.org/anthology/P13-1042" title="Large-scale Semantic Parsing via Schema Matching and Lexicon Extension" target="_blank" rel="noopener">Cai Q 等 2013</a>，<a href="https://www.aclweb.org/anthology/D13-1160" title="Semantic Parsing on Freebase from Question-Answer Pairs" target="_blank" rel="noopener">Berant J 等 2013</a>，<a href="https://www.aclweb.org/anthology/D13-1161" title="Scaling Semantic Parsers with On-the-fly Ontology Matching" target="_blank" rel="noopener">Kwiatkowski T 等 2013</a>）的新的问答系统被提出，它只具有少量标记数据。但仍需要耗费大量精力去仔细设计词汇，语法和知识库。</li>
<li>所以本文（2014 年）提出了基于嵌入式的问答模型。据我们所知，这是以前从未尝试过的。</li>
</ol>
<h1 id="开放域问答"><a href="#开放域问答" class="headerlink" title="开放域问答"></a>开放域问答</h1><p>&emsp;&emsp;本文使用 <a href="https://www.aclweb.org/anthology/P13-1158" target="_blank" rel="noopener">Fader 2013</a> 的问答框架，并使用了相同的数据。</p>
<h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>&emsp;&emsp;我们将回答问题的任务看作为：给定一个问题 q，对应的答案由 KB 中的三元组 t 给出。这意味着我们的问题由<strong>一组三元组 t</strong> 提供对问题及其答案的解释，例如：</p>
<blockquote>
<p>q: What environment does a dodo live in?（渡渡鸟生活在什么样的环境中？）<br>t: (dodo.e, live-in.r, makassar.e)<br>q: What are the symbols for Hannukah?（光明节的象征是什么？）<br>t: (menorah.e, be-for.r, hannukah.e)<br>q: What is a laser used for?（极光可以用来做什么？）<br>t: (hologram.e,be-produce-with.r,laser.e)</p>
</blockquote>
<p>&emsp;&emsp;这里每个问题我们只给出一个 t，但是实际上它可以有很多，所以上文说是一组三元组。本文其余部分，<strong>使用 <script type="math/tex">\kappa</script>（读作 kappa） 代表 KB ，使用 <script type="math/tex">\epsilon</script> 代表 KB 中的实体或者关系。问题的词表用 V 表示，<script type="math/tex">n_V</script> <script type="math/tex">n_{\epsilon}</script>分别表示 V 和 <script type="math/tex">\epsilon</script> 的大小</strong>。<br>&emsp;&emsp;我们的模型在于<strong>函数 S(·)</strong>，它可以为 question-answer triple pairs (q,t) 打分。因此，找到问题 q 的 top-ranked 的答案 <script type="math/tex">\hat{t}</script>(q) 直接由以下公式得出：</p>
<script type="math/tex; mode=display">
\hat{t}(q) = arg \max_{t' \in \kappa}S(q, t')</script><p>&emsp;&emsp;为了处理多个答案，我们将结果呈现为排完序的列表并对其评分，而不是直接采用最前面的预测结果。<br>&emsp;&emsp;使用评分函数可以直接查询 KB，而不需要在<strong>语义分析系统</strong>中一样为问题定义一个中间的结构化逻辑表示。我们的目标是学习 S(·)，余下将讲述用于训练的数据的创建步骤。</p>
<h2 id="用于训练的数据"><a href="#用于训练的数据" class="headerlink" title="用于训练的数据"></a>用于训练的数据</h2><div class="note info">
            <p>待续</p>
          </div>
<h1 id="Embedding-based-model"><a href="#Embedding-based-model" class="headerlink" title="Embedding-based model"></a>Embedding-based model</h1><p>&emsp;&emsp;模型使用了词嵌入（2019 年了，应该谁都知道了，不做解释）。</p>
<h2 id="Question-KB-Triple-Scoring"><a href="#Question-KB-Triple-Scoring" class="headerlink" title="Question-KB Triple Scoring"></a>Question-KB Triple Scoring</h2><p>&emsp;&emsp;我们的框架关注的是函数 S(q,t) 的学习，该函数的目的是对一个<strong>问题 q</strong> 和 一个<strong>来自 <script type="math/tex">\kappa</script> 的三元组 t</strong> 进行打分。该评分方法受到了先前工作 labeling images withwords 的启发（<a href="https://link.springer.com/content/pdf/10.1007/s10994-010-5198-3.pdf" target="_blank" rel="noopener">Weston 2013</a>），我们采用该方法将图片和标签替换成了问题和三元组。直观来讲就是：<br>有点难翻译，故给出原文：</p>
<blockquote>
<p>&emsp;&emsp;Intuitively, it consists of projecting questions, treated as a bag of words(and possibly n-grams as well), on the one hand, and triples on the other hand,into a shared embedding space and then computing a similarity measure (the dot  product  in  this  paper)  between  both  projections.<br>&emsp;&emsp;大致意思，将问题和三元组使用词袋模型（也可以是 n-gram 模型）投射到共享的嵌入空间，然后计算二者的相似度（本文使用点积的方式）。</p>
</blockquote>
<p>&emsp;&emsp;那么评分函数为:</p>
<script type="math/tex; mode=display">
S(q,t) = f(q)^Tg(t)</script><p>&emsp;&emsp;<strong>其中 f(·) 将问题中的单词映射到 <script type="math/tex">\mathbb{R}^{\kappa}</script>，<script type="math/tex">f(q) = V^T \Theta(q)</script>。V 是关于 <script type="math/tex">\mathbb{R}^{n_v \times \kappa}</script> 包含所有词嵌入 v 的矩阵。<script type="math/tex">\Theta(q)</script>是 q（<script type="math/tex">\in \{0,1\}^{n_v}</script>） 的二进制（稀疏）表示。同样，g(·) 将 KB 三元组中的实体和关系映射到 <script type="math/tex">\mathbb{R}^{\kappa}</script>，<script type="math/tex">g(t) = W^T\Psi(t)</script>，W 是关于 <script type="math/tex">\mathbb{R}^{n_e \times \kappa}</script> 包含所有实体和关系的嵌入 w 的矩阵，<script type="math/tex">\Psi(t)</script> 是 t（<script type="math/tex">\in \{0,1\}^{n_e}</script>） 的二进制（稀疏）表示。</strong><br><div class="note info">
            <p>&emsp;&emsp;注：上一段太长了，解释一下。f(q) 就是词向量，g(t) 就是实体和关系的向量（下一段原文写到 g(t) 是将三元组中的嵌入全部相加）。</p>
          </div></p>
<p>&emsp;&emsp;将单词表示为词袋模型似乎有一点局限性，但是由于我们特定的设置，语法都很简单，因此含有的信息十分有限，所以词袋模型应该也能带来不错的性能。当然也有反例，比如 <em>What are cats afraid of ?vs.What are afraid of cats ?</em> 这将会有不同的答案。不过这种情况十分罕见。未来考虑将 parse tree features 或者 semantic role labels 作为输入放入嵌入模型中。<br>&emsp;&emsp;与以前的工作（<a href="https://arxiv.org/pdf/1307.7973" target="_blank" rel="noopener">Weston 2013</a>）不同的是，在我们的模型中，实体出现三元组的不同侧面（左右侧）时，实体并非拥有相同的嵌入。KB 中的关系并不是对称的，所以会出现三元组中左侧和右侧的实体是不同的情况。<strong>由于 g(·) 是将三元组中的所有成分相加，所以每一个实体我们都需要两个嵌入</strong>。<br>&emsp;&emsp;这样就可以很容易地对任何三元组进行评分：</p>
<script type="math/tex; mode=display">
\hat{t}(q) = arg \max_{t' \in \kappa}S(q, t') = arg \max_{t' \in \kappa}(f(q)^Tg(t'))</script><p>&emsp;&emsp;接下来花了好几段讲怎么训练。</p>
<h2 id="Fine-tuning-the-Similarity-between-Embeddings"><a href="#Fine-tuning-the-Similarity-between-Embeddings" class="headerlink" title="Fine-tuning the Similarity between Embeddings"></a>Fine-tuning the Similarity between Embeddings</h2><p>&emsp;&emsp;由于受到数据大小的限制，需要使用微调来改进性能。</p>
<h1 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h1><p>&emsp;&emsp;通读论文之后还是有点搞不清论文是怎么训练的，后来看了一下 CCF ADL100 刘康老师的 PPT ，感觉有点理解了，以下是训练步骤：</p>
<ol>
<li>输入自然语言表达的问题，比如：姚明的老婆的是哪里人？</li>
<li>使用 entity linking（论文中貌似没有这步，我在看 PPT 时也是一知半解，好在前几天我刚好在一篇论文中看到了这个 entity linking！<a href="https://yan624.github.io/论文/19、Semantic Parsing via Staged Query Graph Generation：Question Answering with Knowledge Base.html#链接主题实体">博客地址</a>，entity linking 源于<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）找到 main entity，main entity 周围的 entity 均是候选 entity。如下图，姚明是 main entity，姚明周围的实体都算作候选 entity，比如叶莉、火箭队、上海等。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Open Question Answering with Weakly supervised Embedding Models/姚明的老婆是谁的知识图谱的子图.jpg" alt="姚明的老婆是谁的知识图谱的子图"></li>
<li>计算问题和候选 entity 的相似度，其中问题由词向量表示，候选 entity 是一个三元组的形式，难以直接用词向量表示，方法是将三元组中的三个对象分别用词向量表示，然后将三个词向量相加。这样就得到了问题的词向量和 entity 的词向量，点乘获得相似度。</li>
<li>由于候选 entity 不一定只有一个，所以可以获得多个相似度。进行排序即可获得最相似的候选 entity。</li>
</ol>
<div class="note danger">
            <p>&emsp;&emsp;以上的训练步骤并不是论文中的训练步骤，只是我为了给自己加深映像写的，具体的训练步骤在原论文第 4 节，具体在 4.1。</p>
          </div>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/20、Open Question Answering with Weakly supervised Embedding Models.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/19、Semantic Parsing via Staged Query Graph Generation：Question Answering with Knowledge Base.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/19、Semantic Parsing via Staged Query Graph Generation：Question Answering with Knowledge Base.html" class="post-title-link" itemprop="url">论文笔记：Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-03 10:11:46" itemprop="dateCreated datePublished" datetime="2019-07-03T10:11:46+08:00">2019-07-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-25 09:34:48" itemprop="dateModified" datetime="2019-07-25T09:34:48+08:00">2019-07-25</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>&emsp;&emsp;此论文为 2015 年的论文。<br>&emsp;&emsp;本文会出现一个名为<strong>谓语序列（predicate sequence）</strong>的名词，论文中没有详细说明。但是估计就是：一个实体至另一个实体的有向路径上的所有谓语的连接形式。如下文第一张图 Family Guy-&gt;cvt1-&gt;Mila Kunis 的谓语序列就是 cast-actor。</p>
<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;节选自摘要部分：</p>
<blockquote>
<p>&emsp;&emsp;论文提出了一个基于知识库问答的新的语义解析（semantic parsing）框架。首先定义一个类似于知识库的<strong>子图（subgraph）</strong>的查询图（query graph），可以直接映射到一个语义的逻辑形式（如<script type="math/tex">\lambda</script>-calculus）。所以<strong>语义分析简化为查询图的生成</strong>，并将其表示为一个阶段性搜索问题。然后通过使用先进的实体链接系统（<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）以及深度卷积网络来实现问题与谓语序列之间的匹配。在 WEBQUESTIONS 的数据集上，F1 指标达到了 52.5% 的水平，高于以前的方法。</p>
</blockquote>
<p>&emsp;&emsp;以下大型知识库已经成为支持开放领域问答的重要资源：</p>
<ul>
<li>DBPedia</li>
<li>Freebase</li>
</ul>
<p>&emsp;&emsp;最先进的 KB-QA 方法都是基于<strong>语义解析</strong>的，在语义解析中一个问题或者一种表达被映射到它具有一定意义的表示上（如逻辑形式，具体来说可以是 <script type="math/tex">\lambda</script>-calculus），即将自然语言映射为表达式，然后被翻译为一个 <strong>KB 查询</strong>。最后，只需要执行查询就可以检索问题的答案。<strong>但是大多数<u>传统的</u>语义解析方法在很大程度上都<u>脱离</u>知识库</strong>。由于没有前人的贡献累积，因此 QA 问题面临着一系列的挑战。例如：</p>
<ul>
<li>当在逻辑形式中使用与知识库中的谓语不同的谓语时，可能需要用到本体匹配（ontology matching）的问题（Kwiatkowski et al., 2013）。</li>
<li>即使表示语言与知识库的模式接近，从知识库中的大量词汇表中寻找正确的谓语与语句的描述相关联仍然是一个难题（Berant and Liang, 2014）。</li>
</ul>
<p>&emsp;&emsp;由（Yao and Van Durme, 2014; Bao etal., 2014）的启发，该论文提出了一个语义解析框架，定义一个查询图可以直接地映射到由 <script type="math/tex">\lambda</script>-calculus 表达的逻辑形式。从语义上来讲，与 <script type="math/tex">\lambda</script>-DCS（Liang, 2013）十分接近。将解析行为分为 3 步：</p>
<ol>
<li>定位问题中的主题实体；</li>
<li>找到回答与主题实体之间的主要关联；</li>
<li>（通过额外的约束扩大查询图，约束即回答需要附加的额外属性，如最早时间等）或者（答案与其他实体之间的关联）。</li>
</ol>
<p>&emsp;&emsp;至此将一个语义解析问题划分成了一系列的子问题。例如 entity linking 和 relation matching。</p>
<h2 id="文章内容介绍"><a href="#文章内容介绍" class="headerlink" title="文章内容介绍"></a>文章内容介绍</h2><ol>
<li>Sec. 2: 介绍了图知识库（估计就是知识图谱）的概念和查询图的设计；</li>
<li>Sec. 3: 介绍了基于搜索方法的查询图生成；</li>
<li>Sec. 4: 实验结果；</li>
<li>Sec. 5: 论文中的方法和其他相关工作的比较；</li>
<li>Sec. 6: 总结。</li>
</ol>
<h1 id="Knowledge-Base"><a href="#Knowledge-Base" class="headerlink" title="Knowledge Base"></a>Knowledge Base</h1><p>&emsp;&emsp;论文中的知识库 K 是一个包含主语、谓语、宾语的三元组（e1, p, e2）的集合，其中 e1 和 e2<script type="math/tex">\in</script>E，是一个实体。p<script type="math/tex">\in</script>P，是一个谓语。这种形式的知识库通常称为知识图谱。每一个实体是一个节点，两个相关联的实体由谓语标记的有向边连接，边的方向是从主语实体到宾语实体。如下图就是一个 Freebase 的子图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Freebase subgraph of Family Guy.jpg" alt="Freebase subgraph of Family Guy"></p>
<div class="note info">
            <p>&emsp;&emsp;Freebase 中有一个叫 <a href="https://developers.google.com/freebase/guide/basic_concepts#cvts" target="_blank" rel="noopener">CVT</a>（此链接需要翻墙访问） 的特殊实体类型，它不是一个真正的实体，而是用于收集事件或特殊的关联的多个字段。</p>
          </div>
<h1 id="Query-graph"><a href="#Query-graph" class="headerlink" title="Query graph"></a>Query graph</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>&emsp;&emsp;给定一个知识图谱。执行逻辑形式的查询等价于寻找一个子图，该子图的表现形式可以映射到查询动作。之后解析绑定的变量。<br><div class="note warning">
            <p>&emsp;&emsp;接下来，以实体这个属性来表示真实世界的实体和 CVT 实体以及日期或高度等属性，这些实体之间的区别对于论文中的方法来说并不重要。</p>
          </div><br>&emsp;&emsp;就像知识图谱一样，查询图中的相关节点也是通过有向边连接，并用 K 中的谓语标记。查询图由四中类型的节点组成：</p>
<ol>
<li>grounded entity：圆角矩形表示。grounded entity 是在知识库 K 中已存的实体。</li>
<li>existential variable：圆形表示。existential variable 是 un-grounded entity。</li>
<li>lambda variable：阴影圆形表示。lambda variable 是 un-grounded entity。尤其，该论文表示希望<strong>检索</strong>能够映射到 lambda variable 的所有实体<strong>作为</strong>最终答案。其也被称为<strong>answer 节点</strong>。</li>
<li>aggregation function：菱形表示。aggregation function 被用于操作特定的实体，该实体通常具有一些数值属性。</li>
</ol>
<p>&emsp;&emsp;下图展示了一个查询图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Query graph that represents the question “Who first voiced Meg on Family Guy？”.jpg" alt="Query graph that represents the question “Who first voiced Meg on Family Guy？”"></p>
<p>&emsp;&emsp;上图是“谁第一次为 Family Guy 中的 Meg 配音？”的问题。MegGriffin 和 FamilyGuy 由圆角矩形表示，圆圈节点 y 表示应该存在一个实体来描述扮演关系，比如角色、演员和开始饰演此角色的时间。阴影圆圈节点也被称为 <strong>answer 节点</strong>。菱形节点 argmin 限制答案必须是扮演此角色的最早的演员。同样不含聚合函数的<script type="math/tex">\lambda-calculus</script>逻辑形式查询为<script type="math/tex">\lambda x.\exists y.cast(FamilyGuy,y) \Lambda actor(y,x) \Lambda character(y,MegGriffin)</script>。在使用聚合函数之前，对 K 运行此查询图会匹配 LaceyChabert 以及 MilaKunis，请看第一张图。但是只有 LaceyChabert 是正确答案，因为是她最早开始扮演这个角色。<br><div class="note info">
            <p>查询图的设计灵感来源于（Reddyet al., 2014），但是他的查询图是从问题的 CCG 解析中映射出来的，在映射到子图前还需要进一步的转换。从语义上来说，该论文的查询图更像简单的 <script type="math/tex">\lambda-DCS</script>。</p>
          </div></p>
<h2 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h2><p>&emsp;&emsp;<strong>首先</strong>树图（tree graph）的根由一个实体节点组成，称为主题实体（topic entity）。<strong>其次</strong>，只有一个 lambda 变量 x 作为答案节点，从根到 x 有一个定向路径，其中含有 0 个或多个 existential variables。论文中将此路径称为图的核心推理链，因为它描述了答案和主题实体之间的主要关系。这个链除了根节点外只有变量节点。<strong>最后</strong>，可以将 0 个或多个实体或者聚合函数节点附加到每个变量节点，包括 answer 节点。例如，上图 Family Guy 是根，而 Family Guy-&gt;y-&gt;x 是核心推理链，分支 y-&gt;MegGriffin 阐述了角色，而 y-&gt;argmin 限制答案必须是该角色最早的参与者。<br>&emsp;&emsp;定义状态（state）集合<script type="math/tex">S = \{\phi, S_e, S_p, S_c\}</script>，其中每个状态可以是一个空的图（<script type="math/tex">\phi</script>），一个主题实体的单节点图（<script type="math/tex">S_e</script>），一个核心推理链（<script type="math/tex">S_p</script>）或者带有额外约束的更复杂的查询图（<script type="math/tex">S_c</script>）。<br>&emsp;&emsp;定义动作（action）集合<script type="math/tex">A = \{A_e, A_p, A_c, A_a\}</script>，其中<script type="math/tex">A_e</script>选取实体节点，<script type="math/tex">A_p</script>确定核心推理链，<script type="math/tex">A_c</script>和<script type="math/tex">A_a</script>分别约束和聚合节点。<br>&emsp;&emsp;给出一个示例<script type="math/tex">q_{ex}</script> = “Who first voiced Meg of Family Guy?”。</p>
<h3 id="链接主题实体"><a href="#链接主题实体" class="headerlink" title="链接主题实体"></a>链接主题实体</h3><p>&emsp;&emsp;从初始状态<script type="math/tex">S_0</script>开始，正确的操作是创建一个与给定问题中的主题实体相对应单节点图。例如，<script type="math/tex">q_{ex}</script>中可能的主题实体是 Family Guy 和 MegGriffin，如下图所示。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Two possible topic entity linking actionsapplied to an empty graph, for question “Who firstvoiced[Meg]on[Family Guy]？”.jpg" alt="Two possible topic entity linking actionsapplied to an empty graph, for question “Who firstvoiced[Meg]on[Family Guy]？”"></p>
<p>&emsp;&emsp;使用的<strong>实体链接系统</strong>是专为短且有噪声的文本设计的，源于（<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）。具体不做赘述，详情可参考相关论文。</p>
<h3 id="确定核心推理链"><a href="#确定核心推理链" class="headerlink" title="确定核心推理链"></a>确定核心推理链</h3><p>&emsp;&emsp;给定与主题实体 e 对应的单节点图的状态 s，扩展该图的正确操作是确定核心推理链，即主题实体和答案之间的关系。下图展示了扩展<script type="math/tex">s_1</script>中的单节点图的三个可能的链。具体做法是，当中间的 existential variable 链接 CVT 时，探索长度为 2 的所有路径，如果没有链接，则探索长度为 1 的路径。<br><div class="note primary">
            <p>&emsp;&emsp;本节主要描述了如何确定核心推理链，不过上文一段先描述了如何确定候选的核心推理链。具体做法上一段也已经给出，但是由于原论文讲的也有点不清楚，此处加以说明，以下只是推测。</p><ol><li>扩展主题节点 Family Guy 的三个可能的核心推理链，应该是从知识库 K 中入手。请看第一张图，它是知识库 K 中的一张子图。从 Family Guy 中开始可以看到有三条边，两条边上是 cast，一条边上是 writer。由于两条边相同，于是就融为了一条推理链。至于最后一条推理链的谓语是 genre，可能是第一张图的子图中没有标出造成的。总而言之，那三条推理链就是从知识库 K 中获取。</li><li>existential variable 即 y，lambda variable 即 x。可以把知识库 K 中的 CVT 节点看作是 y，答案看作是 x。</li></ol>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Candidate core inferential chains start from the entity FamilyGuy.jpg" alt="Candidate core inferential chains start from the entity FamilyGuy"></p>
<p>&emsp;&emsp;这样做的目的是将自然表达映射到正确的谓语序列上。对于问题“Who first voiced Meg on [Family Guy]?”，需要衡量的是在{cast-actor, writer-start, genre}中每个序列（<em>注：这个元组就是上图的三个候选核心推理链上的谓语</em>）正确捕捉 Family Guy 和 Who 之间关系的可能性。因此将这个问题简化为使用神经网络测量语义相似度。</p>
<h4 id="Deep-Convolutional-Neural-Networks"><a href="#Deep-Convolutional-Neural-Networks" class="headerlink" title="Deep Convolutional Neural Networks"></a>Deep Convolutional Neural Networks</h4><p>&emsp;&emsp;虽然是陈述一个相同的问题，但是以语义等价的方式来重新表达该问题仍旧拥有巨大的多样性。并且还存在自然语言表达与知识库中的谓语不匹配的情况。<strong>为了处理上述两个问题</strong>，论文建议使用 Siamese neural networks（<a href="http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf" target="_blank" rel="noopener">Bromley et al., 1993</a>）来识别核心推理链（暹（xiān）罗神经网络，也可以叫连体神经网络。看见这个中文就很好理解了。Siamese neural networks 可以进行语义相似度分析，QA 的匹配等操作。详情可以先看看<a href="https://www.jianshu.com/p/92d7f6eaacf5" target="_blank" rel="noopener">这篇</a>博客）。注：由于上图可以得知一个问题可以获得几个候选得到核心推理链，这就是因为语言的多样性造成的，所以需要一个方法来识别一条最核心的推理链。<br>&emsp;&emsp;例如，将一个问题映射到一种<strong>模式</strong>上，方法是将实体替换为通用符号 &lt;e&gt;，然后将其与<strong>候选链</strong>比较。比如问题“who first voiced meg on &lt;e&gt;”和 cast-actor。该模型由两个神经网络组成，一个处理<strong>模式</strong>，一个处理<strong>核心推理链</strong>（这个模型说白了就是 Siamese neural networks）。两个神经网络都映射到 k 维向量作为网络的输出，最后使用距离函数（如余弦相似度）计算语义相似度。<br><div class="note info">
            <p>&emsp;&emsp;该论文处理<strong>匹配问题</strong>使用了 CNN 模型。你可能会有点疑惑<strong>匹配问题</strong>是什么问题，前面压根就没提到过。是的，论文里也没说过，我只能猜测，这里的 CNN 其实就是上述模型的两个神经网络的具体实现。处理模型和处理核心推理链可能都用了 CNN 模型。另外论文中也没有说如何将核心推理链送入 CNN 中。论文中倒是稍微提了一下如何将问题送入 CNN 中，使用 word hashing 技术（<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf" target="_blank" rel="noopener">Huang et al., 2013</a>）。</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/CNN架构.jpg" alt="CNN架构"></p>
<h3 id="增加约束和聚合函数"><a href="#增加约束和聚合函数" class="headerlink" title="增加约束和聚合函数"></a>增加约束和聚合函数</h3><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>&emsp;&emsp;<strong>Topic Entity</strong>：由实体链接系统返回的分数直接作为特征。<br>&emsp;&emsp;<strong>Core Inferential Chain</strong>：使用不同的 CNN 模型的相似度分数来衡量核心推理链的质量，以下为 3 个模型。</p>
<ul>
<li><strong>PatChain</strong>：比较模式和谓语序列。</li>
<li><strong>QuesEP</strong>：将主题实体的名称与谓语序列拼接完成之后，将其与原问题比较。</li>
<li><strong>ClueWeb</strong>：使用 ClueWeb 语料库的 Freebase 注释训练 ClueWeb 模型</li>
</ul>
<p>&emsp;&emsp;<strong>Constraints &amp; Aggregations</strong>：当查询图中有约束节点，使用一些简单的特征来检查问题中是否存在单词可以与约束实体或者属性相关联。相似地，也可以使用一些预定义的关键字，比如“first”、“current”或者“latest”作为 argmin 节点的特征。<br>&emsp;&emsp;<strong>Overall</strong>：回答节点的个数和总节点个数也都作为特征。<br>&emsp;&emsp;比如下图，（1）属于 Topic Entity，（2）（3）（4）属于 Core Inferential Chain，（5）（6）（7）属于 Constraints &amp; Aggregations，（8）（9）属于 Overall：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/特征举例.jpg" alt="特征举例"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;使用 WEBQUESTIONS 数据集，评价指标有：precision，recall 和 F1。其中 F1 的平均值作为主要的评价指标。</p>
<h1 id="其他参考资料"><a href="#其他参考资料" class="headerlink" title="其他参考资料"></a>其他参考资料</h1><p>&emsp;&emsp;在浏览此篇论文时，发现还有其他人也看过这篇论文并且留下了笔记（中文）。<br>&emsp;&emsp;<a href="https://bigquant.com/community/t/topic/121147" target="_blank" rel="noopener">笔记1</a><br>&emsp;&emsp;<a href="https://blog.csdn.net/qq_32782771/article/details/82773048" target="_blank" rel="noopener">笔记2</a><br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/19、Semantic Parsing via Staged Query Graph Generation：Question Answering with Knowledge Base.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/在hexo中添加绘制流程图及其他图的功能.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/在hexo中添加绘制流程图及其他图的功能.html" class="post-title-link" itemprop="url">在hexo中添加绘制流程图及其他图的功能</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-01 15:28:27 / 修改时间：15:45:51" itemprop="dateCreated datePublished" datetime="2019-07-01T15:28:27+08:00">2019-07-01</time>
            

            
              

              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/hexo/" itemprop="url" rel="index"><span itemprop="name">hexo</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>hexo 本身不支持绘制流程图，但是可以使用以下命令安装插件来实现此功能。<br><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --<span class="built_in">save</span> hexo-<span class="built_in">filter</span>-flowchart</span><br></pre></td></tr></table></figure></p>
<p><a href="https://github.com/bubkoo/hexo-filter-flowchart" target="_blank" rel="noopener">插件地址</a><br>语法可以<a href="https://cloud.tencent.com/developer/article/1142260" target="_blank" rel="noopener">在这</a>找<br>一个简单的例子<br>(`乘3)flow<br>st=&gt;start: Start|past:&gt;<a href="http://www.google.com[blank" target="_blank" rel="noopener">http://www.google.com[blank</a>]<br>e=&gt;end: End:&gt;<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br>op1=&gt;operation: My Operation|past<br>op2=&gt;operation: Stuff|current<br>sub1=&gt;subroutine: My Subroutine|invalid<br>cond=&gt;condition: Yes<br>or No?|approved:&gt;<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br>c2=&gt;condition: Good idea|rejected<br>io=&gt;inputoutput: catch something…|request</p>
<p>st-&gt;op1(right)-&gt;cond<br>cond(yes, right)-&gt;c2<br>cond(no)-&gt;sub1(left)-&gt;op1<br>c2(yes)-&gt;io-&gt;e<br>c2(no)-&gt;op2-&gt;e<br>(`乘3)<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/在hexo中添加绘制流程图及其他图的功能.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">记录学习问题，积累做的 leetcode 题目</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">124</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">23</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
			  
			  <!-- 不蒜子/busuanzi -->
			  <div class="site-state-item site-state-posts">
			  	<span class="site-state-item-count">117.5k</span>
			  	<span class="site-state-item-name">总字数</span>
			  </div>
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

  
  <!-- 自己新增的所有 js 文件 -->
  <script src="/lib/my-utils.js"></script>
<!--图片缩放插件-->
<script src="/lib/zoomify/zoomify.min.js"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- 背景插件 -->
<script src="https://cdn.bootcss.com/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(11), {
		duration:90000,//1min半一换
		fade: 1500
	});
</script>
<!-- 图片缩放 -->
<script>
$('#content img').zoomify({duration: 500, });
  $('#content img').on('zoom-in.zoomify', function () {
    $('#sidebar').css('display', 'none');
  });
  $('#content img').on('zoom-out-complete.zoomify', function () {
    $('#sidebar').css('display', '');
  });
</script>

	

</body>
</html>
