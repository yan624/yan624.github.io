<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-simple.min.css?v=1.0.2">























  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


<!--图片缩放插件样式-->
<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">

  <meta name="description" content="记录学习问题，积累做的 leetcode 题目">
<meta name="keywords" content="博客，java，javaWeb，NLP，python，机器学习，深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/page/5/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="记录学习问题，积累做的 leetcode 题目">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="记录学习问题，积累做的 leetcode 题目">






  <link rel="canonical" href="http://yan624.github.io/page/5/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
	<!--加载flower canvas-->
<script>
var pathname = window.location.pathname;
if(pathname == '/flower.html'){
	var body =  document.getElementsByTagName('body')[0];
	var canvas = document.createElement("canvas")
	canvas.setAttribute('id', 'sakura')
	// '<canvas id="sakura"></canvas>'
	body.appendChild(canvas)
}
</script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">低阶炼金术士</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">18</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">22</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">113</span></a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/bug/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/bug/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html" class="post-title-link" itemprop="url">Jupyter出现gbk codec cant decode byte 0x93 in position 3136：illegal multibyte sequence</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-27 14:45:05" itemprop="dateCreated datePublished" datetime="2019-04-27T14:45:05+08:00">2019-04-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:22:36" itemprop="dateModified" datetime="2019-06-04T13:22:36+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>一般来说是open()方法没有加encoding=’utf-8’，但是没用，试了其他办法没一个能用。<br>解决办法：重启Jupyter。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/bug/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html" class="post-title-link" itemprop="url">吴恩达深度学习学习笔记：自然语言处理与词嵌入</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-25 23:58:24" itemprop="dateCreated datePublished" datetime="2019-04-25T23:58:24+08:00">2019-04-25</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:20:44" itemprop="dateModified" datetime="2019-06-04T13:20:44+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>我们一直使用<a href="https://yan624.github.io/学习笔记/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型.html#one hot编码">one hot编码</a>，这在之前已经记过笔记。这种表示方法的最大缺点是将每个词孤立起来，并且泛化能力不强。由于每个向量的内积都是0，所以它们之间的距离都是一样的。比如</p>
<ol>
<li>I want a glass of orange juice.</li>
<li>I want a glass of apple <em>_</em>.<br>这两个句子是很常见的句子，所以自然而然的想到划线处应该是juice。但是由于one hot编码，程序并不知道orange和apple之间的关系，也就猜不出来。</li>
</ol>
<h2 id="Featurized-representation：-word-embedding"><a href="#Featurized-representation：-word-embedding" class="headerlink" title="Featurized representation： word embedding"></a>Featurized representation： word embedding</h2><p>既然one hot有问题，那么自然就有人发明了新的算法。<br>使用特征来表示每个词。如果适应特征化来表示，那么最后发现orange和apple的特征差不多，就可以推测出划线处应该填写什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Featurized representation： word embedding.jpg" alt="Featurized representation： word embedding"></p>
<h2 id="Visualizing-word-embedding"><a href="#Visualizing-word-embedding" class="headerlink" title="Visualizing word embedding"></a>Visualizing word embedding</h2><p>可以使用t-SNE算法将数据可视化为二维的图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Visualizing word embedding.jpg" alt="Visualizing word embedding"></p>
<h1 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h1><h2 id="类比"><a href="#类比" class="headerlink" title="类比"></a>类比</h2><p>看下图中的表格，现在已知对应关系man-&gt;woman，能否推出king对应于queen？也就是说king-&gt;<em>_</em>，填空题。<br>解法是：<br>求出man和woman之间的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-1\\
0.01\\
0.03\\
0.09\\
\end{pmatrix} - 
\begin{pmatrix}
1\\
0.02\\
0.02\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>假设计算king和queen的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-0.95\\
0.93\\
0.70\\
0.02\\
\end{pmatrix} - 
\begin{pmatrix}
0.97\\
0.95\\
0.69\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>算法的原理就是找到一个词使得man和woman的差与king和新词的差接近。翻译为代码就是<script type="math/tex">find\ word\ w: argmax\ sim(e_w, e_{king} - e_{man} + e _{woman})</script>。但是算法的准确度只有30%-75%。</p>
<h3 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h3><p>余弦相似度也可以计算相似度。公式为<script type="math/tex">sim(u,v) = \frac{u^Tv}{\parallel u\parallel_2\parallel v\parallel_2}</script></p>
<h1 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h1><p>略。大致意思是一个嵌入矩阵E乘上one hot编码可以得到一个单词的特征向量。E就是全部单词的特征矩阵。</p>
<h1 id="如何train一个词嵌入矩阵"><a href="#如何train一个词嵌入矩阵" class="headerlink" title="如何train一个词嵌入矩阵"></a>如何train一个词嵌入矩阵</h1><p>在早期深度学习的研究人员都是使用比较复杂的算法，但是随着时间的推移，这些复杂的算法被慢慢的简化。以至于现在的新手看到这些简化版的算法时，会疑惑这样简单的算法时怎么工作的。所以现在先介绍一个比较复杂的算法，再慢慢介绍简化版的。<br><div class="note info">
            <p>这节好像是用来讲如何建立神经语言模型的，以后再看。之前讲了嵌入矩阵E，但是E中全部的特征向量是已经假定存在的，那么这些特征从何而来呢？就是这节讲的，去训练得来的。但是其实有已经训练好的，我们可以直接拿来用，网上有很多。</p>
          </div></p>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><h1 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h1><h1 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h1><p>就是词嵌入中可能带有一些偏见，比如男女偏见、种族偏见等。现在的目的就是除去这种偏见。<br>暂且不看，其他的算法都还没学。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html" class="post-title-link" itemprop="url">吴恩达李宏毅综合学习笔记：RNN入门</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-23 16:46:12" itemprop="dateCreated datePublished" datetime="2019-04-23T16:46:12+08:00">2019-04-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-05 13:25:31" itemprop="dateModified" datetime="2019-08-05T13:25:31+08:00">2019-08-05</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>课程</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>2~8</td>
<td>吴恩达深度学习</td>
<td>one hot编码、RNN包括双向和深层、GRU、LSTM</td>
</tr>
<tr>
<td>9~14</td>
<td>李宏毅机器学习</td>
<td>RNN包括双向和深层、LSTM、RNN反向传播、seq2seq</td>
</tr>
<tr>
<td>15~20</td>
<td>李宏毅深度学习</td>
<td>计算图、语言模型中的深度学习</td>
</tr>
</tbody>
</table>
</div>
<h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>假设：<br>x: Harry Potter and Hermione Granger invented a new spell.<br>y: 1 1 0 1 1 0 0 0 0<br>其中1代表人名地名之类的单词。这句话一共有九个单词，则x可以表示为：<script type="math/tex">x^{<1>} x^{<2>} \cdots x^{<t>} \cdots x^{<9>}</script>。<br>则y可以表示为：<script type="math/tex">y^{<1>} y^{<2>} \cdots y^{<t>} \cdots y^{<9>}</script><br>输入的长度表示为<script type="math/tex">T_x</script>，则<script type="math/tex">T_x = 9</script>。<br>输出的长度表示为<script type="math/tex">T_y</script>，则<script type="math/tex">T_y = 9</script>。<br>之前在神经网络中<script type="math/tex">X^i</script>或<script type="math/tex">X^(i)</script>代表第i个训练样本。现在在序列模型中，<script type="math/tex">X^{(i)<t>}</script>代表代表第i个训练样本的第t个元素。对应地，<script type="math/tex">T^i_x</script>就代表第i个样本的输入长度。</p>
<h1 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one hot编码"></a>one hot编码</h1><p>在我们做自然语言处理时，一件需要事先决定的是，怎么表示一个序列里的单词。<br>第一件事就是做一张词表（Vocabulary）有时也叫字典（Dictionary），然后将表示方法中要使用的单词列出一列。最后将一个单词用一个稀疏向量表示，如Harry表示为<script type="math/tex">\begin{pmatrix}0&0&0&\cdots&1&0&\cdots&0\end{pmatrix}</script>。1所在位置就是Harry这个单词在词表中的所在位置。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/ont%20hot%E4%BE%8B%E5%AD%90.jpg" alt="ont hot例子"></p>
<h1 id="循环神经网络——RNN"><a href="#循环神经网络——RNN" class="headerlink" title="循环神经网络——RNN"></a>循环神经网络——RNN</h1><div class="note primary">
            <p>RNN解决了什么问题。</p>
          </div>
<p>与Simple Neural Network不同的是，循环神经网络的每一层都要有输入x和输出y。<br>第一步与Simple Neural Network类似，<script type="math/tex">a_1 = w_{aa} * x^{<1>} + b_a</script>，这样就获得了激活值a，但是这时需要使用sigmoid函数或者其他函数直接算出y，另外与Simple Neural Network不同的是，它在计算激活值时需要附带加上前一层的激活值乘上一个权重，此权重与其他的权重类似，也是NN自己训练的。所以第二个序列的计算公式是<script type="math/tex">a_2 = w_{aa} * a_1 + w_{ax} * x^{<2>} + b_a</script>。后面的序列就跟第二个序列一样。<strong>注意一点，RNN中平行方向是时间序列，并不是隐藏层，并且此例中为了方便起见，垂直方向只有一个隐藏层。那几个圆圈是神经元</strong>。看下图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84RNN%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="吴恩达深度学习中的RNN示意图"></p>
<ol>
<li><p>由于为了一般化，第一层需要修改成跟后面的计算类似，所以引入一个零向量<script type="math/tex">a_0</script>来计算<script type="math/tex">a_1</script>。<br>所以RNN的计算公式为：</p>
<script type="math/tex; mode=display">
\left\{ 
 \begin{array}{c}
     a^{<1>} = g_1(w_{aa} * a^{<0>} + w_{ax} * x^{<1>} + b_a)\\
     \hat{y}^{<1>} = g_2(w_{ya} * a^{<1>} + b_y)\\
     a^{<2>} = g_1(w_{aa} * a^{<1>} + w_{ax} * x^{<2>} + b_a)\\
     \hat{y}^{<2>} = g_2(w_{ya} * a^{<2>} + b_y)\\
     \vdots\\
     a^{<t>} = g_1(w_{aa} * a^{<t-1>} + w_{ax} * x^{<t>} + b_a)\\
     \hat{y}^{<t>} = g_2(w_{ya} * a^{<t>} + b_y)\\
 \end{array}
\right.</script><p>注意上式中的<script type="math/tex">w_{aa}</script>、<script type="math/tex">w_{ax}</script>、<script type="math/tex">w_{ya}</script>、<script type="math/tex">b_{a}</script>和<script type="math/tex">b_{y}</script>并没有上标或者下标，所以意味着每一层同一个符号的权重值和偏差值都是一样的。另外对于激活函数也是用户自行选择，在<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html">对神经网络整体的理解</a>一文中已经解释的很清楚了，为了区分输入与输出的激活函数不同，我特意使用了不同的下标，这个下标仅代表这个意思。</p>
</li>
<li><p>为了进一步地一般化，我们将<script type="math/tex">w_{aa}</script>、<script type="math/tex">w_{ax}</script>合并成为<script type="math/tex">w_{a}</script>，如果表示为矩阵形式就是<script type="math/tex">w_{a} = \begin{pmatrix}w_{aa} | w_{ax}\end{pmatrix}</script>，然后将1中的最后两行表达式一般化为：</p>
<script type="math/tex; mode=display">
a^{<t>} = g_1(w_{a} * [a^{<t-1>}, x^{<t>}] + b_a)\\
\hat{y}^{<t>} = g_2(w_{y} * a^{<t>} + b_y)\\</script><p>表达式<script type="math/tex">[a^{<t-1>}, x^{<t>}]</script>的意思是将两个向量堆起来，如果表示为矩阵形式就是<script type="math/tex">\begin{pmatrix} a^{<t-1>}\\ x^{<t>}\\ \end{pmatrix}</script>，上式为了排版问题就不写成矩阵形式了。</p>
</li>
</ol>
<h2 id="RNN的反向传播"><a href="#RNN的反向传播" class="headerlink" title="RNN的反向传播"></a>RNN的反向传播</h2><p>跟Simple Neural Network类似，也要先定义一个cost function，可以选择crossentropy。由于RNN每一层都有输出值y，所以需要对每一层都求出代价，最后将这些代价值加起来</p>
<div class="note primary">
    <p>吴恩达老师在讲反向传播的实现时并没有讲计算过程，所以有点糊里糊涂的。从代价函数到激活值反向传播还可以理解，但是从后一层到前一层的反向传播理解不了。另外由于权重值一样，那么权重值到底该怎么更新？</p>
</div>

<h2 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h2><p>上面讲到的都是<script type="math/tex">T_x = T_y</script>，但是有时候输入和输出的长度并不相同。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84RNN%E5%AE%9E%E4%BE%8B.jpg" alt="不同类型的RNN实例"><br>多对多（many to many）、多对一（many to one）、一对一（one to one）、一对多（one to many）架构<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg" alt="不同类型的RNN结构"></p>
<h2 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h2><h2 id="对新序列采样"><a href="#对新序列采样" class="headerlink" title="对新序列采样"></a>对新序列采样</h2><h2 id="长期依赖，梯度消失"><a href="#长期依赖，梯度消失" class="headerlink" title="长期依赖，梯度消失"></a>长期依赖，梯度消失</h2><p>观察两个句子：</p>
<ul>
<li>The cat, which already ate…, was full.</li>
<li>The cats, which already ate…, were full.</li>
</ul>
<p>这两个句子只有复数形式上的不同，但是开头的名词影响到了最后面的be动词。但是我们目前见到的最基本的RNN不擅长捕获这种长期依赖效应。<br>用梯度消失解释一下为什么，其实原理相同的，这里引用之前的文章<br><a href="https://yan624.github.io//%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8">梯度消失和梯度爆炸</a></p>
<h1 id="GRU单元——Gate-Recurrent-Unit"><a href="#GRU单元——Gate-Recurrent-Unit" class="headerlink" title="GRU单元——Gate Recurrent Unit"></a>GRU单元——Gate Recurrent Unit</h1><p>中文名为门控循环单元。它解决了梯度消失的问题。</p>
<h2 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h2><p>c = memonry cell，使用<script type="math/tex">c^{<t>}</script>符号表示输出，其中<script type="math/tex">c^{<t>} = a^{<t>}</script>，由于后面的LSTM的c和a代表意思不同，所以这里直接使用c来表示输出值。所以本小章下的c你都看作是a即可。</p>
<h2 id="GRU工作流程"><a href="#GRU工作流程" class="headerlink" title="GRU工作流程"></a>GRU工作流程</h2><p>由于通过<script type="math/tex">c^{<t-1>}</script>来更新<script type="math/tex">c^{<t>}</script>的值，但是现在我们使用GRU，GRU就是来控制是否更新<script type="math/tex">c^{<t>}</script>的值的，这里使用“更新”的名词可能有点怪，因为<script type="math/tex">c^{<t>}</script>实际上是通过<script type="math/tex">c^{<t-1>}</script><strong>计算</strong>出来的。那么公式<script type="math/tex">a^{<t>} = g_1(w_{a} * [a^{<t-1>}, x^{<t>}] + b_a)</script>变为<script type="math/tex">\tilde{c}^{<t>} = tanh(w_{c} * [c^{<t-1>}, x^{<t>}] + b_c)</script>，这里的<script type="math/tex">\tilde{c}^{<t>}</script>是一个候选值——candidate value，类似于中间变量，而激活函数我们选择tanh。<br>GRU的核心是有一个Gate，就是上面说的是否更新值的功能，它的公式为<script type="math/tex">\Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)</script>，<script type="math/tex">\Gamma_u</script>的u的意思是update，sigmoid函数的输出范围在0-1之间，所以就完成了类似更新的功能。如果是0就代表不让你更新，如果是1就代表让你更新，这里听起来还有点绕，没关系看下面的表达式。<br>这时开始执行更新步骤：<script type="math/tex">c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}</script>，这一步可以看出如果<script type="math/tex">\Gamma_u</script>等于1就将<script type="math/tex">c^{<t>}</script>更新为<script type="math/tex">\tilde{c}^{<t>}</script>，如果等于0就相当于不让你更新，结果还是上一个的c，即<script type="math/tex">c^{<t-1>}</script>。<br>将公式写在一起，GRU的工作流程就是：</p>
<script type="math/tex; mode=display">
\begin{cases}
    \tilde{c}^{<t>} = tanh(w_{c} * [c^{<t-1>}, x^{<t>}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)\\
    c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}\\
\end{cases}</script><h2 id="GRU完整版"><a href="#GRU完整版" class="headerlink" title="GRU完整版"></a>GRU完整版</h2><p>可以看到下式中就多了一个<script type="math/tex">\Gamma_r</script>，但是为什么不用上面的简化版呢？那是因为经研究者多年的尝试，发现下面的版本是很实用的，也算是一个标准版，你可以自己开发不同的版本。</p>
<script type="math/tex; mode=display">
\begin{cases}
    \tilde{c}^{<t>} = tanh(w_{c} * [\Gamma_r * c^{<t-1>}, x^{<t>}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)\\
    \Gamma_r = \sigma(w_{r} * [c^{<t-1>}, x^{<t>}] + b_r)\\
    c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}\\
\end{cases}</script><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>吴恩达老师讲得感觉理解起来有点费劲，因为他觉得图片比文字更难理解，所以写了一大堆公式，只是再后面补充了图片。所以我建议看李宏毅老师的深度学习视频来理解LSTM。李宏毅老师的视频用了一张图片很好的解释了LSTM，并且他还举了一个例子，更加生动形象。<br>可能是东西方的差异，我感觉是图片好理解点，所以我选择看李宏毅老师的视频。这里就不写了，因为我在<strong>下面写了</strong>李宏毅老师课程的<strong>笔记</strong>。</p>
<h1 id="双向神经网络"><a href="#双向神经网络" class="headerlink" title="双向神经网络"></a>双向神经网络</h1><h1 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h1><hr>
<p>李宏毅机器学习</p>
<hr>
<h1 id="字母表示-1"><a href="#字母表示-1" class="headerlink" title="字母表示"></a>字母表示</h1><p>跟吴恩达老师讲的类似，李宏毅老师也讲了文字如何表示，与吴恩达老师不同的是，李宏毅老师多讲了几个。<br>最简单的方法利用向量来表示文字，就是上面说过的one-hot：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/1%20of%20N%20encoding.jpg" alt="1 of N encoding"><br>因为会出现某些单词没见到过，所以需要使用other这一维来表示。并且在右边的图中还可以使用字母来表示。然后理想上只要将词向量放入神经网络就会出现结果。但是Feedforward Network其实没办法解决这问题。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/beyond%201-of-N%20encoding.jpg" alt="beyond 1-of-N encoding"><br>可以看到下图，由于Feedforward Network没有记忆，所以两个句子对它来说是一个意思，但是对人来说可以很明显判断出第一句话台北是目的地，第二句话台北是出发地。Feedforward Network它只能训练当前的词，前一个词是什么它并不知道。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/Feedforward%20Network%E6%97%A0%E6%B3%95%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="Feedforward Network无法解决的问题"></p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>上面讲到Feedforward Network由于没有记忆，无法记住前一个或者前几个词，所以就诞生了RNN。RNN其实也没那么神秘，就是每次输入并交给激活函数计算完毕后，将计算结果存入缓存中，并且在下一次计算时，将缓存取出来一起计算。就是下图的蓝色方框，由于是第一次计算，其中初始化为0。下图第一遍已经在计算了，实际上已经准备更新蓝色方框中的值了。RNN在上面的章节中其实已经写过了，都是类似的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/%E4%B8%80%E4%B8%AARNN%E7%9A%84%E5%B0%8F%E5%9E%8B%E4%BE%8B%E5%AD%90.jpg" alt="一个RNN的小型例子"><br>经过上面的例子发现，当前的输入已经在依赖前一个的缓存了，所以当顺序有所变化，或者前一个数据有所变化时，RNN可以察觉到，输出的结果也自然不同。</p>
<h2 id="deep-RNN"><a href="#deep-RNN" class="headerlink" title="deep RNN"></a>deep RNN</h2><p>我一共写了两个RNN的笔记，无论是吴恩达老师的还是李宏毅老师的到目前为止，RNN其实都不是deep的，之前也在疑惑，RNN横轴有很多层，但是实际上那些层只是不同时间的输入，根本不算deep。今天继续看下去，发现这个问题终于有解了，RNN也可以是deep的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/deep%20RNN.jpg" alt="deep RNN"></p>
<h2 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h2><p>上面讲的RNN都被称为Elman Network。还有另一种辩题叫做Jordan Network，它将输出值缓存起来。传说之中Jordan Network可以有更好的性能。<br><div class="note primary">
            <p>为什么有更好的性能</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/Elman%20Network%E5%92%8Cordan%20Network.jpg" alt="Elman Network和ordan Network"></p>
<h2 id="双向RNN——Bidirectional-RNN"><a href="#双向RNN——Bidirectional-RNN" class="headerlink" title="双向RNN——Bidirectional RNN"></a>双向RNN——Bidirectional RNN</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/Bidirectional%20RNN.jpg" alt="RNN——Bidirectional RNN"></p>
<h1 id="长短期记忆——Long-Short-term-Memory-LSTM"><a href="#长短期记忆——Long-Short-term-Memory-LSTM" class="headerlink" title="长短期记忆——Long Short-term Memory(LSTM)"></a>长短期记忆——Long Short-term Memory(LSTM)</h1><div class="note primary">
            <p>LSTM的神经元个数不同有什么区别？其他的NN架构也有同样的疑问</p>
          </div>
<p>上面讲的memory实际上是最简单的，LSTM才是现在最常用的Memory。Menory在RNN中实际只是一个神经元而已，它负责输入和输出。它们之间的关联是：RNN依旧是RNN，只不过把RNN中的神经元换成了LSTM。我们知道神经元的逻辑其实很简单，只有输入——计算——输入到激活函数——输出激活值，而LSTM只不过麻烦一点罢了。<br>下图就是一个LSTM。Input Gate中如果f(z)是1就代表Gate打开，也就是f(z)*g(z) = 1 * g(z) = g(z)，就相当于可以让外界输入。如果f(z)=0，Gate被关闭，那么 f(z)*g(z)=0，是不是就像不允许外界输入一样？因为你输入多少都被置为0。而Forget Gate也类似，当f(z)=1时，即Forget Gate被打开，这里与直觉有点相反，因为Gate打开，有点感觉像遗忘。但是其实c*f(z) = 1，所以Forget Gate为1其实是记住原本的c的意思。<br>另外图中也写到了，Gate的激活函数一般选sigmoid，里面的值就代表Gate的打开程度。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/LSTM%E7%A4%BA%E4%BE%8B.jpg" alt="LSTM示例"></p>
<h2 id="LSTM的例子"><a href="#LSTM的例子" class="headerlink" title="LSTM的例子"></a>LSTM的例子</h2><p>例子介绍：只有一个LSTM，输入有3维，输出有1维。<script type="math/tex">x_2 = 1</script>则<script type="math/tex">x_1</script>的值就会被存到Memory中，<script type="math/tex">x_2 = -1</script>则重置Memory，<script type="math/tex">x_3 = 1</script>则输出。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/LSTM%E4%BE%8B%E5%AD%90%E4%BB%8B%E7%BB%8D.jpg" alt="LSTM例子介绍"><br>注：下图中的蓝色数字和灰色数字是权重值。<br><div class="note primary">
            <p>权重值是初始化的？还是固定的？还是初始化后自己可以训练的？其实就是LSTM的反向传播算法要弄懂。</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/LSTM%E4%BE%8B%E5%AD%90%E8%AE%A1%E7%AE%97.jpg" alt="LSTM例子计算"></p>
<ol>
<li>Input Gate：<br>将偏差设为-10是因为我们通过x2来对Input Gate控制。平常x2=0，计算x*w+b=-10，那么通过sigmoid function就会得到一个接近于0的值，所以就实现了将Input Gate关闭的功能。而如果x2=1，那么x2*100=100，通过sigmoid function就会得到一个接近于1的值，Input Gate就实现了打开的功能。</li>
<li>Forget Gate: 这里的功能跟Input Gate类似。</li>
<li>Output Gate: 如果Output Gate被关闭，那么输出0.</li>
</ol>
<h2 id="多个LSTM工作场景"><a href="#多个LSTM工作场景" class="headerlink" title="多个LSTM工作场景"></a>多个LSTM工作场景</h2><p>里面的<script type="math/tex">x^t</script>就是对应于NN中的一个向量，它分别乘上4个参数矩阵得到4个不同的向量，以此操控LSTM，而LSTM实际上就等于神经元，说白了就是一个类似激活函数的功能。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF.jpg" alt="LSTM实际工作场景"><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF2.jpg" alt="LSTM实际工作场景2"><br>多个LSTM连起来工作就是像下面一样，红线和红线旁边的那个黑色曲线链接的值之前没有讲过，但是下图的这样才是LSTM实际的长相，所以之前讲的那么复杂实际上还是LSTM的简化版。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="LSTM实际工作流程"></p>
<h1 id="RNN反向传播"><a href="#RNN反向传播" class="headerlink" title="RNN反向传播"></a>RNN反向传播</h1><p>BPTT——backpropagation through time，与NN的backpropagation类似，李宏毅老师也没讲原理直接跳过了。<br>然而不幸的是，RNN的training是很困难的。下面蓝色的线是希望的结果，但是实际上是绿色的线，会出现剧烈地抖动，最后在某个点出现NAN。这就是类似梯度消失问题。可以使用一些办法解决，但是现在用得最多的方法是LSTM。<br><div class="note primary">
            <p>为什么LSTM能解决RNN的难题</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/RNN%20training%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="RNN trWaining碰到的问题"></p>
<h1 id="其他解决梯度消失的办法"><a href="#其他解决梯度消失的办法" class="headerlink" title="其他解决梯度消失的办法"></a>其他解决梯度消失的办法</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/%E5%85%B6%E4%BB%96%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8A%9E%E6%B3%95.jpg" alt="其他解决梯度消失的办法"></p>
<h1 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h1><h2 id="Many-to-one"><a href="#Many-to-one" class="headerlink" title="Many to one"></a>Many to one</h2><p>输入一个向量sequence，只输出一个向量。</p>
<ol>
<li>语义分析。比如分析电影评论是好是坏。</li>
<li>key term extraction。对文档提取关键词。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many to One.jpg" alt="Many to One"></p>
<h2 id="Many-to-many-Output-is-shorter"><a href="#Many-to-many-Output-is-shorter" class="headerlink" title="Many to many(Output is shorter)"></a>Many to many(Output is shorter)</h2><p>输入和输出都是向量sequence，但是输出要短。</p>
<ol>
<li>Speech Recognition 。语音辨识。</li>
</ol>
<h2 id="Many-to-many-No-limitation"><a href="#Many-to-many-No-limitation" class="headerlink" title="Many to many(No limitation)"></a>Many to many(No limitation)</h2><p>输入和输出都是序列且长短不一。被称为 <strong>Sequence to sequence learning</strong> 。</p>
<ol>
<li>Machine Translation. 机器翻译。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many to Many（No Limitation）.jpg" alt="Many to Many(No Limitation)"></p>
<h2 id="Beyond-Sequence"><a href="#Beyond-Sequence" class="headerlink" title="Beyond Sequence"></a>Beyond Sequence</h2><ol>
<li>Syntactic parsing</li>
</ol>
<hr>
<p>李宏毅深度学习</p>
<hr>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>此系列视频还有两个Review视频，分别为第一个视频：Basic Structures for Deep Learning Models(Part 1)， 第二个视频：Basic Structures for Deep Learning Models(Part 2)。<br>个人认为Review视频不需要看，而且这两个视频时间贼长，加起来得有两个多小时。没必要浪费时间，即使你根本没学过Review中的知识点也不用去看。他的Review里不会讲很深，基本上就过过场，就算有很深的东西也完全不影响继续往下学。1P时长80分钟，说实话如果自己属于小白阶段，去看那么长的视频是挺打击人的兴趣的，如果是大佬或者已经入门的人当然看得津津有味了。<br><a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#字母表示-1">此文</a>记录了李宏毅机器学习视频中讲解的RNN的笔记。</p>
<h1 id="Computational-Graph-amp-Backpropagation"><a href="#Computational-Graph-amp-Backpropagation" class="headerlink" title="Computational Graph &amp; Backpropagation"></a>Computational Graph &amp; Backpropagation</h1><div class="note danger">
            <p>2019年6月7号更新：关于计算图这章，现在才发现原来很重要，因为这是完成<strong>自动求导</strong>的关键。学了 pytorch 之后才发现的。</p>
          </div>
<h2 id="什么是Computational-Graph"><a href="#什么是Computational-Graph" class="headerlink" title="什么是Computational Graph"></a>什么是Computational Graph</h2><p>这实际上跟要学的深度学习没什么关系，只是名字好听点，无视就好，如下图就是一个Computational Graph。主要用来在计算神经网络一些输出时，便于理解。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E4%BE%8B%E5%AD%90.jpg" alt="Computational Graph例子"><br>在看一个比较贴近实际的例子，顺便复习一下链式求导法则。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99%E7%A4%BA%E4%BE%8B.jpg" alt="Computational Graph链式求导法则示例"></p>
<h2 id="通过链式求导的例子理解反向传播（Backpropagation）算法"><a href="#通过链式求导的例子理解反向传播（Backpropagation）算法" class="headerlink" title="通过链式求导的例子理解反向传播（Backpropagation）算法"></a>通过链式求导的例子理解反向传播（Backpropagation）算法</h2><p>首先进行正向链式求导，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E6%AD%A3%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="正向链式求导"><br>图中要求计算e对a求偏导，首先给出a=3, b=2。其中c=a+b, d=b+1。<br>按照李宏毅老师使用链式求导法则，先要计算c对a求导得到1。e再对c求导得到b+1，带入b=2，得到3。所以3对a求偏导等于1*3=3。<br>上面这种链式求导法则有点乱，如果没仔细学过<em>微积分</em>可能难以理解。其实对于方程e = (a+b) * (b+1)，e对a求偏导，直接看出来都可以。利用考研时的口诀“左导右不导，左不导右导”（也就是<a href="https://baike.baidu.com/item/%E8%8E%B1%E5%B8%83%E5%B0%BC%E8%8C%A8%E5%85%AC%E5%BC%8F/8779293?fr=aladdin" target="_blank" rel="noopener">莱布尼茨公式</a>），直接得到结果<script type="math/tex">\frac{\partial e}{\partial a} = b+1</script>。<br>然后将b=2带入b+1得到结果还是3。</p>
<p>接着进行反向模式，如下图:<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%8F%8D%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="反向链式求导"><br>现在图中要求计算<script type="math/tex">\frac{\partial e}{\partial a}</script>以及<script type="math/tex">\frac{\partial e}{\partial b}</script>，当然你可以分别进行两次链式求导，得到结果。但是如果从e出发，也就是反向，那么就可以同时得到<script type="math/tex">\frac{\partial e}{\partial a}</script>以及<script type="math/tex">\frac{\partial e}{\partial b}</script>的结果。<br>不要在意e为什么等于1，只不过一个输入而已。<br>此外，如果阅读过《deep learning and neural network》一书，看过吴恩达机器学习视频或者其它资料的应该已经能反应出来。连接线上的求偏导实际上就跟神经网络上的权重一个意思，然后也是一层一层地反向传播。<br>这个输入e实际上就是神经网络中的反向传播算法中的输入。就是最后一层神经元的误差<script type="math/tex">\delta^l = h-y</script>。这里吴恩达老师和《deep learning and neural network》作者的最后一层误差公式不一样，<strong>目前不明</strong>，暂时不做解释，这里的公式是吴恩达老师的。<br>然后就是误差*权重+偏差得到前一层的误差，具体不展开。</p>
<h2 id="反向传播的好处"><a href="#反向传播的好处" class="headerlink" title="反向传播的好处"></a>反向传播的好处</h2><p>如果你的root只有一个，那么这个Computational Graph中的所有偏微分就都可以一次性算出。对应于神经网络，我们就是要这样的效果。</p>
<h2 id="参数共享（Parameter-sharing）"><a href="#参数共享（Parameter-sharing）" class="headerlink" title="参数共享（Parameter sharing）"></a>参数共享（Parameter sharing）</h2><p>略，看了一眼貌似挺简单。16:20</p>
<h2 id="Computational-Graph-for-Feedforword-Net"><a href="#Computational-Graph-for-Feedforword-Net" class="headerlink" title="Computational Graph for Feedforword Net"></a>Computational Graph for Feedforword Net</h2><p>李宏毅深度学习p3从21:16到52:48讲解梯度下降算法、前馈神经网络以及反向传播算法的具体数学原理<br>一直没看懂原理，以后再看。</p>
<h2 id="Computational-Graph-for-Recurrent-Network"><a href="#Computational-Graph-for-Recurrent-Network" class="headerlink" title="Computational Graph for Recurrent Network"></a>Computational Graph for Recurrent Network</h2><h1 id="Deep-Learning-for-Language-Modeling"><a href="#Deep-Learning-for-Language-Modeling" class="headerlink" title="Deep Learning for Language Modeling"></a>Deep Learning for Language Modeling</h1><p>语言模型就是预测一个word sequence出现的几率有多大。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Language%20Modeling.jpg" alt="Language Modeling"></p>
<h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>N-gram是自然语言处理中的算法。2-gram读作bi-gram。</p>
<h3 id="传统做法"><a href="#传统做法" class="headerlink" title="传统做法"></a>传统做法</h3><ul>
<li>怎么预测一句话出现的几率</li>
<li>收集大量文本作为训练数据<ul>
<li>然后计算<script type="math/tex">w_1\cdots w_n</script>这句话在训练数据中出现的概率</li>
</ul>
</li>
<li>N-gram语言模型：<ul>
<li>如何计算一小部分的概率？例如下图的p(beach|nice)出现的概率。就是将nice beach出现的次数除以nice出现的次数。</li>
</ul>
</li>
</ul>
<p>前两条是理想的处理办法，但是麻烦的是要预测的句子在语料库——corpus中八成一次都没出现过。于是就需要使用N-gram模型。它的处理办法就是将句子拆成比较小的部分——component，再把每个小部分的概率乘起来就是句子出现的几率。像下图这种只考虑前一个单词的模型叫做2-gram model。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/N-gram.jpg" alt="N-gram"></p>
<h3 id="NN-based-LM"><a href="#NN-based-LM" class="headerlink" title="NN-based LM"></a>NN-based LM</h3><p>怎么做基于NN的N-gram？<br>做法：</p>
<ol>
<li>搜集training数据</li>
<li>learn一个Neural Network，通过两个词predict下一个词，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/NN-based%20LM.jpg" alt="NN-based LM"></li>
<li>使用cross entropy minimize</li>
<li>有了Neural Network后算一个句子的几率，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9A%84%E5%87%A0%E7%8E%87.jpg" alt="计算句子的几率"><br>其中STRAT是一个token，代表句子的起始。</li>
</ol>
<h3 id="RNN-based-LM"><a href="#RNN-based-LM" class="headerlink" title="RNN-based LM"></a>RNN-based LM</h3><p>往上翻<strong>循环神经网络——RNN</strong>，原理就是这个。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门：RNN序列模型/RNN-based LM.jpg" alt="RNN-based LM"></p>
<h3 id="Challenge-of-N-gram"><a href="#Challenge-of-N-gram" class="headerlink" title="Challenge of N-gram"></a>Challenge of N-gram</h3><h4 id="NN-based-model"><a href="#NN-based-model" class="headerlink" title="NN-based model"></a>NN-based model</h4><p>为什么要使用NN-based model。相较于传统方法有什么好处。<br>就是概率估不准，因为永远没有足够的数据。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Challenge%20of%20N-gram.jpg" alt="Challenge of N-gram"><br><div class="note info">
            <p>视频13:20~27:01仔细讲解了为什么要使用NN，而且把我困惑了快一个月的问题解决了，就是将文字转为数字之后进行训练的意义。</p>
          </div></p>
<h4 id="RNN-based-model"><a href="#RNN-based-model" class="headerlink" title="RNN-based model"></a>RNN-based model</h4><p>为什么要使用RNN-based model。相较于传统方法有什么好处。</p>
<h1 id="Spatial-Transformer-Layer"><a href="#Spatial-Transformer-Layer" class="headerlink" title="Spatial Transformer Layer"></a>Spatial Transformer Layer</h1><h1 id="Highway-Network-amp-Grid-LSTM"><a href="#Highway-Network-amp-Grid-LSTM" class="headerlink" title="Highway Network &amp; Grid LSTM"></a>Highway Network &amp; Grid LSTM</h1><p>前馈神经网络（就是Simple NN）和RNN的对比。</p>
<ol>
<li>Feedforward NN不是每一步都有输入。</li>
<li>Feedforward NN每一层都有不同的参数。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward NN和RNN的对比.jpg" alt="Feedforward NN和RNN的对比"></li>
</ol>
<h1 id="Recusive-Network"><a href="#Recusive-Network" class="headerlink" title="Recusive Network"></a>Recusive Network</h1><p>Recursive Network是Recurrent Network的General版本。Recurrent Network是Recursive Network的一个特殊的例子，如果翻译成中文的话，实际上名字都一样。所以可以称之为递归式网络。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/git学习记录.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/git学习记录.html" class="post-title-link" itemprop="url">git学习记录</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-22 23:51:08" itemprop="dateCreated datePublished" datetime="2019-04-22T23:51:08+08:00">2019-04-22</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-24 23:34:20" itemprop="dateModified" datetime="2019-04-24T23:34:20+08:00">2019-04-24</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是在学习<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">该教程</a>时/后做的笔记。<br>我现在用git基本都是用<a href="https://desktop.github.com/" target="_blank" rel="noopener">Github Desktop</a>，前面的是下载地址。用起来方便又快捷。事实上我也不会用git的命令o(<em>￣︶￣</em>)o所以今天稍微学一下。</p>
<h1 id="git-init"><a href="#git-init" class="headerlink" title="git init"></a>git init</h1><p>切换到想要创建仓库的文件夹，执行命令<code>git init</code>就会在该文件夹下创建一个.git的文件夹，这个文件夹是隐藏的。</p>
<h1 id="git-add-git-commit"><a href="#git-add-git-commit" class="headerlink" title="git add/git commit"></a>git add/git commit</h1><p>使用命令<code>git add whatever.txt</code>将文件添加到仓库。使用命令<code>git commit -m &quot;wrote a file&quot;</code>将文件提交到仓库，-m后面的是描述这份文件你改了什么。其实就是相当于desktop的一个按钮，按一下就把全部有改动文件都提交了。<br>这样就完成了提交一份文件。这里就会有疑问了，为什么设计成先add再commit？直接commit不就行了？因为commit可以提交多份文件，你可以使用add命令一份一份地添加文件，再使用commit一次性提交到仓库。<br>该命令指示推送到本地仓库，并非远程仓库。</p>
<h1 id="git-status"><a href="#git-status" class="headerlink" title="git status"></a>git status</h1><p>查看仓库当前的修改状态</p>
<h1 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h1><p>发现某份文件被修改了，但是忘记改了什么怎么办？使用命令<code>git diff modified_file.txt</code>查看，它会显示文件哪里被修改了。diff就是difference的意思。</p>
<h1 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h1><p>git可以记住你的历史提交版本，如果有一天电脑损坏，自己干了什么完全忘记，可以使用<code>git log</code>命令。它会显示以前所有的历史记录。这条命令会显示很详细的信息，但是就是因为信息太详细了，人可能看不过来，可以加上<code>--pretty=oneline</code>来限制。类似<code>c3fca95239a4bbe21ee2991e0a914fb522060e74</code>这种是版本号（commit id）。</p>
<h1 id="git-rest"><a href="#git-rest" class="headerlink" title="git rest"></a>git rest</h1><p>该命令可以回退版本。<code>git reset --hard HEAD^</code>，命令里的HEAD代表当前版本，^代表上一个版本，如果想要回退至前100个版本，可以使用HEAD~100。<br>也可以直接指定commit id，如<code>git reset --hard c3fca95239a4bbe21ee2991e0a914fb522060e74</code>，commit id可以不写全，写个开头就行了<code>git reset --hard c3fca</code><br>注意回退版本后，如果关闭git bash那么就无法查询到该版本之后的所有版本。</p>
<h1 id="git-reflog"><a href="#git-reflog" class="headerlink" title="git reflog"></a>git reflog</h1><p>该命令记住了你每一步操作，如果回退版本后后悔了，可以使用该命令查询以前的commit id。</p>
<h1 id="git-checkout-—filename"><a href="#git-checkout-—filename" class="headerlink" title="git checkout —filename"></a>git checkout —filename</h1><p>把文件夹在工作区的修改全部撤销。总之，就是让这个文件回到最近一次git commit或git add时的状态。<br>参考<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374831943254ee90db11b13d4ba9a73b9047f4fb968d000" target="_blank" rel="noopener">文章</a></p>
<h1 id="git-rm"><a href="#git-rm" class="headerlink" title="git rm"></a>git rm</h1><p>删除文件，与linux命令类似。</p>
<h1 id="git-remote"><a href="#git-remote" class="headerlink" title="git remote"></a>git remote</h1><p>将本地的仓库和远程的仓库关联，使用命令<code>git remote add origin git@github.com:github_account_name/repository_name.git</code><br>注意将github_account_name和repository_name分别替换成github账号名和仓库名。添加关联后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的。下一步，就可以把本地库的所有内容推送到远程库上。</p>
<h1 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h1><p><code>git push -u origin master</code></p>
<blockquote>
<p>把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。<br>由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。<br>从现在起，只要本地作了提交，就可以通过命令：<br><code>git push origin master</code><br>把本地master分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！</p>
</blockquote>
<h1 id="git-clone"><a href="#git-clone" class="headerlink" title="git clone"></a>git clone</h1><p>上面说了将本地仓库和远程仓库关联，并将本地仓库的文件推送到远程仓库，那么自然也可以从远程仓库clone文件到本地仓库。<br>使用命令：<code>git clone git@github.com:github_account_name/repository_name.git</code><br>还可以从<a href="https://github.com/yan624/yan624.github.io.git这样的地址克隆" target="_blank" rel="noopener">https://github.com/yan624/yan624.github.io.git这样的地址克隆</a></p>
<h1 id="对分支的管理"><a href="#对分支的管理" class="headerlink" title="对分支的管理"></a>对分支的管理</h1><p>字太多，不想打了。看下面教程。<br><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840038939c291467cc7c747b1810aab2fb8863508000" target="_blank" rel="noopener">教程</a></p>
<h2 id="git-checkout-b-branch-name"><a href="#git-checkout-b-branch-name" class="headerlink" title="git checkout -b branch_name"></a>git checkout -b branch_name</h2><p>创建名为branch_name的分支并切换到该分支，-b参数代表切换。</p>
<h2 id="git-branch"><a href="#git-branch" class="headerlink" title="git branch"></a>git branch</h2><p>查看当前分支，如果分支之前有*就代表这个分支是主分支。</p>
<h2 id="git-merge-branch-name"><a href="#git-merge-branch-name" class="headerlink" title="git merge branch_name"></a>git merge branch_name</h2><p>合并分支</p>
<h2 id="git-branch-d-branch-name"><a href="#git-branch-d-branch-name" class="headerlink" title="git branch -d branch_name"></a>git branch -d branch_name</h2><p>删除名为branch_name分支</p>
<h2 id="合并分支发生冲突"><a href="#合并分支发生冲突" class="headerlink" title="合并分支发生冲突"></a>合并分支发生冲突</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840202368c74be33fbd884e71b570f2cc3c0d1dcf000" target="_blank" rel="noopener">解决办法</a></p>
<h2 id="强大的分支功能"><a href="#强大的分支功能" class="headerlink" title="强大的分支功能"></a>强大的分支功能</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013758410364457b9e3d821f4244beb0fd69c61a185ae0000" target="_blank" rel="noopener">创建分支的策略</a></p>
<h2 id="bug分支。将当前工作暂存，先修改出现的bug"><a href="#bug分支。将当前工作暂存，先修改出现的bug" class="headerlink" title="bug分支。将当前工作暂存，先修改出现的bug"></a>bug分支。将当前工作暂存，先修改出现的bug</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137602359178794d966923e5c4134bc8bf98dfb03aea3000" target="_blank" rel="noopener">暂存命令</a></p>
<h2 id="feature分支"><a href="#feature分支" class="headerlink" title="feature分支"></a>feature分支</h2><p>与上面类似，无非概念不同</p>
<h2 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013760174128707b935b0be6fc4fc6ace66c4f15618f8d000" target="_blank" rel="noopener">教程</a></p>
<h2 id="git-rebase"><a href="#git-rebase" class="headerlink" title="git rebase"></a>git rebase</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="使用github"><a href="#使用github" class="headerlink" title="使用github"></a>使用github</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="使用码云"><a href="#使用码云" class="headerlink" title="使用码云"></a>使用码云</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00150154460073692d151e784de4d718c67ce836f72c7c4000" target="_blank" rel="noopener">教程</a></p>
<h1 id="配置文件的更多配置"><a href="#配置文件的更多配置" class="headerlink" title="配置文件的更多配置"></a>配置文件的更多配置</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00150154460073692d151e784de4d718c67ce836f72c7c4000" target="_blank" rel="noopener">教程</a></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/git学习记录.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/更改hexo-next主题的fontawesome版本至最新.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/更改hexo-next主题的fontawesome版本至最新.html" class="post-title-link" itemprop="url">更改hexo next主题的fontawesome版本至最新</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-22 13:23:47 / 修改时间：15:09:00" itemprop="dateCreated datePublished" datetime="2019-04-22T13:23:47+08:00">2019-04-22</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/hexo/" itemprop="url" rel="index"><span itemprop="name">hexo</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>由于在后续又发现在其他地方也用到了icon，所以需要大量更改配置。<strong>如果不会写程序的还是别改了。</strong></p>
<p>next的fontawesome默认版本是4.6.2，在写本文时，fontawesome的最新版本是5.8.1。貌似fontawesome在5.0.0版本之后改版了。总之一直出现方框乱码。<br>后来发现，现在的fontawesome链接已经跟以前不一样了，它现在分为3大类别。<br>现在的使用方法是：在next主题的_config.xml中搜索fontawesome，并更改属性<code>fontawesome: //cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css</code>，注意里面的文件是all.min.css，而不是font-awesom.min.css。<br>但是这样更改之后还是会出现方框乱码，原因是next默认使用的fa的类，而有时候我们需要使用fab或其他的类。所以需要修改一下源代码。<br>找到layout/_macro/menu/menu-item.swig，定位class=”menu-item-icon，将后面的“fa fa-fw fa-”删去。以后再修改icon不能只加一个名字了。可以像我这样修改：<code>assorted: /assorted || fa fa-fw fa-layer-group</code>。<br>可以看到我将icon的名称补全了。如果想用fab的类，可以像这样修改：<code>python: /python || fab fa-fw fa-python</code>。以此类推。</p>
<p>这样修改以后，如果不想用fontawesome了，想用其他的icon库，改起来也很方便。</p>
<p>layout/_macro/menu/menu-item.swig被layout/_partials/header/sub-menu.swig引用。</p>
<p>另外由于fontawesome版本改动，社交软件的icon也需要更改，在_config.xml中搜索github，将icon改为<code>fab fa-fw fa-github</code>。找到ayout/_macro/siderbar.swig，搜索fa fa-fw fa-，看看定位的地点上面是不是<code>{百分号  if theme.social_icons.enable 百分号}</code>。是的话将fa fa-fw fa-删除。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/更改hexo-next主题的fontawesome版本至最新.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/ViewPager无法刷新数据.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/ViewPager无法刷新数据.html" class="post-title-link" itemprop="url">ViewPager无法刷新数据</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-18 22:15:24 / 修改时间：22:38:54" itemprop="dateCreated datePublished" datetime="2019-04-18T22:15:24+08:00">2019-04-18</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/android/" itemprop="url" rel="index"><span itemprop="name">android</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>实现ViewPager刷新数据功能，在网上找了很多资料都已经过时了。<br>由于本人并不是android开发出身，完全是做app玩的。所以很多术语都不知道，如果看不懂就算了。。。</p>
<p>实现PagerAdapter类，我命名为HomePagerAdapter<br><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HomePagerAdapter</span> <span class="keyword">extends</span> <span class="title">PagerAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;View&gt; pageView;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HomePagerAdapter</span><span class="params">(ArrayList&lt;View&gt; pageView)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pageView = pageView;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mChildCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">notifyDataSetChanged</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mChildCount = getCount();</span><br><span class="line">        <span class="keyword">super</span>.notifyDataSetChanged();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getItemPosition</span><span class="params">(Object object)</span>   </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> ( mChildCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            mChildCount --;</span><br><span class="line">            <span class="keyword">return</span> POSITION_NONE;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">return</span> <span class="keyword">super</span>.<span class="title">getItemPosition</span><span class="params">(object)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//获取当前窗体界面数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="function"><span class="keyword">return</span> pageView.<span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//判断是否由对象生成界面</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isViewFromObject</span><span class="params">(View arg0, Object arg1)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="keyword">return</span> arg0==arg1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">destroyItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position, Object object)</span> </span>&#123;</span><br><span class="line">        container.removeView(pageView.get(position));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Object <span class="title">instantiateItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position)</span> </span>&#123;</span><br><span class="line">        View view = pageView.get(position);</span><br><span class="line">        container.addView(view);</span><br><span class="line">        <span class="keyword">return</span> view;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">finishUpdate</span><span class="params">(ViewGroup container)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(container.getChildCount() == <span class="number">0</span>)&#123;</span><br><span class="line">            pageView.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意destroyItem、instantiateItem方法，PagerAdapter中还有两个同名的方法，但是已被废弃，注意参数不同。<br>实现刷新数据主要在destroyItem、instantiateItem、finishUpdate三个方法。其他的方法其实别人的教程也写了，但是我这三个方法是我自己研究的，我没见别人写过。</p>
<p>解释流程。<br>第一步，改变数据，我是将数据保存在了<code>private ArrayList&lt;View&gt; pageView;</code>中。<br>第二步，调用<code>adapter.notifyDataSetChanged();</code>方法，它首先会销毁item，即调用<code>destroyItem(ViewGroup container, int position, Object object)</code>方法。随即调用<code>instantiateItem(ViewGroup container, int position)</code>方法。<br>一般来说大家都是这么干的，因为将数据改变后，调用<code>adapter.notifyDataSetChanged();</code>方法。直觉认为这么做合乎常理。<br>但是这里注意一点，假设<code>private ArrayList&lt;View&gt; pageView;</code>中原先保存两个View，改变数据将这个View删除，从新添加三个新的View，那么在<code>destroyItem(ViewGroup container, int position, Object object)</code>方法中，它无法删除，仔细看里面的代码<code>container.removeView(pageView.get(position));</code>，发现它是通过position这个索引获取对象，再在container容器中通过对象查找删除。那么问题来了，你之前已经将两份View删除了，它还怎么通过position获取到呢？所以在这一步出了问题。<br>当然这一步出了问题后，后面的创建页面步骤更是稀巴烂。</p>
<p>正确步骤如下：<br><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将页面刷新，渲染新的数据集</span></span><br><span class="line"><span class="selector-tag">homePagerAdapter</span><span class="selector-class">.notifyDataSetChanged</span>();</span><br><span class="line"><span class="selector-tag">changeData</span>(inflater, data);</span><br><span class="line"><span class="selector-tag">homePagerAdapter</span><span class="selector-class">.notifyDataSetChanged</span>();</span><br></pre></td></tr></table></figure></p>
<p>第一步，不要更改数据，直接调用<code>homePagerAdapter.notifyDataSetChanged();</code>，目的是让其删除原先的view。<br>第二步，更改数据。<br>第三步，再次调用<code>homePagerAdapter.notifyDataSetChanged();</code>，完成页面的创建。由于container中已经没有view了，所以删除那个步骤做了也等于没做，但是由于数据已经更新页面还是会被创建出来。</p>
<p>最后强调用一点。在HomePagerAdapter类中一个<code>finishUpdate(ViewGroup container)</code>方法，注意看里面的代码。<strong>以上的所有步骤，全部依赖于这几句代码。</strong><br>上面第一步说到直接调用notifyDataSetChanged()方法目的是删除原先的view，但是view删除后，你必须将<code>private ArrayList&lt;View&gt; pageView;</code>中的数据也删除。<strong>这里补充一点，pageView内是我创建的View，而container中是android自己维护的界面</strong>，我也不知道怎么称呼，就将其称为界面吧。<br>在finishUpdate()方法中判断，如果container中已经没有界面了，那就直接移除pageView中所有的数据，也就是算更新数据了。值得注意的是，这里面逻辑及其复杂，这行清空数据的代码，只有放在<code>finishUpdate(ViewGroup container)</code>中执行，并且必须加上那个if条件判断，app才能正常运行。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/ViewPager无法刷新数据.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/bug/在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/bug/在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式.html" class="post-title-link" itemprop="url">在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-18 16:23:46 / 修改时间：16:27:54" itemprop="dateCreated datePublished" datetime="2019-04-18T16:23:46+08:00">2019-04-18</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/hexo/" itemprop="url" rel="index"><span itemprop="name">hexo</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>之前写的数学表达式明明可以渲染，但是接下去隔了n行的数学表达式无法渲染。推测是因为单行数学表达式在文字前面换行。<br>比如说：</p>
<blockquote>
<p>文字文字文字文字：·￥￥·<br>该表达式渲染正常。</p>
</blockquote>
<p>如果，</p>
<blockquote>
<p>文字文字文字文字：<br>·￥￥·<br>那么下面的数学表达式将全部无法渲染。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/bug/在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></blockquote></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/运用其他的插件，在hexo中添加提示弹窗.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/运用其他的插件，在hexo中添加提示弹窗.html" class="post-title-link" itemprop="url">运用其他的插件，在hexo中添加提示弹窗</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-18 14:30:17 / 修改时间：14:49:00" itemprop="dateCreated datePublished" datetime="2019-04-18T14:30:17+08:00">2019-04-18</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/hexo/" itemprop="url" rel="index"><span itemprop="name">hexo</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>由于我写了很多学习记录，但是这些都是自己看书或者看视频学来的。万一有错误的地方正好被人看见，他又是新手，误以为我的是对的，这样就不好了。所以准备做一个提示弹窗，在所有的带有“学习笔记”的标签的文章中自动弹出提示。</p>
<ol>
<li>下载一个自己喜欢的弹窗插件，可以去<a href="http://www.jq22.com/" target="_blank" rel="noopener">jQuery插件库</a>找。</li>
<li>将css和js放在next主题下的source文件夹中，如下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/next%E4%B8%BB%E9%A2%98%E7%9B%AE%E5%BD%95.jpg" alt="next主题目录"><br>css文件放入css文件，js文件放入js文件夹。我自己创了一个spop的文件夹，用于单独放置我的弹窗插件。</li>
<li>打开layout文件夹，进入_macro文件夹，找到post.swig文件。搜索class=”post-block”，这个标签的位置在下图箭头所指的地方：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/%E5%BC%B9%E7%AA%97%E6%8F%92%E4%BB%B6%E4%BB%A3%E7%A0%81%E5%86%99%E5%85%A5%E7%9A%84%E4%BD%8D%E7%BD%AE.png" alt="弹窗插件代码写入的位置"><br>如果打开了这个文件，找了post-block标签，可以看到标签内部第一行代码为<code>&lt;link itemprop=&quot;mainEntityOfPage&quot; href=&quot;{ config.url }{ url_for(post.path) }&quot;/&gt;</code>。由于hexo渲染问题我将href属性里的值去掉了一对{}。<br>将下面的代码放在上述代码上面或者下面即可。<figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="name">for</span></span> tag <span class="keyword">in</span> post.tags %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml">	<span class="comment">&lt;!--判断该文章是否为学习笔记--&gt;</span></span></span><br><span class="line"><span class="xml">	</span><span class="template-tag">&#123;% <span class="name"><span class="name">if</span></span> tag.name == '学习笔记' and !is_home() %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml">  		<span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"/css/spop/spop.min.css"</span>&gt;</span></span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"/js/spop/spop.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span></span><br><span class="line"><span class="xml">			spop(&#123;</span></span><br><span class="line"><span class="xml">				template: '<span class="tag">&lt;<span class="name">h4</span> <span class="attr">class</span>=<span class="string">"spop-title"</span>&gt;</span>注意<span class="tag">&lt;/<span class="name">h4</span>&gt;</span>此文章仅为博主的学习笔记，其中可能含有极大的理论错误。',</span></span><br><span class="line"><span class="xml">				group: 'tips',</span></span><br><span class="line"><span class="xml">				position  : 'bottom-center',</span></span><br><span class="line"><span class="xml">				style: 'success',</span></span><br><span class="line"><span class="xml">				autoclose: 5500,</span></span><br><span class="line"><span class="xml">				onOpen: function () &#123;</span></span><br><span class="line"><span class="xml">					//这里设置灰色背景色</span></span><br><span class="line"><span class="xml">				&#125;,</span></span><br><span class="line"><span class="xml">				onClose: function() &#123;</span></span><br><span class="line"><span class="xml">					//这里可以取消背景色</span></span><br><span class="line"><span class="xml">					spop(&#123;</span></span><br><span class="line"><span class="xml">						template: 'ε = = (づ′▽`)づ',</span></span><br><span class="line"><span class="xml">						group: 'tips',</span></span><br><span class="line"><span class="xml">						position  : 'bottom-center',</span></span><br><span class="line"><span class="xml">						style: 'success',</span></span><br><span class="line"><span class="xml">						autoclose: 1500</span></span><br><span class="line"><span class="xml">					&#125;);</span></span><br><span class="line"><span class="xml">				&#125;</span></span><br><span class="line"><span class="xml">			&#125;);</span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="xml">	</span><span class="template-tag">&#123;% <span class="name"><span class="name">endif</span></span> %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="name">endfor</span></span> %&#125;</span><span class="xml"></span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>is_home()是next主题的方法，用于判断当前页面是否在主页。因为主页一次性加载了所有的文章，如果不加这个方法，会在主页弹出无数个弹窗。效果如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/%E6%8F%90%E7%A4%BA%E5%BC%B9%E7%AA%97%E6%95%88%E6%9E%9C%E5%9B%BE.jpg" alt="提示弹窗效果图"><br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/运用其他的插件，在hexo中添加提示弹窗.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/bug/Android开发，使用腾讯云的API请求对象存储中的资源始终失败.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/bug/Android开发，使用腾讯云的API请求对象存储中的资源始终失败.html" class="post-title-link" itemprop="url">Android开发，使用腾讯云的API请求对象存储中的资源始终失败</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-17 10:55:07 / 修改时间：12:05:37" itemprop="dateCreated datePublished" datetime="2019-04-17T10:55:07+08:00">2019-04-17</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/android/" itemprop="url" rel="index"><span itemprop="name">android</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <ol>
<li>腾讯云api内部在调用时，把url转义了。我的链接是<a href="http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt?abcdefg，它内部给我转义成http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt%3Fabcdefg，就是把&quot;?&quot;转义成了&quot;%3F&quot;。总而言之，我使用api一直获取不到资源，然后我在浏览器上试验了一下。发现把%3F改回?就可以访问了，实际上应该不是这样，反正就给我产生了误导。我想尽办法都不能将其转义回来，最后只好放弃。腾讯云api内部肯定自己转义了一下，真的坑爹。" target="_blank" rel="noopener">http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt?abcdefg，它内部给我转义成http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt%3Fabcdefg，就是把&quot;?&quot;转义成了&quot;%3F&quot;。总而言之，我使用api一直获取不到资源，然后我在浏览器上试验了一下。发现把%3F改回?就可以访问了，实际上应该不是这样，反正就给我产生了误导。我想尽办法都不能将其转义回来，最后只好放弃。腾讯云api内部肯定自己转义了一下，真的坑爹。</a></li>
<li>尝试自己写代码请求资源，结果发现如果url的协议是https就可以访问到资源了。这可能是android的问题，于是我又使用腾讯的api，配置更改如下：<figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">CosXmlServiceConfig</span> serviceConfig = <span class="function"><span class="keyword">new</span> <span class="title">Builder</span>()</span></span><br><span class="line"><span class="function">				.<span class="title">isHttps</span>(true)</span></span><br><span class="line"><span class="function">                .<span class="title">setRegion</span>(region)</span></span><br><span class="line"><span class="function">                .<span class="title">setDebuggable</span>(true)</span></span><br><span class="line"><span class="function">                .<span class="title">builder</span>();</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>将isHttps设为ture，协议就改为了https。可是又报了另一个错：The specified key does not exist.它说我密钥不存在。</p>
<ol>
<li>如果使用http协议访问就会说无法解析域名，总之用腾讯云的api无法访问到资源就对了。</li>
<li>放弃腾讯云的api，自己手写代码去请求资源！</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/bug/Android开发，使用腾讯云的API请求对象存储中的资源始终失败.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/对神经网络整体的理解.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/对神经网络整体的理解.html" class="post-title-link" itemprop="url">对神经网络整体的理解</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-12 20:12:28" itemprop="dateCreated datePublished" datetime="2019-04-12T20:12:28+08:00">2019-04-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-07 10:25:57" itemprop="dateModified" datetime="2019-08-07T10:25:57+08:00">2019-08-07</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>本文疑问的总结地址<a href="https://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">在这</a></p>
          </div>
<h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th>描述的内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2~4</td>
<td>神经网络和深度学习的发展史。</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>从二元分类开始。</td>
</tr>
<tr>
<td style="text-align:center">6~10</td>
<td>浅层神经网络的介绍。如神经网络中一些参数代表的意思、激活函数、梯度下降、随机初始化。</td>
</tr>
<tr>
<td style="text-align:center">11~15</td>
<td>深层神经网络的介绍。正向传播和反向传播中向量化后的计算、参数和超参数、神经网络和大脑的关系。</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td>一个Simple NN的例子。</td>
</tr>
<tr>
<td style="text-align:center">17~22</td>
<td>深度学习的实用性层面。数据切分、偏差与方差、正则化、dropout、其他正则化方法、均值归一化、梯度消失和梯度爆炸、梯度检验。</td>
</tr>
<tr>
<td style="text-align:center">23~25</td>
<td>一些优化算法。Mini-batch、指数加权平均、Momentum、RMSprop、Adam、Adagrad。</td>
</tr>
<tr>
<td style="text-align:center">26~29</td>
<td>超参数调试、Batch正则化、激活函数以及一些深度学习框架。</td>
</tr>
<tr>
<td style="text-align:center">30~end</td>
<td>本文略长，后序的文章请看对应章节的链接。</td>
</tr>
</tbody>
</table>
</div>
<h1 id="神经网络和深度学习的发展"><a href="#神经网络和深度学习的发展" class="headerlink" title="神经网络和深度学习的发展"></a>神经网络和深度学习的发展</h1><p>TODO</p>
<h1 id="神经网络和深度学习的关系"><a href="#神经网络和深度学习的关系" class="headerlink" title="神经网络和深度学习的关系"></a>神经网络和深度学习的关系</h1><p>TODO</p>
<h1 id="为什么要深度学习"><a href="#为什么要深度学习" class="headerlink" title="为什么要深度学习"></a>为什么要深度学习</h1><p>TODO</p>
<h1 id="从二元分类开始"><a href="#从二元分类开始" class="headerlink" title="从二元分类开始"></a>从二元分类开始</h1><p>暂时省略，因为这里已经会了。</p>
<h1 id="神经网络的表示"><a href="#神经网络的表示" class="headerlink" title="神经网络的表示"></a>神经网络的表示</h1><p>规定如下，l：第几层；w：权重值；b：偏差；z：输出值；a：激活值；i，j：都代表第几个神经元，如<script type="math/tex">w^l_i</script>代表第l层的第i个权重值；W：向量化后的权重值；Z：向量化后的输出值；A：向量化后的激活值；<script type="math/tex">\alpha</script>：学习速率；<script type="math/tex">\lambda</script>：正则化项；</p>
<p>如果输出值z和激活值a无法理解或者区分，没关系，继续往下看就知道了。<br>如下图所示，一般规定input layer为第0层，不算入神经网络的层数中，所以下图是一个三层神经网络架构。</p>
<ol>
<li>input layer的输入值被称为x，下图一共有三个输入所以分别被称为<script type="math/tex">x_1\ x_2\ x_3</script>。为了方便起见，可以将input layer的值x以<script type="math/tex">a^0</script>来代替，下面解释a代表什么。</li>
<li>hidden layer中的值被称为a——<strong>激活值</strong>（activations），图中有四个神经元，所以分别被称为<script type="math/tex">a^1_1\ a^1_2\ a^1_3</script>，上标代表着所在神经网络中的第几层，下标代表着所在层中的第几个神经元。如果表示成向量形式就是<script type="math/tex; mode=display">
\begin{pmatrix}
 x_1\\
 x_2\\
 x_3\\
\end{pmatrix} = 
\begin{pmatrix}
 a^0_1\\
 a^0_2\\
 a^0_3\\
\end{pmatrix} 和
\begin{pmatrix}
 a^1_1\\
 a^1_2\\
 a^1_3\\
 a^1_4\\
\end{pmatrix} 和
\begin{pmatrix}
 a^2_1\\
 a^2_2\\
 a^2_3\\
 a^2_4\\
\end{pmatrix} 和
\begin{pmatrix}
 a^3_1\\
\end{pmatrix}</script></li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%8E%9F%E5%9B%BE.jpg" alt="神经网络架构原图"></p>
<h2 id="神经网络中神经元的一些参数的含义，特别解释w的含义"><a href="#神经网络中神经元的一些参数的含义，特别解释w的含义" class="headerlink" title="神经网络中神经元的一些参数的含义，特别解释w的含义"></a>神经网络中神经元的一些参数的含义，特别解释w的含义</h2><p>hidden layer和output layer的每个神经元都有几个参数。分别为<script type="math/tex">w^l\ b^l</script>，对照上图，这里的<script type="math/tex">w^l</script>是一个(4,3)的矩阵，<script type="math/tex">b^l</script>是一个(4,1)的向量。解释如下：</p>
<script type="math/tex; mode=display">
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4\\
\end{cases}</script><p>可以看到一个公式中有三个w和一个b，一共有四个公式。<script type="math/tex">w^l_{ij}</script>代表第l-1层的第j个神经元到第l层的第i个神经元上的w。如<script type="math/tex">w^1_{12}</script>代表第0层的第2个神经元到第1层的第1个神经元上的w。注意这里的i和j实际上是与直觉相反的，也就是说按直觉来看应该是<script type="math/tex">w^l_{ji}</script>才正常。如果对w的表示有疑惑的，可以看<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%ADweight%E7%9A%84%E8%A1%A8%E7%A4%BA%E9%97%AE%E9%A2%98.html">这篇</a>。<br>注意下这里的z是<strong>输出值</strong>，之前一直在说hidden layer中的值是a——激活值，其实a就是将z放到一个<strong>激活函数</strong>（activation function）中得到的一个值，这个激活函数是随用户挑选的，如果不能理解激活函数是什么，就暂时理解为激活函数自己想设成什么就设成什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg" alt="神经网络架构图"></p>
<h1 id="神经网络中的输出是怎么计算的"><a href="#神经网络中的输出是怎么计算的" class="headerlink" title="神经网络中的输出是怎么计算的"></a>神经网络中的输出是怎么计算的</h1><h2 id="以一个样本为例"><a href="#以一个样本为例" class="headerlink" title="以一个样本为例"></a>以一个样本为例</h2><p>第0层是输入层，所以是不需要计算的，x我本来就有，我还计算什么？对吧。从hidden layer1开始到output layer每一层都需要计算一连串的值，下面给出第一层的计算公式：</p>
<script type="math/tex; mode=display">
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1，a^1_1 = \sigma(z^1_1)\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2，a^1_2 = \sigma(z^1_2)\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3，a^1_3 = \sigma(z^1_3)\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4，a^1_3 = \sigma(z^1_4)\\
\end{cases}</script><p>这里的<script type="math/tex">\sigma(z)</script>函数其实就是上面说的<strong>激活函数</strong>，一般来讲<script type="math/tex">\sigma</script>这个符号特指sigmoid function: <script type="math/tex">\frac{1}{1+e^{-z}}</script>。<br>这4行公式其实在上面已经给出部分，每一行包含两个公式，也就是说一个神经元中实际上先得到了z，然后再通过激活函数将z转为a。这里可能会有疑惑，已经得到z了为什么还要用一个函数将z转为a呢？这样不是毫无意义？下面有一部分会具体解释，也可以看下面几篇的解释：<br><a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">神经网络激励函数的作用是什么？有没有形象的解释？</a><br><a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a><br>现在回到本文，正如我上面所说，我一共写了四个公式（激活函数现在暂时不看），所以我要分别计算四个公式，也就是说要计算四次。那么有没有办法只计算一次就得到所有结果呢？答案是<strong>向量化</strong>（vectorization），现在开始用向量化来解决这个问题。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    z^1_1\\
    z^1_2\\
    z^1_3\\
    z^1_4\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&w^1_{12}&w^1_{13}\\
    w^1_{21}&w^1_{22}&w^1_{23}\\
    w^1_{31}&w^1_{32}&w^1_{33}\\
    w^1_{41}&w^1_{42}&w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^0_1\\
    a^0_2\\
    a^0_3\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^1 = w^1 * a^0 + b^1</script><p>以下是整个神经网络的计算过程，也就是说只需要下面6行就可以代替上文占据几个屏幕的内容。</p>
<script type="math/tex; mode=display">
\begin{array}{c|}
    z^1 = w^1 * a^0 + b^1\\
    a^1 = \sigma(z^1)\\
    z^2 = w^2 * a^1 + b^2\\ 
    a^2 = \sigma(z^2)\\
    z^3 = w^3 * a^2 + b^3\\
    a^3 = \sigma(z^3)\\ 
\end{array} =>记为P</script><p>最后一个a就是整个神经网络的输出值，也就是预测值（prediction），也可以用<script type="math/tex">\hat{y}</script>表示，自然<script type="math/tex">\hat{y} = a^3</script>。</p>
<h2 id="向量化计算多个样本"><a href="#向量化计算多个样本" class="headerlink" title="向量化计算多个样本"></a>向量化计算多个样本</h2><p>上面我没有特意地说明其实我们只使用了一个样本，我们一直在使用<script type="math/tex">a^0_1\ a^0_2\ a^0_3</script>，但是<script type="math/tex">a^0_1\ a^0_2\ a^0_3</script>实际上只是<strong>一个</strong>样本。<script type="math/tex">a^0</script>代表的是一个样本，<script type="math/tex">a^0_1</script>代表的是样本中的第一个特征，如果不明白我可以举个例子：<script type="math/tex">a^0_1</script>代表天气样本中的第一个特征——温度，<script type="math/tex">a^0_2</script>代表湿度，<script type="math/tex">a^0_3</script>代表PM2.5，<script type="math/tex">a^0</script>代表整一个天气样本。<br>那么如果有成千上万个样本，总不能使用P计算成千上万次吧。这里再次使用向量化进行计算。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    z^{11}_1&z^{12}_1&\cdots\\
    z^{11}_2&z^{12}_2&\cdots\\
    z^{11}_3&z^{12}_3&\cdots\\
    z^{11}_4&z^{12}_4&\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&w^1_{12}&w^1_{13}\\
    w^1_{21}&w^1_{22}&w^1_{23}\\
    w^1_{31}&w^1_{32}&w^1_{33}\\
    w^1_{41}&w^1_{42}&w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{01}_1&a^{01}_1&\cdots\\
    a^{01}_2&a^{02}_1&\cdots\\
    a^{01}_3&a^{03}_1&\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^1 = w^1 * a^0 + b^1</script><script type="math/tex; mode=display">
\begin{pmatrix}
    z^{21}_1&z^{22}_1&\cdots\\
    z^{21}_2&z^{22}_2&\cdots\\
    z^{21}_3&z^{22}_3&\cdots\\
    z^{21}_4&z^{22}_4&\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^2_{11}&w^2_{12}&w^2_{13}&w^2_{14}\\
    w^2_{21}&w^2_{22}&w^2_{23}&w^2_{24}\\
    w^2_{31}&w^2_{32}&w^2_{33}&w^2_{34}\\
    w^2_{41}&w^2_{42}&w^2_{43}&w^2_{44}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{11}_1&a^{11}_1&\cdots\\
    a^{11}_2&a^{12}_1&\cdots\\
    a^{11}_3&a^{13}_1&\cdots\\
    a^{11}_3&a^{14}_1&\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^2_1\\
    b^2_2\\
    b^2_3\\
    b^2_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^2 = w^2 * a^1 + b^2</script><p>省略号代表后面有无数个样本，同理矩阵相乘也可以只用一个字母表示。上标的第二个数字代表是第几个样本，第一个数字依旧是代表所属第几层。</p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a></p>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p>上文中我们一直假设使用sigmoid function作为激活函数。但是事实上还有很多其他选择，甚至其他的激活函数比sigmoid funtion效果要更好。<br>上面讲过<script type="math/tex">\sigma(z)</script>特指sigmoid function，现在我们将表达式改为：<script type="math/tex">a = g(z)</script>，用g来表示激活函数，它可以是线性的，也可以是非线性的。<br>引用吴恩达在深度学习视频中的话：</p>
<blockquote>
<p>有一个函数总是比sigmoid function表现得更好，就是tanh函数或者叫双曲正切函数，公式为：<script type="math/tex">\frac{e^z-e^{-z}}{e^z+e^{-z}}</script>，在数学上实际是<script type="math/tex">\sigma</script>函数平移后的版本。<br>事实证明，如果将<script type="math/tex">g(z)</script>选为tanh函数，效果几乎总比<script type="math/tex">\sigma(z)</script>函数要好。</p>
</blockquote>
<p>有一个例外是output layer，它还是使用sigmoid funtion，因为output layer跟普通的分类问题没什么区别，它要得到0~1之间的一个概率。<br>sigmoid function的值总是位于0~1之间，tanh function的值总是位于-1~1之间。</p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>但是不管是<script type="math/tex">\sigma</script>或者tanh函数都一个缺点，那就是当z非常大或者非常小时，函数的斜率（导数的梯度）很小。这样会拖慢梯度下降。在机器学习中还有一个函数，即ReLU函数——Rectified Linear Unit，表达式为<script type="math/tex">max(0, z)</script>。<br>所以在选择激活函数时有一些经验法则：</p>
<ol>
<li>如果你的输出值是0或1，那么<script type="math/tex">\sigma</script>函数很适合做output layer的激活函数，非二元分类的情况下使用tanh函数几乎都比<script type="math/tex">\sigma</script>优越。藏层单元全用ReLU函数，现在ReLU函数已经是隐藏层的默认激活函数了，大多数人都这么做。</li>
<li>还有个叫Leaky ReLU的函数比ReLU稍微好点，但是目前暂时不是很多人用。</li>
</ol>
<h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>Sogmoid函数<script type="math/tex">\sigma = \frac{1}{1 + e^{-z}}</script>适用于二元分类，那么碰到多元分类怎么么办呢？Softmax函数就可以解决这个问题。<br>Softmax函数计算步骤如下，假设是n元分类：</p>
<script type="math/tex; mode=display">
Z^L = W^L * A^{L-1} + b^L\\
t = e^{Z^L}\\
A^L = \frac{e^{Z^L}}{\sum^n_{i=1}t_i},\quad A^L_i = \frac{t_i}{\sum^n_{i=1}t_i}\\</script><p>多元分类中每一个神经元代表对应标签的概率是多少，并且将概率相加等于1。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/softmax%E4%BE%8B%E5%AD%90.jpg" alt="softmax例子"></p>
<h2 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h2><div class="note primary">
            <p>为什么<script type="math/tex">w*x+b=z</script>之后，需要使用一个激活函数<script type="math/tex">a = \sigma(z)</script>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
          </div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。<br>那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。</p>
<script type="math/tex; mode=display">
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w' * a^0 + b' \qquad由于w和b只是一堆常数，所以将w'代替w^2 * w^1，b'同理
    \end{array}
\right.</script><p>通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<script type="math/tex">a^2 = w' * a^0 + b'</script>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。<br>只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降，反向传播算法——backpropagation解析"><a href="#梯度下降，反向传播算法——backpropagation解析" class="headerlink" title="梯度下降，反向传播算法——backpropagation解析"></a>梯度下降，反向传播算法——backpropagation解析</h1><div class="note primary">
            <p>首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？</p>
          </div>
<p>本节的示例均建立在一个样本的情况下，如果是多个样本经过神经网络，可能略微不同。我看了吴恩达老师的深度学习课程，发现多个样本与一个样本的区别，可能只在偏差b那里会有点不同。<br>下图以一个三层神经网络为例，说明正向与反向传播过程。由于神经元之间的链接太多会导致混乱，所以下图只链接了第一个神经元。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="使用导数解释反向传播算法"><br>下图略微简化一下反向传播算法中的导数项，并且完成了最后的权重值优化。值得注意的是：如果cost function不同，下面求导结果会略微不同，本文统一使用<script type="math/tex">cost = \frac{1}{m} * \sum{(\hat{y} - y)^2}</script>，但是神经网络一般是使用<strong>交叉熵</strong>——crossentropy，其公式为：<script type="math/tex">cost = -\frac{1}{m} * (y * log(\hat{y}) + (1 - y) * log(1 - \hat{y}))</script>。使用前者，<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}} = (a^{(3)} - y) * g'(z^{(3)})</script>；如果使用后者，<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}} = a^{(3)} - y</script>。可以看到使用两个不同的代价函数，会有不同的结果，这是因为两个函数求导的结果不一样。而两者对表达式<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}}</script>的结果只差了一个<script type="math/tex">g'(z^{(3)})</script>，这完全是巧合罢了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%AE%80%E5%8C%96%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="简化反向传播算法"><br><strong>另外再提醒一下自己，这里全是以一个样本为例。但是仅仅这样权重已经是一个二维矩阵了，要是如果传入多个样本，权重岂不是是一个三维矩阵？然而不管传入几个样本权重实际上对于不同的样本是没有变化的，所以还是二维矩阵。</strong></p>
<h1 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h1><p>对于逻辑回归可以将<strong>权重</strong>（weight）全部初始化为0，但是对于神经网络来说，将个权重初始化为0，再使用梯度下降会完全无效。实际上将偏差b初始化为0是可以的，但是权重不行。<br>解释起来太麻烦，详情看吴恩达深度学习——01神经网络和深度学习第三周浅层神经网络，3.11随机初始化。吴恩达老师解释地还是很清楚的。<br>可以像以下这样设置weight：<script type="math/tex">w^l = np.random.randn((2, 2)) * 0.01</script>//这可以产生参数为(2, 2)的高斯分布随机变量，后面再成一个很小的数，比如0.01。而对于b，之前说了初始化为0也可以。<br>对于上式的0.01可能会感到很疑惑，为什么要乘这么一个值。因为我们一般将weight初始化为很小的值，如果weight值很大，最终导致z也很大，那么会落在sigmoid function或者tanh function的平缓部分，会使梯度的写了很小，意味着梯度下降算法会非常慢，所以学习得很慢。</p>
<h2 id="初始化补充"><a href="#初始化补充" class="headerlink" title="初始化补充"></a>初始化补充</h2><p>经在作业中做的测试得出如下结论：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Train accuracy</strong></th>
<th><strong>Problem/Comment</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>3-layer NN with <strong>zeros initialization</strong></td>
<td>50%</td>
<td>fails to break symmetry</td>
</tr>
<tr>
<td>3-layer NN with large <strong>random initialization</strong></td>
<td>83%</td>
<td>too large weights</td>
</tr>
<tr>
<td>3-layer NN with <strong>He initialization</strong></td>
<td>99%</td>
<td><strong>recommended method</strong></td>
</tr>
</tbody>
</table>
</div>
<p>其中”He initialization”最近（论文是2015年的）新搞出来得初始化算法，现在推荐使用此算法进行初始化。</p>
<h1 id="核对矩阵维数"><a href="#核对矩阵维数" class="headerlink" title="核对矩阵维数"></a>核对矩阵维数</h1><p>w的维数应该与dw的维数相同。b和db的维数相同</p>
<h1 id="为什么使用深度表示——Why-deep-representations"><a href="#为什么使用深度表示——Why-deep-representations" class="headerlink" title="为什么使用深度表示——Why deep representations"></a>为什么使用深度表示——Why deep representations</h1><p>引用在2017course深度学习课程上吴恩达老师的话</p>
<blockquote>
<p>深度神经网络能解决很多问题，其实并不需要很大的神经网络，但是得有深度。得有比较多的隐藏层。</p>
</blockquote>
<p>为什么深度神经网络会很好用？</p>
<ol>
<li>深度神经网络到底在计算什么？假设现在在做一个人脸识别系统。那么神经网络的第一层会去找照片里的边缘部分；第二层会去识别人类的特征，比如耳朵，鼻子，嘴巴；第三层会去识别不同的人脸。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%9B%B4%E8%A7%82%E6%98%A0%E5%83%8F.jpg" alt="深度表示的直观映像"><br>这种识别模式可能难以理解，但是会在卷积神经网络——Convolutional Neural Network中详细解释。<br>这视频的这一章节有点难以总结，可以看看<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>，总共也就10分钟。</li>
</ol>
<h1 id="深层神经网络块"><a href="#深层神经网络块" class="headerlink" title="深层神经网络块"></a>深层神经网络块</h1><p>此视频中画出了深度神经网络的<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701023" target="_blank" rel="noopener">代码流程</a>。</p>
<h1 id="参数VS超参数"><a href="#参数VS超参数" class="headerlink" title="参数VS超参数"></a>参数VS超参数</h1><p>有如下超参数（hyperparameters）：W, b, lerning rate <script type="math/tex">\alpha</script>, iterations, hidden layer L, hidden units, choice of activatation function.这些超参数都需要自己设置。<br>上面这些都是基础的，实际上还有其他的超参数，稍后会涉及到。 </p>
<h1 id="神经网络和大脑有什么关系？"><a href="#神经网络和大脑有什么关系？" class="headerlink" title="神经网络和大脑有什么关系？"></a>神经网络和大脑有什么关系？</h1><p>计算机视觉、其他深度学习领域或者其他学科在早期可能都受过人类大脑的启发，但是近年来人类将神经网络类比为大脑的次数越来越少，也就是说近年来大家都不怎么认为这二者有关联。</p>
<h1 id="一个Simple-NN的例子"><a href="#一个Simple-NN的例子" class="headerlink" title="一个Simple NN的例子"></a>一个Simple NN的例子</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">Simple Neural Network</a>例子</p>
<hr>
<p>本节以下开始利用算法改善深层神经网络</p>
<hr>
<h1 id="训练-开发-测试集"><a href="#训练-开发-测试集" class="headerlink" title="训练/开发/测试集"></a>训练/开发/测试集</h1><p>训练集——training set<br>开发集/交叉验证集/验证集——dev set/cross validation set/validation set<br>测试集——test set</p>
<p>以前数据量小的时候，比如100个样本、10000个样本。一般将数据按三七分，七份训练集，三份测试集。验证集（以下均称验证集）在训练集中再细分，比如二八分，八份训练集。<br>但是现在进入大数据时代，验证集和测试集已经没有必要占大量比例了。比如现在有100万的样本，那么验证集和测试集只需要各抽取大约10000的样本即可。也就是98/1/1的比例，甚至验证集和测试集可以再降低占比。</p>
<h2 id="训练集和验证集-测试集分布不匹配"><a href="#训练集和验证集-测试集分布不匹配" class="headerlink" title="训练集和验证集/测试集分布不匹配"></a>训练集和验证集/测试集分布不匹配</h2><p>如下图，吴恩达老师建议最好让<strong>验证集</strong>和<strong>测试集</strong>匹配，即来自同一源，要都来自网络高清图，要么都来自手机低像素拍摄。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E5%BC%80%E5%8F%91%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%86%E5%B8%83%E4%B8%8D%E5%8C%B9%E9%85%8D.jpg" alt="训练集和验证集测试集分布不匹配"><br>如果直接不设置测试集也是可以的。</p>
<h1 id="偏差-方差"><a href="#偏差-方差" class="headerlink" title="偏差/方差"></a>偏差/方差</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%86%B3%E7%AD%96%E7%95%8C%E9%99%90.jpg" alt="欠拟合和过拟合的决策界限"></p>
<p>下图讲述了什么是<strong>过拟合</strong>，什么是<strong>欠拟合</strong>，如图所示，该神经网络用于判断一张图片是猫还是狗。<br>左边。训练样本中的误差为1%，这个值已经很小了，但是在验证集上的误差有11%。这就代表了过拟合，试想一下，在训练集上误差很小是因为你的决策界限划分的很好，在上图中的最后一个例子，整条决策界限画的十分完美，但是我们要知道在验证集中，这样一条完美的线肯定不能再拟合的很好。因为训练集和验证集即使来源于同一份数据，他们之间的分布也是不一样的，你训练出一条完美的曲线，在另一份数据集上肯定是过于完美了。所以导致了下图中验证集上的误差有11%。我们称这种情况为<strong>高方差</strong>——high variance。<br>中间。训练样本中的误差为15%，这已经不需要再看验证集上的误差了。因为训练集上的误差那么大，肯定是没有拟合好，所以这就是欠拟合，我们称为<strong>高偏差</strong>——high bais。<br>右边。如果训练集中的误差很高，验证集上的误差更高，那么可以判断为同时具有高方差和高偏差。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88.jpg" alt="欠拟合和过拟合"><br>如果训练集上的误差为0.5%，验证集上的误差为1%。那这就是低方差和低偏差，这是很好结果。<br>最后一点，以上均建立在人眼判断的误差为0%上以及训练集和验证集来自相同分布。如果人眼判断的误差也高达15%，那么中间的例子也算是可以的结果一般来说<strong>最优误差</strong>也被称为<strong>贝叶斯误差</strong>。<br>关于上图同时高方差和高方差，就如同下图紫色线条的决策界限一般。过渡拟合了数据，但是拟合的数据其实狗屁不通。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E5%90%8C%E6%97%B6%E9%AB%98%E6%96%B9%E5%B7%AE%E5%92%8C%E9%AB%98%E5%81%8F%E5%B7%AE%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84.jpg" alt="同时高方差和高偏差是怎么样的"></p>
<h2 id="机器学习遇到偏差或方差的解决办法"><a href="#机器学习遇到偏差或方差的解决办法" class="headerlink" title="机器学习遇到偏差或方差的解决办法"></a>机器学习遇到偏差或方差的解决办法</h2><div class="note info">
    <p>笔记中都记了，懒得再写一遍了。补充一点，遇到偏差或方差都可以更换神经网络架构，比如换成CNN或者RNN，如果是高偏差还可以使用更大的神经网络。</p>
</div>

<p>可以看这个6分半中的小视频，<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702115" target="_blank" rel="noopener">机器学习基础</a>。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>如果出现了过拟合，即高方差的情况，第一件想到的事是<strong>正则化</strong>——regularization。当然也可以增加数据，不过有时候数据不是那么容易获取的。可以对W使用<a href="https://www.baidu.com/s?wd=L2%E8%8C%83%E6%95%B0" target="_blank" rel="noopener">L2范数</a>进行正则化，当然对b也可以进行L2范数正则化，不过一般不加。L2范数的公式为<script type="math/tex">||w||^2_2 = \sum_{j=1}^n w^2_j = W^T * W</script><br>因此代价函数修改为<script type="math/tex">cost = \frac{1}{m} \sum^m_{i=1} g(\hat{y}^i, y^i) + \frac{\lambda}{2m}||w||^2_2</script>，<script type="math/tex">\lambda</script>是正则化的超参数。这里的w实际上是一个二维矩阵，所以L2范数需要把里面的每一个值的平方都加起来。<br>如果加入了正则化项，那么在计算dW时有点变化。将会变为：<script type="math/tex">dW = dZ * A\_prev + \frac{\lambda}{m} w ^ l</script></p>
<h2 id="为什么正则化可以防止过拟合"><a href="#为什么正则化可以防止过拟合" class="headerlink" title="为什么正则化可以防止过拟合"></a>为什么正则化可以防止过拟合</h2><p>略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702116" target="_blank" rel="noopener">1.5 为什么正则化可以减少过拟合？</a></p>
<h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><div class="note primary">
            <p>那么问题又来了，dropout背后的原理是什么？</p>
          </div>
<p>dropout，中文翻译为<strong>随机失活</strong>。<br>先将神经网络复制一遍，然后dropout会遍历神经网络的每一层，并设置消除神经网络中结点的概率，比如设置0.5。下图的带X的结点就是准备消除的。另外每一层的概率都可以是不同的，如果在某一层不担心会过拟合可以将概率设为1.0，比如输出层。如果觉得某些层比其他层更容易过拟合，可以把那些层的keep-prob设置的更低。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/dropout%E5%BE%85%E5%88%A0%E9%99%A4%E7%BB%93%E7%82%B9.jpg" alt="dropout待删除结点"><br>下图则是消除后的神经网络。将结点的进出的链接全部删除。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8dropout%E5%90%8E%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg" alt="使用dropout后的神经网络"><br>dropout使用之后，就让一个样本进入神经网络进行训练。而对于其他样本也如法炮制，需要再进行复制一遍神经网络，并进行dropout。<br>以上均是逻辑上的做法，接下来讲实际编码该怎么做。</p>
<ol>
<li>设置一个结点保留的概率——keep-prob，假设为0.8。<script type="math/tex">d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) < keep-prob</script>，这样会得到一个True和False的数组，但是python中Ture等于1，False等于0。</li>
<li>让<script type="math/tex">a^3</script>乘上这个向量。<script type="math/tex">a^3 = np.multiply(a^3, d^3)</script>。由于False等于0，所以变相地将<script type="math/tex">a^3</script>中的值失活了。</li>
<li>最后一步看起来有点奇怪，<script type="math/tex">a^3 /= keep-prob</script>。<br>完整代码如下：<script type="math/tex; mode=display">
\begin{cases}
 d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) < keep-prob\\
 a^3 = np.multiply(a^3, d^3)\\
 a^3 /= keep-prob\\
\end{cases}</script></li>
</ol>
<p>对于最后一步，由于<script type="math/tex">Z^4 = W^4 * A^3 + b^4</script>，由于<script type="math/tex">A^3</script>被dropout减少0.2，为了使得<script type="math/tex">Z^4</script>不受影响，所以对<script type="math/tex">A^3</script>除0.8，来保证<script type="math/tex">A^3</script>的值不变。由于早期的版本没有除于keep-prob，使得测试阶段，平均值越来越复杂。<br>最后，从技术上来讲，输入值也可以使用dropout，但是基本不这么做，直接把keep-prob设为1.0即可，当然0.9也可以。不过太低的值一般不会去设置。<br>以上的步骤被称为<strong>Inverted dropout</strong>——<strong>反向随机失活</strong>。<br>dropout在计算机视觉中用的非常多，甚至成了标配。但要记住一点，dropout是一种正则化方法，为了预防过拟合。所以除非算法过拟合，不然不会使用dropout。由于计算机视觉的特殊性，他们才经常用dropout。<br>dropout的缺点是使我们失去了代价函数这一调试功能。我们经常使用代价函数得到误差，从而画出曲线图。但是使用dropout之后，这样的曲线图就不再准确了。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在测试阶段不再使用dropout，因为我们不希望输出结果是随机的，如果使用dropout预测会受到干扰。</p>
<h3 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h3><div class="note primary">
    <p>略。有点晦涩。</p>
</div>

<p>看<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a>。。</p>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ol>
<li>Data augment——数据增强。如果拟合猫咪图片分类器，可以对原图片做一些处理，来增加数据，比如翻转、旋转、随机裁剪等。</li>
<li>Early stopping。在训练时画出代价的曲线图，x轴为迭代次数，再绘制验证时的误差。然后选择验证误差曲线图中最低点的迭代次数，下次训练时就改用这个迭代次数，或者也可以在程序中写一个条件判断。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/early%20stopping.jpg" alt="early stopping"></li>
</ol>
<h1 id="均值归一化输入"><a href="#均值归一化输入" class="headerlink" title="均值归一化输入"></a>均值归一化输入</h1><div class="note info">
    <p>略。其实很简单。</p>
</div>

<h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702118&amp;cid=2001699114" target="_blank" rel="noopener">视频</a><br><a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另外一个参考视频</a>，08:37开始。<br><a href="https://www.bilibili.com/video/av10590361/?p=37" target="_blank" rel="noopener">另一个</a>13:50~18左右</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701047" target="_blank" rel="noopener">视频</a></p>
<h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>Gradient checking(Grad check).<br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701048" target="_blank" rel="noopener">原理视频</a><br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702119" target="_blank" rel="noopener">实战视频</a><br>梯度检验可以帮助我们发现神经网络中的一些bug。具体原理是，通过数学上导数的定义来确认反向传播算法是否正确。如果学过高数就会知道，使用导数的定义求解和直接使用公式求解，两者结果十分接近或者一模一样。如果二者不一样说明肯定是求错了。<br>对应于神经网络，那就肯定是代码写错了。具体操作可在视频中看见，每个视频都不超过10分钟。</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果梯度检验确实发现问题，要检查每一项，看看是哪个i的w和b有问题。</li>
<li>记得正则化项，它也被包含在w的梯度中。</li>
<li>梯度检验不能和dropout一起用。</li>
<li><del>在随机初始化时就运行一遍梯度检验；或许在训练一会后可以再运行一遍梯度检验。当W和b接近于0时，梯度下降正确执行在现实中几乎不太可能。</del>吴恩达老师说这条他在现实中几乎不会这么做，并且第五条的翻译，个人感觉翻得有问题，然后看了英文原文后，感觉原文表达得也不是很好，我看不太懂，所以这条就不算进注意事项了。</li>
</ol>
<h1 id="Mini-batch梯度下降"><a href="#Mini-batch梯度下降" class="headerlink" title="Mini-batch梯度下降"></a>Mini-batch梯度下降</h1><div class="note primary">
            <p>为什么Mini-batch比普通的梯度下降快？</p>
          </div>
<p>普通的梯度下降——vanilla gradient descent，是将整个数据集同时做运算，而Mini-batch梯度下降算法是以一组为单位，分别进行梯度下降，所有组执行完毕后再进行下一次迭代。<br>假设现在有m个样本。</p>
<script type="math/tex; mode=display">
X = \begin{pmatrix}x^1&x^2&x^3&\cdots&x^m\end{pmatrix}\\
Y = \begin{pmatrix}y^1&y^2&x^3&\cdots&y^m\end{pmatrix}\\</script><p>使用Mini-batch，假设每1000个样本为一组：</p>
<script type="math/tex; mode=display">
X = \begin{pmatrix}\underbrace{x^1\cdots x^{1000}}_{X^{\{1\}}} & \underbrace{x^{1001}\cdots x^{2000}}_{X^{\{2\}}} & \cdots&\underbrace{\cdots x^m}_{X^{\{t\}}}\end{pmatrix}\\
Y = \begin{pmatrix}\underbrace{y^1\cdots y^{1000}}_{Y^{\{1\}}} & \underbrace{y^{1001}\cdots y^{2000}}_{Y^{\{2\}}} & \cdots&\underbrace{\cdots y^m}_{Y^{\{t\}}}\end{pmatrix}\\</script><p>如果使用代码实现就是类似下面这样的伪代码：<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>, ..., t</span><br><span class="line">	forwardprop <span class="keyword">on</span> X^&#123;t&#125;</span><br><span class="line">	compute cost</span><br><span class="line">	backprop <span class="keyword">to</span> compute grads</span><br><span class="line">	update weights <span class="keyword">and</span> bais</span><br></pre></td></tr></table></figure></p>
<p>for循环完成之后就完成了神经网络的第一次迭代。</p>
<h2 id="理解mini-batch"><a href="#理解mini-batch" class="headerlink" title="理解mini-batch"></a>理解mini-batch</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/Mini-batch%E5%92%8C%E6%99%AE%E9%80%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="Mini-batch和普通梯度下降的区别"></p>
<ol>
<li>如果将batch设为m，那它就是普通的梯度下降算法。</li>
<li>如果将batch设为1，就叫做随机梯度下降——SGD</li>
<li>batch在1到m之间就是mini-batch</li>
</ol>
<p>SGD和普通梯度下降的区别，“+”代表代价最小点。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/SGD%E5%92%8Cvanilla%20gradient%20descent%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="SGD和vanilla gradient descent的区别"><br>SGD和mini-batch的区别。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/SGD%E5%92%8Cmini-batch%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="SGD和mini-batch的区别"></p>
<p><strong>应该记住的有：</strong></p>
<ol>
<li>普通梯度下降、mini-batch和SGD之间的区别就是执行一次参数更新所需的样本数量不同。</li>
<li>你需要自己调整学习速率<script type="math/tex">\alpha</script>。</li>
<li>当mini-batch的量调整良好时，它通常优于普通梯度下降和SGD（尤其是在训练集特别大时）。</li>
</ol>
<h2 id="mini-bacth实现步骤"><a href="#mini-bacth实现步骤" class="headerlink" title="mini-bacth实现步骤"></a>mini-bacth实现步骤</h2><ol>
<li>打乱数据。创建一个打乱数据之后的副本，其中X和Y的每一列都代表一个训练样本。注意X和Y是同步地随机打乱样本，即X中第<script type="math/tex">i^{th}</script>个样本和Y中第<script type="math/tex">i^{th}</script>标签在打乱之后还是是对应的。此步骤确保样本被随机地分割到不同的mini-batches中。下图是步骤示意图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E7%AC%AC%E4%B8%80%E6%AD%A5%E6%89%93%E4%B9%B1%E6%95%B0%E6%8D%AE.jpg" alt="mini-batch第一步打乱数据"></li>
<li>切分。将打乱数据后的XY切分进<code>mini_batch_size</code>大小（下图是64）的mini-batches中。不过注意训练样本的数量并不总能被<code>mini_batch_size</code>整除。最后的mini-batch可能要小点，但是不需要担心这点。使用<code>math.floor()</code>向上取整即可。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E7%AC%AC%E4%BA%8C%E6%AD%A5%E5%88%87%E5%88%86.png" alt="mini-batch第二步切分"></li>
</ol>
<h2 id="一些经验"><a href="#一些经验" class="headerlink" title="一些经验"></a>一些经验</h2><p>如果小数据量（大约小于2000）的话，<strong>只</strong>执行步骤1即可；如果样本数目较大，执行步骤1和步骤2，一般将batch设置在64~512之间，考虑到电脑的内存设置和使用方式，batch的大小设置为2的次方，代码的运行速度会比较快；</p>
<h1 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h1><p>为了更好地理解其他优化算法，需要使用到指数加权平均。这章介绍一下它。<br>Exponentially weighted averages，在统计学中被称为指数加权移动平均——Exponentially weighted moving averages。<br>指数加权平均有一个公式：<script type="math/tex">V_t = \beta * V_{t-1} + (1- \beta) * \theta_t</script>，<script type="math/tex">V_0 = 0</script>，其目的是使用<script type="math/tex">V_t</script>代替<script type="math/tex">\theta_t</script>。<script type="math/tex">V_t</script>可视为<strong>约等于</strong><script type="math/tex">\frac{1}{1 - \beta}</script>个样本的平均值。这里可能会有疑问，为什么<script type="math/tex">V_t</script>可视为约等于<script type="math/tex">\frac{1}{1 - \beta}</script>个样本的平均值？其实我也不知道，也不想知道，我又不是学统计学或者数学的。<br>下图中的数据为伦敦一年之间的温度，来源于吴恩达的深度学习视频，可以看到其中的数据十分杂乱，也就是常在网络上看到别人所说的“噪点”多。我们可以使用<strong>指数加权平均</strong>来画出一条线，就是下图的红线，来代表温度变化的趋势，这样会使得更容易让人类理解和观察。<br>下图中的<script type="math/tex">\beta</script>为0.9。而<script type="math/tex">\frac{1}{1 - 0.9} = 10</script>，所以<script type="math/tex">V_t</script>代表过去<em>十天</em>内的平均温度。如果<script type="math/tex">\beta</script>为0.98，那么<script type="math/tex">V_t</script>代表过去<em>五十</em>天内的平均温度<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E4%BE%8B%E5%AD%90.jpg" alt="指数加权平均例子"><br>下图是不同<script type="math/tex">\beta</script>值的对比。注意到一点，绿色（<script type="math/tex">\beta</script>=0.98）的线比红色的线要平坦一点，这是因为你多平均了几天的温度，所以这根线波动更新、更平坦。但是缺点是曲线进一步向右移，拟合的不是很好。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%B8%8D%E5%90%8Cbeta%E5%80%BC%E7%9A%84%E5%AF%B9%E6%AF%94.jpg" alt="不同beta值的对比"><br>现在看到了平均了10天和50天温度的曲线，现在试试<script type="math/tex">\beta=0.5</script>，也就是只平均两天的温度。由于只平均了两天的温度，数据太少，所以曲线有更多的噪声，更有可能出现异常值。但是这个曲线能更快适应温度变化。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/beta%E5%80%BC%E7%AD%89%E4%BA%8E0.5%E7%9A%84%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87.jpg" alt="beta值等于0.5的指数加权平均"></p>
<h2 id="理解其作用"><a href="#理解其作用" class="headerlink" title="理解其作用"></a>理解其作用</h2><div class="note primary">
            <p>略。至今看不懂。</p>
          </div>
<h2 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h2><p>之前的曲线其实都是理想状态下的，回想绿色的曲线是50天内的温度平均值。但是其实绿色曲线会是紫色曲线那样的轨迹。初始化<script type="math/tex">V_0=0</script>，原数据中<script type="math/tex">\theta_0 = 40</script>，所以其实<script type="math/tex">V_1 = 0.02 * 40 = 8</script>，从而绿色曲线的起点实际上很低。因为起点并没有计算50天内的温度平均，我们默认将<script type="math/tex">V_0</script>初始化为0。<br>我们可以用下图右边的公式将其修正。算出<script type="math/tex">V_t</script>后再做如下计算：<script type="math/tex">\frac{V_t}{1 - \beta^t}</script>，其中<script type="math/tex">\beta</script>的上标t是指<strong>t次方</strong>。<br>另外由于t越大，<script type="math/tex">\beta^t</script>的值越接近0，所以对后面的值几乎没影响。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%81%8F%E5%B7%AE%E4%BF%AE%E6%AD%A3.jpg" alt="指数加权平均偏差修正"></p>
<h1 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h1><h2 id="动量梯度下降——Momentum"><a href="#动量梯度下降——Momentum" class="headerlink" title="动量梯度下降——Momentum"></a>动量梯度下降——Momentum</h2><p><div class="note primary">
    <p>算法的意图是明白了，可是算法的原理还是没搞明白。</p>
</div><br>普通的梯度下降，不管是mini-batch、SGD还是其他的什么，都是通过<script type="math/tex">W -= \alpha * dW</script>来更新权重。<br>但是在动量梯度下降中，使用到了<strong>指数加权平均</strong>。尤其是针对mini-batch算法，因为mini-batch算法抖动过大，上面的章节介绍了mini-batch的梯度下降误差曲线，指数加权平均正好可以解决。<br>可以观察下图发现，梯度下降的波动比较大，也就是噪点较多，我们可以使用指数加权平均来减少噪点。下面的公式就是用其减少了梯度dW和db。下式中还对指数加权平均进行了优化，使用了<strong>偏差修正</strong>。</p>
<script type="math/tex; mode=display">
    \begin{cases}
        compute\ dW, db\\
        V_{dW} = \beta * (V_{dW})_{prev} + (1 - \beta) * dW,\quad V_{db} = \beta * (V_{db})_{prev} + (1 - \beta) * db\\
        V^{corrected}_{dW} = \frac{V_{dW}}{1 - \beta^t},\quad V^{corrected}_{db} = \frac{V_{db}}{1 - \beta^t} \\
        W -= \alpha * V^{corrected}_{dW}\\
        b -= \alpha * V^{corrected}_{db}\\
    \end{cases}</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="梯度下降示意图"><br>在梯度下降时的这种波动减慢了下降的速度，无法使用更大的学习速率。因为梯度已经很大了，如果使用更大的学习速率，可能梯度直接爆炸了，直接无法收敛。为了避免摆动过大需要使用较小的学习速率。</p>
<p><div class="note primary">
    <p>
        <blockquote class="blockquote-center"><p>在梯度下降时的这种波动减慢了下降的速度</p>
</blockquote>
        为什么？暂时将结果记住。
    </p>
</div><br>还可以从另一种角度看待。我们希望在纵轴上学习的慢点，我们希望摆动小点，不就是希望纵轴小点吗。而在横轴上我们又希望学习的快点，因为我们希望越快接近中心越好。<br>这个<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">视频</a>讲的直观一点，可以参考一下，从36::00开始看，虽然讲的是RMSprop但是讲的原理跟Momentum的原理一样。</p>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>在课后练习中有更详细的说明，在此补充一下。</p>
<blockquote>
<p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.</p>
</blockquote>
<p>大致意思就是使用Momentum可以使得mini-batch的振荡更小，观察下图。。。说实话我并没有观察出什么，不知道Coursera是怎么想的。我把此图的提示贴出来：</p>
<blockquote>
<p>Figure 3: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence  v  and then take a step in the direction of  v .</p>
</blockquote>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E4%BD%BF%E7%94%A8Momentum%E5%90%8E.png" alt="mini-batch使用Momentum后"></p>
<blockquote>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable  v . Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of  v  as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.</p>
</blockquote>
<p>Momentum考虑到了之前的梯度，从而用其来缓和参数更新。我们将前一次梯度的“方向”存进变量v。后续不翻译了。</p>
<h2 id="均方根传播——RMSprop"><a href="#均方根传播——RMSprop" class="headerlink" title="均方根传播——RMSprop"></a>均方根传播——RMSprop</h2><p>Root mean square prop.<br>一个类似Momentum的算法，没必要死记公式，略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d#/learn/content?type=detail&amp;id=2001702124" target="_blank" rel="noopener">视频地址</a>。<br>或者<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另一个参考视频</a>，36:00开始。<br><strong>RMSprop 没有使用偏差修正。</strong>但是在 Adam 中的 RMSprop 使用了偏差修正。</p>
<script type="math/tex; mode=display">
    \begin{cases}
        compute\ dW, db\\
        S_{dW} = \beta_2 * (S_{dW})_{prev} + (1 - \beta_2) * (dW)^2,\quad S_{db} = \beta_2 * (S_{db})_{prev} + (1 - \beta_2) * (db)^2\\
        W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}\\
        b -= \alpha * \frac{db}{\sqrt{S_{db}} + \epsilon}\\
    \end{cases}</script><p>在更新 W 和 b 时的算法与之前的 Momentum 算法略微不同。另外为了防止 dW 和 db 等于 0，导致分母为 0，所以在分母加了一个极小值<script type="math/tex">\epsilon</script>，在 Keras 中取了 1e-7， 吴恩达老师说 1e-8 是个不错的选择。<br><strong>RMSprop 算法也是使用了指数加权平均算法。</strong>并且还结合了 Adagrad。<br><div class="note info">
            <p>&emsp;&emsp;对于理解 RMSprop。<strong>可以观察出 RMSprop 和 Momentum 长得有点像，但是这两个算法的具体关系暂时不清楚</strong>。并且 RMSprop 其实还有简化版的算法，叫做 Adagrad。之前对这些优化算法（Momentum, RMSprop, Adam 等）的理解都是<em>改变 W 和 b 的大小从而使得梯度下降更快</em>。但是又今天看了一遍<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">李宏毅老师的视频</a>，发现还有其他的理解。其实这些算法都在<strong>改变学习速率的大小</strong>。<br>&emsp;&emsp;比如 RMSprop 算法，观察<script type="math/tex">W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}</script>，我们可以改写成<script type="math/tex">W = W - \frac{\alpha}{\sqrt{S_{dW}} + \epsilon} * dW</script>。看dW之前的那项<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>实际上就是对学习速率<script type="math/tex">\alpha</script>乘上了<script type="math/tex">\frac{1}{\sqrt{S_{dW}} + \epsilon}</script>。<br>&emsp;&emsp;所以对RMSprop的理解是：<strong>如果梯度过大<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>就会相对减小，如果梯度过小<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>就会相对增大</strong>。因为其实<script type="math/tex">S_{dW}</script>就是 dW 算出来的，而梯度过大就是 dW 过大，dW过大就是<script type="math/tex">S_{dW}</script>过大。一个很大的数取倒数，这个数就变很小了。梯度过小同理。<br>&emsp;&emsp;而为什么梯度过大就要是学习速率<script type="math/tex">\alpha</script>变小呢？因为梯度过大就是说梯度较为陡峭，可以想象一座陡峭的山，如果跨一大步是不是直接掉下去了？而掉在哪是未知的，很有可能掉到最低点的前面，这样大概率是回不到最低点的（或者是极小值点）。而如果<script type="math/tex">\alpha</script>小点就很好了，因为可以一小步一小步的走，最终可能会走到极小值点（或者最小值点）。梯度过小同理。平原地方肯定要大跨步走，你小步伐走要走到什么时候才能走到极小值点？<br>&emsp;&emsp;<strong>另外 RMSprop 可以算是 Adagrad 算法的改进版，但是这二者的具体关系未知。</strong></p>
          </div></p>
<h2 id="优化算法历史介绍"><a href="#优化算法历史介绍" class="headerlink" title="优化算法历史介绍"></a>优化算法历史介绍</h2><blockquote>
<p>在深度学习的历史中，有不少学者，包括许多知名学者，提出了优化算法并解决了一些问题。但之后这些算法被指出并不能一般化，并不能适用于多种神经网络。<br>时间久了，深度学习圈子里的人开始多少有点质疑全新的优化算法。<br>但是RMSprop和Adam是少有的经受住人们考验的两种算法。已被证明适用于不同的深度学习结构。</p>
</blockquote>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>全称：Adaptive Moment Estimation<br>这里的 RMSprop 使用了偏差修正。<br><strong>Adam 算法是 Momentum 和 RMSprop 结合起来的算法。</strong>Momentum算法解决算法在纵轴上波动过大的问题，它可以使用类似于物理中的动量来累积梯度。而RMSprop可以在横轴上收敛速度更快同时使得波动的幅度更小。所以将两种算法结合起来表现可能会更好。<br><div class="note primary">
            <p>我的理解是 RMSprop 算法也算是在累计梯度。所以我感觉只使用 RMSprop 和使用 Adam 差不多。</p>
          </div></p>
<script type="math/tex; mode=display">
\begin{array}{l}
    compute\ dW, db\\
    V_{dW} = \beta_1 * V_{dW} + (1 - \beta_1) * dW,\quad V_{db} = \beta_1 * V_{db} + (1 - \beta_1) * db\\
    S_{dW} = \beta_2 * S_{dW} + (1 - \beta_2) * (dW)^2,\quad S_{db} = \beta_2 * S_{db} + (1 - \beta_2) * (db)^2\\
    V^{corrected}_{dW} = \frac{V_{dW}}{1 - \beta_1},\quad V^{corrected}_{db} = \frac{V_{db}}{1 - \beta_1}\\
    S^{corrected}_{dW} = \frac{S_{dW}}{1 - \beta_2},\quad S^{corrected}_{db} = \frac{S_{db}}{1 - \beta_2}\\
    W -= \alpha * \frac{V^{corrected}_{dW}}{\sqrt{S^{corrected}_{dW}} + \epsilon}\\
    b -= \alpha * \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}} + \epsilon}\\
\end{array}</script><p><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">adam paper</a>在这。</p>
<h3 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h3><ol>
<li><script type="math/tex">\alpha</script>需要自行调整。</li>
<li><script type="math/tex">\beta_1</script>一般设置为0.9，计算<script type="math/tex">dW</script>。</li>
<li><script type="math/tex">\beta_2</script>Adam的作者推荐0.999，计算<script type="math/tex">(dW)^2</script>。</li>
<li><script type="math/tex">\epsilon</script>其实不是很重要，但是Adam作者推荐设置为<script type="math/tex">10^{-8}</script>。其实不设置也可以，并不会影响算法的性能。</li>
</ol>
<p>所以在该算法中其实只要调整<script type="math/tex">\alpha</script>就够了，其他的参数也可以调整，但是一般不调整。</p>
<h2 id="其他的学习速率衰减算法"><a href="#其他的学习速率衰减算法" class="headerlink" title="其他的学习速率衰减算法"></a>其他的学习速率衰减算法</h2><p>减小学习速率的方法有多种，李宏毅老师讲解过一个 Adagrad。<br><a href="https://www.bilibili.com/video/av10590361/?p=6" target="_blank" rel="noopener">李宏毅 Adagrad 参考视频</a>，从06:30开始。<br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d#/learn/content?type=detail&amp;id=2001702125" target="_blank" rel="noopener">吴恩达深度学习——学习速率衰减</a></p>
<h2 id="优化算法总结"><a href="#优化算法总结" class="headerlink" title="优化算法总结"></a>优化算法总结</h2><div class="table-container">
<table>
<thead>
<tr>
<th><strong>optimization method</strong></th>
<th><strong>accuracy</strong></th>
<th><strong>cost shape</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient descent</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Momentum</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Adam</td>
<td>94%</td>
<td>smoother</td>
</tr>
</tbody>
</table>
</div>
<h1 id="如何为超参数选择范围"><a href="#如何为超参数选择范围" class="headerlink" title="如何为超参数选择范围"></a>如何为超参数选择范围</h1><p>上面说了那么多算法，其中包括了许多超参数，那么应该怎么为超参数选择值呢？</p>
<h2 id="超参数的重要程度"><a href="#超参数的重要程度" class="headerlink" title="超参数的重要程度"></a>超参数的重要程度</h2><p>按照吴恩达老师的排序，超参数的重要程度如下：</p>
<ol>
<li>learning rate<script type="math/tex">\alpha</script></li>
<li>Momentum的<script type="math/tex">\beta</script>, hidden layer units, mini-batch size</li>
<li>layer的数量，learning rate decay</li>
<li>Adam中的<script type="math/tex">\beta_1\quad \beta_2\quad \epsilon</script>不是很重要，一般按<script type="math/tex">0.9\quad 0.99\quad 10^{-8}</script>设置</li>
</ol>
<h2 id="超参数的取值"><a href="#超参数的取值" class="headerlink" title="超参数的取值"></a>超参数的取值</h2><ol>
<li>随机取值</li>
<li>从粗糙到精细的策略。首先进行随机取值，发现某个点的效果很好，并且附近的点也很好，然后放大这块区域，进行更密集地取值。下图被圈出来的蓝点就是效果不错的，然后被方框画出一大块区域进行密集地取值或者也可以在这块区域随机取值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BB%8E%E7%B2%97%E7%B3%99%E5%88%B0%E7%B2%BE%E7%BB%86%E7%9A%84%E5%8F%96%E5%80%BC%E7%AD%96%E7%95%A5.jpg" alt="从粗糙到精细的取值策略"></li>
</ol>
<h2 id="选择合适的范围"><a href="#选择合适的范围" class="headerlink" title="选择合适的范围"></a>选择合适的范围</h2><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701053" target="_blank" rel="noopener">视频1</a><br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701054" target="_blank" rel="noopener">视频2</a></p>
<h1 id="batch-normalization——对激活值均值归一化"><a href="#batch-normalization——对激活值均值归一化" class="headerlink" title="batch normalization——对激活值均值归一化"></a>batch normalization——对激活值均值归一化</h1><p>第25章写了均值归一化，它对输入值进行了均值归一，更易于算法优化。而batch normalization对激活值进行了均值归一化，说白了是一个东西。<br>但是。。。我看不懂。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036#/learn/content?type=detail&amp;id=2001701055&amp;cid=2001693088" target="_blank" rel="noopener">视频地址</a><br>这几个视频都看不太懂，可能是没有实战的原因。因为上面的大部分算法在以前学机器学习的时候，我都有实战过。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>代价函数为<script type="math/tex">cost(\hat{y}, y) = - \sum^n_{j=1} y_j * log(\hat{y_j})</script>。<br><div class="note primary">
            <p>但是这里可能会有点奇怪。因为二元分类的代价函数是<script type="math/tex">cost(\hat{y}, y) = - \sum^n_{j=1} (y_j * log(\hat{y_j}) + (1 - y_j) * log(\hat{1-y_j}))</script>。怎么多元分类的表达式那么短？</p>
          </div></p>
<h1 id="选择深度学习框架"><a href="#选择深度学习框架" class="headerlink" title="选择深度学习框架"></a>选择深度学习框架</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E9%80%89%E6%8B%A9.jpg" alt="深度学习框架选择"></p>
<h1 id="序列模型——RNN"><a href="#序列模型——RNN" class="headerlink" title="序列模型——RNN"></a>序列模型——RNN</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">传送门</a></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/对神经网络整体的理解.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">记录学习问题，积累做的 leetcode 题目</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">113</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">18</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
            <br>
						<!--同个ip访问本站页面次数-->
						<div class="site-state-item site-state-posts" style="border-left:none;">
								<span class="site-state-item-count" id="busuanzi_value_site_pv"></span>
								<span class="site-state-item-name">浏览量</span>
						</div>
						<!--不同ip访问本站次数-->
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count" id="busuanzi_value_site_uv"></span>
								<span class="site-state-item-name">访客量</span>
						</div>
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count">97.7k</span>
								<span class="site-state-item-name">总字数</span>
						</div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        








        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

	<script src="/lib/my-utils.js"></script>
<!--图片缩放插件-->
<script src="/lib/zoomify/zoomify.min.js"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!--不蒜子统计-->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(4), {
		duration:90000,//1min半一换
		fade: 1500
	});
</script>
<!-- 图片缩放 -->
<script>
$('#content img').zoomify({duration: 500, });
  $('#content img').on('zoom-in.zoomify', function () {
    $('#sidebar').css('display', 'none');
  });
  $('#content img').on('zoom-out-complete.zoomify', function () {
    $('#sidebar').css('display', '');
  });
</script>

	

</body>
</html>
