<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-simple.min.css?v=1.0.2">























  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


<!--图片缩放插件样式-->
<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">

  <meta name="description" content="记录学习问题，积累做的 leetcode 题目">
<meta name="keywords" content="博客，java，javaWeb，NLP，python，机器学习，深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/page/5/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="记录学习问题，积累做的 leetcode 题目">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="记录学习问题，积累做的 leetcode 题目">






  <link rel="canonical" href="http://yan624.github.io/page/5/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
	<!--加载flower canvas-->
<script>
var pathname = window.location.pathname;
if(pathname == '/flower.html'){
	var body =  document.getElementsByTagName('body')[0];
	var canvas = document.createElement("canvas")
	canvas.setAttribute('id', 'sakura')
	// '<canvas id="sakura"></canvas>'
	body.appendChild(canvas)
}
</script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">低阶炼金术士</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">18</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">22</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">117</span></a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/梯度下降算法的推导.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/梯度下降算法的推导.html" class="post-title-link" itemprop="url">梯度下降算法的推导</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-10 19:55:24" itemprop="dateCreated datePublished" datetime="2019-05-10T19:55:24+08:00">2019-05-10</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:20:32" itemprop="dateModified" datetime="2019-06-04T13:20:32+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>梯度下降算法大家都知道，公式是<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，其中J是代价函数。但是这个算法具体是怎么来的，可能不太清楚。<br>本文参考<br><a href="https://mp.weixin.qq.com/s/k26Fm0GL3fdVA9VbQIVAuQ" target="_blank" rel="noopener">微信公众号</a><br><a href="https://baike.baidu.com/item/梯度/13014729" target="_blank" rel="noopener">百度百科</a><br>由于没有专业的制图工具，所以只能手画了。。。</p>
<h1 id="梯度下降问题"><a href="#梯度下降问题" class="headerlink" title="梯度下降问题"></a>梯度下降问题</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/梯度下降草图.jpg" alt="梯度下降草图"><br>由图中可以观察到，我们将参数初始化到A点，我们的目标是将点移动到最小值点（或者极小值点）。那么问题就是如何移动了。<br>先给出梯度下降公式：<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，J是代价函数，这个公式应该不陌生。</p>
<h1 id="一阶泰勒展开式"><a href="#一阶泰勒展开式" class="headerlink" title="一阶泰勒展开式"></a>一阶泰勒展开式</h1><p>如果学过高数，应该知道<strong>一阶泰勒展开式</strong>的公式是：<script type="math/tex">f(x) = f(x_0) + (x - x_0) * f'(x_0) + R_n(x)</script>，其中<script type="math/tex">R_n(x)</script>是泰勒公式的余项，可以理解为一个无穷小量。既然是无穷小量那么便可以省略不写，但是即使是无穷小，其实等式的左右边还是有点差距的，所以将等式修改为<strong>约等于号</strong>。</p>
<script type="math/tex; mode=display">
f(x) \approx f(x_0) + (x - x_0) * f'(x_0)</script><p>但是由于我们最小化的代价函数的参数是<script type="math/tex">\theta</script>，所以我们可以将x替换为<script type="math/tex">\theta</script>，即</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script><p>如果不知道泰勒公式，可以看下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/泰勒公式线性近似.webp" alt="泰勒公式线性近似"><br>在点<script type="math/tex">\theta_0</script>处，找一条极短的直线来表示曲线，则直线的斜率为<script type="math/tex">f'(\theta_0)</script>，并且已知<script type="math/tex">\theta_0</script>，那么根据初中数学，可以获得直线公式<script type="math/tex">f(\theta) = f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>（还不懂看这个：<script type="math/tex">y-y_0=k(x-x_0)</script>===&gt;<script type="math/tex">y = y_0 + k(x-x_0)</script>）。<br><div class="note warning">
            <p>如果仔细看到了上一行的推导，你也许要问：为什么直线斜率是<script type="math/tex">f'(\theta_0)</script>。百度。</p>
          </div><br><div class="note warning">
            <p>如果对上式没有问题，可能要问为什么这个红线的箭头要向下，不能向上？我有强迫症，我就要让它向上，并且我还要让<script type="math/tex">\theta</script>在<script type="math/tex">\theta_0</script>右边。这个下面会讲，但是现在假定以下的步骤均围绕上图展开。</p>
          </div><br>至此准备工作完成。</p>
<h1 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h1><p>我们将<script type="math/tex">f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>的<script type="math/tex">\theta - \theta_0</script>用字母<script type="math/tex">\alpha v</script>代替，实际上只用<script type="math/tex">v</script>代替也可以。但是还是使用<script type="math/tex">\alpha v</script>吧。</p>
<script type="math/tex; mode=display">
\theta - \theta_0 = \alpha v</script><p>所以公式被简化为如下形式，并且将导数的表示做一下改变，用<strong>倒三角</strong>表示</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + \alpha v * \nabla f(\theta_0)</script><p>由于我们的目标是使得<script type="math/tex">f(\theta)</script>比<script type="math/tex">f(\theta_0)</script>小，也就是使得<script type="math/tex">f(\theta) - f(\theta_0) < 0</script>。那么将公式转变为</p>
<script type="math/tex; mode=display">
f(\theta) -  f(\theta_0) \approx \alpha v * \nabla f(\theta_0) < 0</script><p>省略一部分</p>
<script type="math/tex; mode=display">
\alpha v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">\alpha</script>一般为正值，所以</p>
<script type="math/tex; mode=display">
v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>实际上都是向量。所以上式就转换为<strong>两个向量相乘在什么时候是小于0的</strong>，并且我们希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好，也就是<script type="math/tex">v * \nabla f(\theta_0)</script>越小越好。那么问题又转化为<strong>两个向量相乘在什么时候是最小的</strong>。<br><div class="note warning">
            <p>问题1：为什么<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>是向量。<br>由于以上都是使用二维的图来描述，所以无法体现是向量。但是实际上<script type="math/tex">\theta</script>不只有一个。<br>问题2：为什么希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好。<br>因为希望<script type="math/tex">f(\theta)</script>这一步迈远一点。</p>
          </div><br>以下为向量乘积的三种形式，由初中的知识可以得知，当向量相反时<script type="math/tex">cos(\alpha)</script>为-1，即cos函数的最小值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/向量的乘积.webp" alt="向量的乘积"><br>由于公式可以转为如下，其中<script type="math/tex">\beta</script>是向量夹角</p>
<script type="math/tex; mode=display">
|v| * |\nabla f(\theta_0)| * cos(\beta) < 0</script><p>所以当<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>正好相反时，<script type="math/tex">cos(\beta) = -1</script>。也就是说当<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>反向，<script type="math/tex">v * \nabla f(\theta_0)</script>最小。<br>众所周知，<script type="math/tex">\nabla f(\theta_0)</script>就是梯度，也就是梯度方向。所以只需要<script type="math/tex">v</script>为<script type="math/tex">\nabla f(\theta_0)</script>的反方向即可。所以</p>
<script type="math/tex; mode=display">
v  = -\frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}\\</script><p>之所以要除以<script type="math/tex">\nabla f(\theta_0)</script>的模，是因为<script type="math/tex">v</script>是单位向量。<br><div class="note warning">
            <p><script type="math/tex">v</script>为什么是单位向量。不太清楚，原博主没说明。</p>
          </div><br>将<script type="math/tex">v</script>带入到<script type="math/tex">\theta - \theta_0 = \alpha * v</script>中</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha * \frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}</script><p>一般地，因为<script type="math/tex">|\nabla f(\theta_0)|</script>是标量，可以并入到中，即简化为：</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha *\nabla f(\theta_0)</script>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/梯度下降算法的推导.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/深度学习中的一些疑问总结.html" class="post-title-link" itemprop="url">深度学习中的一些疑问总结</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-03 13:54:56" itemprop="dateCreated datePublished" datetime="2019-05-03T13:54:56+08:00">2019-05-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:20:26" itemprop="dateModified" datetime="2019-06-04T13:20:26+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h1><div class="note primary">
            <p>为什么<script type="math/tex">w*x+b=z</script>之后，需要使用一个激活函数<script type="math/tex">a = \sigma(z)</script>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
          </div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。<br>那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。</p>
<script type="math/tex; mode=display">
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w' * a^0 + b' \qquad由于w和b只是一堆常数，所以将w'代替w^2 * w^1，b'同理
    \end{array}
\right.</script><p>通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<script type="math/tex">a^2 = w' * a^0 + b'</script>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。<br>只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降的意义"><a href="#梯度下降的意义" class="headerlink" title="梯度下降的意义"></a>梯度下降的意义</h1><div class="note primary">
            <p>首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？</p>
          </div>
<h1 id="dropout背后的原理"><a href="#dropout背后的原理" class="headerlink" title="dropout背后的原理"></a>dropout背后的原理</h1><div class="note primary">
            <p>dropout背后的原理是什么？</p>
          </div>
<p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a></p>
<h1 id="为什么Mini-batch比普通的梯度下降快？"><a href="#为什么Mini-batch比普通的梯度下降快？" class="headerlink" title="为什么Mini-batch比普通的梯度下降快？"></a>为什么Mini-batch比普通的梯度下降快？</h1><div class="note primary">
            <p>为什么Mini-batch比普通的梯度下降快？</p>
          </div>
<h1 id="指数加权平均的作用"><a href="#指数加权平均的作用" class="headerlink" title="指数加权平均的作用"></a>指数加权平均的作用</h1><div class="note primary">
            <p>指数加权平均的作用</p>
          </div>
<h1 id="为什么要deep"><a href="#为什么要deep" class="headerlink" title="为什么要deep"></a>为什么要deep</h1><div class="note primary">
            <p>学了有一段时间的深度学习，但是有个问题一直没想明白。那就是将hidden layer叠多层的意义是什么？</p>
          </div>
<p>可以观察下图发现，确实越deep代价越小。左边的两列显示了，层数越多代价越小。右边两列显示即使整个神经网络参数类似，但是明显越deep代价越小。同一行代表hidden layer的参数接近。那个size指的是神经网络中的参数。再看最后一行，它显示即使参数暴增到16k，代价也不是很低。图中红框还显示了2层2k的model比1层16k的model好多了。<br>下图是由底下的论文的作者做的实验得出的结论。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/隐藏层层数对cost的影响.jpg" alt="隐藏层层数对cost的影响"><br><strong>那么为什么神经网络越深效果越好呢？</strong><br>这其实归功于modularization——模块化。如下图所示，如果直接写一个model用于将4种人分类，那么可能会出现某类人的数据并不多的情况，比如长发的男生的数据可能并不多。那么分类的准确率可能会不是很高。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/解释why deep的例子.jpg" alt="解释why deep的例子"><br>下图中先将其分类为男女以及长短发，然后再进一步分类。虽然说长发男生的数据比较少，但是男女和长短发的数据有很多，我们可以得到一个很好的模型。之后我们再叠一层用于进一步分类，此时，由于我们已经做了上一步的分类，所以新的一层可以使用上一层的特征。<br>上一层的分类已经把难的事情——辨别男女，辨别长短发等解决了，所以后一层只要使用少量的数据就能进行分类。<br><strong>没有使用模块化</strong>的那个模型，它是用少量的数据硬生生地去识别长发男生。<strong>使用模组化</strong>的模型是先识别男女以及长短发，再通过调用前一步的特征判断。下图4个分类器区别可能只是辨别的方式不同而已，比如权重不同。输入一张图片之后第一层已经可以辨别是男还是女，长发还是短发，然后后一层经过简单的运算就可以确定了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/模块化后.jpg" alt="模块化后"><br>经过上面的解释，可能已经大致理解是什么意思了。但是真要讲清还有点问题，尤其是模块化怎么做。<br>但是李宏毅老师说模块化其实是神经网络从数据中<strong>自动</strong>学到的。</p>
<h2 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h2><p>用数电的逻辑门来举例，但是我没怎么学过数电，所以没有理解。<br>另一个比较贴近生活的例子，就是剪窗花。没有人会一瓣花一瓣花的去剪窗花，都是将纸先折好，然后一步剪完。这就是模块化了。</p>
<p>还有其他领域的人也有过解读，个人理解why deep这个问题可能到现在没有一个官方的回应，可能前辈也是误打误撞才发现deep learning很牛。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/其他领域对为什么要deep的解读.jpg" alt="其他领域对为什么要deep的解读"></p>
<h2 id="吴恩达老师的解释"><a href="#吴恩达老师的解释" class="headerlink" title="吴恩达老师的解释"></a>吴恩达老师的解释</h2><p><a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>也做了解释。</p>
<h1 id="词向量乘上权重以及做梯度下降有什么意义"><a href="#词向量乘上权重以及做梯度下降有什么意义" class="headerlink" title="词向量乘上权重以及做梯度下降有什么意义"></a>词向量乘上权重以及做梯度下降有什么意义</h1><p><a href="https://mooc.study.163.com/learn/2001280005?tid=2001391038&amp;_trace_c_p_k2_=023fecd41c524f0d9485b18d2d773f53#/learn/content?type=detail&amp;id=2001770038" target="_blank" rel="noopener">本文灵感</a><br>本文疑问：</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<h2 id="准备词向量"><a href="#准备词向量" class="headerlink" title="准备词向量"></a>准备词向量</h2><p>假设有这么一句话：I want a glass of orange ___.<br>要做的是估计划线处应该填入什么词。答案是juice。<br>首先我们需要一个词典——vocabulary，每个单词对应一个索引，这是通用步骤。词表大小为10000。<br>然后将上述的句子，从单词转成索引形式。即：<br>I want a glass of orange —-&gt; 4343 9665 1 3852 6163 6257<br>此外每一个单词都会对应一个词向量，而词表中所有单词的词向量就组合一个词嵌入矩阵。词表以及词向量都是可以找一些预训练的，比如<strong>GloVe</strong>。<br>梳理一遍就是：<br>单词:索引<br>索引:词向量<br>所以可以通过单词间接地获取到词向量。关于索引对应词向量，实际上是里面没有索引的因为一个矩阵它本身就有一个属性表示索引，如第0行就是代表第0个单词，第1行就是代表第一个单词。<br>总而言之，我们通过单词获取索引后，就能通过该索引直接获取词向量。伪代码可以表示为：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index = vocabulary.get_index('want') <span class="comment"># 索引为9665</span></span><br><span class="line">word_vector = embedding_matrix[index, :] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p><strong>对于词嵌入矩阵的行代表词向量，还是列代表词向量不必纠结。</strong>你要乐意可以改成<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_vector = embedding_matrix[:, index] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p>如果使用one hot编码来执行上述代码就是将9665转为one hot编码，即除了9665位置为1，其余位置全为0。然后<script type="math/tex">word\_vector = embedding\_matrix^T * word\_one\_hot</script>。这样也能得到词向量，但是由于one hot编码全是0，算起来速度太慢了。<br>现在有了句子“I want a glass of orange”的所有词向量，接下来要做的是将这些词向量从头到尾拼在一起，接成一个更长的向量，也就是6倍长的向量。原词向量是300维，拼接完成后是1800维。然后将这个向量输入一个神经网络中，最后经过softmax函数进行预测，预测范围是在10000个单词中，看谁的概率大。</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>将词向量送入神经网络中当然还需要梯度下降进行迭代。这里会有很多疑问，</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<p>我进行逐一思考，本文仅为自己的理解。<br>首先其实有一件事很多视频没讲，可能他们认为这是一件很平常的事，所以没讲。<br>上述的这个步骤并不是预测步骤，而是在进行迭代，所以是一个训练步骤。人家之所以说<em>我们可以通过这个神经网络预测出单词为juice</em>，是因为逻辑上是这样的。<br>由于是训练步骤，所以我们有一个很重要的数据，最终结果。最终结果我们是知道的，然而我们初学者在考虑整个流程时，没把最终结果算进去，因为老师说<em>我们可以通过这个神经网络预测出单词为juice</em>，由于是<em>预测</em>，那么结果肯定没有啊。这很合乎逻辑。所以就陷入了一个思维的怪圈，<strong>正确的逻辑是：</strong></p>
<ol>
<li>首先我们知道最终结果，所以当第一次迭代时，所有的权重都是随机初始化的，1、2两个问题也就没有意义了。第一次迭代完毕后，结果肯定稀巴烂，所以进行梯度下降。</li>
<li>这里面我们又会碰到一个问题，就是梯度下降到底在算什么？其实这里的疑问来自我们的潜意识始终将句子当做文字在看，自然而然就意识不到梯度下降在干什么。而其实我们在几步之前就已经将文字转为词向量了。<br>词向量说白了就是一堆浮点型数字，而最终结果juice也是一个词向量，所以实际上就是将一个权重矩阵乘上一个1800维的向量，得到一个输出值（may be 激活值），然后将这个输出值和juice的向量放入代价函数中进行计算，接下来的梯度下降其实就是跟正常的步骤一样。</li>
<li><strong>梯度下降就是在寻找一个合适的权重矩阵使得权重矩阵乘1800维向量得到的值接近juice的向量。</strong><br>这里在解释第3个问题时，顺便也解释了第1、2个问题。<strong>权重值实际上就是用来使得预测值和实际结果越接近越好</strong></li>
<li>由于开头就说了我们实际上是知道划线处应该填juice，所以第4个问题压根不需要解答，因为我们本来就知道应该填juice，也没必要预测。之前之所以有这个问题，是因为我们潜意识觉得老师说<em>我们可以通过这个神经网络预测出单词为juice</em>。</li>
<li>至于第5个问题如何预测其他句子。打个比方，现在预测一个新的句子：I want a glass of apple ___.<br>由于我们知道词嵌入矩阵是由很多单词的词向量组成的。而一个单词词向量其实就是一堆特征组成的，对比两个句子，apple和orange的词向量肯定很接近，因为它们都是水果，它们的特征（水多不多，好不好吃，是不是水果，有没有性别特征，是不是动词等）都类似。<br>而我们之前已经训练了一个神经网络，我们得到了所需要的权重值，我们直接把这个权重值乘上新句子的词向量，那么结果肯定和权重值乘上之前句子的词向量的结果很接近。因为它们之间的区别仅仅是apple和orange的词向量不同，并且apple和orange的词向量其实也并不是完全不同，只是略有不同。所以二者的结果自然也差不多。<br>最后经过softmax函数在10000个单词之中预测，输出的结果必然都是juice。</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>这里还会有一个问题，那么如果需要预测的句子单词数不一致怎么办？<br>之前两个例子的句子都是6个单词，预测第7个单词。那要是前面有10个单词，我要预测第11个单词怎么办？<br>其实可以只考虑划线处前几个单词，比如只考虑划线处前4个单词，这样输入的维度就相同了。<br>在本文开头的参考视频里，吴恩达老师讲得很清楚了。由古圣先贤总结，一般来说取前4个单词即可，当然你自己也可以用其他办法。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/深度学习中的一些疑问总结.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/深度学习学习记录大纲.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/深度学习学习记录大纲.html" class="post-title-link" itemprop="url">深度学习学习记录大纲</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-02 13:54:55" itemprop="dateCreated datePublished" datetime="2019-05-02T13:54:55+08:00">2019-05-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-24 10:53:05" itemprop="dateModified" datetime="2019-07-24T10:53:05+08:00">2019-07-24</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/outline/" itemprop="url" rel="index"><span itemprop="name">outline</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题"><a href="#《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题" class="headerlink" title="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题"></a>《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题</h1><p><a href="https://yan624.github.io/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">文章地址</a><br>由于神经网络中参数太多，而有些参数的表现形式太过复杂， 比如文中权重——<script type="math/tex">w^l_{ji}</script>有太多上标下标，所以写了一篇文章记录一下。</p>
<h1 id="对神经网络整体的理解"><a href="#对神经网络整体的理解" class="headerlink" title="对神经网络整体的理解"></a>对神经网络整体的理解</h1><p><a href="https://yan624.github.io/学习笔记/对神经网络整体的理解.html">文章地址</a><br>通常学习深度学习从一个最简单的神经网络开始，但是由于对深度学习时0基础，所以需要同时学习大量算法以及其原理，比如梯度下降，Momentum，Adam，RMSprop，adagrad等等算法。所以写了一篇文章记录一下大部分的算法以及原理。</p>
<h1 id="吴恩达李宏毅综合学习笔记：RNN入门"><a href="#吴恩达李宏毅综合学习笔记：RNN入门" class="headerlink" title="吴恩达李宏毅综合学习笔记：RNN入门"></a>吴恩达李宏毅综合学习笔记：RNN入门</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">文章地址</a><br>学习完神经网络之后，可以学习其他的神经网络模型。由于本人初步决定学习nlp，所以基本没有看CNN，直接学了RNN。本文就是学习RNN的记录，包括了许多算法。</p>
<h1 id="吴恩达深度学习学习笔记：自然语言处理与词嵌入"><a href="#吴恩达深度学习学习笔记：自然语言处理与词嵌入" class="headerlink" title="吴恩达深度学习学习笔记：自然语言处理与词嵌入"></a>吴恩达深度学习学习笔记：自然语言处理与词嵌入</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html">文章地址</a><br>学习完RNN之后，就可以学习 NLP 的概念了，这里面讲得虽然还是神经网络，但是其实都是 NLP 领域的知识。</p>
<h1 id="练习Keras-RNN的代码"><a href="#练习Keras-RNN的代码" class="headerlink" title="练习Keras RNN的代码"></a>练习Keras RNN的代码</h1><p><a href="https://yan624.github.io/学习笔记/练习Keras RNN的代码.html">文章地址</a><br>学习完Simple NN，RNN 和 NLP 之后，就可以练习一下了。文中使用 Keras 框架，写了几个例子练习。</p>
<h1 id="疑问总结"><a href="#疑问总结" class="headerlink" title="疑问总结"></a>疑问总结</h1><p><a href="https://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">文章地址</a><br>深度学习入门后必然有很多疑问待解答，此篇解决疑问。</p>
<h1 id="开始继续学习机器学习"><a href="#开始继续学习机器学习" class="headerlink" title="开始继续学习机器学习"></a>开始继续学习机器学习</h1><p>待续。。。</p>
<h1 id="开始CNN"><a href="#开始CNN" class="headerlink" title="开始CNN"></a>开始CNN</h1><p>待续。。。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/深度学习学习记录大纲.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/练习Keras RNN的代码.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/练习Keras RNN的代码.html" class="post-title-link" itemprop="url">练习Keras RNN的代码</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-27 15:28:20" itemprop="dateCreated datePublished" datetime="2019-04-27T15:28:20+08:00">2019-04-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:46:39" itemprop="dateModified" datetime="2019-06-04T13:46:39+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="《Python深度学习》第6章预测imdb的影评"><a href="#《Python深度学习》第6章预测imdb的影评" class="headerlink" title="《Python深度学习》第6章预测imdb的影评"></a>《Python深度学习》第6章预测imdb的影评</h1><p><a href="https://github.com/yan624/deep-learning-notes/tree/master/RNN_learning/imdb_predication" target="_blank" rel="noopener">imdb影评代码</a></p>
<h1 id="第五课第二周作业：Emojify"><a href="#第五课第二周作业：Emojify" class="headerlink" title="第五课第二周作业：Emojify"></a>第五课第二周作业：Emojify</h1><p>本文实现吴恩达深度学习第五课第二周的Emojify作业。目的是了解一个小型nlp系统的构建流程。<br>首先需要导入以下模块。由于在Jupyter中打开的ipynb文件全是英文，所以本文的大部分标题也用英文，方便ctrl F。<br>Emojifier-V1略。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>加了很多注释，但是代码的顺序我做了很大的改动。下面博客里面的代码，是作业里面的代码，基本没改几个字。<br><a href="https://github.com/yan624/deep-learning-notes/tree/master/RNN_learning/emojify" target="_blank" rel="noopener">emojify_V2代码</a></p>
<h2 id="Emojifier-V2-Using-LSTMs-in-Keras"><a href="#Emojifier-V2-Using-LSTMs-in-Keras" class="headerlink" title="Emojifier-V2: Using LSTMs in Keras"></a>Emojifier-V2: Using LSTMs in Keras</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.seed</span>(<span class="number">0</span>)</span><br><span class="line">from keras<span class="selector-class">.models</span> import Model</span><br><span class="line">from keras<span class="selector-class">.layers</span> import Dense, Input, Dropout, LSTM, Activation</span><br><span class="line">from keras<span class="selector-class">.layers</span><span class="selector-class">.embeddings</span> import Embedding</span><br><span class="line">from keras<span class="selector-class">.preprocessing</span> import sequence</span><br><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.seed</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br></pre></td></tr></table></figure>
<h3 id="Overview-of-the-model"><a href="#Overview-of-the-model" class="headerlink" title="Overview of the model"></a>Overview of the model</h3><h3 id="Keras-and-mini-batching"><a href="#Keras-and-mini-batching" class="headerlink" title="Keras and mini-batching"></a>Keras and mini-batching</h3><p>本练习中，我们使用mini-batch算法训练Kears。大部分深度学习框架要求在相同的mini-batch中所有序列都要等长。这使得可以执行向量化，如果你有一个3个单词的句子和一个4个单词的句子，它们之间的计算会不同（一个需要3个timestep，一个需要4个timestep，也就是说需要的LSTM个数不同），所有同时计算它们是不可能的，即无法向量化。<br>通用的解决办法是使用padding。具体来说，设置一个序列的最大长度，然后使其他的序列都与该长度等长。比如序列的最大长度是20，那么将其他的序列在后面补充0，知道长度等于20。所以句子“I love you”会在“you”后面被补充17个0。即<script type="math/tex">\begin{pmatrix}e_i & e_{love} & e_{you} & \overrightarrow{0} & \overrightarrow{0} & \cdots & \overrightarrow{0}\end{pmatrix}</script>，e代表词向量。如果长度大于20的话会被裁剪。<br>以下代码实现将句子转换为句子中的每个单词转为索引，这个索引是GloVe词嵌入的，每一个单词对应一个id。id就是索引。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sentences_to_indices</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将句子转为索引形式，短于max_len的句子后面补充0，长于max_len的句子直接截断</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape</span></span><br><span class="line">    X_indices = np.zeros((m, max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] =word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure></p>
<p>以下设计Embedding层，使用keras的Embedding类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pretrained_embedding_layer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此方法创建了一个Embedding层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># GloVe的总单词数量</span></span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                  <span class="comment"># adding 1 to fit Keras embedding (requirement)</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    <span class="comment"># 词嵌入矩阵，之前V1压根没用词嵌入矩阵，将这个矩阵放入Embedding层中供keras使用</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable.</span></span><br><span class="line">    <span class="comment"># Use Embedding(...). Make sure to set trainable=False.</span></span><br><span class="line">    embedding_layer = Embedding(vocab_len, emb_dim, trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure></p>
<h3 id="Building-the-Emojifier-V2"><a href="#Building-the-Emojifier-V2" class="headerlink" title="Building the Emojifier-V2"></a>Building the Emojifier-V2</h3><p>以下代码完成Emojify。主要是keras代码。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: Emojify_V2</span></span><br><span class="line"></span><br><span class="line">def Emojify_V2(input_shape, word_to_vec_map, word_to_index):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    <span class="attr">sentence_indices</span> = Input(input_shape, <span class="attr">dtype='int32')</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    <span class="attr">embedding_layer</span> = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    <span class="attr">embeddings</span> = embedding_layer(sentence_indices)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    <span class="comment"># 这个128是神经元的个数</span></span><br><span class="line">    <span class="attr">X</span> = LSTM(<span class="number">128</span>, <span class="attr">return_sequences=True)(embeddings)</span></span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    <span class="attr">X</span> = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    <span class="attr">X</span> = LSTM(<span class="number">128</span>, <span class="attr">return_sequences=False)(X)</span></span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    <span class="attr">X</span> = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    <span class="attr">X</span> = Dense(<span class="number">5</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    <span class="attr">X</span> = Activation('softmax')(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    <span class="attr">model</span> = Model(<span class="attr">inputs=sentence_indices,</span> <span class="attr">outputs=X)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    return model</span><br></pre></td></tr></table></figure></p>
<h2 id="系统整体流程"><a href="#系统整体流程" class="headerlink" title="系统整体流程"></a>系统整体流程</h2><p>RNN</p>
<ol>
<li>读取GloVe文件和训练数据</li>
<li>将训练数据的label转为one hot表示</li>
<li>求出所有训练数据中最长句子的长度，该长度就是LSTM的个数。由于向量化的要求，LSTM的个数需要相同，以最长长度作为LSTM的个数，当然并不需要每个项目都这么设置，完全可以自己选，随便举几个例子比如20,50，100等。</li>
<li>设计Embedding层</li>
<li>建立神经网络模型</li>
<li>将每句话转换为索引表示，如果长度不够就填0，够了就截断。《Python深度学习》中使用了pad_sequences类</li>
<li>使用模型预测，第6条就是输入的训练数据，第2条就是输入的标签</li>
</ol>
<p>Embedding层需要输入一个词嵌入矩阵，就是一个二维数组。每行代表一个单词的特征向量，行数就是单词的索引。而输入的训练数据被处理成单词的索引形式。如一组训练样本，每个单词被替换成唯一的索引。<br>Embedding层太过复杂，故不作详解。第一章的代码中全部有注释。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/练习Keras RNN的代码.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/bug/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/bug/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html" class="post-title-link" itemprop="url">Jupyter出现gbk codec cant decode byte 0x93 in position 3136：illegal multibyte sequence</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-27 14:45:05" itemprop="dateCreated datePublished" datetime="2019-04-27T14:45:05+08:00">2019-04-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:22:36" itemprop="dateModified" datetime="2019-06-04T13:22:36+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>一般来说是open()方法没有加encoding=’utf-8’，但是没用，试了其他办法没一个能用。<br>解决办法：重启Jupyter。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/bug/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html" class="post-title-link" itemprop="url">吴恩达深度学习学习笔记：自然语言处理与词嵌入</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-25 23:58:24" itemprop="dateCreated datePublished" datetime="2019-04-25T23:58:24+08:00">2019-04-25</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:20:44" itemprop="dateModified" datetime="2019-06-04T13:20:44+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>我们一直使用<a href="https://yan624.github.io/学习笔记/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型.html#one hot编码">one hot编码</a>，这在之前已经记过笔记。这种表示方法的最大缺点是将每个词孤立起来，并且泛化能力不强。由于每个向量的内积都是0，所以它们之间的距离都是一样的。比如</p>
<ol>
<li>I want a glass of orange juice.</li>
<li>I want a glass of apple <em>_</em>.<br>这两个句子是很常见的句子，所以自然而然的想到划线处应该是juice。但是由于one hot编码，程序并不知道orange和apple之间的关系，也就猜不出来。</li>
</ol>
<h2 id="Featurized-representation：-word-embedding"><a href="#Featurized-representation：-word-embedding" class="headerlink" title="Featurized representation： word embedding"></a>Featurized representation： word embedding</h2><p>既然one hot有问题，那么自然就有人发明了新的算法。<br>使用特征来表示每个词。如果适应特征化来表示，那么最后发现orange和apple的特征差不多，就可以推测出划线处应该填写什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Featurized representation： word embedding.jpg" alt="Featurized representation： word embedding"></p>
<h2 id="Visualizing-word-embedding"><a href="#Visualizing-word-embedding" class="headerlink" title="Visualizing word embedding"></a>Visualizing word embedding</h2><p>可以使用t-SNE算法将数据可视化为二维的图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Visualizing word embedding.jpg" alt="Visualizing word embedding"></p>
<h1 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h1><h2 id="类比"><a href="#类比" class="headerlink" title="类比"></a>类比</h2><p>看下图中的表格，现在已知对应关系man-&gt;woman，能否推出king对应于queen？也就是说king-&gt;<em>_</em>，填空题。<br>解法是：<br>求出man和woman之间的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-1\\
0.01\\
0.03\\
0.09\\
\end{pmatrix} - 
\begin{pmatrix}
1\\
0.02\\
0.02\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>假设计算king和queen的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-0.95\\
0.93\\
0.70\\
0.02\\
\end{pmatrix} - 
\begin{pmatrix}
0.97\\
0.95\\
0.69\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>算法的原理就是找到一个词使得man和woman的差与king和新词的差接近。翻译为代码就是<script type="math/tex">find\ word\ w: argmax\ sim(e_w, e_{king} - e_{man} + e _{woman})</script>。但是算法的准确度只有30%-75%。</p>
<h3 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h3><p>余弦相似度也可以计算相似度。公式为<script type="math/tex">sim(u,v) = \frac{u^Tv}{\parallel u\parallel_2\parallel v\parallel_2}</script></p>
<h1 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h1><p>略。大致意思是一个嵌入矩阵E乘上one hot编码可以得到一个单词的特征向量。E就是全部单词的特征矩阵。</p>
<h1 id="如何train一个词嵌入矩阵"><a href="#如何train一个词嵌入矩阵" class="headerlink" title="如何train一个词嵌入矩阵"></a>如何train一个词嵌入矩阵</h1><p>在早期深度学习的研究人员都是使用比较复杂的算法，但是随着时间的推移，这些复杂的算法被慢慢的简化。以至于现在的新手看到这些简化版的算法时，会疑惑这样简单的算法时怎么工作的。所以现在先介绍一个比较复杂的算法，再慢慢介绍简化版的。<br><div class="note info">
            <p>这节好像是用来讲如何建立神经语言模型的，以后再看。之前讲了嵌入矩阵E，但是E中全部的特征向量是已经假定存在的，那么这些特征从何而来呢？就是这节讲的，去训练得来的。但是其实有已经训练好的，我们可以直接拿来用，网上有很多。</p>
          </div></p>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><h1 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h1><h1 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h1><p>就是词嵌入中可能带有一些偏见，比如男女偏见、种族偏见等。现在的目的就是除去这种偏见。<br>暂且不看，其他的算法都还没学。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html" class="post-title-link" itemprop="url">吴恩达李宏毅综合学习笔记：RNN入门</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-23 16:46:12" itemprop="dateCreated datePublished" datetime="2019-04-23T16:46:12+08:00">2019-04-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-16 12:38:55" itemprop="dateModified" datetime="2019-08-16T12:38:55+08:00">2019-08-16</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>课程</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>2~8</td>
<td>吴恩达深度学习</td>
<td>one hot编码、RNN包括双向和深层、GRU、LSTM</td>
</tr>
<tr>
<td>9~14</td>
<td>李宏毅机器学习</td>
<td>RNN包括双向和深层、LSTM、RNN反向传播、seq2seq</td>
</tr>
<tr>
<td>15~20</td>
<td>李宏毅深度学习</td>
<td>计算图、语言模型中的深度学习、几个有用的网络架构。到原视频的 p12 结束，由于后续部分涉及到了 GAN 等其他模型，所以不在此处做笔记，详见<a href="https://yan624.github.io/zcy/对神经网络整体的理解.html">对神经网络整体的理解</a>博文中靠后的几节</td>
</tr>
</tbody>
</table>
</div>
<h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>假设：<br>x: Harry Potter and Hermione Granger invented a new spell.<br>y: 1 1 0 1 1 0 0 0 0<br>其中1代表人名地名之类的单词。这句话一共有九个单词，则x可以表示为：<script type="math/tex">x^{<1>} x^{<2>} \cdots x^{<t>} \cdots x^{<9>}</script>。<br>则y可以表示为：<script type="math/tex">y^{<1>} y^{<2>} \cdots y^{<t>} \cdots y^{<9>}</script><br>输入的长度表示为<script type="math/tex">T_x</script>，则<script type="math/tex">T_x = 9</script>。<br>输出的长度表示为<script type="math/tex">T_y</script>，则<script type="math/tex">T_y = 9</script>。<br>之前在神经网络中<script type="math/tex">X^i</script>或<script type="math/tex">X^(i)</script>代表第i个训练样本。现在在序列模型中，<script type="math/tex">X^{(i)<t>}</script>代表代表第i个训练样本的第t个元素。对应地，<script type="math/tex">T^i_x</script>就代表第i个样本的输入长度。</p>
<h1 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one hot编码"></a>one hot编码</h1><p>在我们做自然语言处理时，一件需要事先决定的是，怎么表示一个序列里的单词。<br>第一件事就是做一张词表（Vocabulary）有时也叫字典（Dictionary），然后将表示方法中要使用的单词列出一列。最后将一个单词用一个稀疏向量表示，如Harry表示为<script type="math/tex">\begin{pmatrix}0&0&0&\cdots&1&0&\cdots&0\end{pmatrix}</script>。1所在位置就是Harry这个单词在词表中的所在位置。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/ont%20hot%E4%BE%8B%E5%AD%90.jpg" alt="ont hot例子"></p>
<h1 id="循环神经网络——RNN"><a href="#循环神经网络——RNN" class="headerlink" title="循环神经网络——RNN"></a>循环神经网络——RNN</h1><div class="note primary">
            <p>RNN解决了什么问题。</p>
          </div>
<p>与Simple Neural Network不同的是，循环神经网络的每一层都要有输入x和输出y。<br>第一步与Simple Neural Network类似，<script type="math/tex">a_1 = w_{aa} * x^{<1>} + b_a</script>，这样就获得了激活值a，但是这时需要使用sigmoid函数或者其他函数直接算出y，另外与Simple Neural Network不同的是，它在计算激活值时需要附带加上前一层的激活值乘上一个权重，此权重与其他的权重类似，也是NN自己训练的。所以第二个序列的计算公式是<script type="math/tex">a_2 = w_{aa} * a_1 + w_{ax} * x^{<2>} + b_a</script>。后面的序列就跟第二个序列一样。<strong>注意一点，RNN中平行方向是时间序列，并不是隐藏层，并且此例中为了方便起见，垂直方向只有一个隐藏层。那几个圆圈是神经元</strong>。看下图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84RNN%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="吴恩达深度学习中的RNN示意图"></p>
<ol>
<li><p>由于为了一般化，第一层需要修改成跟后面的计算类似，所以引入一个零向量<script type="math/tex">a_0</script>来计算<script type="math/tex">a_1</script>。<br>所以RNN的计算公式为：</p>
<script type="math/tex; mode=display">
\left\{ 
 \begin{array}{c}
     a^{<1>} = g_1(w_{aa} * a^{<0>} + w_{ax} * x^{<1>} + b_a)\\
     \hat{y}^{<1>} = g_2(w_{ya} * a^{<1>} + b_y)\\
     a^{<2>} = g_1(w_{aa} * a^{<1>} + w_{ax} * x^{<2>} + b_a)\\
     \hat{y}^{<2>} = g_2(w_{ya} * a^{<2>} + b_y)\\
     \vdots\\
     a^{<t>} = g_1(w_{aa} * a^{<t-1>} + w_{ax} * x^{<t>} + b_a)\\
     \hat{y}^{<t>} = g_2(w_{ya} * a^{<t>} + b_y)\\
 \end{array}
\right.</script><p>注意上式中的<script type="math/tex">w_{aa}</script>、<script type="math/tex">w_{ax}</script>、<script type="math/tex">w_{ya}</script>、<script type="math/tex">b_{a}</script>和<script type="math/tex">b_{y}</script>并没有上标或者下标，所以意味着每一层同一个符号的权重值和偏差值都是一样的。另外对于激活函数也是用户自行选择，在<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html">对神经网络整体的理解</a>一文中已经解释的很清楚了，为了区分输入与输出的激活函数不同，我特意使用了不同的下标，这个下标仅代表这个意思。</p>
</li>
<li><p>为了进一步地一般化，我们将<script type="math/tex">w_{aa}</script>、<script type="math/tex">w_{ax}</script>合并成为<script type="math/tex">w_{a}</script>，如果表示为矩阵形式就是<script type="math/tex">w_{a} = \begin{pmatrix}w_{aa} | w_{ax}\end{pmatrix}</script>，然后将1中的最后两行表达式一般化为：</p>
<script type="math/tex; mode=display">
a^{<t>} = g_1(w_{a} * [a^{<t-1>}, x^{<t>}] + b_a)\\
\hat{y}^{<t>} = g_2(w_{y} * a^{<t>} + b_y)\\</script><p>表达式<script type="math/tex">[a^{<t-1>}, x^{<t>}]</script>的意思是将两个向量堆起来，如果表示为矩阵形式就是<script type="math/tex">\begin{pmatrix} a^{<t-1>}\\ x^{<t>}\\ \end{pmatrix}</script>，上式为了排版问题就不写成矩阵形式了。</p>
</li>
</ol>
<h2 id="RNN的反向传播"><a href="#RNN的反向传播" class="headerlink" title="RNN的反向传播"></a>RNN的反向传播</h2><p>跟Simple Neural Network类似，也要先定义一个cost function，可以选择crossentropy。由于RNN每一层都有输出值y，所以需要对每一层都求出代价，最后将这些代价值加起来</p>
<div class="note primary">
    <p>吴恩达老师在讲反向传播的实现时并没有讲计算过程，所以有点糊里糊涂的。从代价函数到激活值反向传播还可以理解，但是从后一层到前一层的反向传播理解不了。另外由于权重值一样，那么权重值到底该怎么更新？</p>
</div>

<h2 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h2><p>上面讲到的都是<script type="math/tex">T_x = T_y</script>，但是有时候输入和输出的长度并不相同。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84RNN%E5%AE%9E%E4%BE%8B.jpg" alt="不同类型的RNN实例"><br>多对多（many to many）、多对一（many to one）、一对一（one to one）、一对多（one to many）架构<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg" alt="不同类型的RNN结构"></p>
<h2 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h2><h2 id="对新序列采样"><a href="#对新序列采样" class="headerlink" title="对新序列采样"></a>对新序列采样</h2><h2 id="长期依赖，梯度消失"><a href="#长期依赖，梯度消失" class="headerlink" title="长期依赖，梯度消失"></a>长期依赖，梯度消失</h2><p>观察两个句子：</p>
<ul>
<li>The cat, which already ate…, was full.</li>
<li>The cats, which already ate…, were full.</li>
</ul>
<p>这两个句子只有复数形式上的不同，但是开头的名词影响到了最后面的be动词。但是我们目前见到的最基本的RNN不擅长捕获这种长期依赖效应。<br>用梯度消失解释一下为什么，其实原理相同的，这里引用之前的文章<br><a href="https://yan624.github.io//%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8">梯度消失和梯度爆炸</a></p>
<h1 id="GRU单元——Gate-Recurrent-Unit"><a href="#GRU单元——Gate-Recurrent-Unit" class="headerlink" title="GRU单元——Gate Recurrent Unit"></a>GRU单元——Gate Recurrent Unit</h1><p>中文名为门控循环单元。它解决了梯度消失的问题。</p>
<h2 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h2><p>c = memonry cell，使用<script type="math/tex">c^{<t>}</script>符号表示输出，其中<script type="math/tex">c^{<t>} = a^{<t>}</script>，由于后面的LSTM的c和a代表意思不同，所以这里直接使用c来表示输出值。所以本小章下的c你都看作是a即可。</p>
<h2 id="GRU工作流程"><a href="#GRU工作流程" class="headerlink" title="GRU工作流程"></a>GRU工作流程</h2><p>由于通过<script type="math/tex">c^{<t-1>}</script>来更新<script type="math/tex">c^{<t>}</script>的值，但是现在我们使用GRU，GRU就是来控制是否更新<script type="math/tex">c^{<t>}</script>的值的，这里使用“更新”的名词可能有点怪，因为<script type="math/tex">c^{<t>}</script>实际上是通过<script type="math/tex">c^{<t-1>}</script><strong>计算</strong>出来的。那么公式<script type="math/tex">a^{<t>} = g_1(w_{a} * [a^{<t-1>}, x^{<t>}] + b_a)</script>变为<script type="math/tex">\tilde{c}^{<t>} = tanh(w_{c} * [c^{<t-1>}, x^{<t>}] + b_c)</script>，这里的<script type="math/tex">\tilde{c}^{<t>}</script>是一个候选值——candidate value，类似于中间变量，而激活函数我们选择tanh。<br>GRU的核心是有一个Gate，就是上面说的是否更新值的功能，它的公式为<script type="math/tex">\Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)</script>，<script type="math/tex">\Gamma_u</script>的u的意思是update，sigmoid函数的输出范围在0-1之间，所以就完成了类似更新的功能。如果是0就代表不让你更新，如果是1就代表让你更新，这里听起来还有点绕，没关系看下面的表达式。<br>这时开始执行更新步骤：<script type="math/tex">c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}</script>，这一步可以看出如果<script type="math/tex">\Gamma_u</script>等于1就将<script type="math/tex">c^{<t>}</script>更新为<script type="math/tex">\tilde{c}^{<t>}</script>，如果等于0就相当于不让你更新，结果还是上一个的c，即<script type="math/tex">c^{<t-1>}</script>。<br>将公式写在一起，GRU的工作流程就是：</p>
<script type="math/tex; mode=display">
\begin{cases}
    \tilde{c}^{<t>} = tanh(w_{c} * [c^{<t-1>}, x^{<t>}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)\\
    c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}\\
\end{cases}</script><h2 id="GRU完整版"><a href="#GRU完整版" class="headerlink" title="GRU完整版"></a>GRU完整版</h2><p>可以看到下式中就多了一个<script type="math/tex">\Gamma_r</script>，但是为什么不用上面的简化版呢？那是因为经研究者多年的尝试，发现下面的版本是很实用的，也算是一个标准版，你可以自己开发不同的版本。</p>
<script type="math/tex; mode=display">
\begin{cases}
    \tilde{c}^{<t>} = tanh(w_{c} * [\Gamma_r * c^{<t-1>}, x^{<t>}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)\\
    \Gamma_r = \sigma(w_{r} * [c^{<t-1>}, x^{<t>}] + b_r)\\
    c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}\\
\end{cases}</script><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>吴恩达老师讲得感觉理解起来有点费劲，因为他觉得图片比文字更难理解，所以写了一大堆公式，只是再后面补充了图片。所以我建议看李宏毅老师的深度学习视频来理解LSTM。李宏毅老师的视频用了一张图片很好的解释了LSTM，并且他还举了一个例子，更加生动形象。<br>可能是东西方的差异，我感觉是图片好理解点，所以我选择看李宏毅老师的视频。这里就不写了，因为我在<strong>下面写了</strong>李宏毅老师课程的<strong>笔记</strong>。</p>
<h1 id="双向神经网络"><a href="#双向神经网络" class="headerlink" title="双向神经网络"></a>双向神经网络</h1><h1 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h1><hr>
<p>李宏毅机器学习</p>
<hr>
<h1 id="字母表示-1"><a href="#字母表示-1" class="headerlink" title="字母表示"></a>字母表示</h1><p>跟吴恩达老师讲的类似，李宏毅老师也讲了文字如何表示，与吴恩达老师不同的是，李宏毅老师多讲了几个。<br>最简单的方法利用向量来表示文字，就是上面说过的one-hot：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/1%20of%20N%20encoding.jpg" alt="1 of N encoding"><br>因为会出现某些单词没见到过，所以需要使用other这一维来表示。并且在右边的图中还可以使用字母来表示。然后理想上只要将词向量放入神经网络就会出现结果。但是Feedforward Network其实没办法解决这问题。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/beyond%201-of-N%20encoding.jpg" alt="beyond 1-of-N encoding"><br>可以看到下图，由于Feedforward Network没有记忆，所以两个句子对它来说是一个意思，但是对人来说可以很明显判断出第一句话台北是目的地，第二句话台北是出发地。Feedforward Network它只能训练当前的词，前一个词是什么它并不知道。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward%20Network%E6%97%A0%E6%B3%95%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="Feedforward Network无法解决的问题"></p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>上面讲到Feedforward Network由于没有记忆，无法记住前一个或者前几个词，所以就诞生了RNN。RNN其实也没那么神秘，就是每次输入并交给激活函数计算完毕后，将计算结果存入缓存中，并且在下一次计算时，将缓存取出来一起计算。就是下图的蓝色方框，由于是第一次计算，其中初始化为0。下图第一遍已经在计算了，实际上已经准备更新蓝色方框中的值了。RNN在上面的章节中其实已经写过了，都是类似的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%80%E4%B8%AARNN%E7%9A%84%E5%B0%8F%E5%9E%8B%E4%BE%8B%E5%AD%90.jpg" alt="一个RNN的小型例子"><br>经过上面的例子发现，当前的输入已经在依赖前一个的缓存了，所以当顺序有所变化，或者前一个数据有所变化时，RNN可以察觉到，输出的结果也自然不同。</p>
<h2 id="deep-RNN"><a href="#deep-RNN" class="headerlink" title="deep RNN"></a>deep RNN</h2><p>我一共写了两个RNN的笔记，无论是吴恩达老师的还是李宏毅老师的到目前为止，RNN其实都不是deep的，之前也在疑惑，RNN横轴有很多层，但是实际上那些层只是不同时间的输入，根本不算deep。今天继续看下去，发现这个问题终于有解了，RNN也可以是deep的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/deep%20RNN.jpg" alt="deep RNN"></p>
<h2 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h2><p>上面讲的RNN都被称为Elman Network。还有另一种辩题叫做Jordan Network，它将输出值缓存起来。传说之中Jordan Network可以有更好的性能。<br><div class="note primary">
            <p>为什么有更好的性能</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Elman%20Network%E5%92%8Cordan%20Network.jpg" alt="Elman Network和ordan Network"></p>
<h2 id="双向RNN——Bidirectional-RNN"><a href="#双向RNN——Bidirectional-RNN" class="headerlink" title="双向RNN——Bidirectional RNN"></a>双向RNN——Bidirectional RNN</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Bidirectional%20RNN.jpg" alt="RNN——Bidirectional RNN"></p>
<h1 id="长短期记忆——Long-Short-term-Memory-LSTM"><a href="#长短期记忆——Long-Short-term-Memory-LSTM" class="headerlink" title="长短期记忆——Long Short-term Memory(LSTM)"></a>长短期记忆——Long Short-term Memory(LSTM)</h1><div class="note primary">
            <p>LSTM的神经元个数不同有什么区别？其他的NN架构也有同样的疑问</p>
          </div>
<p>上面讲的memory实际上是最简单的，LSTM才是现在最常用的Memory。Menory在RNN中实际只是一个神经元而已，它负责输入和输出。它们之间的关联是：RNN依旧是RNN，只不过把RNN中的神经元换成了LSTM。我们知道神经元的逻辑其实很简单，只有输入——计算——输入到激活函数——输出激活值，而LSTM只不过麻烦一点罢了。<br>下图就是一个LSTM。Input Gate中如果f(z)是1就代表Gate打开，也就是f(z)*g(z) = 1 * g(z) = g(z)，就相当于可以让外界输入。如果f(z)=0，Gate被关闭，那么 f(z)*g(z)=0，是不是就像不允许外界输入一样？因为你输入多少都被置为0。而Forget Gate也类似，当f(z)=1时，即Forget Gate被打开，这里与直觉有点相反，因为Gate打开，有点感觉像遗忘。但是其实c*f(z) = 1，所以Forget Gate为1其实是记住原本的c的意思。<br>另外图中也写到了，Gate的激活函数一般选sigmoid，里面的值就代表Gate的打开程度。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E7%A4%BA%E4%BE%8B.jpg" alt="LSTM示例"></p>
<h2 id="LSTM的例子"><a href="#LSTM的例子" class="headerlink" title="LSTM的例子"></a>LSTM的例子</h2><p>例子介绍：只有一个LSTM，输入有3维，输出有1维。<script type="math/tex">x_2 = 1</script>则<script type="math/tex">x_1</script>的值就会被存到Memory中，<script type="math/tex">x_2 = -1</script>则重置Memory，<script type="math/tex">x_3 = 1</script>则输出。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E4%BE%8B%E5%AD%90%E4%BB%8B%E7%BB%8D.jpg" alt="LSTM例子介绍"><br>注：下图中的蓝色数字和灰色数字是权重值。<br><div class="note primary">
            <p>权重值是初始化的？还是固定的？还是初始化后自己可以训练的？其实就是LSTM的反向传播算法要弄懂。</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E4%BE%8B%E5%AD%90%E8%AE%A1%E7%AE%97.jpg" alt="LSTM例子计算"></p>
<ol>
<li>Input Gate：<br>将偏差设为-10是因为我们通过x2来对Input Gate控制。平常x2=0，计算x*w+b=-10，那么通过sigmoid function就会得到一个接近于0的值，所以就实现了将Input Gate关闭的功能。而如果x2=1，那么x2*100=100，通过sigmoid function就会得到一个接近于1的值，Input Gate就实现了打开的功能。</li>
<li>Forget Gate: 这里的功能跟Input Gate类似。</li>
<li>Output Gate: 如果Output Gate被关闭，那么输出0.</li>
</ol>
<h2 id="多个LSTM工作场景"><a href="#多个LSTM工作场景" class="headerlink" title="多个LSTM工作场景"></a>多个LSTM工作场景</h2><p>里面的<script type="math/tex">x^t</script>就是对应于NN中的一个向量，它分别乘上4个参数矩阵得到4个不同的向量，以此操控LSTM，而LSTM实际上就等于神经元，说白了就是一个类似激活函数的功能。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF.jpg" alt="LSTM实际工作场景"><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF2.jpg" alt="LSTM实际工作场景2"><br>多个LSTM连起来工作就是像下面一样，红线和红线旁边的那个黑色曲线链接的值之前没有讲过，但是下图的这样才是LSTM实际的长相，所以之前讲的那么复杂实际上还是LSTM的简化版。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="LSTM实际工作流程"></p>
<h1 id="RNN反向传播"><a href="#RNN反向传播" class="headerlink" title="RNN反向传播"></a>RNN反向传播</h1><p>BPTT——backpropagation through time，与NN的backpropagation类似，李宏毅老师也没讲原理直接跳过了。<br>然而不幸的是，RNN的training是很困难的。下面蓝色的线是希望的结果，但是实际上是绿色的线，会出现剧烈地抖动，最后在某个点出现NAN。这就是类似梯度消失问题。可以使用一些办法解决，但是现在用得最多的方法是LSTM。<br><div class="note primary">
            <p>为什么LSTM能解决RNN的难题</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/RNN%20training%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="RNN trWaining碰到的问题"></p>
<h1 id="其他解决梯度消失的办法"><a href="#其他解决梯度消失的办法" class="headerlink" title="其他解决梯度消失的办法"></a>其他解决梯度消失的办法</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%85%B6%E4%BB%96%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8A%9E%E6%B3%95.jpg" alt="其他解决梯度消失的办法"></p>
<h1 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h1><h2 id="Many-to-one"><a href="#Many-to-one" class="headerlink" title="Many to one"></a>Many to one</h2><p>输入一个向量sequence，只输出一个向量。</p>
<ol>
<li>语义分析。比如分析电影评论是好是坏。</li>
<li>key term extraction。对文档提取关键词。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many to One.jpg" alt="Many to One"></p>
<h2 id="Many-to-many-Output-is-shorter"><a href="#Many-to-many-Output-is-shorter" class="headerlink" title="Many to many(Output is shorter)"></a>Many to many(Output is shorter)</h2><p>输入和输出都是向量sequence，但是输出要短。</p>
<ol>
<li>Speech Recognition 。语音辨识。</li>
</ol>
<h2 id="Many-to-many-No-limitation"><a href="#Many-to-many-No-limitation" class="headerlink" title="Many to many(No limitation)"></a>Many to many(No limitation)</h2><p>输入和输出都是序列且长短不一。被称为 <strong>Sequence to sequence learning</strong> 。</p>
<ol>
<li>Machine Translation. 机器翻译。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many to Many（No Limitation）.jpg" alt="Many to Many(No Limitation)"></p>
<h2 id="Beyond-Sequence"><a href="#Beyond-Sequence" class="headerlink" title="Beyond Sequence"></a>Beyond Sequence</h2><ol>
<li>Syntactic parsing</li>
</ol>
<hr>
<p>李宏毅深度学习</p>
<hr>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>此系列视频还有两个Review视频，分别为第一个视频：Basic Structures for Deep Learning Models(Part 1)， 第二个视频：Basic Structures for Deep Learning Models(Part 2)。<br>个人认为Review视频不需要看，而且这两个视频时间贼长，加起来得有两个多小时。没必要浪费时间，即使你根本没学过Review中的知识点也不用去看。他的Review里不会讲很深，基本上就过过场，就算有很深的东西也完全不影响继续往下学。1P时长80分钟，说实话如果自己属于小白阶段，去看那么长的视频是挺打击人的兴趣的，如果是大佬或者已经入门的人当然看得津津有味了。<br><a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#字母表示-1">此文</a>记录了李宏毅机器学习视频中讲解的RNN的笔记。</p>
<h1 id="Computational-Graph-amp-Backpropagation"><a href="#Computational-Graph-amp-Backpropagation" class="headerlink" title="Computational Graph &amp; Backpropagation"></a>Computational Graph &amp; Backpropagation</h1><div class="note danger">
            <p>2019年6月7号更新：关于计算图这章，现在才发现原来很重要，因为这是完成<strong>自动求导</strong>的关键。学了 pytorch 之后才发现的。</p>
          </div>
<h2 id="什么是Computational-Graph"><a href="#什么是Computational-Graph" class="headerlink" title="什么是Computational Graph"></a>什么是Computational Graph</h2><p>这实际上跟要学的深度学习没什么关系，只是名字好听点，无视就好，如下图就是一个Computational Graph。主要用来在计算神经网络一些输出时，便于理解。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E4%BE%8B%E5%AD%90.jpg" alt="Computational Graph例子"><br>在看一个比较贴近实际的例子，顺便复习一下链式求导法则。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99%E7%A4%BA%E4%BE%8B.jpg" alt="Computational Graph链式求导法则示例"></p>
<h2 id="通过链式求导的例子理解反向传播（Backpropagation）算法"><a href="#通过链式求导的例子理解反向传播（Backpropagation）算法" class="headerlink" title="通过链式求导的例子理解反向传播（Backpropagation）算法"></a>通过链式求导的例子理解反向传播（Backpropagation）算法</h2><p>首先进行正向链式求导，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E6%AD%A3%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="正向链式求导"><br>图中要求计算e对a求偏导，首先给出a=3, b=2。其中c=a+b, d=b+1。<br>按照李宏毅老师使用链式求导法则，先要计算c对a求导得到1。e再对c求导得到b+1，带入b=2，得到3。所以3对a求偏导等于1*3=3。<br>上面这种链式求导法则有点乱，如果没仔细学过<em>微积分</em>可能难以理解。其实对于方程e = (a+b) * (b+1)，e对a求偏导，直接看出来都可以。利用考研时的口诀“左导右不导，左不导右导”（也就是<a href="https://baike.baidu.com/item/%E8%8E%B1%E5%B8%83%E5%B0%BC%E8%8C%A8%E5%85%AC%E5%BC%8F/8779293?fr=aladdin" target="_blank" rel="noopener">莱布尼茨公式</a>），直接得到结果<script type="math/tex">\frac{\partial e}{\partial a} = b+1</script>。<br>然后将b=2带入b+1得到结果还是3。</p>
<p>接着进行反向模式，如下图:<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%8F%8D%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="反向链式求导"><br>现在图中要求计算<script type="math/tex">\frac{\partial e}{\partial a}</script>以及<script type="math/tex">\frac{\partial e}{\partial b}</script>，当然你可以分别进行两次链式求导，得到结果。但是如果从e出发，也就是反向，那么就可以同时得到<script type="math/tex">\frac{\partial e}{\partial a}</script>以及<script type="math/tex">\frac{\partial e}{\partial b}</script>的结果。<br>不要在意e为什么等于1，只不过一个输入而已。<br>此外，如果阅读过《deep learning and neural network》一书，看过吴恩达机器学习视频或者其它资料的应该已经能反应出来。连接线上的求偏导实际上就跟神经网络上的权重一个意思，然后也是一层一层地反向传播。<br>这个输入e实际上就是神经网络中的反向传播算法中的输入。就是最后一层神经元的误差<script type="math/tex">\delta^l = h-y</script>。这里吴恩达老师和《deep learning and neural network》作者的最后一层误差公式不一样，<strong>目前不明</strong>，暂时不做解释，这里的公式是吴恩达老师的。<br>然后就是误差*权重+偏差得到前一层的误差，具体不展开。</p>
<h2 id="反向传播的好处"><a href="#反向传播的好处" class="headerlink" title="反向传播的好处"></a>反向传播的好处</h2><p>如果你的root只有一个，那么这个Computational Graph中的所有偏微分就都可以一次性算出。对应于神经网络，我们就是要这样的效果。</p>
<h2 id="参数共享（Parameter-sharing）"><a href="#参数共享（Parameter-sharing）" class="headerlink" title="参数共享（Parameter sharing）"></a>参数共享（Parameter sharing）</h2><p>略，看了一眼貌似挺简单。16:20</p>
<h2 id="Computational-Graph-for-Feedforword-Net"><a href="#Computational-Graph-for-Feedforword-Net" class="headerlink" title="Computational Graph for Feedforword Net"></a>Computational Graph for Feedforword Net</h2><p>李宏毅深度学习p3从21:16到52:48讲解梯度下降算法、前馈神经网络以及反向传播算法的具体数学原理<br>一直没看懂原理，以后再看。</p>
<h2 id="Computational-Graph-for-Recurrent-Network"><a href="#Computational-Graph-for-Recurrent-Network" class="headerlink" title="Computational Graph for Recurrent Network"></a>Computational Graph for Recurrent Network</h2><h1 id="Deep-Learning-for-Language-Modeling"><a href="#Deep-Learning-for-Language-Modeling" class="headerlink" title="Deep Learning for Language Modeling"></a>Deep Learning for Language Modeling</h1><p>语言模型就是预测一个word sequence出现的几率有多大。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Language%20Modeling.jpg" alt="Language Modeling"></p>
<h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>N-gram是自然语言处理中的算法。2-gram读作bi-gram。</p>
<h3 id="传统做法"><a href="#传统做法" class="headerlink" title="传统做法"></a>传统做法</h3><ul>
<li>怎么预测一句话出现的几率</li>
<li>收集大量文本作为训练数据<ul>
<li>然后计算<script type="math/tex">w_1\cdots w_n</script>这句话在训练数据中出现的概率</li>
</ul>
</li>
<li>N-gram语言模型：<ul>
<li>如何计算一小部分的概率？例如下图的p(beach|nice)出现的概率。就是将nice beach出现的次数除以nice出现的次数。</li>
</ul>
</li>
</ul>
<p>前两条是理想的处理办法，但是麻烦的是要预测的句子在语料库——corpus中八成一次都没出现过。于是就需要使用N-gram模型。它的处理办法就是将句子拆成比较小的部分——component，再把每个小部分的概率乘起来就是句子出现的几率。像下图这种只考虑前一个单词的模型叫做2-gram model。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/N-gram.jpg" alt="N-gram"></p>
<h3 id="NN-based-LM"><a href="#NN-based-LM" class="headerlink" title="NN-based LM"></a>NN-based LM</h3><p>怎么做基于NN的N-gram？<br>做法：</p>
<ol>
<li>搜集training数据</li>
<li>learn一个Neural Network，通过两个词predict下一个词，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/NN-based%20LM.jpg" alt="NN-based LM"></li>
<li>使用cross entropy minimize</li>
<li>有了Neural Network后算一个句子的几率，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9A%84%E5%87%A0%E7%8E%87.jpg" alt="计算句子的几率"><br>其中STRAT是一个token，代表句子的起始。</li>
</ol>
<h3 id="RNN-based-LM"><a href="#RNN-based-LM" class="headerlink" title="RNN-based LM"></a>RNN-based LM</h3><p>往上翻<strong>循环神经网络——RNN</strong>，原理就是这个。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/RNN-based LM.jpg" alt="RNN-based LM"></p>
<h3 id="Challenge-of-N-gram"><a href="#Challenge-of-N-gram" class="headerlink" title="Challenge of N-gram"></a>Challenge of N-gram</h3><h4 id="NN-based-model"><a href="#NN-based-model" class="headerlink" title="NN-based model"></a>NN-based model</h4><p>为什么要使用NN-based model。相较于传统方法有什么好处。<br>就是概率估不准，因为永远没有足够的数据。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Challenge%20of%20N-gram.jpg" alt="Challenge of N-gram"><br><div class="note info">
            <p>视频13:20~27:01仔细讲解了为什么要使用NN，而且把我困惑了快一个月的问题解决了，就是将文字转为数字之后进行训练的意义。</p>
          </div></p>
<h4 id="RNN-based-model"><a href="#RNN-based-model" class="headerlink" title="RNN-based model"></a>RNN-based model</h4><p>为什么要使用RNN-based model。相较于传统方法有什么好处。</p>
<h1 id="几个有用的network架构"><a href="#几个有用的network架构" class="headerlink" title="几个有用的network架构"></a>几个有用的network架构</h1><h2 id="Spatial-Transformer-Layer"><a href="#Spatial-Transformer-Layer" class="headerlink" title="Spatial Transformer Layer"></a>Spatial Transformer Layer</h2><p>&emsp;&emsp;<a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" target="_blank" rel="noopener">论文地址</a>，中文可以叫<strong>空间变换层</strong>。<br>&emsp;&emsp;此神经网络架构的出现的原因：CNN 对图片的缩放以及旋转无所谓（CNN is invariant to scaling androtation）。比如说在图片的局部地区中，一个人移动一点点距离，对 CNN 来说其实没什么多大区别。不过距离有点远的话，还是有点影响的。</p>
<h2 id="Highway-Network"><a href="#Highway-Network" class="headerlink" title="Highway Network"></a>Highway Network</h2><p>&emsp;&emsp;先对前馈神经网络和 RNN 进行一下对比。</p>
<ol>
<li>Feedforward NN 不是每一步都有输入。</li>
<li>Feedforward NN 每一层都有不同的参数。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward NN和RNN的对比.jpg" alt="Feedforward NN和RNN的对比"></li>
</ol>
<p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Network 论文地址</a>；<a href="https://arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="noopener">Highway Network 实战论文地址</a><br>&emsp;&emsp;Highway Network 的想法就是把 RNN <strong>立</strong>起来，把它当做前馈神经网络来用。<br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">Highway Network 的改进版论文地址</a>，这个就是<strong>残差神经网络</strong>。</p>
<h2 id="Grid-LSTM"><a href="#Grid-LSTM" class="headerlink" title="Grid LSTM"></a>Grid LSTM</h2><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1507.01526.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;太复杂了，估计以后也很难用到。。。</p>
<h2 id="Recusive-Network"><a href="#Recusive-Network" class="headerlink" title="Recusive Network"></a>Recusive Network</h2><p>&emsp;&emsp;Recursive Network 是 Recurrent Network 更 Generalize 的版本。Recurrent Network 是 Recursive Network 的一个特殊的例子，如果翻译成中文的话，实际上名字都一样。所以可以称之为递归式网络。以下是 RNN 和 Recursive Network 的对比图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Recursive Network示意图.jpg" alt="Recursive Network示意图"></p>
<p>&emsp;&emsp;在做  Recursive Network 之前，需要考虑输入的序列的结构。图中将 <script type="math/tex">x_1</script> 和 <script type="math/tex">x_2</script> 一同输入进一个 function，但是其实可以不这么做，具体要怎么输入，取决于输入数据的结构。<strong>而由于 f 与 f 前后相接，所以在写代码时需要预先做好设计</strong>。<br>&emsp;&emsp;举个具体的例子，要判断“not very good”包含什么情绪，可以先使用语法解析，将句子结构化，然后根据句子的语法结构来使用 Recursive Network 进行训练，如下图：<br>&emsp;&emsp;“very”的词向量和“good”的词向量一同放入 f 中训练，我们可以将得到的向量看做是“very good”的意思。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/根据句子语法结构训练1.jpg" alt="根据句子语法结构训练1" title="根据句子语法结构训练1"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/根据句子语法结构训练2.jpg" alt="根据句子语法结构训练2" title="根据句子语法结构训练2"></p>
<p>&emsp;&emsp;当然两个词向量不能是简单的相加，具体做法可以自行选择。最简单的做法可以参考下图的上半部分，而下图的下半部分被称为 <strong>Recursive Neural Tensor Network</strong>，总而言之就是一个很复杂的做法来解决两个词向量不仅仅是进行简单的拼接或者相加。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Recursive Neural Tensor Network.jpg" alt="Recursive Neural Tensor Network"></p>
<p>&emsp;&emsp;对于 f 还有其他的做法，如 Matrix-Vector Recursive Network，<a href="https://arxiv.org/pdf/1503.00075.pdf" target="_blank" rel="noopener">Tree LSTM 2015</a> 等。具体就不记了，以后可以查 Recursive Network 相关论文。</p>
<h1 id="Conditional-Generation-by-RNN-amp-Attention"><a href="#Conditional-Generation-by-RNN-amp-Attention" class="headerlink" title="Conditional Generation by RNN &amp; Attention"></a>Conditional Generation by RNN &amp; Attention</h1><p>&emsp;&emsp;注意本文讲的是 RNN <strong>入门</strong>，而下面的部分也只是讲普通的 RNN Generation，甚至连 decoder 部分都没用。下图是生成文字，其实也可以生成图片、音频等，我就不一一截图了，第二张图将这些<strong>想法</strong>及其<strong>论文</strong>都汇总了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/一个简单的Generation.jpg" alt="一个简单的Generation"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Generation汇总.jpg" alt="Generation汇总"></p>
<h2 id="Conditional-Generation"><a href="#Conditional-Generation" class="headerlink" title="Conditional Generation"></a>Conditional Generation</h2><p>&emsp;&emsp;但是在真实的场景中，我们不仅仅是希望只生成随机的句子，我们更偏向于生成一些基于某些条件的句子，比如：当看见一张一个人正在跳舞的图片，我们希望电脑生成“A young girl is dancing”；当给予一个条件“Hello”时，我们希望电脑生成“Hello, nice to see you.”。<br>&emsp;&emsp;一个实际的例子，我们可以将一张图片输入进 CNN，从而产生一个向量，再把该向量输入进 decoder 部分，最后生成句子。如下图所示，其他类型的<strong>条件生成</strong>也类似。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Image Caption Generation.jpg" alt="Image Caption Generation" title="Image Caption Generation"></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>&emsp;&emsp;将 <script type="math/tex">z_0</script> 与 <script type="math/tex">h_1 h_2 h_3 h_4</script> 分别做一次 match，至于 match 怎么计算可以看下图右边，总而言之，就是可以由你自己设计。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制.jpg" alt="Attention机制"></p>
<p>&emsp;&emsp;然后获得 <script type="math/tex">a^1_0 a^2_0 a^3_0 a^4_0</script>，之后将它们输入 softmax 层（有实验发现其实不经过 softmax 层也可以，甚至效果更好），最后将所有 a 分别乘上它们对应的向量并且相加，得到一个向量 <script type="math/tex">c^0</script>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制计算score.jpg" alt="Attention机制计算score"></p>
<p>&emsp;&emsp;使用 Attention 机制计算完毕后，将向量 <script type="math/tex">c^0</script> 输入进 decoder 即可，接下来的计算都是以此类推。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制计算完毕后输入到decoder.jpg" alt="Attention机制计算完毕后输入到decoder"></p>
<h3 id="Attention应用到Speach-Recognition"><a href="#Attention应用到Speach-Recognition" class="headerlink" title="Attention应用到Speach Recognition"></a>Attention应用到Speach Recognition</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention for Speach Recognition.jpg" alt="Attention for Speach Recognition"></p>
<h2 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h2><p>&emsp;&emsp;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;Memory Network 最先被用在 Reading Comprehension，说白了就是一个 Attention 机制。下图就是一个简易的 Memory Network，其实还有更复杂的版本，但是由于文章写太长不好，我就不截图了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/一个简易的Memory Network.jpg" alt="一个简易的Memory Network" title="一个简易的Memory Network"></p>
<h2 id="Neural-Turing-Machine"><a href="#Neural-Turing-Machine" class="headerlink" title="Neural Turing Machine"></a>Neural Turing Machine</h2><h2 id="Tips-for-Generation"><a href="#Tips-for-Generation" class="headerlink" title="Tips for Generation"></a>Tips for Generation</h2><p>&emsp;&emsp;这里听不太懂，跳过了。有 Beam Search 之类的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/对Generation的建议1.jpg" alt="对Generation的建议1"></p>
<h1 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h1><p>&emsp;&emsp;<a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;举一个简单的例子助于理解 Pointer Network。在二维坐标系中任意给出 4 个点，我们的目标是找到几个点，将它们连起来形成一个封闭圈，剩下的那几个点要正好在这个封闭圈之中，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/助于理解Pointer Network的一个例子.jpg" alt="助于理解Pointer Network的一个例子"></p>
<p>&emsp;&emsp;当然，这肯定已经有一些算法可以解了，比如在坐标系中计算距离。但是今天我们使用硬 train 一发的方法，即不管三七二十一将它输入到神经网络里面训练。可以制造一些训练数据，然后给 encoder-deocder 训练。<strong>具体的训练步骤为</strong>：输入点的坐标，输出表示 <code>{1,2,3,4,END}</code> 的五维向量，碰到 END 则代表解码完毕。<br>&emsp;&emsp;但是结果是训练不起来的，因为在上述的例子中我们只输入了 4 个点，我们的目的是得到 1-4 个点。但是如果我们的测试数据是输入 400 个点呢？那么我们也只会得到 1-4 个点，因为 <code>{1,2,3,4,END}</code> 是预先定义好的。你可能会想那就多定义一点啊，但是下次我要是输入 4000 个点呢？要是 40000 个点呢？总有你无法预先定义解决得到时候。<br>&emsp;&emsp;所以我们需要 <strong>Pointer Network</strong> 来<strong>动态的改变类别</strong>（具体做法详见下一小节），注意我这里直接说成类别了，我们可以把 decoder 部分看作是多元分类的工作，如输出 4000 个点，就是 4000元分类。<br>&emsp;&emsp;<strong>上面的例子其实是 Pointer Netwoek 论文中的其中一个例子，但是对于这个例子来说，使用 Pointer Network 其实没多大意义，因为本身有更简单的解法，下面说一下有意义的用途。</strong><br>&emsp;&emsp;Pointer Network 应用于 <strong>Summarization</strong>，<a href="https://arxiv.org/pdf/1704.04368.pdf" target="_blank" rel="noopener">论文地址</a>。对于给定一篇文档，让机器做出总结来说，我们会碰到很多<strong>生僻的地名、人民</strong>等等字词。我们可以使用 Pointer Network 来解决这个问题。<br>&emsp;&emsp;下图就是做法，整张图的意思就是在做文本摘要的工作，输入一个句子，输出摘要。先不看 <script type="math/tex">p_{gen}</script>，看看其他部分就是很普通的 encoder-decoder。但是对于这个 encoder-decoder 来说，词表中并没有 <em>Aregentina</em> 这个单词。那么我们就可以使用 Pointer Network，这个 <script type="math/tex">p_{gen}</script> 就是类似做 Attention 计算的 <script type="math/tex">z^0</script>。最后结果就是我们将注意力关注到 <em>Aregentina</em> 这个单词。当然对于 encoder-decoder 这部分的工作也是要做的，我们可以将两个结果加起来，从而判断出最终要产生哪个单词，做法详见原论文。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Pointer Network for Summarization.jpg" alt="Pointer Network for Summarization"></p>
<p>&emsp;&emsp;还可用于 <strong>machine learning</strong>、<strong>chatbot</strong> 等。</p>
<h2 id="具体做法"><a href="#具体做法" class="headerlink" title="具体做法"></a>具体做法</h2><p>&emsp;&emsp;具体的实现就是个你下图一样，首先在输入的序列之前加入一个 END 序列，然后将 decoder 删掉。我们还是使用 Attention 机制计算每个序列的 attention score，但是这次的 score 不再乘上它对应的向量，而是直接当做向量输出，意思就是把所以的 score 做一次 max，最大的就输出 1。而<strong>停止条件就是 END 这个序列的 score 是最大的，即为 1 就停止训练。</strong><br>&emsp;&emsp;这样的做法乍一看好像无法理解，我解释一下。由于 encoder 是对序列的长度不敏感的，也就是说如果预先定义的类别是 40 维，而我输入 400 个点，那么对于 encoder 来说，它可以增加神经元的数量从而使得 400 个点<strong>正好</strong>全部输入进 encoder。但是对于 decoder 来说，它输出只能是 40 维。<strong>那么 Pointer Network 的做法是将 decoder 删除，把输出的工作也交给 encoder 去做。所以我输入 400 个点，自然也就可以输出 400 维的类别</strong>（这里应该是 401 维，因为还有一个 END 序列）。看下图的 encoder，<script type="math/tex">h^4</script> 的分数是 0.7，所以我们的输出就是 4，当然对于向量来说就是 <script type="math/tex">(0, 0, 0, 0, 1)</script>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Pointer Network的做法.jpg" alt="Pointer Network的做法"></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/git学习记录.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/git学习记录.html" class="post-title-link" itemprop="url">git学习记录</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-22 23:51:08" itemprop="dateCreated datePublished" datetime="2019-04-22T23:51:08+08:00">2019-04-22</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-24 23:34:20" itemprop="dateModified" datetime="2019-04-24T23:34:20+08:00">2019-04-24</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是在学习<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">该教程</a>时/后做的笔记。<br>我现在用git基本都是用<a href="https://desktop.github.com/" target="_blank" rel="noopener">Github Desktop</a>，前面的是下载地址。用起来方便又快捷。事实上我也不会用git的命令o(<em>￣︶￣</em>)o所以今天稍微学一下。</p>
<h1 id="git-init"><a href="#git-init" class="headerlink" title="git init"></a>git init</h1><p>切换到想要创建仓库的文件夹，执行命令<code>git init</code>就会在该文件夹下创建一个.git的文件夹，这个文件夹是隐藏的。</p>
<h1 id="git-add-git-commit"><a href="#git-add-git-commit" class="headerlink" title="git add/git commit"></a>git add/git commit</h1><p>使用命令<code>git add whatever.txt</code>将文件添加到仓库。使用命令<code>git commit -m &quot;wrote a file&quot;</code>将文件提交到仓库，-m后面的是描述这份文件你改了什么。其实就是相当于desktop的一个按钮，按一下就把全部有改动文件都提交了。<br>这样就完成了提交一份文件。这里就会有疑问了，为什么设计成先add再commit？直接commit不就行了？因为commit可以提交多份文件，你可以使用add命令一份一份地添加文件，再使用commit一次性提交到仓库。<br>该命令指示推送到本地仓库，并非远程仓库。</p>
<h1 id="git-status"><a href="#git-status" class="headerlink" title="git status"></a>git status</h1><p>查看仓库当前的修改状态</p>
<h1 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h1><p>发现某份文件被修改了，但是忘记改了什么怎么办？使用命令<code>git diff modified_file.txt</code>查看，它会显示文件哪里被修改了。diff就是difference的意思。</p>
<h1 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h1><p>git可以记住你的历史提交版本，如果有一天电脑损坏，自己干了什么完全忘记，可以使用<code>git log</code>命令。它会显示以前所有的历史记录。这条命令会显示很详细的信息，但是就是因为信息太详细了，人可能看不过来，可以加上<code>--pretty=oneline</code>来限制。类似<code>c3fca95239a4bbe21ee2991e0a914fb522060e74</code>这种是版本号（commit id）。</p>
<h1 id="git-rest"><a href="#git-rest" class="headerlink" title="git rest"></a>git rest</h1><p>该命令可以回退版本。<code>git reset --hard HEAD^</code>，命令里的HEAD代表当前版本，^代表上一个版本，如果想要回退至前100个版本，可以使用HEAD~100。<br>也可以直接指定commit id，如<code>git reset --hard c3fca95239a4bbe21ee2991e0a914fb522060e74</code>，commit id可以不写全，写个开头就行了<code>git reset --hard c3fca</code><br>注意回退版本后，如果关闭git bash那么就无法查询到该版本之后的所有版本。</p>
<h1 id="git-reflog"><a href="#git-reflog" class="headerlink" title="git reflog"></a>git reflog</h1><p>该命令记住了你每一步操作，如果回退版本后后悔了，可以使用该命令查询以前的commit id。</p>
<h1 id="git-checkout-—filename"><a href="#git-checkout-—filename" class="headerlink" title="git checkout —filename"></a>git checkout —filename</h1><p>把文件夹在工作区的修改全部撤销。总之，就是让这个文件回到最近一次git commit或git add时的状态。<br>参考<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374831943254ee90db11b13d4ba9a73b9047f4fb968d000" target="_blank" rel="noopener">文章</a></p>
<h1 id="git-rm"><a href="#git-rm" class="headerlink" title="git rm"></a>git rm</h1><p>删除文件，与linux命令类似。</p>
<h1 id="git-remote"><a href="#git-remote" class="headerlink" title="git remote"></a>git remote</h1><p>将本地的仓库和远程的仓库关联，使用命令<code>git remote add origin git@github.com:github_account_name/repository_name.git</code><br>注意将github_account_name和repository_name分别替换成github账号名和仓库名。添加关联后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的。下一步，就可以把本地库的所有内容推送到远程库上。</p>
<h1 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h1><p><code>git push -u origin master</code></p>
<blockquote>
<p>把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。<br>由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。<br>从现在起，只要本地作了提交，就可以通过命令：<br><code>git push origin master</code><br>把本地master分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！</p>
</blockquote>
<h1 id="git-clone"><a href="#git-clone" class="headerlink" title="git clone"></a>git clone</h1><p>上面说了将本地仓库和远程仓库关联，并将本地仓库的文件推送到远程仓库，那么自然也可以从远程仓库clone文件到本地仓库。<br>使用命令：<code>git clone git@github.com:github_account_name/repository_name.git</code><br>还可以从<a href="https://github.com/yan624/yan624.github.io.git这样的地址克隆" target="_blank" rel="noopener">https://github.com/yan624/yan624.github.io.git这样的地址克隆</a></p>
<h1 id="对分支的管理"><a href="#对分支的管理" class="headerlink" title="对分支的管理"></a>对分支的管理</h1><p>字太多，不想打了。看下面教程。<br><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840038939c291467cc7c747b1810aab2fb8863508000" target="_blank" rel="noopener">教程</a></p>
<h2 id="git-checkout-b-branch-name"><a href="#git-checkout-b-branch-name" class="headerlink" title="git checkout -b branch_name"></a>git checkout -b branch_name</h2><p>创建名为branch_name的分支并切换到该分支，-b参数代表切换。</p>
<h2 id="git-branch"><a href="#git-branch" class="headerlink" title="git branch"></a>git branch</h2><p>查看当前分支，如果分支之前有*就代表这个分支是主分支。</p>
<h2 id="git-merge-branch-name"><a href="#git-merge-branch-name" class="headerlink" title="git merge branch_name"></a>git merge branch_name</h2><p>合并分支</p>
<h2 id="git-branch-d-branch-name"><a href="#git-branch-d-branch-name" class="headerlink" title="git branch -d branch_name"></a>git branch -d branch_name</h2><p>删除名为branch_name分支</p>
<h2 id="合并分支发生冲突"><a href="#合并分支发生冲突" class="headerlink" title="合并分支发生冲突"></a>合并分支发生冲突</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840202368c74be33fbd884e71b570f2cc3c0d1dcf000" target="_blank" rel="noopener">解决办法</a></p>
<h2 id="强大的分支功能"><a href="#强大的分支功能" class="headerlink" title="强大的分支功能"></a>强大的分支功能</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013758410364457b9e3d821f4244beb0fd69c61a185ae0000" target="_blank" rel="noopener">创建分支的策略</a></p>
<h2 id="bug分支。将当前工作暂存，先修改出现的bug"><a href="#bug分支。将当前工作暂存，先修改出现的bug" class="headerlink" title="bug分支。将当前工作暂存，先修改出现的bug"></a>bug分支。将当前工作暂存，先修改出现的bug</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137602359178794d966923e5c4134bc8bf98dfb03aea3000" target="_blank" rel="noopener">暂存命令</a></p>
<h2 id="feature分支"><a href="#feature分支" class="headerlink" title="feature分支"></a>feature分支</h2><p>与上面类似，无非概念不同</p>
<h2 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013760174128707b935b0be6fc4fc6ace66c4f15618f8d000" target="_blank" rel="noopener">教程</a></p>
<h2 id="git-rebase"><a href="#git-rebase" class="headerlink" title="git rebase"></a>git rebase</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="使用github"><a href="#使用github" class="headerlink" title="使用github"></a>使用github</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="使用码云"><a href="#使用码云" class="headerlink" title="使用码云"></a>使用码云</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00150154460073692d151e784de4d718c67ce836f72c7c4000" target="_blank" rel="noopener">教程</a></p>
<h1 id="配置文件的更多配置"><a href="#配置文件的更多配置" class="headerlink" title="配置文件的更多配置"></a>配置文件的更多配置</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00150154460073692d151e784de4d718c67ce836f72c7c4000" target="_blank" rel="noopener">教程</a></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/git学习记录.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/更改hexo-next主题的fontawesome版本至最新.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/更改hexo-next主题的fontawesome版本至最新.html" class="post-title-link" itemprop="url">更改hexo next主题的fontawesome版本至最新</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-22 13:23:47 / 修改时间：15:09:00" itemprop="dateCreated datePublished" datetime="2019-04-22T13:23:47+08:00">2019-04-22</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/hexo/" itemprop="url" rel="index"><span itemprop="name">hexo</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>由于在后续又发现在其他地方也用到了icon，所以需要大量更改配置。<strong>如果不会写程序的还是别改了。</strong></p>
<p>next的fontawesome默认版本是4.6.2，在写本文时，fontawesome的最新版本是5.8.1。貌似fontawesome在5.0.0版本之后改版了。总之一直出现方框乱码。<br>后来发现，现在的fontawesome链接已经跟以前不一样了，它现在分为3大类别。<br>现在的使用方法是：在next主题的_config.xml中搜索fontawesome，并更改属性<code>fontawesome: //cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css</code>，注意里面的文件是all.min.css，而不是font-awesom.min.css。<br>但是这样更改之后还是会出现方框乱码，原因是next默认使用的fa的类，而有时候我们需要使用fab或其他的类。所以需要修改一下源代码。<br>找到layout/_macro/menu/menu-item.swig，定位class=”menu-item-icon，将后面的“fa fa-fw fa-”删去。以后再修改icon不能只加一个名字了。可以像我这样修改：<code>assorted: /assorted || fa fa-fw fa-layer-group</code>。<br>可以看到我将icon的名称补全了。如果想用fab的类，可以像这样修改：<code>python: /python || fab fa-fw fa-python</code>。以此类推。</p>
<p>这样修改以后，如果不想用fontawesome了，想用其他的icon库，改起来也很方便。</p>
<p>layout/_macro/menu/menu-item.swig被layout/_partials/header/sub-menu.swig引用。</p>
<p>另外由于fontawesome版本改动，社交软件的icon也需要更改，在_config.xml中搜索github，将icon改为<code>fab fa-fw fa-github</code>。找到ayout/_macro/siderbar.swig，搜索fa fa-fw fa-，看看定位的地点上面是不是<code>{百分号  if theme.social_icons.enable 百分号}</code>。是的话将fa fa-fw fa-删除。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/更改hexo-next主题的fontawesome版本至最新.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/ViewPager无法刷新数据.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/ViewPager无法刷新数据.html" class="post-title-link" itemprop="url">ViewPager无法刷新数据</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-18 22:15:24 / 修改时间：22:38:54" itemprop="dateCreated datePublished" datetime="2019-04-18T22:15:24+08:00">2019-04-18</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/android/" itemprop="url" rel="index"><span itemprop="name">android</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>实现ViewPager刷新数据功能，在网上找了很多资料都已经过时了。<br>由于本人并不是android开发出身，完全是做app玩的。所以很多术语都不知道，如果看不懂就算了。。。</p>
<p>实现PagerAdapter类，我命名为HomePagerAdapter<br><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HomePagerAdapter</span> <span class="keyword">extends</span> <span class="title">PagerAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;View&gt; pageView;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HomePagerAdapter</span><span class="params">(ArrayList&lt;View&gt; pageView)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pageView = pageView;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mChildCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">notifyDataSetChanged</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mChildCount = getCount();</span><br><span class="line">        <span class="keyword">super</span>.notifyDataSetChanged();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getItemPosition</span><span class="params">(Object object)</span>   </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> ( mChildCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            mChildCount --;</span><br><span class="line">            <span class="keyword">return</span> POSITION_NONE;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">return</span> <span class="keyword">super</span>.<span class="title">getItemPosition</span><span class="params">(object)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//获取当前窗体界面数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="function"><span class="keyword">return</span> pageView.<span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//判断是否由对象生成界面</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isViewFromObject</span><span class="params">(View arg0, Object arg1)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="keyword">return</span> arg0==arg1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">destroyItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position, Object object)</span> </span>&#123;</span><br><span class="line">        container.removeView(pageView.get(position));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Object <span class="title">instantiateItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position)</span> </span>&#123;</span><br><span class="line">        View view = pageView.get(position);</span><br><span class="line">        container.addView(view);</span><br><span class="line">        <span class="keyword">return</span> view;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">finishUpdate</span><span class="params">(ViewGroup container)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(container.getChildCount() == <span class="number">0</span>)&#123;</span><br><span class="line">            pageView.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意destroyItem、instantiateItem方法，PagerAdapter中还有两个同名的方法，但是已被废弃，注意参数不同。<br>实现刷新数据主要在destroyItem、instantiateItem、finishUpdate三个方法。其他的方法其实别人的教程也写了，但是我这三个方法是我自己研究的，我没见别人写过。</p>
<p>解释流程。<br>第一步，改变数据，我是将数据保存在了<code>private ArrayList&lt;View&gt; pageView;</code>中。<br>第二步，调用<code>adapter.notifyDataSetChanged();</code>方法，它首先会销毁item，即调用<code>destroyItem(ViewGroup container, int position, Object object)</code>方法。随即调用<code>instantiateItem(ViewGroup container, int position)</code>方法。<br>一般来说大家都是这么干的，因为将数据改变后，调用<code>adapter.notifyDataSetChanged();</code>方法。直觉认为这么做合乎常理。<br>但是这里注意一点，假设<code>private ArrayList&lt;View&gt; pageView;</code>中原先保存两个View，改变数据将这个View删除，从新添加三个新的View，那么在<code>destroyItem(ViewGroup container, int position, Object object)</code>方法中，它无法删除，仔细看里面的代码<code>container.removeView(pageView.get(position));</code>，发现它是通过position这个索引获取对象，再在container容器中通过对象查找删除。那么问题来了，你之前已经将两份View删除了，它还怎么通过position获取到呢？所以在这一步出了问题。<br>当然这一步出了问题后，后面的创建页面步骤更是稀巴烂。</p>
<p>正确步骤如下：<br><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将页面刷新，渲染新的数据集</span></span><br><span class="line"><span class="selector-tag">homePagerAdapter</span><span class="selector-class">.notifyDataSetChanged</span>();</span><br><span class="line"><span class="selector-tag">changeData</span>(inflater, data);</span><br><span class="line"><span class="selector-tag">homePagerAdapter</span><span class="selector-class">.notifyDataSetChanged</span>();</span><br></pre></td></tr></table></figure></p>
<p>第一步，不要更改数据，直接调用<code>homePagerAdapter.notifyDataSetChanged();</code>，目的是让其删除原先的view。<br>第二步，更改数据。<br>第三步，再次调用<code>homePagerAdapter.notifyDataSetChanged();</code>，完成页面的创建。由于container中已经没有view了，所以删除那个步骤做了也等于没做，但是由于数据已经更新页面还是会被创建出来。</p>
<p>最后强调用一点。在HomePagerAdapter类中一个<code>finishUpdate(ViewGroup container)</code>方法，注意看里面的代码。<strong>以上的所有步骤，全部依赖于这几句代码。</strong><br>上面第一步说到直接调用notifyDataSetChanged()方法目的是删除原先的view，但是view删除后，你必须将<code>private ArrayList&lt;View&gt; pageView;</code>中的数据也删除。<strong>这里补充一点，pageView内是我创建的View，而container中是android自己维护的界面</strong>，我也不知道怎么称呼，就将其称为界面吧。<br>在finishUpdate()方法中判断，如果container中已经没有界面了，那就直接移除pageView中所有的数据，也就是算更新数据了。值得注意的是，这里面逻辑及其复杂，这行清空数据的代码，只有放在<code>finishUpdate(ViewGroup container)</code>中执行，并且必须加上那个if条件判断，app才能正常运行。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/ViewPager无法刷新数据.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">记录学习问题，积累做的 leetcode 题目</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">117</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">18</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
            <br>
						<!--同个ip访问本站页面次数-->
						<div class="site-state-item site-state-posts" style="border-left:none;">
								<span class="site-state-item-count" id="busuanzi_value_site_pv"></span>
								<span class="site-state-item-name">浏览量</span>
						</div>
						<!--不同ip访问本站次数-->
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count" id="busuanzi_value_site_uv"></span>
								<span class="site-state-item-name">访客量</span>
						</div>
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count">101k</span>
								<span class="site-state-item-name">总字数</span>
						</div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        








        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

	<script src="/lib/my-utils.js"></script>
<!--图片缩放插件-->
<script src="/lib/zoomify/zoomify.min.js"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!--不蒜子统计-->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(6), {
		duration:90000,//1min半一换
		fade: 1500
	});
</script>
<!-- 图片缩放 -->
<script>
$('#content img').zoomify({duration: 500, });
  $('#content img').on('zoom-in.zoomify', function () {
    $('#sidebar').css('display', 'none');
  });
  $('#content img').on('zoom-out-complete.zoomify', function () {
    $('#sidebar').css('display', '');
  });
</script>

	

</body>
</html>
