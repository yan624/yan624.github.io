<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2">























  

<link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.10.2/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


<!--图片缩放插件样式-->
<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">

  <meta name="description" content="记录学习问题，积累做的 leetcode 题目">
<meta name="keywords" content="博客，java，javaWeb，NLP，python，机器学习，深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/page/5/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="记录学习问题，积累做的 leetcode 题目">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="记录学习问题，积累做的 leetcode 题目">






  <link rel="canonical" href="http://yan624.github.io/page/5/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
	<!--加载flower canvas-->
<script>
var pathname = window.location.pathname;
if(pathname == '/flower.html'){
	var body =  document.getElementsByTagName('body')[0];
	var canvas = document.createElement("canvas")
	canvas.setAttribute('id', 'sakura')
	// '<canvas id="sakura"></canvas>'
	body.appendChild(canvas)
}
</script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">低阶炼金术士</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">19</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">23</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">124</span></a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/机器学习算法（七）：K-NN.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/机器学习算法（七）：K-NN.html" class="post-title-link" itemprop="url">机器学习算法（七）：K-NN</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-01 15:14:48" itemprop="dateCreated datePublished" datetime="2019-07-01T15:14:48+08:00">2019-07-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-06 13:10:25" itemprop="dateModified" datetime="2019-07-06T13:10:25+08:00">2019-07-06</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p>K-NN 算法采用测量不同特征值之间的距离的方法进行分类。<br>工作原理：<br>存在一个<strong>样本数据集</strong>，也称作训练样本集，并且样本集中每个数据都存在标签。输入<strong>没有标签的新数据</strong>后，将<strong>新数据</strong>的每个特征与<strong>样本集</strong>中的数据对应特征进行比较，然后算法提取样本集中特征最相似的数据（最邻近）的分类<strong>标签</strong>。一般来说，只选择样本数据集中前 k 个最相似的数据，这就是 k-NN 算法中 k 的出处，通常 k 是不大于 20 的整数。<br>最后选择在 k 个最相似的数据中出现次数最多的分类，作为新数据的分类。</p>
</blockquote>
<div id="flowchart-0" class="flow-chart"></div>

<p>简单来说，K-NN 算法使用了一种计算特征之间的距离的公式，然后选择距离前 k 近的数据，获取这些数据的标签。通过一个简单的统计，获取这 k 项数据中最多的类别。最后我们将新数据看作是这个类别。</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    group = np.array([[<span class="number">1.0</span>, <span class="number">1.1</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.1</span>]])</span><br><span class="line">    labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(input, dataset, labels, k)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    K-NN 分类</span></span><br><span class="line"><span class="string">    :param input: 输入数据，即待分类的数据</span></span><br><span class="line"><span class="string">    :param dataset: 训练数据集</span></span><br><span class="line"><span class="string">    :param labels: dataset 对应的标签</span></span><br><span class="line"><span class="string">    :param k: 显而易见</span></span><br><span class="line"><span class="string">    :return: input 的类别</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算特征之间的距离，只是一个很简单的算法</span></span><br><span class="line">    <span class="comment"># 先算差，再平方，然后将一个项数据的所有特征累加，最后开方</span></span><br><span class="line">    all_distances = np.sqrt(np.sum(np.power((input - dataset), <span class="number">2</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 对距离进行逆序排序</span></span><br><span class="line">    sorted_distance_indices = all_distances.argsort()</span><br><span class="line">    <span class="comment"># 对类别进行计数</span></span><br><span class="line">    class_count = &#123;&#125;</span><br><span class="line">    <span class="comment"># 选取前 k 项数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        <span class="comment"># 第 i 项数据的标签</span></span><br><span class="line">        label = labels[sorted_distance_indices[i]]</span><br><span class="line">        <span class="comment"># 标签存在则加 1，不存在就默认是 0 再加 1</span></span><br><span class="line">        class_count[label] = class_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据标签的数量排序</span></span><br><span class="line">    sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_class_count[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    X, Y = create_dataset()</span><br><span class="line">    res = classify([<span class="number">0</span>, <span class="number">0</span>], X, Y, <span class="number">3</span>)</span><br><span class="line">    print(res)</span><br></pre></td></tr></table></figure>
<h2 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h2><p>由于有些数据范围波动较大，可以进行均值归一化处理。</p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>引用《机器学习实战》中的应用。</p>
<ol>
<li>可以分类电影的类别，已知数据：打斗镜头、接吻镜头、<strong>电影的类别</strong>。如果给定一部新电影，则可以根据该电影的打斗镜头、接吻镜头来计算此部电影属于哪种类别。</li>
<li>改进约会网站配对效果。已知数据：每年获得的飞行常客里程数、玩视频游戏所耗时间百分比、每周消费的冰淇淋公升数、<strong>用户交往对象的类别</strong>。其中<strong>用户交往对象的类别</strong>指：<ul>
<li>不喜欢的人</li>
<li>魅力一般的人</li>
<li>极具魅力的人<br>则可以输入一个新的约会对象的数据，从而判断此人属于哪种类别，如果属于不喜欢的人的类别，那么用户可以提前得知，并且决定不去约会。</li>
</ul>
</li>
<li>甚至可以识别手写数字。将图片转换成 0 1 表示，即数字部分用 1 表示，其他部分用 0 表示。组成一个 32 x 32 数字矩阵，然后将矩阵转换为 1 x 1024 的向量。其中的每一维度的值可以看作为一个特征。算法类似。</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/机器学习算法（七）：K-NN.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/2019 CCF会议总结.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019 CCF会议总结.html" class="post-title-link" itemprop="url">2019 CCF会议总结</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-27 21:33:09" itemprop="dateCreated datePublished" datetime="2019-06-27T21:33:09+08:00">2019-06-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-07 15:13:48" itemprop="dateModified" datetime="2019-09-07T15:13:48+08:00">2019-09-07</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/conference/" itemprop="url" rel="index"><span itemprop="name">conference</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h1><h2 id="知识图谱问答系统概述"><a href="#知识图谱问答系统概述" class="headerlink" title="知识图谱问答系统概述"></a>知识图谱问答系统概述</h2><p>现在的<strong>搜索引擎</strong>工作流程是输入要搜索的内容，搜索引擎返回一大堆内容，供你自己选择。<br><strong>问答系统</strong>是下一代的搜索引擎的基本形态。</p>
<blockquote>
<p>以直接而准确的方式回答用户自然语言提问的自动问答系统将构成下一代搜索引擎的基本形态。</p>
</blockquote>
<p>下图展示问答系统在近几十年的发展历史。</p>
<ol>
<li>1960 年的问答系统属于专家系统（模版系统）</li>
<li>1990 - 2000 年的问答系统属于基于信息检索的 QA 系统</li>
<li>2000 - 2010 年的问答系统属于社区 QA 系统</li>
<li>2011 年之后的问答系统属于基于知识图谱的 QA 系统</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/问答系统的历史.jpg" alt="问答系统的历史"></p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>问答系统的分类（或者说三个阶段）：</p>
<ol>
<li>IR-based QA：基于<strong>关键词匹配 + 信息抽取</strong>，任然是基于<strong>浅层语义分析</strong></li>
<li>Community QA：依赖于网民贡献，问答过程任然依赖于<strong>关键词检索技术</strong></li>
<li>KB-based QA：Knowledge Base，例如：WolfframAlpha</li>
</ol>
<p>根据问答形式分类：</p>
<ol>
<li>一问一答：字面意思，也是演讲的主题</li>
<li>交互式问答：就是进行连续的复杂的问答</li>
<li>阅读理解</li>
</ol>
<div class="note warning">
            <p>KB-QA 现在只能解决事实性的问题，无法解决：</p><ol><li>怎么去天安门</li><li>西红柿炒鸡蛋怎么做等提问</li></ol><p>某公司（在会议上没听清，可能是一个公司）只有 5% 的问题能用 KB-QA 解决。</p>
          </div>
<h2 id="什么是知识图谱"><a href="#什么是知识图谱" class="headerlink" title="什么是知识图谱"></a>什么是知识图谱</h2><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/知识图谱示例.jpg" alt="知识图谱示例"></p>
<h3 id="知识图谱基本架构"><a href="#知识图谱基本架构" class="headerlink" title="知识图谱基本架构"></a>知识图谱基本架构</h3><p>图中三元组中的 Ent1、Ent2 等指的是 entity。entity 可以在架构中选取，比如将 concept 作为 entity 或者将 instance 作为 entity。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/知识图谱基本架构.jpg" alt="知识图谱基本架构"></p>
<h3 id="运用知识图谱问答"><a href="#运用知识图谱问答" class="headerlink" title="运用知识图谱问答"></a>运用知识图谱问答</h3><p>语义如何表示是其中的一个问题：</p>
<ol>
<li>使用符号表示的形式（传统方法）</li>
<li>使用分布式表示方法</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/运用知识图谱问答.jpg" alt="运用知识图谱问答"></p>
<h3 id="知识图谱问答的两类方法（根据技术路线分）"><a href="#知识图谱问答的两类方法（根据技术路线分）" class="headerlink" title="知识图谱问答的两类方法（根据技术路线分）"></a>知识图谱问答的两类方法（根据技术路线分）</h3><ol>
<li>语义解析(Semantic Parsing)：问句转换成形式化的查询语句，进行结构化查询得到答案</li>
<li>语义检索（Answer Retrieval &amp; Ranking）：简单的搜索得到候选答案，利用问句和候选答案的匹配程度(特征)抽取答案</li>
</ol>
<h2 id="公开的评测数据集"><a href="#公开的评测数据集" class="headerlink" title="公开的评测数据集"></a>公开的评测数据集</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/公开的评测数据集.jpg" alt="公开的评测数据集"><br>例如：</p>
<script type="math/tex; mode=display">
    \text{图数据结构}
    \begin{cases}
        QALD \\
        WebQuestions \\
        Simple Question\\
    \end{cases}\\
    \text{表数据结构}
    \begin{cases}
        WikiSQL & \text{一个表} \\
        Spider & \text{多个表} \\
    \end{cases}</script><h2 id="知识图谱问答基于的几种方法"><a href="#知识图谱问答基于的几种方法" class="headerlink" title="知识图谱问答基于的几种方法"></a>知识图谱问答基于的几种方法</h2><ol>
<li>基于符号语义解析的知识图谱问答<ul>
<li>语义表示（lambda 验算，DCS Tree）</li>
<li>语义解析方法（CCG）<ul>
<li>还有许多语义解析方法，略</li>
</ul>
</li>
</ul>
</li>
<li>基于语义检索的知识图谱问答<ul>
<li>基于显示特征的知识检索</li>
<li>基于端到端的知识图谱问答</li>
</ul>
</li>
<li>基于神经符号计算的知识图谱问答<ul>
<li>基于序列学习的解析方法</li>
<li>基于动作序列的解析方法</li>
<li>基于对战神经网络的端到端问答方法</li>
</ul>
</li>
</ol>
<h3 id="基于符号语义解析的知识图谱问答"><a href="#基于符号语义解析的知识图谱问答" class="headerlink" title="基于符号语义解析的知识图谱问答"></a>基于符号语义解析的知识图谱问答</h3><p>两种技术的具体实现过程略过，对比如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/Lambda演算vs.DCSTree.jpg" alt="Lambda演算vs.DCSTree"></p>
<h3 id="基于语义检索的知识图谱问答"><a href="#基于语义检索的知识图谱问答" class="headerlink" title="基于语义检索的知识图谱问答"></a>基于语义检索的知识图谱问答</h3><ul>
<li>基于显示特征的知识检索<ul>
<li>关键词检索</li>
<li>文本蕴含推理</li>
<li>逻辑表达式</li>
<li><div class="note primary">
            <p>给出了许多研究进展。</p>
          </div></li>
</ul>
</li>
<li>基于端到端的知识图谱问答<ul>
<li>LSTM</li>
<li>Attention Model</li>
<li>Memory Network</li>
<li><div class="note primary">
            <p>其中有部分问题：</p><ol><li>如何学习？<ul><li>RNN</li><li>CNN</li><li>Transformer</li></ul></li><li>问句如何表示？<ul><li>取所有词向量的平均值</li><li>关注答案不同的部分，问句的表示应该问句的不同部分</li><li>等</li></ul></li><li><strong>考虑多维度的相似度</strong><ul><li>从多个角度计算问句和知识的语义匹配（语义相似度）</li><li>问句如何表示？</li><li>依据问答特点，考虑答案不同维度的信息</li></ul></li></ol><p>PPT 中给出了许多研究进展，包括最基本的做法。</p>
          </div>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/基于语义检索的知识图谱问答.jpg" alt="基于语义检索的知识图谱问答"></li>
</ul>
</li>
</ul>
<h3 id="基于神经符号计算的知识图谱问答"><a href="#基于神经符号计算的知识图谱问答" class="headerlink" title="基于神经符号计算的知识图谱问答"></a>基于神经符号计算的知识图谱问答</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/符号语义解析vs.深度学习.jpg" alt="符号语义解析vs.深度学习"></p>
<ul>
<li><p>基于序列学习的解析方法</p>
<ul>
<li>seq2seq<ul>
<li>RNN-based</li>
<li>with Attention</li>
</ul>
</li>
<li><p>基于序列学习的神经符号计算</p>
<div class="note primary">
            <p>就是运用<strong>基于符号语义解析的知识图谱问答</strong>的原理，让神经网络生成这些符号，而不是生成文字。</p>
          </div>
<blockquote>
<p>基于序列学习的方法将问句和答案的逻辑表达式看作为两个序列</p>
<ul>
<li>使用序列转换的神经网络模型（如 Seq2Seq）来建模</li>
<li>神经网络生成的逻辑表达式可能不合语法规范</li>
</ul>
</blockquote>
<ul>
<li>Seq2Tree</li>
</ul>
</li>
</ul>
</li>
<li>基于动作序列的解析方法<ul>
<li>Seq2Action</li>
</ul>
</li>
<li>基于对战神经网络的端到端问答方法</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>限定域的深度问答的准确度比较高，开放域的深度问答的准确度还是处于较低的水平。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/深度问答的性能.jpg" alt="深度问答的性能"></p>
<h1 id="对话系统"><a href="#对话系统" class="headerlink" title="对话系统"></a>对话系统</h1><p>对话系统也可以直白的称为聊天机器人。<br>目前 54% 的用户会使用闲聊（开放域对话）功能。26% 的用户会选择使用某些功能性功能，比如查出行路线、查天气等。其余小部分用户使用其他的功能。<br>目前大部分的聊天机器人都基于<strong>微软小冰</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/各种聊天机器人.jpg" alt="各种聊天机器人"></p>
<p>聊天机器人一共分为两种：</p>
<ol>
<li>检索式</li>
<li>生成式</li>
</ol>
<h2 id="Response-Selection-for-Retrieval-based-Chatbots"><a href="#Response-Selection-for-Retrieval-based-Chatbots" class="headerlink" title="Response Selection for Retrieval-based Chatbots"></a>Response Selection for Retrieval-based Chatbots</h2><p>检索式又分为单轮和多轮。<br>单轮不考虑回复历史。下图展示了一个单轮回复的场景，用户提出一个问题，机器人需要在一堆回复中检索出一个最有可能的结果来对用户进行回复。多轮回复与单轮类似，只不过多轮需要考虑上下文的对话。最后也是选择一个最优可能的结果进行回复。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复.jpg" alt="检索式单轮回复"></p>
<div class="note info">
            <p>&emsp;&emsp;对于单轮：<br>&emsp;&emsp;回复不只回复 Top1 的候选回复，而是要训练一个 classifier，从而随机地返回一个回复。因为如果回复总是为同一个，用户可能会感觉很无聊。<br>&emsp;&emsp;对于多轮：<br>&emsp;&emsp;有一些挑战：</p><ul><li>A hierarchical data structure<ul><li>Words -&gt; utterances -&gt; session</li></ul></li><li>Information redundancy<ul><li>Not all words and utterances are useful for response selection</li></ul></li><li>Logics<ul><li>Order of utterances matters in response selection</li><li>Long-term dependencies among words and utterances</li><li>Constraints to proper responses</li></ul></li></ul>
          </div>
<p>下面是检索式单轮回复系统架构图和多轮回复系统架构图的对比。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复架构图.jpg" alt="检索式单轮回复架构图"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式多轮回复架构图.jpg" alt="检索式多轮回复架构图"></p>
<h3 id="单轮回复中使用的模型"><a href="#单轮回复中使用的模型" class="headerlink" title="单轮回复中使用的模型"></a>单轮回复中使用的模型</h3><p>一共有两种框架，分别为：Framework I 和 Framework II。<br><strong>Framework I 和 Framework II 的区别是</strong>：</p>
<ol>
<li>Framework I 是将句子表示为向量，Framework II 将字表示为向量。</li>
</ol>
<p><strong>Framework I 和 Framework II 的比较：</strong></p>
<ul>
<li>Efficacy（功效）：<ol>
<li>一般来讲，在外界公布出的数据集上，Framework II 模型比 Framework I 模型更好。因为在 Framework II 中的 interaction 充分保留了一个 message-response pair 中的匹配信息。</li>
</ol>
</li>
<li>Efficiency（效率）：<ol>
<li>由于过多的 interaction，Framework II 的模型普遍比 Framework I 的模型在计算上代价更大。</li>
<li>由于可以预先计算 messages and responses 的表示并将它们以索引形式存储。所以当对线上响应时间有严格要求时， Framework I 的模型更可取。</li>
</ol>
</li>
</ul>
<p>下图是 Framework I 的架构，其中最下层的 sentence embedding layer 大概就是词向量，然后需要经过一个 Representation function（这个 function 下面会给出架构）。最后将已经经过 Representation function 转换后的 q 和 r 送入 Matching layer，该层有一个 Matching function（这个 function 下面也会给出架构）。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I.jpg" alt="检索式单轮回复的 Framework I"></p>
<p>下图是 Representation funtion 的结构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I 的 Representation funtion.jpg" alt="检索式单轮回复的 Framework I 的 Representation funtion"></p>
<p>下图是 Matching funtion 的结构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I 的 Matching funtion.jpg" alt="检索式单轮回复的 Framework I 的 Matching funtion"></p>
<p><strong><em>有一些特殊的模型：Arc-I，Attentive LSTM 等</em></strong></p>
<p>Framework II 的架构与 Framework I 类似，只是多了一个 Interaction Function。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework II.jpg" alt="检索式单轮回复的 Framework II"></p>
<p>Interaction 由两种形式：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework II 中 Interaction 的两种类型.jpg" alt="检索式单轮回复的 Framework II 中 Interaction 的两种类型"></p>
<p><strong><em>有一些特殊的模型：Match Pyramid，Match LSTM 等</em></strong></p>
<p>PPT 中有数据集。以及很多 reference。</p>
<h3 id="多轮回复中使用的模型"><a href="#多轮回复中使用的模型" class="headerlink" title="多轮回复中使用的模型"></a>多轮回复中使用的模型</h3><p>&emsp;&emsp;对于多轮回复也有两种框架，分别为：Framework I 和 Framework II。<br>&emsp;&emsp;具体的架构略。PPT 里都有。</p>
<h1 id="技术总结"><a href="#技术总结" class="headerlink" title="技术总结"></a>技术总结</h1><h2 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h2><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019 CCF会议总结.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/深度学习算法（三）：RNN 各种机制.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/深度学习算法（三）：RNN 各种机制.html" class="post-title-link" itemprop="url">深度学习算法（三）：RNN 各种机制</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-23 17:07:17" itemprop="dateCreated datePublished" datetime="2019-05-23T17:07:17+08:00">2019-05-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-15 15:22:24" itemprop="dateModified" datetime="2019-08-15T15:22:24+08:00">2019-08-15</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>&emsp;&emsp;该<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#长短期记忆——Long-Short-term-Memory-LSTM">博客</a>中描述了一个 LSTM 的例子，已经把大部分的东西概括了。但是今天看了别人的代码，这是第一次见到代码形式的 LSTM，感觉还是有些地方有问题。以下就记录这些问题。<br>&emsp;&emsp;下图是吴恩达深度学习第五周作业中的图片，是一个 LSTM 单元。与李宏毅老师做的图有略微不同，并且在下图中将 input gate 称为了 update gate。</p>
<ol>
<li>首先是<strong>输入的问题</strong>。一般来说一个 LSTM 的输入是前一<strong>个</strong> LSTM 的输出值 <script type="math/tex">a</script> 以及输入值 <script type="math/tex">x</script>（对于第 2 层的 LSTM 的 输入值就是前一<strong>层</strong>的输出值）。但是众所周知，<strong>LSTM 每个门的输入肯定只有一个向量，<script type="math/tex">a</script> 和 <script type="math/tex">x</script> 是两个向量，那么如何处理呢？</strong> <strong>1)</strong>在下图中使用了 <script type="math/tex">[a^{<t-1>},x^{<t>}]</script> 进行向量拼接。<strong>2)</strong>在我看的代码中直接使用了加法进行相加，代码<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch/blob/master/seq2seq/atis/lstm/main.py" target="_blank" rel="noopener">在这</a>，但是代码量太大了，随便看看就行了。二者的差别无非是向量的维度。</li>
<li>第一个 LSTM 的输入值怎么处理？因为它不存在前一个 LSTM。答：<strong>暂且使用随机初始化，具体还要补充</strong>（感觉 0 也可以，婴儿出生的时候不就是一张白纸吗。。。）。</li>
<li>记忆单元中的数据也可以随机初始化或者直接为 0。</li>
<li>每一层的 LSTM 都权重共享。意思是每一层都有多个 LSTM，里面的权重值其实是同一份。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/LSTM cell.jpg" alt="LSTM cell"></p>
<p>&emsp;&emsp;下图是多个 LSTM 运行的示意图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/多个 LSTM.jpg" alt="多个 LSTM"></p>
<p>&emsp;&emsp;下图是 LSTM 的反向传播，被称为 BPTT（backpropagation through time）。由于还没遇到过，并且 pytorch 都已经是自动求导，所以目前处于待补充状态。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/LSTM反向传播.jpg" alt="LSTM反向传播"></p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><p>&emsp;&emsp;该<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#seq2seq">博文</a>也可供参考。以前想过 encoder-decoder 如何实现，以为很复杂。现在自己实现了一下，实际上就是很简单的代码，<a href="https://github.com/yan624/machine_learning_algorithms/blob/master/dl/seq2seq.py" target="_blank" rel="noopener">代码地址</a>。<br>&emsp;&emsp;由于 seq2seq 有两个输入。对于 encoder 输入包含隐藏状态 a 以及 输入值 x。对于 decoder 输入包含隐藏状态 a 和 encoder 的输出。这实际上跟 LSTM 差不多，如果 seq2seq 的神经元使用 LSTM 的话，实际上就一模一样了。我只使用了简单的加法，将两个输入合并。当然也可以使用其他方法，比如拼接。其他的方法可以自行发挥。<br>&emsp;&emsp;另外对于 seq2seq 学习，已经不需要每个输入的长度都相等了，可以在句子的最后加入一个结束符，如<code>&lt;EOS&gt;</code>，以此判断输入是否结束。但是这样做如何进行向量化呢？暂时未知，待补充。</p>
<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>&emsp;&emsp;<a href="https://www.zhihu.com/question/68482809/answer/264632289" target="_blank" rel="noopener">参考文章</a>；<a href="https://www.cnblogs.com/robert-dlut/p/5952032.html" target="_blank" rel="noopener">Attention历史</a><br>&emsp;&emsp;实际上九几年的时候在CV领域已经有这概念了。RNN 领域第一篇文章《Recurrent Models of Visual Attention》。<br>&emsp;&emsp;<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#Attention">博客</a>中有写到如何计算 Attention。</p>
<h2 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h2><p>&emsp;&emsp;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#Memory-Network">博客</a>有记一些基础的东西。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/深度学习算法（三）：RNN 各种机制.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/深度学习算法（二）：simple RNN 推导与理解.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/深度学习算法（二）：simple RNN 推导与理解.html" class="post-title-link" itemprop="url">深度学习算法（二）：simple RNN 推导与理解</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-23 15:48:00" itemprop="dateCreated datePublished" datetime="2019-05-23T15:48:00+08:00">2019-05-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-11-12 17:12:28" itemprop="dateModified" datetime="2019-11-12T17:12:28+08:00">2019-11-12</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_recurrent_neural_network" target="_blank" rel="noopener">github地址</a><br>simple RNN只实现了正向传播，反向传播没有实现。是因为simple RNN有梯度消失的问题，索性直接不写了。下一节直接写LSTM。</p>
<h1 id="simple-RNN和simple-NN的对比"><a href="#simple-RNN和simple-NN的对比" class="headerlink" title="simple RNN和simple NN的对比"></a>simple RNN和simple NN的对比</h1><p>本来觉得simple RNN挺简单的，只不过是simple NN的扩展，区别无非是将simple NN的神经元换成一个RNN cell。但是实际上没那么简单，特别是加上将数据向量化后计算。感觉简直和simple NN是两种架构。</p>
<h2 id="input"><a href="#input" class="headerlink" title="input"></a>input</h2><p>下图是一个simple NN的架构，其中的一层也叫做全连接层。所以也叫做前馈神经网络。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>网络中输入的x是一个纯数字，而不是向量。如果输入的是向量，就代表一次输入了多条样本。总的来说，一条样本向量化为：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1\\
    x_2\\
    x_3\\
\end{pmatrix}</script><p>多条样本向量化为：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 & x_4\\
    x_2 & x_5\\
    x_3 & x_6\\
\end{pmatrix}</script><p>但是在simple <strong>R</strong>NN中，因为RNN可以用做自然语言处理，在NLP领域一个数据就是一个单词或者一个词组，我们需要先将词组用数字表示，但是一个字只用一个数表示显然是不现实的。我们通常使用词向量（word vector）表示，所以问题就来了。如果我用simple NN来做自然语言处理，我该怎么处理这些数据。<br>我需要输入这句话——我 是 一名 学生。将其用变量一一对应：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 = \text{我}\\
    x_2 = \text{是}\\
    x_3 = \text{一名}\\
    x_4 = \text{学生}\\
\end{pmatrix}</script><p>如果再输入一句话——今天 天气 好像 不错。将其用变量一一对应：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 = \text{我} & x_5 = \text{今天}\\
    x_2 = \text{是} & x_6 = \text{天气}\\
    x_3 = \text{一名} & x_7 = \text{好像}\\
    x_4 = \text{学生} & x_8 = \text{不错}\\
\end{pmatrix}</script><p>这样会出现<strong>极大的问题</strong>，即文字无法进行数学运算。所以需要将文字转为词向量。但是问题是我如何在一个神经元输入一个向量？我如果输入多条样本，那么我整个输入值x就会变成<strong>3维的矩阵</strong>。simple NN显然是处理不了的。所以有个折中的方法，即将每个词的词向量加起来除以词的个数。即：</p>
<script type="math/tex; mode=display">
\frac{(v_{\text{我}} + v_{\text{是}} + v_{\text{一名}} + v_{\text{学生}})}{4} =  v_{\text{我是一名学生}}</script><p>其中v代表某个词对应的词向量。这样就将4个向量合并成一个向量了。然后simple NN就可以处理了。<br>但是这样处理肯定太勉强了，所以就出现了simple RNN，它可以处理上述这种问题。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84RNN%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="吴恩达深度学习中的RNN示意图"><br>上图就是一个simple <strong>R</strong>NN架构，看起来跟simple NN不一样。但是其实你只要将图片往右旋转90度，就一样了。还有一点不太一样，就是simple <strong>R</strong>NN中的一个神经元跟simple NN中的一层神经元一样。<br><div class="note info">
            <p>simple <strong>R</strong>NN其实就是simple NN的神经元被替换成了simple <strong>R</strong>NN的神经元。而simple <strong>R</strong>NN的一个神经元本身里面还有多个单元，图上就是一个神经元里有4个单元。<br>所以simple <strong>R</strong>NN的神经元可以被叫做记忆细胞。<br>如果重新描述一遍就是simple <strong>R</strong>NN其实就是simple NN的神经元被替换成了simple <strong>R</strong>NN的记忆细胞。<br>而simple <strong>R</strong>NN的记忆细胞里面有多个神经元用来处理进行向量计算。</p>
          </div></p>
<h2 id="simple-RNN的另一个输入值a"><a href="#simple-RNN的另一个输入值a" class="headerlink" title="simple RNN的另一个输入值a"></a>simple RNN的另一个输入值a</h2><p>下图是simple RNN的一个记忆细胞。由于其概念大都需要数值来演示，但是大量的数值难以书写，并且用语言实在难以描述。所以以下均使用一个矩阵的字母表示来演示。<br>此处会有几个问题：</p>
<ol>
<li>乍一看很简单，但是在实现代码的时候，脑子会转不过来。<strong>因为你碰到的是向量化后的运算</strong>。所以你对各个W，b，A以及X的形状难以确定。不信就自己写一下代码，如果你以前没写过simple RNN的代码，肯定要在确定形状这卡至少一个小时。</li>
<li>A的形状尤其难确定，会一时之间绕不过来。</li>
<li>如下图所示，虽然在图里看上去是执行加法，但是在计算时，A 与 X 其实是做向量拼接。<script type="math/tex; mode=display">
W_{ax} x^{<t>} + W_{aa} a^{<t-1>} = 
\begin{pmatrix} 
 W_{ax} & W_{aa}
\end{pmatrix} * 
\begin{pmatrix} 
 x^{<t>} \\
 a^{<t-1>} \\
\end{pmatrix}</script></li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/simple%20rnn%20cell.png" alt="simple rnn cell"></p>
<h3 id="以一个记忆细胞为例"><a href="#以一个记忆细胞为例" class="headerlink" title="以一个记忆细胞为例"></a>以一个记忆细胞为例</h3><p>设词向量的维度为300，一个记忆细胞的units为32——keras代码表示：simpleRNN(32)，并且进行的是18元分类问题。</p>
<h4 id="以一个样本为例"><a href="#以一个样本为例" class="headerlink" title="以一个样本为例"></a>以一个样本为例</h4><p>，输入样本数为1。<br>则运算过程为：</p>
<script type="math/tex; mode=display">
\begin{align}
    A\_next & = tanh(Wx_{32, 300} * X_{300, 1} + Wa_{32, 32} * A\_prev_{32, 1} + ba_{32, 1})\\
    y & = softmax(Wy_{18, 32} * A\_next + by_{18, 1})
\end{align}</script><p>(数字, 数字) 的表示形式是在线性代数里表示形式，这样看起来方便点。第一个数字表示行，第二个数字表示列。tanh和softmax就不解释了。<br>我依次解释：</p>
<ol>
<li>权重值Wx比较好理解，由于输入值X是一个 (300, 1) 的矩阵（也可以叫向量）。Wx为 (32, 300) 是因为记忆细胞的一个单元为32，并且词的特征为300。如果将一个记忆细胞看作是一个隐藏层，那么神经元个数就是32，300就代表前一层的输入。</li>
<li>权重值Wa为什么是 (32, 32) ?由于a实际上就是激活值，激活值我们可以通过w * x计算得到。显然结果是 (32, 1) ，那么权重值Wa的第二个参数就可以确定大小了（矩阵相乘，第一个矩阵的第二维和第二个矩阵的第一维必须一样），权重值Wa的第一个参数实际上跟权重值Wx一样，都是units。</li>
<li>Wy为什么是 (18, 32) ?18是因为这是一个18元分类问题，32是因为A的第一维是32。</li>
</ol>
<p>其中最难确定的就是Wa的形状。经过上面推导就可以知道了，第一维代表units，第二维代表需要与A的第一维，即为A的第一维的大小。</p>
<h4 id="以128个样本为例"><a href="#以128个样本为例" class="headerlink" title="以128个样本为例"></a>以128个样本为例</h4><p>如果上面没懂可以再看一遍多个样本</p>
<script type="math/tex; mode=display">
\begin{align}
    A\_next & = tanh(Wx_{32, 300} * X_{300, 128} + Wa_{32, 32} * A\_prev_{32, 128} + ba_{32, 1})\\
    y & = softmax(Wy_{18, 32} * A\_next + by_{18, 1})
\end{align}</script><p>说白了A其实激活值，看起来比较晕是因为多了一个初始的A0，实际上A0就是激活值，只不过需要初始化一下，它的形状就是激活值的形状。</p>
<h3 id="以多个记忆细胞为例"><a href="#以多个记忆细胞为例" class="headerlink" title="以多个记忆细胞为例"></a>以多个记忆细胞为例</h3><p>过程就是上面的过程，唯一有区别的是输入值X。因为有了多个记忆细胞，所以X变成了3维，第3维就是timestep——时间步。<br>但是其实计算，跟上面一模一样，假设现在有6个时间步。伪代码如下：<br><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">rnn_cell</span><span class="params">(A_prev, Xt, parameters)</span></span>:</span><br><span class="line">	<span class="keyword">do</span>上面的操作</span><br><span class="line"></span><br><span class="line">timesteps = <span class="number">6</span></span><br><span class="line">X = rand((<span class="number">300</span>, <span class="number">128</span>, timesteps))</span><br><span class="line">A0 = rand((<span class="number">32</span>, <span class="number">128</span>))</span><br><span class="line">parameters = 初始化所有参数</span><br><span class="line"><span class="keyword">for</span> ts <span class="keyword">in</span> range(timesteps):</span><br><span class="line">	rnn_cell(A0, X[:, :, ts], parameters)</span><br></pre></td></tr></table></figure></p>
<p>其实就是遍历每一个时间步而已。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/RNN.png" alt="RNN"></p>
<h1 id="simple-RNN-的缺陷"><a href="#simple-RNN-的缺陷" class="headerlink" title="simple-RNN 的缺陷"></a>simple-RNN 的缺陷</h1><p>RNN一个最大的缺陷就是梯度消失与梯度爆炸问题，由于这一缺陷，使得RNN在长文本中难以训练，这才诞生了LSTM及各种变体，来源于<a href="https://zhuanlan.zhihu.com/p/44163528" target="_blank" rel="noopener">专栏</a>。梯度消失的原因：参考<a href="https://zhuanlan.zhihu.com/p/28687529" target="_blank" rel="noopener">专栏</a></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/深度学习算法（二）：simple RNN 推导与理解.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/2017CS224n学习笔记.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/2017CS224n学习笔记.html" class="post-title-link" itemprop="url">2017CS224n学习笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-20 10:51:09" itemprop="dateCreated datePublished" datetime="2019-05-20T10:51:09+08:00">2019-05-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-30 13:14:00" itemprop="dateModified" datetime="2019-10-30T13:14:00+08:00">2019-10-30</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="开场白"><a href="#开场白" class="headerlink" title="开场白"></a>开场白</h1><p>&emsp;&emsp;略</p>
<h1 id="词向量表示：word2vec"><a href="#词向量表示：word2vec" class="headerlink" title="词向量表示：word2vec"></a>词向量表示：word2vec</h1><p>&emsp;&emsp;课程计划如下：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/课程计划.jpg" alt="课程计划"></p>
<p>&emsp;&emsp;<strong>神经网络词嵌入学习的通用做法</strong>：定义一个模型，根据中心词 <script type="math/tex">w_t</script> 去预测上下文单词。给定 <script type="math/tex">w_t</script> 的条件下 context 的概率。</p>
<script type="math/tex; mode=display">
p(context|w_t) = \dots</script><p>&emsp;&emsp;然后用损失函数判断预测的准确性，例如：</p>
<script type="math/tex; mode=display">
J = 1 - p(w_{-t}|w_t), \quad  \text{-t 代表 t 周围的单词}</script><p>&emsp;&emsp;如果可以精准地根据 t 预测到这些单词，那么概率就为 1，于是损失就没有了。但通常情况下，做不到这点。<strong>所以我们应该调整词汇表示，从而使损失最小化</strong>。<br>&emsp;&emsp;下图是以前的低维词向量表示方法，2003 年 Bengio 发表的这篇现在属于开创性的论文其实并没有太多人关注，因为那时候深度学习并没有很流行。但是当这篇论文开始流行的时候，就开始大行其道了。于是 2008 年 Collobert 和 Weston 开启了一个新方向，<strong>他们觉得如果我们只想要得到好的单词表示，我们甚至不需要构建一个具有预测功能的概率语言模型（probabilistic language model），我们只需要找到一种学习单词表示的方法即可</strong>。于是 2013 年有了 word2vec 模型。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/以前的低维词向量表示方法.jpg" alt="以前的低维词向量表示方法"></p>
<p>&emsp;&emsp;word2vec 是一个软件，实际上，它里面包含很多东西。有两个用于生成词汇向量的算法（Hierarchical softamx，negative sampling），还有两套效率中等的训练方法（Skip-grams，CBOW）。<em>这里的软件应该指的不是那种可以运行 exe 文件</em>。本节只讲 skip-grams 算法，并且不会讲那两个高效的词向量生成算法，而是将一个效率极低的算法（因为比较简单且包含了基本概念）。<br>&emsp;&emsp;skip-grams 模型的概念是：在每一个估算步中，都取一个词为中心词汇，然后尝试预测它<strong>一定范围内</strong>的上下文的词汇。这个模型将定义一个概率分布：<strong>给定一个中心词汇预测某个单词在它上下文中出现的概率</strong>。如下图所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/skip-grams模型.jpg" alt="skip-grams模型"></p>
<p>&emsp;&emsp;我们将会选取词汇的向量表示，以让概率分布值最大化。</p>
<h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>&emsp;&emsp;我们需要做的是定义一个半径 m，然后从中心词汇开始到距离为 m 的位置来预测周围的词汇。这句话比较抽象，因为到这为止，你还是构建不出一个模型（优化目标）。下面先给出模型的公式，注意一撇不是求导：</p>
<script type="math/tex; mode=display">
J'(\theta) = \prod^T_{t=1} \prod_{-m \leq m, j \neq 0} p(w_{t+j}|w_t;\theta)</script><p>&emsp;&emsp;其中定义一句话有 T 个单词，word t = 1 <script type="math/tex">\dots</script> T。上式中 m 为半径窗口，j 为整个窗口之中的索引。先不看第一个累乘符号，当 t = 1 时，也就是当中心词的索引为 1 时，以 m 为半径，预测该中心词汇的上下文单词出现的概率，并将所有的概率累乘。即公式： <script type="math/tex">\prod_{-m \leq m, j \neq 0} p(w_{t+j}|w_t;\theta)</script>。而第一个累乘符号指的是，将句子中每一个字都当做一次中心词汇，然后将概率再累乘起来。当然当中心词的索引比较靠前时，可能窗口会超出句子的前部，比如 中心词汇所以为 1，而 m = 5，则需要预测 -4，-3… 的位置，这显然不可能，所以需要自己做一下处理。<br>&emsp;&emsp;公式中的 <script type="math/tex">\theta</script> 是模型唯一的参数，让上下文所有词汇出现的概率都尽可能的高，其实 <script type="math/tex">\theta</script> 就是词向量，而模型的输入就是 one-hot 表示。但是，处理概率问题是一件很不爽的事，我们要做最大化操作，实际上就是解决对数分布的问题。这样求积就会变成求和，如下所示：</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{T} \sum^T_{t=1} \sum_{-m \leq m, j \neq 0} log \, p(w_{t+j}|w_t;\theta)</script><p>&emsp;&emsp;这样我们就得到了<strong>负的对数似然</strong>，上述公式就是最终版。但是这里还有一小点就是 m 其实也算是模型的参数，但是确是<strong>超参数</strong>，需要自己手动改的。所以上面说“<em>公式中的 <script type="math/tex">\theta</script> 是模型唯一的参数</em>”也没错。事实上这个模型还有很多其他的超参数，但是现在暂且视为常数。<br>&emsp;&emsp;公式前面有个负号，是因为我们要求最小化问题，而原式只能取最大值，所以取了个负号。</p>
<h3 id="确定相应的概率分布"><a href="#确定相应的概率分布" class="headerlink" title="确定相应的概率分布"></a>确定相应的概率分布</h3><p>&emsp;&emsp;那么我们具体应该怎么通过中心词汇来预测周围单词出现的概率呢？也就是说公式中的函数 p 应该是什么。其实 p 就是 softmax 函数。具体来说就是用由词向量构成的中心词汇去预测周围词汇的概率分布。下图就是 softmax 函数。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/softmax.jpg" alt="softmax"></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>&emsp;&emsp;最前面讲到需要有一个损失函数来判断预测的准确性。我们使用 cross-entropy loss。</p>
<h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><script type="math/tex; mode=display">
\begin{align}
     & \frac{\partial}{\partial v_c} log \frac{exp(u^T_o v_c)}{\sum^v_{w=1} exp(u^T_w v_c))} \\
    = & \frac{\partial}{\partial v_c} (\underbrace{log \, exp(u^T_o v_c)}_{1} - \underbrace{log \, \sum^v_{w=1} exp(u^T_w v_c)}_2) \\
     & \frac{\partial}{\partial v_c} log \, exp(u^T_o v_c) & \text{1} \\
    = & \frac{\partial}{\partial v_c} u^T_o v_c = u_o  \\
     & \frac{\partial}{\partial v_c} log \, \sum^v_{w=1} exp(u^T_w v_c) & \text{2} \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \frac{\partial}{\partial v_c} \sum^v_{x=1} exp(u^T_x v_c) \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \sum^v_{x=1} \frac{\partial}{\partial v_c} exp(u^T_x v_c) \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \sum^v_{x=1} exp(u^T_x v_c) \, u_x \\
    = & \sum^v_{x=1} \frac{exp(u^T_x v_c)}{\sum^v_{w=1} exp(u^T_w v_c)} \, u_x \\
    = & \sum^v_{x=1} p(x|c) u_x \\
     & u_o - \sum^v_{x=1} p(x|c) u_x & \text{合并}\\
\end{align}</script><h1 id="高级词向量表示"><a href="#高级词向量表示" class="headerlink" title="高级词向量表示"></a>高级词向量表示</h1><p>&emsp;&emsp;略。</p>
<h1 id="Word-Window分类与神经网络"><a href="#Word-Window分类与神经网络" class="headerlink" title="Word Window分类与神经网络"></a>Word Window分类与神经网络</h1><p>&emsp;&emsp;略。</p>
<h1 id="反向传播和项目建议"><a href="#反向传播和项目建议" class="headerlink" title="反向传播和项目建议"></a>反向传播和项目建议</h1><p>&emsp;&emsp;讲反向传播，略。</p>
<h1 id="依存分析"><a href="#依存分析" class="headerlink" title="依存分析"></a>依存分析</h1><h2 id="语言结构分析历史回顾"><a href="#语言结构分析历史回顾" class="headerlink" title="语言结构分析历史回顾"></a>语言结构分析历史回顾</h2><h2 id="Dependency-Grammar-and-Dependency-Structure"><a href="#Dependency-Grammar-and-Dependency-Structure" class="headerlink" title="Dependency Grammar and Dependency Structure"></a>Dependency Grammar and Dependency Structure</h2><p>&emsp;&emsp;</p>
<h1 id="Tensorflow-入门"><a href="#Tensorflow-入门" class="headerlink" title="Tensorflow 入门"></a>Tensorflow 入门</h1><p>&emsp;&emsp;略，不学 tensorflow。</p>
<h1 id="RNN和语言模式"><a href="#RNN和语言模式" class="headerlink" title="RNN和语言模式"></a>RNN和语言模式</h1><p>&emsp;&emsp;详解 RNN 很多问题。</p>
<h1 id="机器翻译和高级循环神经网络"><a href="#机器翻译和高级循环神经网络" class="headerlink" title="机器翻译和高级循环神经网络"></a>机器翻译和高级循环神经网络</h1><p>&emsp;&emsp;花了二三十分钟讲机器翻译，然后讲解各类 RNN。</p>
<h1 id="神经机器翻译和注意力模型"><a href="#神经机器翻译和注意力模型" class="headerlink" title="神经机器翻译和注意力模型"></a>神经机器翻译和注意力模型</h1><p>&emsp;&emsp;先将机器翻译，后讲 attention。</p>
<h2 id="神经机器翻译"><a href="#神经机器翻译" class="headerlink" title="神经机器翻译"></a>神经机器翻译</h2><h2 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h2><p>&emsp;&emsp;下图是 attention 的工作原理。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/attention机制.jpg" alt="attention机制"></p>
<ol>
<li>图中的 a 代表 score，<script type="math/tex">\bar{h}_s</script> 代表 encoder 中每个 time step 生成的隐藏状态向量，<script type="math/tex">c_t</script> 代表 attention 之后的向量；</li>
<li>首先将开始标志输入到一个 decoder，代表开始进行翻译，输出一个单词后，将该 decoder 的<strong>隐藏状态</strong>（注意是隐藏状态而不是输出值，此节课视频中有明确指出）与 encoder 中的<strong>隐藏状态</strong>进行计算得到一个 score。打分的公式为 <script type="math/tex">score(h_{t - 1}, \bar{h}_s)</script>，score 具体是什么公式可以自己定义，最简单就是向量内积，下面会细说；</li>
<li>关于 score 函数，它有多种选择，<strong>注意一点</strong>下面的 score 函数只是对<strong>一个</strong>时间步上的隐藏状态打分，<script type="math/tex">\bar{h}_s</script> 也可以是个矩阵，即一步计算所有时间步的 attention score（这做法是最好的）。以下罗列几种做法，被广泛采用（2017 年的说法，现不知）的是第二个表达式，第三个表达式的 <script type="math/tex">v_a</script> 也是一个向量参数。另外对于第三个表达式 <script type="math/tex">v_a tanh(W_a [h_t;\bar{h}_s])</script>，它不是 score function，而是 Bahdanau，不知道为什么把它放到 score function 这。<script type="math/tex; mode=display">
score(h_t, \bar{h}_s) = 
\begin{cases}
h^T_t \bar{h}_s \\
h^T_t W_a \bar{h}_s \\
v_a tanh(W_a [h_t;\bar{h}_s])
\end{cases}</script></li>
<li>将 score 送入 softmax 得到概率；</li>
<li>通过公式 <script type="math/tex">c_t = \sum_s a_t(s)\bar{h}_s</script>，将所有的向量乘上注意力分数加起来；</li>
<li>将此新向量当做下一个 decoder 的输入；</li>
</ol>
<p>&emsp;&emsp;下图是加 attention 机制和不加的区别。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/加入attention机制后的性能.jpg" alt="加入attention机制后的性能"></p>
<h2 id="coverage"><a href="#coverage" class="headerlink" title="coverage"></a>coverage</h2><p>&emsp;&emsp;coverage = more attention，想法源于计算机视觉，请看下图。神经网络读入一张图片，要求输出一段话。但是我们知道一段话不仅要描写图中的鸟，还要描写鸟旁边的事物，所以就引出了多次注意，即神经网络需要注意图中更多的地方。将这一想法引入 NLP 中，其实就是多做几次 attention，<em>注：这一想法貌似就是后来 transformer 的 multi-head attention</em>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/more attention（coverage）.jpg" alt="more attention(coverage)"></p>
<h2 id="search"><a href="#search" class="headerlink" title="search"></a>search</h2><p>&emsp;&emsp;</p>
<h1 id="GRU及NMT的其他议题"><a href="#GRU及NMT的其他议题" class="headerlink" title="GRU及NMT的其他议题"></a>GRU及NMT的其他议题</h1><h2 id="GRUs-LSTMs"><a href="#GRUs-LSTMs" class="headerlink" title="GRUs/LSTMs"></a>GRUs/LSTMs</h2><h2 id="NMT-evaluation"><a href="#NMT-evaluation" class="headerlink" title="NMT evaluation"></a>NMT evaluation</h2><h1 id="语音处理的端对端模型"><a href="#语音处理的端对端模型" class="headerlink" title="语音处理的端对端模型"></a>语音处理的端对端模型</h1><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h1 id="树RNN和短语句分析"><a href="#树RNN和短语句分析" class="headerlink" title="树RNN和短语句分析"></a>树RNN和短语句分析</h1><p>&emsp;&emsp;人类语言具有嵌套结构（训练结构、树结构），如：[The man from [the company that you spoke with about [the project] yesterday]]。<br>&emsp;&emsp;那么如何使用向量来表示这些句子的语义呢？可以使用 tree RNN，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/Recursive vs. recurrent neural network.jpg" alt="Recursive vs. recurrent neural network"></p>
<blockquote>
<p>&emsp;&emsp;<strong>Tree recursive neural network 的问题在于你需要得到一个树形结构</strong>，这是一个比较大的问题。树形网络并没有火遍全球，在语言方面确实有原因喜欢这类的模型（原因后面会有讲到），但是如果你在 arxiv 里面找，人们在语言神经网络研究中所使用的的方法时，你会发现人们并不多使用树形结构模型。LSTMs 的比例几乎是其十倍之多。<br>&emsp;&emsp;这里面比较大的原因是树形递归神经网络的使用者必须构建一个树形结构。<strong>在你构建完成后，使用反向传播学习模型会是一个问题</strong>。<br>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a> 25分开始。</p>
</blockquote>
<h2 id="simple-tree-RNN"><a href="#simple-tree-RNN" class="headerlink" title="simple tree RNN"></a>simple tree RNN</h2><h3 id="树RNN的计算"><a href="#树RNN的计算" class="headerlink" title="树RNN的计算"></a>树RNN的计算</h3><p>&emsp;&emsp;那么具体如何使用树形递归神经网络计算呢？比如下图中使用向量 [3 3] 和 [8 5] 计算，输入进神经网络之后，就会输出一个向量 [8 3] 和一个分数 1.3，这个分数代表输出的向量 [8 3] 是否合理（即结构是否合理。如果不太理解什么是结构是否合理，请看下两张图以及博客内容即可理解）。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive neural network for training.jpg" alt="recursive neural network for training"></p>
<p>&emsp;&emsp;具体的做法如下图所示。应该很好理解，就不详细说明了，其中对于计算 score 的 U，我猜测可能是一个 trainable 的参数，视频中并没有详细的说明。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive neural network details.jpg" alt="recursive neural network details"></p>
<p>&emsp;&emsp;那么到了真正的实战阶段应该怎么做呢？训练一个贪心的解析器，对于单词两两组合，然后发现最前的两个单词 “The cat” 组成的短语训练之后的分数最高，然后我们将 “The cat” 看作一个成分并且尤其对应的语义 [5 2]。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 1.jpg" alt="parsing a sentence with rnn"></p>
<p>&emsp;&emsp;接下来继续重复做，请注意现在的 “The cat” 是一个成分（可看作单词），而不是两个单词。又做一遍解析之后发现 “the mat” 的分数最高。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 2.jpg" alt="parsing a sentence with rnn——2"></p>
<p>&emsp;&emsp;再将 “the mat” 看作一个成分，并拥有对应的语义。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 3.jpg" alt="parsing a sentence with rnn 3"></p>
<p>&emsp;&emsp;以此类推，我们发现 “on the mat” 的分数最高，然后发现 “sat on the mat” 的分数最高，最后就得到 “The cat sat on the mat” 的分数最高。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 4.jpg" alt="parsing a sentence with rnn 4"></p>
<p>&emsp;&emsp;这是一棵解析树（parse tree），我们会得到这棵解析树的分数，它的分数由每个节点的分数加和得到。<strong>我们要做的就是找到由这堆节点所能组成的分数最高的解析树</strong>。<br>&emsp;&emsp;还需要一个优化目标，similar to max-margin parsing(Taskar et al. 2004), a supervised max-margin objective:</p>
<script type="math/tex; mode=display">J = \sum_i s(x_i, y_i) - \max_{y \in A(x_i)} (s(x_i, y) - \Delta(y, y_i))</script><p>&emsp;&emsp;最后我们还需要反向传播算法进行计算，这一工作早在 20 世纪 90 年代就由几个德国人做过了。Goller 和 Kuchler 提出了这个算法，并命名为 <strong>back propagation through structure</strong>.</p>
<h2 id="Syntactically-Untied-RNN"><a href="#Syntactically-Untied-RNN" class="headerlink" title="Syntactically-Untied RNN"></a>Syntactically-Untied RNN</h2><p>&emsp;&emsp;语义解绑树形递归神经网络，这被证明是构建高质量解析器的一个成功的方法。<br>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a>，50 分开始。</p>
<h2 id="Compositionality-Through-Recursive-Matrix-Vector-Spaces"><a href="#Compositionality-Through-Recursive-Matrix-Vector-Spaces" class="headerlink" title="Compositionality Through Recursive Matrix-Vector Spaces"></a>Compositionality Through Recursive Matrix-Vector Spaces</h2><p>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a>，65 分开始。</p>
<h2 id="related-work-for-parsing"><a href="#related-work-for-parsing" class="headerlink" title="related work for parsing"></a>related work for parsing</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/related work for parsing.jpg" alt="related work for parsing"></p>
<h1 id="共指解析"><a href="#共指解析" class="headerlink" title="共指解析"></a>共指解析</h1><p>&emsp;&emsp;<script type="math/tex">B^3</script>(B-CUBED) 算法用于评估。其他的一些算法：</p>
<ul>
<li>MUC Score (Vilain et al., 1995)</li>
<li>BEAF (Luo 2005); entity based</li>
<li>BLANC (Recasens and Hovy 2011) Cluster RAND-index</li>
</ul>
<p>&emsp;&emsp;在于语言学中，人们常区分两种关系。其中之一是共指，即两个词指代同一个实体，这和文本结构无关；另一种关系是首语重复，它指的是文本中某一项，一个照应语（或一个指代，anaphor）指代的事物由另一项决定，即先行词。<br>&emsp;&emsp;一些共指消解的做法：</p>
<ul>
<li>Mention Pair models</li>
<li>Mention Ranking models</li>
<li>Entity-Mention models</li>
</ul>
<p>&emsp;&emsp;神经共指模型，人们通过深度学习和共指做的内容。</p>
<ul>
<li>Wisemean, Rush, Shieber, and Weston (ACL 2015)<ul>
<li>Mention-pair model. Only partially neural network system over conventional, categorical coreference features</li>
</ul>
</li>
<li>Wiseman, Rush and Shieber (NAACL 2016)<ul>
<li>Use RNNs to learn global representations of entity clusters from mentions</li>
</ul>
</li>
<li>Clark and Manning (ACL 2016)<ul>
<li>An entity-mention model based around clustering using distributed representations of mentions and entity clusters</li>
</ul>
</li>
<li>Clark and Manning (EMNLP 2016)<ul>
<li>Expolores deep reinforcement learning to improve a metion-pair model</li>
</ul>
</li>
</ul>
<div class="note info">
            <p>&emsp;&emsp;此节视频讲了很多共指的理论，我没有记下来。实际内容比这里记的还要多一点。</p>
          </div>
<h1 id="用于回答问题的动态神经网络"><a href="#用于回答问题的动态神经网络" class="headerlink" title="用于回答问题的动态神经网络"></a>用于回答问题的动态神经网络</h1><p>&emsp;&emsp;略，这节听不懂。</p>
<h1 id="NLP的问题和可能性架构"><a href="#NLP的问题和可能性架构" class="headerlink" title="NLP的问题和可能性架构"></a>NLP的问题和可能性架构</h1><p>&emsp;&emsp;</p>
<h1 id="应对深度-NLP-的局限性"><a href="#应对深度-NLP-的局限性" class="headerlink" title="应对深度 NLP 的局限性"></a>应对深度 NLP 的局限性</h1>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/2017CS224n学习笔记.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/深度学习算法（一）：simple NN（前馈神经网络的正反向推导）.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/深度学习算法（一）：simple NN（前馈神经网络的正反向推导）.html" class="post-title-link" itemprop="url">深度学习算法（一）：simple NN（前馈神经网络的正反向推导）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-16 19:53:55" itemprop="dateCreated datePublished" datetime="2019-05-16T19:53:55+08:00">2019-05-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-11 19:54:34" itemprop="dateModified" datetime="2019-07-11T19:54:34+08:00">2019-07-11</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>本文的公式不存在次方的说法，所以看见上标，不要想成是次方。<br>对于权重的表示问题，请看<a href="https://yan624.github.io/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">博客</a>，但是由于是以前的学习笔记，不保证完全正确。<br>如果想了解为什么梯度下降要对w和b求导，可以看<a href="https://yan624.github.io/学习笔记/梯度下降算法的推导.html">这篇</a>。<br><strong>建议边看边写，否则思维跟不上。</strong></p>
          </div>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>参考<a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">文章</a><br>以如下神经网络架构为例。参考<a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">文章</a>中使用了一个2 2 2的神经网络架构，但是现实中神经网络架构不会这么整整齐齐。所以还是使用了略复杂的架构，此外原文中未对bias（偏差）更新。另外原文也没有实现向量化后的计算。虽然在后面的代码写了，但是由于代码太长了，有一种代码我给出来了，你们自己去看的感觉。说实话没多少注释，都没看的欲望(╬￣皿￣)。然后她所使用的符号让我不太习惯，因为看吴恩达以及李宏毅老师使用的符号都是<script type="math/tex">w^l_{ji}\ a^l_i</script>等等，所以自己重新推导一遍，并且使用了数学公式，而不是截图，更好看一点。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>解释一下最下面的神经元，这个神经元初始化为1，也就是意味着1 * b = b。输入值为1，一个偏差乘1还是偏差本身。</p>
<h2 id="关于函数选用"><a href="#关于函数选用" class="headerlink" title="关于函数选用"></a>关于函数选用</h2><p>本文所有激活函数选择sigmoid函数，代价函数选择binary_crossentropy。</p>
<h2 id="一些约定"><a href="#一些约定" class="headerlink" title="一些约定"></a>一些约定</h2><div class="note info">
            <p>本文所有的输入值，激活值，输出值都是<strong>列向量</strong>。</p>
          </div>
<h1 id="初始化数据以及正向传播"><a href="#初始化数据以及正向传播" class="headerlink" title="初始化数据以及正向传播"></a>初始化数据以及正向传播</h1><h2 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h2><p>此处初始化各层的权重值，偏差。由于是演示，所以顺便把输入层也初始化了。<br>设</p>
<script type="math/tex; mode=display">
\begin{cases}
    x_1 = a^0_1 = 0.55, x_2 = a^0_2 = 0.72\\
    y_1 = 0.60, y_2 = 0.54\\
\end{cases}\\
\begin{cases}
    w^1_{11}=0.4236548, w^1_{12}=0.64589411\quad|\quad w^1_{21}=0.43758721, w^1_{22}=0.891773\quad|\quad w^1_{31}=0.96366276, w^1_{32}=0.38344152\\
    b^1_1=0.79172504, b^1_2=0.52889492, b^1_3=0.56804456\\
    w^2_{11}=0.92559664, w^2_{12}=0.07103606, w^2_{13}=0.0871293\quad|\quad w^2_{21}=0.0202184, w^2_{22}=0.83261985, w^2_{23}=0.77815675\\
    b^2_1=0.87001215, b^2_2=0.97861834\\
\end{cases}</script><p>不用多看，反正也用不到几次。。。</p>
<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>对于正向传播，应该是很熟悉了，所以我直接一次写完，不做过多解释。</p>
<h3 id="输入层到隐藏层"><a href="#输入层到隐藏层" class="headerlink" title="输入层到隐藏层"></a>输入层到隐藏层</h3><script type="math/tex; mode=display">
z^1_1 = w^1_{11} * a^0_1 + w^1_{12} * a^0_2 + 1 * b^1_1\\
z^1_2 = w^1_{21} * a^0_1 + w^1_{22} * a^0_2 + 1 * b^1_2\\
z^1_3 = w^1_{31} * a^0_1 + w^1_{32} * a^0_2 + 1 * b^1_3\\</script><p>带入sigmoid函数中，以下开始省略bias乘的1：</p>
<script type="math/tex; mode=display">
a^1_1 = \sigma{(z^1_1)}\\
a^1_2 = \sigma{(z^1_2)}\\
a^1_3 = \sigma{(z^1_3)}\\</script><h3 id="隐藏层到输出层"><a href="#隐藏层到输出层" class="headerlink" title="隐藏层到输出层"></a>隐藏层到输出层</h3><script type="math/tex; mode=display">
z^2_1 = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1\\
z^2_2 = w^2_{21} * a^1_1 + w^2_{22} * a^1_2 + w^2_{13} * a^1_3 + b^2_2\\</script><p>带入sigmoid函数中：</p>
<script type="math/tex; mode=display">
a^2_1 = \sigma{(z^2_1)}\\
a^2_2 = \sigma{(z^2_2)}\\</script><h3 id="计算代价"><a href="#计算代价" class="headerlink" title="计算代价"></a>计算代价</h3><p>以字母J记为代价函数的名称，最后一个表达式为最简版：</p>
<script type="math/tex; mode=display">
\begin{align}
    J & = -[(y_1 * \log(a^2_1) + (1 - y_1) * \log(1 - a^2_1) + (y_2 * \log(a^2_2) + (1 - y_2) * \log(1 - a^2_2)]\\
    J & = -\Sigma^2_{i = 1}{[(y_i * \log(a^2_i) + (1 - y_i) * \log(1 - a^2_i)]}\\
    J & = -\Sigma{[(y * \log(a^2) + (1 - y) * \log(1 - a^2)]}\\
\end{align}</script><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>上述的表达式全部是一个一个列出来的，如果使用向量来表示乘积那就方便很多。可以看到下面只用了五行就写完了上面<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#正向传播">正向传播</a>的所有步骤。<br><div class="note warning">
            <p>如果无法理解这一步那就是不会线性代数的问题，线性代数不在此文的介绍范围之内。</p>
          </div></p>
<script type="math/tex; mode=display">
\begin{align}
    z^1 & = w^1 * a^0 + b^1 & \text{输入层到隐藏层}\\
    a^1 & = \sigma{(z^1)} & \text{带入隐藏层的激活函数}\\
    z^2 & = w^2 * a^1 + b^2 & \text{隐藏层到输出层}\\
    a^2 & = \sigma{(z^2)} & \text{带入输出层的激活函数}\\
    J & = -\Sigma{[(y * \log(a^2) + (1 - y) * \log(1 - a^2)]} & \text{计算代价}\\
\end{align}</script><h2 id="上述表达式代码实现"><a href="#上述表达式代码实现" class="headerlink" title="上述表达式代码实现"></a>上述表达式代码实现</h2><p>最后几节有神经网络numpy实现的全部代码，可以直接跳过本节看下一节，这里的代码只是给出一个直观的理解，可以自己运行看看。<br>受到keras以及万物皆对象的启发，首先建立一个神经元对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">'sigmoid'</span>)</span>:</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line">        self.hyperparameters = dict()</span><br><span class="line">        <span class="comment"># W, b, A_prev的导数</span></span><br><span class="line">        self.grads = dict()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hyperparameters</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化超参数，在神经网络中权重值不能初始化为0，偏差可以</span></span><br><span class="line"><span class="string">        :param shape: 神经元的形状，(units, input_shape)</span></span><br><span class="line"><span class="string">        :return: hyperparameters，该神经元的超参数。可以通过hyperparameters['W']、hyperparameters['b']取值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 由于是演示，所以使用了随机初始化</span></span><br><span class="line">        W = np.random.rand(*shape)</span><br><span class="line">        b = np.random.rand(shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> W, b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构造神经元，目前只执行初始化超参数的步骤</span></span><br><span class="line"><span class="string">        :param input_shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        W, b = self._init_hyperparameters(shape=(self.units, input_shape))</span><br><span class="line">        self.hyperparameters[<span class="string">'W'</span>] = W</span><br><span class="line">        self.hyperparameters[<span class="string">'b'</span>] = b</span><br></pre></td></tr></table></figure></p>
<p>为方便起见，将大部分的函数都放入Model中，下面给出所有的代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> simple_neural_network.activation_function <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> simple_neural_network.cost_function <span class="keyword">import</span> *</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">'sigmoid'</span>)</span>:</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hyperparameters</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化超参数，在神经网络中权重值不能初始化为0，偏差可以</span></span><br><span class="line"><span class="string">        :param shape: 神经元的形状，(units, input_shape)</span></span><br><span class="line"><span class="string">        :return: hyperparameters，该神经元的超参数。可以通过hyperparameters['W']、hyperparameters['b']取值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 用于是演示，所以使用了随机初始化</span></span><br><span class="line">        hyperparameters = &#123;</span><br><span class="line">                <span class="string">'W'</span>: np.random.rand(*shape),</span><br><span class="line">                <span class="string">'b'</span>: np.random.rand(shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hyperparameters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构造神经元，目前只执行初始化超参数的步骤</span></span><br><span class="line"><span class="string">        :param input_shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.hyperparameters = self._init_hyperparameters(shape=(self.units, input_shape))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 全部的神经元，并且根据神经元数量计算神经网络架构的层数，在计算时需要减1因为输入层不算入神经网络层数</span></span><br><span class="line">        self.neurons = list()</span><br><span class="line">        <span class="comment"># 按顺序缓存A, (Z, W, b)，由于输入层不需要任何缓存，所以放入None填充此位置。方便根据索引取值</span></span><br><span class="line">        self.value_caches = [<span class="keyword">None</span>]</span><br><span class="line">        <span class="comment"># 代价函数</span></span><br><span class="line">        self.cost = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, neuron)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        在模型中添加神经元</span></span><br><span class="line"><span class="string">        :param neuron:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.neurons.append(neuron)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(self, A_prev, W, b)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        正向传播，线性运算：Z = W * A + b</span></span><br><span class="line"><span class="string">        :param A_prev: 前一层的激活值</span></span><br><span class="line"><span class="string">        :param W: 权重值</span></span><br><span class="line"><span class="string">        :param b: 偏差</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        Z: 运算结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        cache = A_prev, (Z, W, b)</span><br><span class="line">        self.value_caches.append(cache)</span><br><span class="line">        <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nonlinear_forward</span><span class="params">(self, A_prev, W, b, activation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        进入激活函数进行非线性计算</span></span><br><span class="line"><span class="string">        :param A_prev:</span></span><br><span class="line"><span class="string">        :param W:</span></span><br><span class="line"><span class="string">        :param b:</span></span><br><span class="line"><span class="string">        :param activation:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Z = self.linear_forward(A_prev, W, b)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'sigmoid'</span>:</span><br><span class="line">            <span class="keyword">return</span> sigmoid(Z)</span><br><span class="line">        <span class="keyword">elif</span> activation == <span class="string">'relu'</span>:</span><br><span class="line">            <span class="keyword">return</span> relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deep_forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param X: 输入值</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        A: 最后一层的运算结果，也就是输出层的激活值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 计算神经网络层数，减1是为了去掉输入层，众所周知输入层不需要进行计算</span></span><br><span class="line">        L = len(self.neurons) - <span class="number">1</span></span><br><span class="line">        A = X</span><br><span class="line">        <span class="comment"># 循环整个神经网络，进行正向传播，从1开始，因为索引0是输入层</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 根据索引获取神经元实例</span></span><br><span class="line">            neuron = self.neurons[l]</span><br><span class="line">            A_prev = A</span><br><span class="line">            W = neuron.hyperparameters[<span class="string">'W'</span>]</span><br><span class="line">            b = neuron.hyperparameters[<span class="string">'b'</span>]</span><br><span class="line">            A = self.nonlinear_forward(A_prev, W, b, neuron.activation)</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compile</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, epochs=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入层的神经元添加进去</span></span><br><span class="line">        self.neurons.insert(<span class="number">0</span>, SimpleNN(len(X)))</span><br><span class="line">        <span class="comment"># 初始化神经元的超参数</span></span><br><span class="line">        <span class="keyword">for</span> i, n <span class="keyword">in</span> enumerate(self.neurons[<span class="number">1</span>:]):</span><br><span class="line">            input_shape = self.neurons[i].units</span><br><span class="line">            n.build(input_shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始反向传播</span></span><br><span class="line">        AL = self.deep_forward(X)</span><br></pre></td></tr></table></figure></p>
<p>激活函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> z * (z &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    dZ = np.array(dA, copy=<span class="keyword">True</span>)  <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line"></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    s = sigmoid(Z)</span><br><span class="line">    <span class="keyword">return</span> dA * s * (<span class="number">1</span> - s)</span><br></pre></td></tr></table></figure></p>
<p>测试一下<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入值的大小</span></span><br><span class="line">input_size = 2</span><br><span class="line"><span class="comment"># 输出值的大小</span></span><br><span class="line">output_size = 2</span><br><span class="line"><span class="comment"># 方便书写，截断小数</span></span><br><span class="line">X0 = np.round(np.random.rand(input_size, 1), 2)</span><br><span class="line">Y0 = np.round(np.random.rand(output_size, 1),2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该模型为2 3 2架构</span></span><br><span class="line">model = Model()</span><br><span class="line">model.add(SimpleNN(3))</span><br><span class="line">model.add(SimpleNN(output_size))</span><br><span class="line">model.compile()</span><br><span class="line">model.fit(X0, Y0)</span><br><span class="line">print(model.value_caches[0])</span><br></pre></td></tr></table></figure></p>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><div class="note primary">
            <p>说是说反向传播，实际上整个流程就是在<strong><a href="https://baike.baidu.com/item/链式法则/3314017?fr=aladdin" target="_blank" rel="noopener">链式求导</a></strong>。如果把这点想通了，整个神经网络的难点就只在向量化上了。一定要理解为什么整个流程只是在做链式求导的问题，在这里我并不是随便一提。</p>
          </div>
<p>为了便于查找，把之前的图再放这。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"></p>
<h2 id="首先更新输出层的权重值（W）以及偏差值（b）"><a href="#首先更新输出层的权重值（W）以及偏差值（b）" class="headerlink" title="首先更新输出层的权重值（W）以及偏差值（b）"></a>首先更新输出层的权重值（W）以及偏差值（b）</h2><p>梯度下降公式大家应该都知道：<script type="math/tex">W = W - \alpha * grad</script>。其中的grad实际上就是W的导数，<a href="https://yan624.github.io/学习笔记/梯度下降算法的推导.html">参考</a>。<br>可以对照上图观察，<strong>输出层</strong>的权重值分别为:</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    w^2_{11}&w^2_{12}&w^2_{13}\\
    w^2_{21}&w^2_{22}&w^2_{23}\\
\end{pmatrix}</script><p>所以我们需要分别求：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} \tag{3.1.1}\label{3.1.1}</script><h3 id="求矩阵中第一个w的导数并更新w"><a href="#求矩阵中第一个w的导数并更新w" class="headerlink" title="求矩阵中第一个w的导数并更新w"></a>求矩阵中第一个w的导数并更新w</h3><p>先求<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>，我们知道这个神经网络的代价的表达式是</p>
<script type="math/tex; mode=display">
J = -[(y_1 * \log(a^2_1) + (1 - y_1) * \log(1 - a^2_1) + (y_2 * \log(a^2_2) + (1 - y_2) * \log(1 - a^2_2)]</script><p><strong>为了方便对照我将隐藏层到输出层的正向传播的步骤</strong>也写在下面：</p>
<script type="math/tex; mode=display">
\begin{align}
    z^2_1 & = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1\\
    z^2_2 & = w^2_{21} * a^1_1 + w^2_{22} * a^1_2 + w^2_{13} * a^1_3 + b^2_2\\
    a^2_1 & = \sigma{(z^2_1)} = \frac{1}{1 - e^{-z^2_1}}\\
    a^2_2 & = \sigma{(z^2_2)} = \frac{1}{1 - e^{-z^2_2}}\\
\end{align}</script><p>根据链式求导法则得：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}}</script><p>我们将其拆解，一步一步地求：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] & \text{首先对a求导，此步如果你不会微积分会有疑惑} \tag{3.1.2}\label{3.1.2}\\
    \frac{\partial a^2_1}{\partial z^2_1} & = (a^2_1) * (1 - a^2_1) & \text{这是对sigmoid函数的求导，百度一下求导过程}\\
    \frac{\partial J}{\partial z^2_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1} & \text{其次对z求导}\\
                                      & = \frac{\partial J}{\partial a^2_1} * (a^2_1) * (1 - a^2_1) \tag{3.1.3}\label{3.1.3}\\
    \frac{\partial z^2_1}{\partial w^2_{11}} & = a^1_1\\
    \frac{\partial J}{\partial w^2_{11}} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}} & \text{最后对w求导} \tag{3.1.4}\label{3.1.4}\\
                                         & = \frac{\partial J}{\partial z^2_1} * a^1_1\\
    \frac{\partial J}{\partial w^2_{11}} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1) * a^1_1 & \text{整合在一起}\\
\end{align}</script><p>其中<script type="math/tex">[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1)</script>实际上是可以化简的，化简为<script type="math/tex">a^2_1 - y_1</script>，同时去掉了负号，所以</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = (a^2_1 - y_1) * a^1_1</script><p>我们将数值带入其中，之前的正向传播已经得到了所有激活值。</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial w^2_{11}} = (0.85220348 - 0.60) * 0.81604509 = 0.20580941153491322</script><p>对<script type="math/tex">w^2_{11}</script>更新，</p>
<script type="math/tex; mode=display">
w^2_{11} = w^2_{11} - \alpha * \frac{\partial J}{\partial w^2_{11}}</script><p>学习速率<script type="math/tex">\alpha</script>选1，经过简单的运算，<script type="math/tex">w^2_{11} = 0.92559664 - 1 * 0.20580941153491322 = 0.7197872284650868</script><br><div class="note info">
            <p>如果细心点就会发现，<script type="math/tex">\frac{\partial J}{\partial w^2_{11}}</script>其实就等于这层的z的导数乘上前一层的激活值a。如果没发现也没关系，下面<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#向量化-1">向量化</a>这节会做一个总结。</p>
          </div></p>
<h3 id="求所有w的导数"><a href="#求所有w的导数" class="headerlink" title="求所有w的导数"></a>求所有w的导数</h3><p>同理可以求出所有的导数</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} \tag{\ref{3.1.1}}</script><h3 id="向量化-1"><a href="#向量化-1" class="headerlink" title="向量化"></a>向量化</h3><div class="note danger">
            <p>上面只求了一个w的导数，虽然其他的w的求导都是类似操作，但是真要算起来，对于自己没去算过的人，可能花一天都没有办法将其用<strong>向量化表示</strong>。<br>求导是十分简单的，但是向量化可能会有点问题。问题的主要来源是<strong>想偷懒</strong>。对于这种问题，最好得到解决办法是暴力破解，即求出所有的w的导数，然后再将其向量化。</p>
          </div>
<p>首先观察上述公式<script type="math/tex">\ref{3.1.4}</script>：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}}</script><p>它由两部分组成，一个是<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>，第二部分是<script type="math/tex">\frac{\partial z^2_1}{\partial w^2_{11}}</script>，如果你自己求过导就会发现其实<script type="math/tex">\frac{\partial z^2_1}{\partial w^2_{11}} = a^1_1</script>。为了方便你们观察，我列出所有式子：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial z^2_1} * a^1_1 \quad \frac{\partial J}{\partial w^2_{12}} = \frac{\partial J}{\partial z^2_1} * a^1_2 \quad \frac{\partial J}{\partial w^2_{13}} = \frac{\partial J}{\partial z^2_1} * a^1_3\\
    \frac{\partial J}{\partial w^2_{21}} = \frac{\partial J}{\partial z^2_2} * a^1_1 \quad \frac{\partial J}{\partial w^2_{22}} = \frac{\partial J}{\partial z^2_2} * a^1_2 \quad \frac{\partial J}{\partial w^2_{23}} = \frac{\partial J}{\partial z^2_2} * a^1_3\\
\end{align}</script><p><strong>可能到这你有点烦躁了，因为表达式实在太多了。没关系，下方蓝色的note会给出总结，直接一步求解完毕。</strong><br>有没有发现，里面有一半是重复的元素？我们可以将它们组成向量得到：</p>
<script type="math/tex; mode=display">
\eqref{3.1.1}
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} = 
\begin{pmatrix}
\frac{\partial J}{\partial z^2_1}\\
\frac{\partial J}{\partial z^2_2}\\
\end{pmatrix} * 
\begin{pmatrix}
a^1_1 & a^1_2 & a^1_3
\end{pmatrix} \tag{3.1.5}</script><p>公式3.1.5和上面那六个表达式实际上计算的东西是一样的。进一步缩写为</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2} = \frac{\partial J}{\partial z^2} * (a^1)^T \tag{3.1.6}</script><p>这里加了一个T代表转置，实际上我们所有的输入值，激活值，输出值都是列向量。<br><div class="note info">
            <p>总结一下，在这里<strong>求<script type="math/tex">\frac{\partial J}{\partial w}</script>的步骤为：求权重值所在层的z的导数<script type="math/tex">\frac{\partial J}{\partial z}</script>再乘上前一层的激活值</strong>。这是对一个w求导所做的运算，而对一整个W矩阵求导那就是公式3.1.6的那个向量化操作。但是观察公式3.1.6发现，其实求一个w和求一个W矩阵并无区别，无非是将数字相乘改为向量（矩阵）相乘。<br>另外，其实这对神经网络中每一层的操作都是一样。如果不信可以自己算一下。所以以后理解的时候，可以用这种方式理解，加快理解速度。</p>
          </div></p>
<h3 id="更新偏差"><a href="#更新偏差" class="headerlink" title="更新偏差"></a>更新偏差</h3><p>偏差比权重简单很多。</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] \tag{\ref{3.1.2}}\\
    \frac{\partial J}{\partial z^2_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1}\\
                                      & = \frac{\partial J}{\partial a^2_1} * (a^2_1) * (1 - a^2_1) \tag{\ref{3.1.3}}\\
    \frac{\partial J}{\partial b^2_1} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial b^2_1} \\
                                      & = \frac{\partial J}{\partial z^2_1}\\
    \frac{\partial J}{\partial b^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1)\\
\end{align}</script><p>可以看到</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial b^2_1} = \frac{\partial J}{\partial z^2_1}</script><p>所以偏差的向量化比较简单：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial b^2_1}\\
    \frac{\partial J}{\partial b^2_2}\\
\end{pmatrix} = 
\begin{pmatrix}
    \frac{\partial J}{\partial z^2_1}\\
    \frac{\partial J}{\partial z^2_2}\\
\end{pmatrix}</script><h3 id="整理"><a href="#整理" class="headerlink" title="整理"></a>整理</h3><p>整理一下上一波的求导过程。目标是求得<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>，但是上面我并没有一步求导到底，相反我将每一步都写出来了，这是有原因的。因为<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>会在前一层对w求导时使用，所以在代码上当然需要保存副本。而<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>已经在这次的反向传播中使用过了，它的价值也算是用完了。<br>在<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#首先更新输出层的权重值（W）以及偏差值（b）">首先更新输出层的权重值（W）以及偏差值（b）</a>中<strong>略有瑕疵</strong>的步骤（也就是上述所有步骤）是：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script></li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script></li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script></li>
</ol>
<div class="note info">
            <p>根据上面三步，我们可以观察出，如果需要求出一层的<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script>（<strong>步骤3</strong>），需要<strong>先</strong>求出<strong>同一层</strong>的<script type="math/tex">\frac{\partial J}{\partial a^2}\ \frac{\partial J}{\partial z^2}</script>（<strong>步骤1和2</strong>）。<strong><em>所以</em></strong>如果我们需要求出<strong>前一层</strong>的<script type="math/tex">\frac{\partial J}{\partial w^1}\ \frac{\partial J}{\partial b^1}</script>，必须先求出<strong>前一层</strong>的a的导数<script type="math/tex">\frac{\partial J}{\partial a^1}\ \frac{\partial J}{\partial z^1}</script>，而由于公式<script type="math/tex">z^2_1 = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1</script>，可以观察到上述<strong>步骤2</strong>对z求导之后其实拥有三个选项：</p><ol><li>求w的导数（<strong>步骤3</strong>）</li><li>求b的导数（<strong>步骤3</strong>）</li><li>求上一层a的导数</li></ol>
          </div>
<p>所以正确的步骤是：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script>（<strong>不变</strong>）</li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script>（<strong>不变</strong>）</li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script>（<strong>不变</strong>）</li>
<li>最后求出前一层的<script type="math/tex">\frac{\partial J}{\partial a^1}</script>，准备下一步的计算。</li>
</ol>
<p><strong>也就是说，我们在一层中进行求导，需要分别求4个参数的导数，即当前层的a，w，b以及前一层的a的导数。</strong></p>
<h2 id="更新隐藏层的权重值以及偏差值"><a href="#更新隐藏层的权重值以及偏差值" class="headerlink" title="更新隐藏层的权重值以及偏差值"></a>更新隐藏层的权重值以及偏差值</h2><p>由于上述步骤太多，来回滑动网页略繁琐，我再次把图放出来，以供参考。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>隐藏层的更新与输出层略微不同，由于看公式不太形象，可以看上面的图。观察发现，隐藏层的某一个神经元链接着输出层的<strong>所有</strong>神经元。所以隐藏层的神经元的误差其实来源于与它相连接的输出层的神经元。<br>根据链式求导法则，我们知道：一个函数对一个变量求导，如果有多条路径可以到达该变量，那么就需要对每条路径都求导，最后将结果相加。转换成数学公式就跟下面公式3.2.1的求导过程一样。</p>
<h3 id="对第一个w求导"><a href="#对第一个w求导" class="headerlink" title="对第一个w求导"></a>对第一个w求导</h3><p>我们按照上一节<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#整理">《整理》</a>的四个步骤来做，先求出a的导数：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^1_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial a^1_1} + \frac{\partial J}{\partial a^2_2} * \frac{\partial a^2_2}{\partial a^1_1} & \text{输出层两个神经元均要求导再相加}\\
                                      & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial a^1_1} + \frac{\partial J}{\partial z^2_2} * \frac{\partial z^2_1}{\partial a^1_1} & \text{之前求过z的导数，为了方便书写用它替换}\\
                                      & = \frac{\partial J}{\partial z^2_1} * w^2_{11} + \frac{\partial J}{\partial z^2_2} * w^2_{21} \tag{3.2.1}\label{3.2.1}\\
\end{align}</script><div class="note info">
            <p>我们可以观察到隐藏层的a的导数<script type="math/tex">\frac{\partial J}{\partial a^1_1}</script>实际上就是<strong>输出层</strong>的z的导数<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>乘上与之相连的<strong>输出层</strong>的神经元的w。<br>一般化之后就是：<strong>除了输出层</strong>，其他所有层的<strong>a的导数</strong>都是<strong>后一层</strong>的<strong>z的导数</strong>乘上<strong>后一层</strong>的w。因为输出层的<strong>a的导数</strong>是通过代价函数求的。</p>
          </div>
<p>所以下一步就是求z的导数：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial z^1_1} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} + \frac{\partial J}{\partial z^2_2} * \frac{\partial z^2_1}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1}\\
                                      & = \frac{\partial J}{\partial z^2_1} * w^2_{11} * \frac{\partial a^1_1}{\partial z^1_1}  + \frac{\partial J}{\partial z^2_2} * w^2_{21} * \frac{\partial a^1_1}{\partial z^1_1}\\
                                      & = \frac{\partial J}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} & \text{这里a的导数参考}\ref{3.2.1}\\
    \frac{\partial a^1_1}{\partial z^1_1} & = a^1_1 * (1 - a^1_1) & \text{对sigmoid函数求导，前面已经说过了}\\
\end{align}</script><p>最后求出w的导数</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial w^1_{11}} & = \frac{\partial J}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} * \frac{\partial z^1_1}{\partial w^1_{11}}\\
                                         & = \frac{\partial J}{\partial z^1_1} * \frac{\partial z^1_1}{\partial w^1_{11}}\\
                                         & = \frac{\partial J}{\partial z^1_1} * a^0_1
\end{align}</script><h3 id="向量化-2"><a href="#向量化-2" class="headerlink" title="向量化"></a>向量化</h3><p>你肯定已经想把它向量化了。先列出所有的表达式。</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^1_{11}} = \frac{\partial J}{\partial z^1_1} * a^0_1\\
\frac{\partial J}{\partial w^1_{12}} = \frac{\partial J}{\partial z^1_1} * a^0_2\\
\frac{\partial J}{\partial w^1_{21}} = \frac{\partial J}{\partial z^1_2} * a^0_1\\
\frac{\partial J}{\partial w^1_{22}} = \frac{\partial J}{\partial z^1_2} * a^0_2\\
\frac{\partial J}{\partial w^1_{31}} = \frac{\partial J}{\partial z^1_3} * a^0_1\\
\frac{\partial J}{\partial w^1_{32}} = \frac{\partial J}{\partial z^1_3} * a^0_2\\</script><p>可以发现这其实跟上面的向量化步骤一模一样：</p>
<script type="math/tex; mode=display">
\begin{align}
    \begin{pmatrix}
        \frac{\partial J}{\partial w^1_{11}} & \frac{\partial J}{\partial w^1_{12}}\\
        \frac{\partial J}{\partial w^1_{21}} & \frac{\partial J}{\partial w^1_{22}}\\
        \frac{\partial J}{\partial w^1_{31}} & \frac{\partial J}{\partial w^1_{32}}\\
    \end{pmatrix} & = 
    \begin{pmatrix}
        \frac{\partial J}{\partial z^1_1}\\
        \frac{\partial J}{\partial z^1_2}\\
        \frac{\partial J}{\partial z^1_3}\\
    \end{pmatrix} * 
    \begin{pmatrix}
        a^0_1 & a^0_2
    \end{pmatrix} \\
    \frac{\partial J}{\partial w^1} & = \frac{\partial J}{\partial z^1} * (a^0)^T
\end{align}</script><h3 id="对偏差求导"><a href="#对偏差求导" class="headerlink" title="对偏差求导"></a>对偏差求导</h3><p>这一步更是简单，直接给结果了。</p>
<script type="math/tex; mode=display">
\begin{align}
    \begin{pmatrix}
        \frac{\partial J}{\partial b^1_1}\\
        \frac{\partial J}{\partial b^1_2}\\
        \frac{\partial J}{\partial b^1_3}\\
    \end{pmatrix} & = 
    \begin{pmatrix}
        \frac{\partial J}{\partial z^1_1}\\
        \frac{\partial J}{\partial z^1_2}\\
        \frac{\partial J}{\partial z^1_3}\\
    \end{pmatrix}\\
    \frac{\partial J}{\partial b^1} & = \frac{\partial J}{\partial z^1}
\end{align}</script><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><p>上述步骤看起来没什么问题，但是在实际编程中会有很大问题。在向量化的时候，我直接使用了<script type="math/tex">\frac{\partial J}{\partial z^1}</script>，但是问题就是<script type="math/tex">\frac{\partial J}{\partial z^1}</script>的向量化我直接跳过了。要向量化<script type="math/tex">\frac{\partial J}{\partial z^1}</script>，实际上得先向量化<script type="math/tex">\frac{\partial J}{\partial a^1}</script>。观察表达式<script type="math/tex">\ref{3.2.1}</script>，先给出所有的式子：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial a^1_1} = \frac{\partial J}{\partial z^2_1} * w^2_{11} + \frac{\partial J}{\partial z^2_2} * w^2_{21}\\
\frac{\partial J}{\partial a^1_2} = \frac{\partial J}{\partial z^2_1} * w^2_{12} + \frac{\partial J}{\partial z^2_2} * w^2_{22}\\
\frac{\partial J}{\partial a^1_3} = \frac{\partial J}{\partial z^2_1} * w^2_{13} + \frac{\partial J}{\partial z^2_2} * w^2_{23}\\</script><h4 id="向量化-3"><a href="#向量化-3" class="headerlink" title="向量化"></a>向量化</h4><script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial a^1_1}\\
    \frac{\partial J}{\partial a^1_2}\\
    \frac{\partial J}{\partial a^1_3}\\
\end{pmatrix} = 
\begin{pmatrix}
w^2_{11} & w^2_{12} & w^2_{13}\\
w^2_{21} & w^2_{22} & w^2_{23}\\
\end{pmatrix}^T * 
\begin{pmatrix}
\frac{\partial J}{\partial z^2_1} & \frac{\partial J}{\partial z^2_2}
\end{pmatrix}</script><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在反向传播中，每一层都只需要重复如下几步：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script></li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script></li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script></li>
<li>最后求出前一层的<script type="math/tex">\frac{\partial J}{\partial a^1}</script>，准备下一步的计算。此步骤的向量化操作在<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#注意点">注意点</a>。</li>
</ol>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">使用numpy实现一个简单的神经网络</a></p>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>在做反向传播代码时，验算了很多遍，发现公式推导没有问题，但是梯度却一直在上升，心态都炸了。<br>最后发现，用了大半年的crossentropy在最前面居然要加上一个“-”号。以前由于是偷懒，在求导的时候一般不加负号，在求完导之后再补上。然后由于写习惯了，导致我忘记crossentropy居然是有负号的。</p>
<h1 id="撒花"><a href="#撒花" class="headerlink" title="撒花"></a>撒花</h1><p>在第二节有一步是初始化数据，但是全篇都没用几个地方用到。是因为数据测试起来太麻烦了，我需要在代码里一步一步分析神经网络的计算过程，从而获得数据。以后再补充吧。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/深度学习算法（一）：simple NN（前馈神经网络的正反向推导）.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/linux非root用户配置环境变量.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/linux非root用户配置环境变量.html" class="post-title-link" itemprop="url">linux非root用户配置环境变量</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-15 16:04:44" itemprop="dateCreated datePublished" datetime="2019-05-15T16:04:44+08:00">2019-05-15</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:22:49" itemprop="dateModified" datetime="2019-06-04T13:22:49+08:00">2019-06-04</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><a href="https://www.cnblogs.com/gstblog/p/10160976.html" target="_blank" rel="noopener">参考文章</a><br>本文以配置anaconda的环境变量为例。</p>
<p>切换到用户目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br></pre></td></tr></table></figure></p>
<p>输入，发现有一个名为<code>.bashrc</code>的文件<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ll</span></span><br></pre></td></tr></table></figure></p>
<p>编辑它<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~<span class="string">/.bashrc</span></span><br></pre></td></tr></table></figure></p>
<p>在最后一行加上如下代码，保存并退出。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=~/anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>
<div class="note warning">
            <p>PATH和=之间不能有空格。由于写java代码习惯了，加上了空格，导致报错。</p>
          </div>
<p>更新配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/linux非root用户配置环境变量.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/梯度下降算法的推导.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/梯度下降算法的推导.html" class="post-title-link" itemprop="url">梯度下降算法的推导</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-10 19:55:24" itemprop="dateCreated datePublished" datetime="2019-05-10T19:55:24+08:00">2019-05-10</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:20:32" itemprop="dateModified" datetime="2019-06-04T13:20:32+08:00">2019-06-04</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>梯度下降算法大家都知道，公式是<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，其中J是代价函数。但是这个算法具体是怎么来的，可能不太清楚。<br>本文参考<br><a href="https://mp.weixin.qq.com/s/k26Fm0GL3fdVA9VbQIVAuQ" target="_blank" rel="noopener">微信公众号</a><br><a href="https://baike.baidu.com/item/梯度/13014729" target="_blank" rel="noopener">百度百科</a><br>由于没有专业的制图工具，所以只能手画了。。。</p>
<h1 id="梯度下降问题"><a href="#梯度下降问题" class="headerlink" title="梯度下降问题"></a>梯度下降问题</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/梯度下降草图.jpg" alt="梯度下降草图"><br>由图中可以观察到，我们将参数初始化到A点，我们的目标是将点移动到最小值点（或者极小值点）。那么问题就是如何移动了。<br>先给出梯度下降公式：<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，J是代价函数，这个公式应该不陌生。</p>
<h1 id="一阶泰勒展开式"><a href="#一阶泰勒展开式" class="headerlink" title="一阶泰勒展开式"></a>一阶泰勒展开式</h1><p>如果学过高数，应该知道<strong>一阶泰勒展开式</strong>的公式是：<script type="math/tex">f(x) = f(x_0) + (x - x_0) * f'(x_0) + R_n(x)</script>，其中<script type="math/tex">R_n(x)</script>是泰勒公式的余项，可以理解为一个无穷小量。既然是无穷小量那么便可以省略不写，但是即使是无穷小，其实等式的左右边还是有点差距的，所以将等式修改为<strong>约等于号</strong>。</p>
<script type="math/tex; mode=display">
f(x) \approx f(x_0) + (x - x_0) * f'(x_0)</script><p>但是由于我们最小化的代价函数的参数是<script type="math/tex">\theta</script>，所以我们可以将x替换为<script type="math/tex">\theta</script>，即</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script><p>如果不知道泰勒公式，可以看下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/泰勒公式线性近似.webp" alt="泰勒公式线性近似"><br>在点<script type="math/tex">\theta_0</script>处，找一条极短的直线来表示曲线，则直线的斜率为<script type="math/tex">f'(\theta_0)</script>，并且已知<script type="math/tex">\theta_0</script>，那么根据初中数学，可以获得直线公式<script type="math/tex">f(\theta) = f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>（还不懂看这个：<script type="math/tex">y-y_0=k(x-x_0)</script>===&gt;<script type="math/tex">y = y_0 + k(x-x_0)</script>）。<br><div class="note warning">
            <p>如果仔细看到了上一行的推导，你也许要问：为什么直线斜率是<script type="math/tex">f'(\theta_0)</script>。百度。</p>
          </div><br><div class="note warning">
            <p>如果对上式没有问题，可能要问为什么这个红线的箭头要向下，不能向上？我有强迫症，我就要让它向上，并且我还要让<script type="math/tex">\theta</script>在<script type="math/tex">\theta_0</script>右边。这个下面会讲，但是现在假定以下的步骤均围绕上图展开。</p>
          </div><br>至此准备工作完成。</p>
<h1 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h1><p>我们将<script type="math/tex">f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>的<script type="math/tex">\theta - \theta_0</script>用字母<script type="math/tex">\alpha v</script>代替，实际上只用<script type="math/tex">v</script>代替也可以。但是还是使用<script type="math/tex">\alpha v</script>吧。</p>
<script type="math/tex; mode=display">
\theta - \theta_0 = \alpha v</script><p>所以公式被简化为如下形式，并且将导数的表示做一下改变，用<strong>倒三角</strong>表示</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + \alpha v * \nabla f(\theta_0)</script><p>由于我们的目标是使得<script type="math/tex">f(\theta)</script>比<script type="math/tex">f(\theta_0)</script>小，也就是使得<script type="math/tex">f(\theta) - f(\theta_0) < 0</script>。那么将公式转变为</p>
<script type="math/tex; mode=display">
f(\theta) -  f(\theta_0) \approx \alpha v * \nabla f(\theta_0) < 0</script><p>省略一部分</p>
<script type="math/tex; mode=display">
\alpha v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">\alpha</script>一般为正值，所以</p>
<script type="math/tex; mode=display">
v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>实际上都是向量。所以上式就转换为<strong>两个向量相乘在什么时候是小于0的</strong>，并且我们希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好，也就是<script type="math/tex">v * \nabla f(\theta_0)</script>越小越好。那么问题又转化为<strong>两个向量相乘在什么时候是最小的</strong>。<br><div class="note warning">
            <p>问题1：为什么<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>是向量。<br>由于以上都是使用二维的图来描述，所以无法体现是向量。但是实际上<script type="math/tex">\theta</script>不只有一个。<br>问题2：为什么希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好。<br>因为希望<script type="math/tex">f(\theta)</script>这一步迈远一点。</p>
          </div><br>以下为向量乘积的三种形式，由初中的知识可以得知，当向量相反时<script type="math/tex">cos(\alpha)</script>为-1，即cos函数的最小值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/向量的乘积.webp" alt="向量的乘积"><br>由于公式可以转为如下，其中<script type="math/tex">\beta</script>是向量夹角</p>
<script type="math/tex; mode=display">
|v| * |\nabla f(\theta_0)| * cos(\beta) < 0</script><p>所以当<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>正好相反时，<script type="math/tex">cos(\beta) = -1</script>。也就是说当<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>反向，<script type="math/tex">v * \nabla f(\theta_0)</script>最小。<br>众所周知，<script type="math/tex">\nabla f(\theta_0)</script>就是梯度，也就是梯度方向。所以只需要<script type="math/tex">v</script>为<script type="math/tex">\nabla f(\theta_0)</script>的反方向即可。所以</p>
<script type="math/tex; mode=display">
v  = -\frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}\\</script><p>之所以要除以<script type="math/tex">\nabla f(\theta_0)</script>的模，是因为<script type="math/tex">v</script>是单位向量。<br><div class="note warning">
            <p><script type="math/tex">v</script>为什么是单位向量。不太清楚，原博主没说明。</p>
          </div><br>将<script type="math/tex">v</script>带入到<script type="math/tex">\theta - \theta_0 = \alpha * v</script>中</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha * \frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}</script><p>一般地，因为<script type="math/tex">|\nabla f(\theta_0)|</script>是标量，可以并入到中，即简化为：</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha *\nabla f(\theta_0)</script>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/梯度下降算法的推导.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/深度学习中的一些疑问总结.html" class="post-title-link" itemprop="url">深度学习中的一些疑问总结</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-03 13:54:56" itemprop="dateCreated datePublished" datetime="2019-05-03T13:54:56+08:00">2019-05-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:20:26" itemprop="dateModified" datetime="2019-06-04T13:20:26+08:00">2019-06-04</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h1><div class="note primary">
            <p>为什么<script type="math/tex">w*x+b=z</script>之后，需要使用一个激活函数<script type="math/tex">a = \sigma(z)</script>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
          </div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。<br>那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。</p>
<script type="math/tex; mode=display">
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w' * a^0 + b' \qquad由于w和b只是一堆常数，所以将w'代替w^2 * w^1，b'同理
    \end{array}
\right.</script><p>通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<script type="math/tex">a^2 = w' * a^0 + b'</script>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。<br>只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降的意义"><a href="#梯度下降的意义" class="headerlink" title="梯度下降的意义"></a>梯度下降的意义</h1><div class="note primary">
            <p>首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？</p>
          </div>
<h1 id="dropout背后的原理"><a href="#dropout背后的原理" class="headerlink" title="dropout背后的原理"></a>dropout背后的原理</h1><div class="note primary">
            <p>dropout背后的原理是什么？</p>
          </div>
<p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a></p>
<h1 id="为什么Mini-batch比普通的梯度下降快？"><a href="#为什么Mini-batch比普通的梯度下降快？" class="headerlink" title="为什么Mini-batch比普通的梯度下降快？"></a>为什么Mini-batch比普通的梯度下降快？</h1><div class="note primary">
            <p>为什么Mini-batch比普通的梯度下降快？</p>
          </div>
<h1 id="指数加权平均的作用"><a href="#指数加权平均的作用" class="headerlink" title="指数加权平均的作用"></a>指数加权平均的作用</h1><div class="note primary">
            <p>指数加权平均的作用</p>
          </div>
<h1 id="为什么要deep"><a href="#为什么要deep" class="headerlink" title="为什么要deep"></a>为什么要deep</h1><div class="note primary">
            <p>学了有一段时间的深度学习，但是有个问题一直没想明白。那就是将hidden layer叠多层的意义是什么？</p>
          </div>
<p>可以观察下图发现，确实越deep代价越小。左边的两列显示了，层数越多代价越小。右边两列显示即使整个神经网络参数类似，但是明显越deep代价越小。同一行代表hidden layer的参数接近。那个size指的是神经网络中的参数。再看最后一行，它显示即使参数暴增到16k，代价也不是很低。图中红框还显示了2层2k的model比1层16k的model好多了。<br>下图是由底下的论文的作者做的实验得出的结论。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/隐藏层层数对cost的影响.jpg" alt="隐藏层层数对cost的影响"><br><strong>那么为什么神经网络越深效果越好呢？</strong><br>这其实归功于modularization——模块化。如下图所示，如果直接写一个model用于将4种人分类，那么可能会出现某类人的数据并不多的情况，比如长发的男生的数据可能并不多。那么分类的准确率可能会不是很高。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/解释why deep的例子.jpg" alt="解释why deep的例子"><br>下图中先将其分类为男女以及长短发，然后再进一步分类。虽然说长发男生的数据比较少，但是男女和长短发的数据有很多，我们可以得到一个很好的模型。之后我们再叠一层用于进一步分类，此时，由于我们已经做了上一步的分类，所以新的一层可以使用上一层的特征。<br>上一层的分类已经把难的事情——辨别男女，辨别长短发等解决了，所以后一层只要使用少量的数据就能进行分类。<br><strong>没有使用模块化</strong>的那个模型，它是用少量的数据硬生生地去识别长发男生。<strong>使用模组化</strong>的模型是先识别男女以及长短发，再通过调用前一步的特征判断。下图4个分类器区别可能只是辨别的方式不同而已，比如权重不同。输入一张图片之后第一层已经可以辨别是男还是女，长发还是短发，然后后一层经过简单的运算就可以确定了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/模块化后.jpg" alt="模块化后"><br>经过上面的解释，可能已经大致理解是什么意思了。但是真要讲清还有点问题，尤其是模块化怎么做。<br>但是李宏毅老师说模块化其实是神经网络从数据中<strong>自动</strong>学到的。</p>
<h2 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h2><p>用数电的逻辑门来举例，但是我没怎么学过数电，所以没有理解。<br>另一个比较贴近生活的例子，就是剪窗花。没有人会一瓣花一瓣花的去剪窗花，都是将纸先折好，然后一步剪完。这就是模块化了。</p>
<p>还有其他领域的人也有过解读，个人理解why deep这个问题可能到现在没有一个官方的回应，可能前辈也是误打误撞才发现deep learning很牛。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/其他领域对为什么要deep的解读.jpg" alt="其他领域对为什么要deep的解读"></p>
<h2 id="吴恩达老师的解释"><a href="#吴恩达老师的解释" class="headerlink" title="吴恩达老师的解释"></a>吴恩达老师的解释</h2><p><a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>也做了解释。</p>
<h1 id="词向量乘上权重以及做梯度下降有什么意义"><a href="#词向量乘上权重以及做梯度下降有什么意义" class="headerlink" title="词向量乘上权重以及做梯度下降有什么意义"></a>词向量乘上权重以及做梯度下降有什么意义</h1><p><a href="https://mooc.study.163.com/learn/2001280005?tid=2001391038&amp;_trace_c_p_k2_=023fecd41c524f0d9485b18d2d773f53#/learn/content?type=detail&amp;id=2001770038" target="_blank" rel="noopener">本文灵感</a><br>本文疑问：</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<h2 id="准备词向量"><a href="#准备词向量" class="headerlink" title="准备词向量"></a>准备词向量</h2><p>假设有这么一句话：I want a glass of orange ___.<br>要做的是估计划线处应该填入什么词。答案是juice。<br>首先我们需要一个词典——vocabulary，每个单词对应一个索引，这是通用步骤。词表大小为10000。<br>然后将上述的句子，从单词转成索引形式。即：<br>I want a glass of orange —-&gt; 4343 9665 1 3852 6163 6257<br>此外每一个单词都会对应一个词向量，而词表中所有单词的词向量就组合一个词嵌入矩阵。词表以及词向量都是可以找一些预训练的，比如<strong>GloVe</strong>。<br>梳理一遍就是：<br>单词:索引<br>索引:词向量<br>所以可以通过单词间接地获取到词向量。关于索引对应词向量，实际上是里面没有索引的因为一个矩阵它本身就有一个属性表示索引，如第0行就是代表第0个单词，第1行就是代表第一个单词。<br>总而言之，我们通过单词获取索引后，就能通过该索引直接获取词向量。伪代码可以表示为：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index = vocabulary.get_index('want') <span class="comment"># 索引为9665</span></span><br><span class="line">word_vector = embedding_matrix[index, :] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p><strong>对于词嵌入矩阵的行代表词向量，还是列代表词向量不必纠结。</strong>你要乐意可以改成<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_vector = embedding_matrix[:, index] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p>如果使用one hot编码来执行上述代码就是将9665转为one hot编码，即除了9665位置为1，其余位置全为0。然后<script type="math/tex">word\_vector = embedding\_matrix^T * word\_one\_hot</script>。这样也能得到词向量，但是由于one hot编码全是0，算起来速度太慢了。<br>现在有了句子“I want a glass of orange”的所有词向量，接下来要做的是将这些词向量从头到尾拼在一起，接成一个更长的向量，也就是6倍长的向量。原词向量是300维，拼接完成后是1800维。然后将这个向量输入一个神经网络中，最后经过softmax函数进行预测，预测范围是在10000个单词中，看谁的概率大。</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>将词向量送入神经网络中当然还需要梯度下降进行迭代。这里会有很多疑问，</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<p>我进行逐一思考，本文仅为自己的理解。<br>首先其实有一件事很多视频没讲，可能他们认为这是一件很平常的事，所以没讲。<br>上述的这个步骤并不是预测步骤，而是在进行迭代，所以是一个训练步骤。人家之所以说<em>我们可以通过这个神经网络预测出单词为juice</em>，是因为逻辑上是这样的。<br>由于是训练步骤，所以我们有一个很重要的数据，最终结果。最终结果我们是知道的，然而我们初学者在考虑整个流程时，没把最终结果算进去，因为老师说<em>我们可以通过这个神经网络预测出单词为juice</em>，由于是<em>预测</em>，那么结果肯定没有啊。这很合乎逻辑。所以就陷入了一个思维的怪圈，<strong>正确的逻辑是：</strong></p>
<ol>
<li>首先我们知道最终结果，所以当第一次迭代时，所有的权重都是随机初始化的，1、2两个问题也就没有意义了。第一次迭代完毕后，结果肯定稀巴烂，所以进行梯度下降。</li>
<li>这里面我们又会碰到一个问题，就是梯度下降到底在算什么？其实这里的疑问来自我们的潜意识始终将句子当做文字在看，自然而然就意识不到梯度下降在干什么。而其实我们在几步之前就已经将文字转为词向量了。<br>词向量说白了就是一堆浮点型数字，而最终结果juice也是一个词向量，所以实际上就是将一个权重矩阵乘上一个1800维的向量，得到一个输出值（may be 激活值），然后将这个输出值和juice的向量放入代价函数中进行计算，接下来的梯度下降其实就是跟正常的步骤一样。</li>
<li><strong>梯度下降就是在寻找一个合适的权重矩阵使得权重矩阵乘1800维向量得到的值接近juice的向量。</strong><br>这里在解释第3个问题时，顺便也解释了第1、2个问题。<strong>权重值实际上就是用来使得预测值和实际结果越接近越好</strong></li>
<li>由于开头就说了我们实际上是知道划线处应该填juice，所以第4个问题压根不需要解答，因为我们本来就知道应该填juice，也没必要预测。之前之所以有这个问题，是因为我们潜意识觉得老师说<em>我们可以通过这个神经网络预测出单词为juice</em>。</li>
<li>至于第5个问题如何预测其他句子。打个比方，现在预测一个新的句子：I want a glass of apple ___.<br>由于我们知道词嵌入矩阵是由很多单词的词向量组成的。而一个单词词向量其实就是一堆特征组成的，对比两个句子，apple和orange的词向量肯定很接近，因为它们都是水果，它们的特征（水多不多，好不好吃，是不是水果，有没有性别特征，是不是动词等）都类似。<br>而我们之前已经训练了一个神经网络，我们得到了所需要的权重值，我们直接把这个权重值乘上新句子的词向量，那么结果肯定和权重值乘上之前句子的词向量的结果很接近。因为它们之间的区别仅仅是apple和orange的词向量不同，并且apple和orange的词向量其实也并不是完全不同，只是略有不同。所以二者的结果自然也差不多。<br>最后经过softmax函数在10000个单词之中预测，输出的结果必然都是juice。</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>这里还会有一个问题，那么如果需要预测的句子单词数不一致怎么办？<br>之前两个例子的句子都是6个单词，预测第7个单词。那要是前面有10个单词，我要预测第11个单词怎么办？<br>其实可以只考虑划线处前几个单词，比如只考虑划线处前4个单词，这样输入的维度就相同了。<br>在本文开头的参考视频里，吴恩达老师讲得很清楚了。由古圣先贤总结，一般来说取前4个单词即可，当然你自己也可以用其他办法。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/深度学习中的一些疑问总结.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/深度学习学习记录大纲.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/深度学习学习记录大纲.html" class="post-title-link" itemprop="url">深度学习学习记录大纲</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-02 13:54:55" itemprop="dateCreated datePublished" datetime="2019-05-02T13:54:55+08:00">2019-05-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-24 10:53:05" itemprop="dateModified" datetime="2019-07-24T10:53:05+08:00">2019-07-24</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/outline/" itemprop="url" rel="index"><span itemprop="name">outline</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题"><a href="#《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题" class="headerlink" title="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题"></a>《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题</h1><p><a href="https://yan624.github.io/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">文章地址</a><br>由于神经网络中参数太多，而有些参数的表现形式太过复杂， 比如文中权重——<script type="math/tex">w^l_{ji}</script>有太多上标下标，所以写了一篇文章记录一下。</p>
<h1 id="对神经网络整体的理解"><a href="#对神经网络整体的理解" class="headerlink" title="对神经网络整体的理解"></a>对神经网络整体的理解</h1><p><a href="https://yan624.github.io/学习笔记/对神经网络整体的理解.html">文章地址</a><br>通常学习深度学习从一个最简单的神经网络开始，但是由于对深度学习时0基础，所以需要同时学习大量算法以及其原理，比如梯度下降，Momentum，Adam，RMSprop，adagrad等等算法。所以写了一篇文章记录一下大部分的算法以及原理。</p>
<h1 id="吴恩达李宏毅综合学习笔记：RNN入门"><a href="#吴恩达李宏毅综合学习笔记：RNN入门" class="headerlink" title="吴恩达李宏毅综合学习笔记：RNN入门"></a>吴恩达李宏毅综合学习笔记：RNN入门</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">文章地址</a><br>学习完神经网络之后，可以学习其他的神经网络模型。由于本人初步决定学习nlp，所以基本没有看CNN，直接学了RNN。本文就是学习RNN的记录，包括了许多算法。</p>
<h1 id="吴恩达深度学习学习笔记：自然语言处理与词嵌入"><a href="#吴恩达深度学习学习笔记：自然语言处理与词嵌入" class="headerlink" title="吴恩达深度学习学习笔记：自然语言处理与词嵌入"></a>吴恩达深度学习学习笔记：自然语言处理与词嵌入</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html">文章地址</a><br>学习完RNN之后，就可以学习 NLP 的概念了，这里面讲得虽然还是神经网络，但是其实都是 NLP 领域的知识。</p>
<h1 id="练习Keras-RNN的代码"><a href="#练习Keras-RNN的代码" class="headerlink" title="练习Keras RNN的代码"></a>练习Keras RNN的代码</h1><p><a href="https://yan624.github.io/学习笔记/练习Keras RNN的代码.html">文章地址</a><br>学习完Simple NN，RNN 和 NLP 之后，就可以练习一下了。文中使用 Keras 框架，写了几个例子练习。</p>
<h1 id="疑问总结"><a href="#疑问总结" class="headerlink" title="疑问总结"></a>疑问总结</h1><p><a href="https://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">文章地址</a><br>深度学习入门后必然有很多疑问待解答，此篇解决疑问。</p>
<h1 id="开始继续学习机器学习"><a href="#开始继续学习机器学习" class="headerlink" title="开始继续学习机器学习"></a>开始继续学习机器学习</h1><p>待续。。。</p>
<h1 id="开始CNN"><a href="#开始CNN" class="headerlink" title="开始CNN"></a>开始CNN</h1><p>待续。。。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/深度学习学习记录大纲.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">记录学习问题，积累做的 leetcode 题目</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">124</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">23</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
			  
			  <!-- 不蒜子/busuanzi -->
			  <div class="site-state-item site-state-posts">
			  	<span class="site-state-item-count">117.7k</span>
			  	<span class="site-state-item-name">总字数</span>
			  </div>
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

  
  <!-- 自己新增的所有 js 文件 -->
  <script src="/lib/my-utils.js"></script>
<!--图片缩放插件-->
<script src="/lib/zoomify/zoomify.min.js"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- 背景插件 -->
<script src="https://cdn.bootcss.com/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(11), {
		duration:90000,//1min半一换
		fade: 1500
	});
</script>
<!-- 图片缩放 -->
<script>
$('#content img').zoomify({duration: 500, });
  $('#content img').on('zoom-in.zoomify', function () {
    $('#sidebar').css('display', 'none');
  });
  $('#content img').on('zoom-out-complete.zoomify', function () {
    $('#sidebar').css('display', '');
  });
</script>

	

</body>
</html>
