<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-simple.min.css?v=1.0.2">























  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="朱冲䶮的博客，记录学习时遇到的问题">
<meta name="keywords" content="博客，java，javaWeb，NLP，python，机器学习，深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/page/3/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="朱冲䶮的博客，记录学习时遇到的问题">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="朱冲䶮的博客，记录学习时遇到的问题">






  <link rel="canonical" href="http://yan624.github.io/page/3/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

	<!--图片缩放插件样式-->
	<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">
</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
  <!--加载flower canvas-->
  <script>
	var pathname = window.location.pathname;
	if(pathname == '/flower.html'){
		var body =  document.getElementsByTagName('body')[0];
		var canvas = document.createElement("canvas")
		canvas.setAttribute('id', 'sakura')
		// '<canvas id="sakura"></canvas>'
		body.appendChild(canvas)
	}
  </script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">低阶炼金术士</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">13</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">19</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">88</span></a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/ViewPager无法刷新数据.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/ViewPager无法刷新数据.html" class="post-title-link" itemprop="url">ViewPager无法刷新数据</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-18 22:15:24 / 修改时间：22:38:54" itemprop="dateCreated datePublished" datetime="2019-04-18T22:15:24+08:00">2019-04-18</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/android/" itemprop="url" rel="index"><span itemprop="name">android</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>实现ViewPager刷新数据功能，在网上找了很多资料都已经过时了。<br>由于本人并不是android开发出身，完全是做app玩的。所以很多术语都不知道，如果看不懂就算了。。。</p>
<p>实现PagerAdapter类，我命名为HomePagerAdapter<br><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HomePagerAdapter</span> <span class="keyword">extends</span> <span class="title">PagerAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;View&gt; pageView;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HomePagerAdapter</span><span class="params">(ArrayList&lt;View&gt; pageView)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pageView = pageView;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mChildCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">notifyDataSetChanged</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mChildCount = getCount();</span><br><span class="line">        <span class="keyword">super</span>.notifyDataSetChanged();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getItemPosition</span><span class="params">(Object object)</span>   </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> ( mChildCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            mChildCount --;</span><br><span class="line">            <span class="keyword">return</span> POSITION_NONE;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">return</span> <span class="keyword">super</span>.<span class="title">getItemPosition</span><span class="params">(object)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//获取当前窗体界面数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="function"><span class="keyword">return</span> pageView.<span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//判断是否由对象生成界面</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isViewFromObject</span><span class="params">(View arg0, Object arg1)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="keyword">return</span> arg0==arg1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">destroyItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position, Object object)</span> </span>&#123;</span><br><span class="line">        container.removeView(pageView.get(position));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Object <span class="title">instantiateItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position)</span> </span>&#123;</span><br><span class="line">        View view = pageView.get(position);</span><br><span class="line">        container.addView(view);</span><br><span class="line">        <span class="keyword">return</span> view;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">finishUpdate</span><span class="params">(ViewGroup container)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(container.getChildCount() == <span class="number">0</span>)&#123;</span><br><span class="line">            pageView.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意destroyItem、instantiateItem方法，PagerAdapter中还有两个同名的方法，但是已被废弃，注意参数不同。<br>实现刷新数据主要在destroyItem、instantiateItem、finishUpdate三个方法。其他的方法其实别人的教程也写了，但是我这三个方法是我自己研究的，我没见别人写过。</p>
<p>解释流程。<br>第一步，改变数据，我是将数据保存在了<code>private ArrayList&lt;View&gt; pageView;</code>中。<br>第二步，调用<code>adapter.notifyDataSetChanged();</code>方法，它首先会销毁item，即调用<code>destroyItem(ViewGroup container, int position, Object object)</code>方法。随即调用<code>instantiateItem(ViewGroup container, int position)</code>方法。<br>一般来说大家都是这么干的，因为将数据改变后，调用<code>adapter.notifyDataSetChanged();</code>方法。直觉认为这么做合乎常理。<br>但是这里注意一点，假设<code>private ArrayList&lt;View&gt; pageView;</code>中原先保存两个View，改变数据将这个View删除，从新添加三个新的View，那么在<code>destroyItem(ViewGroup container, int position, Object object)</code>方法中，它无法删除，仔细看里面的代码<code>container.removeView(pageView.get(position));</code>，发现它是通过position这个索引获取对象，再在container容器中通过对象查找删除。那么问题来了，你之前已经将两份View删除了，它还怎么通过position获取到呢？所以在这一步出了问题。<br>当然这一步出了问题后，后面的创建页面步骤更是稀巴烂。</p>
<p>正确步骤如下：<br><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将页面刷新，渲染新的数据集</span></span><br><span class="line"><span class="selector-tag">homePagerAdapter</span><span class="selector-class">.notifyDataSetChanged</span>();</span><br><span class="line"><span class="selector-tag">changeData</span>(inflater, data);</span><br><span class="line"><span class="selector-tag">homePagerAdapter</span><span class="selector-class">.notifyDataSetChanged</span>();</span><br></pre></td></tr></table></figure></p>
<p>第一步，不要更改数据，直接调用<code>homePagerAdapter.notifyDataSetChanged();</code>，目的是让其删除原先的view。<br>第二步，更改数据。<br>第三步，再次调用<code>homePagerAdapter.notifyDataSetChanged();</code>，完成页面的创建。由于container中已经没有view了，所以删除那个步骤做了也等于没做，但是由于数据已经更新页面还是会被创建出来。</p>
<p>最后强调用一点。在HomePagerAdapter类中一个<code>finishUpdate(ViewGroup container)</code>方法，注意看里面的代码。<strong>以上的所有步骤，全部依赖于这几句代码。</strong><br>上面第一步说到直接调用notifyDataSetChanged()方法目的是删除原先的view，但是view删除后，你必须将<code>private ArrayList&lt;View&gt; pageView;</code>中的数据也删除。<strong>这里补充一点，pageView内是我创建的View，而container中是android自己维护的界面</strong>，我也不知道怎么称呼，就将其称为界面吧。<br>在finishUpdate()方法中判断，如果container中已经没有界面了，那就直接移除pageView中所有的数据，也就是算更新数据了。值得注意的是，这里面逻辑及其复杂，这行清空数据的代码，只有放在<code>finishUpdate(ViewGroup container)</code>中执行，并且必须加上那个if条件判断，app才能正常运行。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/ViewPager无法刷新数据.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式.html" class="post-title-link" itemprop="url">在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-18 16:23:46 / 修改时间：16:27:54" itemprop="dateCreated datePublished" datetime="2019-04-18T16:23:46+08:00">2019-04-18</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/hexo/" itemprop="url" rel="index"><span itemprop="name">hexo</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>之前写的数学表达式明明可以渲染，但是接下去隔了n行的数学表达式无法渲染。推测是因为单行数学表达式在文字前面换行。<br>比如说：</p>
<blockquote>
<p>文字文字文字文字：·￥￥·<br>该表达式渲染正常。</p>
</blockquote>
<p>如果，</p>
<blockquote>
<p>文字文字文字文字：<br>·￥￥·<br>那么下面的数学表达式将全部无法渲染。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></blockquote></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/运用其他的插件，在hexo中添加提示弹窗.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/运用其他的插件，在hexo中添加提示弹窗.html" class="post-title-link" itemprop="url">运用其他的插件，在hexo中添加提示弹窗</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-18 14:30:17 / 修改时间：14:49:00" itemprop="dateCreated datePublished" datetime="2019-04-18T14:30:17+08:00">2019-04-18</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/hexo/" itemprop="url" rel="index"><span itemprop="name">hexo</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>由于我写了很多学习记录，但是这些都是自己看书或者看视频学来的。万一有错误的地方正好被人看见，他又是新手，误以为我的是对的，这样就不好了。所以准备做一个提示弹窗，在所有的带有“学习笔记”的标签的文章中自动弹出提示。</p>
<ol>
<li>下载一个自己喜欢的弹窗插件，可以去<a href="http://www.jq22.com/" target="_blank" rel="noopener">jQuery插件库</a>找。</li>
<li>将css和js放在next主题下的source文件夹中，如下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/next%E4%B8%BB%E9%A2%98%E7%9B%AE%E5%BD%95.jpg" alt="next主题目录"><br>css文件放入css文件，js文件放入js文件夹。我自己创了一个spop的文件夹，用于单独放置我的弹窗插件。</li>
<li>打开layout文件夹，进入_macro文件夹，找到post.swig文件。搜索class=”post-block”，这个标签的位置在下图箭头所指的地方：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/%E5%BC%B9%E7%AA%97%E6%8F%92%E4%BB%B6%E4%BB%A3%E7%A0%81%E5%86%99%E5%85%A5%E7%9A%84%E4%BD%8D%E7%BD%AE.png" alt="弹窗插件代码写入的位置"><br>如果打开了这个文件，找了post-block标签，可以看到标签内部第一行代码为<code>&lt;link itemprop=&quot;mainEntityOfPage&quot; href=&quot;{ config.url }{ url_for(post.path) }&quot;/&gt;</code>。由于hexo渲染问题我将href属性里的值去掉了一对{}。<br>将下面的代码放在上述代码上面或者下面即可。<figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="name">for</span></span> tag <span class="keyword">in</span> post.tags %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml">	<span class="comment">&lt;!--判断该文章是否为学习笔记--&gt;</span></span></span><br><span class="line"><span class="xml">	</span><span class="template-tag">&#123;% <span class="name"><span class="name">if</span></span> tag.name == '学习笔记' and !is_home() %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml">  		<span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"/css/spop/spop.min.css"</span>&gt;</span></span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"/js/spop/spop.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span></span><br><span class="line"><span class="xml">			spop(&#123;</span></span><br><span class="line"><span class="xml">				template: '<span class="tag">&lt;<span class="name">h4</span> <span class="attr">class</span>=<span class="string">"spop-title"</span>&gt;</span>注意<span class="tag">&lt;/<span class="name">h4</span>&gt;</span>此文章仅为博主的学习笔记，其中可能含有极大的理论错误。',</span></span><br><span class="line"><span class="xml">				group: 'tips',</span></span><br><span class="line"><span class="xml">				position  : 'bottom-center',</span></span><br><span class="line"><span class="xml">				style: 'success',</span></span><br><span class="line"><span class="xml">				autoclose: 5500,</span></span><br><span class="line"><span class="xml">				onOpen: function () &#123;</span></span><br><span class="line"><span class="xml">					//这里设置灰色背景色</span></span><br><span class="line"><span class="xml">				&#125;,</span></span><br><span class="line"><span class="xml">				onClose: function() &#123;</span></span><br><span class="line"><span class="xml">					//这里可以取消背景色</span></span><br><span class="line"><span class="xml">					spop(&#123;</span></span><br><span class="line"><span class="xml">						template: 'ε = = (づ′▽`)づ',</span></span><br><span class="line"><span class="xml">						group: 'tips',</span></span><br><span class="line"><span class="xml">						position  : 'bottom-center',</span></span><br><span class="line"><span class="xml">						style: 'success',</span></span><br><span class="line"><span class="xml">						autoclose: 1500</span></span><br><span class="line"><span class="xml">					&#125;);</span></span><br><span class="line"><span class="xml">				&#125;</span></span><br><span class="line"><span class="xml">			&#125;);</span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="xml">	</span><span class="template-tag">&#123;% <span class="name"><span class="name">endif</span></span> %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="name">endfor</span></span> %&#125;</span><span class="xml"></span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>is_home()是next主题的方法，用于判断当前页面是否在主页。因为主页一次性加载了所有的文章，如果不加这个方法，会在主页弹出无数个弹窗。效果如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/%E6%8F%90%E7%A4%BA%E5%BC%B9%E7%AA%97%E6%95%88%E6%9E%9C%E5%9B%BE.jpg" alt="提示弹窗效果图"><br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/运用其他的插件，在hexo中添加提示弹窗.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/Android开发，使用腾讯云的API请求对象存储中的资源始终失败.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/Android开发，使用腾讯云的API请求对象存储中的资源始终失败.html" class="post-title-link" itemprop="url">Android开发，使用腾讯云的API请求对象存储中的资源始终失败</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-17 10:55:07 / 修改时间：12:05:37" itemprop="dateCreated datePublished" datetime="2019-04-17T10:55:07+08:00">2019-04-17</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/android/" itemprop="url" rel="index"><span itemprop="name">android</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <ol>
<li>腾讯云api内部在调用时，把url转义了。我的链接是<a href="http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt?abcdefg，它内部给我转义成http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt%3Fabcdefg，就是把&quot;?&quot;转义成了&quot;%3F&quot;。总而言之，我使用api一直获取不到资源，然后我在浏览器上试验了一下。发现把%3F改回?就可以访问了，实际上应该不是这样，反正就给我产生了误导。我想尽办法都不能将其转义回来，最后只好放弃。腾讯云api内部肯定自己转义了一下，真的坑爹。" target="_blank" rel="noopener">http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt?abcdefg，它内部给我转义成http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt%3Fabcdefg，就是把&quot;?&quot;转义成了&quot;%3F&quot;。总而言之，我使用api一直获取不到资源，然后我在浏览器上试验了一下。发现把%3F改回?就可以访问了，实际上应该不是这样，反正就给我产生了误导。我想尽办法都不能将其转义回来，最后只好放弃。腾讯云api内部肯定自己转义了一下，真的坑爹。</a></li>
<li>尝试自己写代码请求资源，结果发现如果url的协议是https就可以访问到资源了。这可能是android的问题，于是我又使用腾讯的api，配置更改如下：<figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">CosXmlServiceConfig</span> serviceConfig = <span class="function"><span class="keyword">new</span> <span class="title">Builder</span>()</span></span><br><span class="line"><span class="function">				.<span class="title">isHttps</span>(true)</span></span><br><span class="line"><span class="function">                .<span class="title">setRegion</span>(region)</span></span><br><span class="line"><span class="function">                .<span class="title">setDebuggable</span>(true)</span></span><br><span class="line"><span class="function">                .<span class="title">builder</span>();</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>将isHttps设为ture，协议就改为了https。可是又报了另一个错：The specified key does not exist.它说我密钥不存在。</p>
<ol>
<li>如果使用http协议访问就会说无法解析域名，总之用腾讯云的api无法访问到资源就对了。</li>
<li>放弃腾讯云的api，自己手写代码去请求资源！</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/Android开发，使用腾讯云的API请求对象存储中的资源始终失败.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/对神经网络整体的理解.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/对神经网络整体的理解.html" class="post-title-link" itemprop="url">对神经网络整体的理解</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-12 20:12:28" itemprop="dateCreated datePublished" datetime="2019-04-12T20:12:28+08:00">2019-04-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-05 09:54:21" itemprop="dateModified" datetime="2019-06-05T09:54:21+08:00">2019-06-05</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>本文疑问的总结地址<a href="https://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">在这</a></p>
          </div>
<h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th>描述的内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2~4</td>
<td>神经网络和深度学习的发展史。</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>从二元分类开始。</td>
</tr>
<tr>
<td style="text-align:center">6~11</td>
<td>浅层神经网络的介绍。如神经网络中一些参数代表的意思、激活函数、梯度下降、随机初始化。</td>
</tr>
<tr>
<td style="text-align:center">12~16</td>
<td>深层神经网络的介绍。正向传播和反向传播中向量化后的计算、参数和超参数、神经网络和大脑的关系。</td>
</tr>
<tr>
<td style="text-align:center">17</td>
<td>一个Simple NN的例子。</td>
</tr>
<tr>
<td style="text-align:center">18~27</td>
<td>深度学习的实用性层面。数据切分、偏差与方差、正则化、dropout、其他正则化方法、均值归一化、梯度消失和梯度爆炸、梯度检验。</td>
</tr>
<tr>
<td style="text-align:center">28~35</td>
<td>一些优化算法。Mini-batch、指数加权平均、Momentum、RMSprop、Adam、Adagrad。</td>
</tr>
<tr>
<td style="text-align:center">36~39</td>
<td>超参数调试、Batch正则化、softmax回归以及一些深度学习框架。</td>
</tr>
<tr>
<td style="text-align:center">40~end</td>
<td>本文略长，后序的文章请看对应章节的链接。</td>
</tr>
</tbody>
</table>
</div>
<h1 id="神经网络和深度学习的发展"><a href="#神经网络和深度学习的发展" class="headerlink" title="神经网络和深度学习的发展"></a>神经网络和深度学习的发展</h1><p>TODO</p>
<h1 id="神经网络和深度学习的关系"><a href="#神经网络和深度学习的关系" class="headerlink" title="神经网络和深度学习的关系"></a>神经网络和深度学习的关系</h1><p>TODO</p>
<h1 id="为什么要深度学习"><a href="#为什么要深度学习" class="headerlink" title="为什么要深度学习"></a>为什么要深度学习</h1><p>TODO</p>
<h1 id="从二元分类开始"><a href="#从二元分类开始" class="headerlink" title="从二元分类开始"></a>从二元分类开始</h1><p>暂时省略，因为这里已经会了。</p>
<h1 id="神经网络的表示"><a href="#神经网络的表示" class="headerlink" title="神经网络的表示"></a>神经网络的表示</h1><p>规定如下，l：第几层；w：权重值；b：偏差；z：输出值；a：激活值；i，j：都代表第几个神经元，如<script type="math/tex">w^l_i</script>代表第l层的第i个权重值；W：向量化后的权重值；Z：向量化后的输出值；A：向量化后的激活值；<script type="math/tex">\alpha</script>：学习速率；<script type="math/tex">\lambda</script>：正则化项；</p>
<p>如果输出值z和激活值a无法理解或者区分，没关系，继续往下看就知道了。<br>如下图所示，一般规定input layer为第0层，不算入神经网络的层数中，所以下图是一个三层神经网络架构。</p>
<ol>
<li>input layer的输入值被称为x，下图一共有三个输入所以分别被称为<script type="math/tex">x_1\ x_2\ x_3</script>。为了方便起见，可以将input layer的值x以<script type="math/tex">a^0</script>来代替，下面解释a代表什么。</li>
<li>hidden layer中的值被称为a——<strong>激活值</strong>（activations），图中有四个神经元，所以分别被称为<script type="math/tex">a^1_1\ a^1_2\ a^1_3</script>，上标代表着所在神经网络中的第几层，下标代表着所在层中的第几个神经元。如果表示成向量形式就是<script type="math/tex; mode=display">
\begin{pmatrix}
 x_1\\
 x_2\\
 x_3\\
\end{pmatrix} = 
\begin{pmatrix}
 a^0_1\\
 a^0_2\\
 a^0_3\\
\end{pmatrix} 和
\begin{pmatrix}
 a^1_1\\
 a^1_2\\
 a^1_3\\
 a^1_4\\
\end{pmatrix} 和
\begin{pmatrix}
 a^2_1\\
 a^2_2\\
 a^2_3\\
 a^2_4\\
\end{pmatrix} 和
\begin{pmatrix}
 a^3_1\\
\end{pmatrix}</script></li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%8E%9F%E5%9B%BE.jpg" alt="神经网络架构原图"></p>
<h2 id="神经网络中神经元的一些参数的含义，特别解释w的含义"><a href="#神经网络中神经元的一些参数的含义，特别解释w的含义" class="headerlink" title="神经网络中神经元的一些参数的含义，特别解释w的含义"></a>神经网络中神经元的一些参数的含义，特别解释w的含义</h2><p>hidden layer和output layer的每个神经元都有几个参数。分别为<script type="math/tex">w^l\ b^l</script>，对照上图，这里的<script type="math/tex">w^l</script>是一个(4,3)的矩阵，<script type="math/tex">b^l</script>是一个(4,1)的向量。解释如下：</p>
<script type="math/tex; mode=display">
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4\\
\end{cases}</script><p>可以看到一个公式中有三个w和一个b，一共有四个公式。<script type="math/tex">w^l_{ij}</script>代表第l-1层的第j个神经元到第l层的第i个神经元上的w。如<script type="math/tex">w^1_{12}</script>代表第0层的第2个神经元到第1层的第1个神经元上的w。注意这里的i和j实际上是与直觉相反的，也就是说按直觉来看应该是<script type="math/tex">w^l_{ji}</script>才正常。如果对w的表示有疑惑的，可以看<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%ADweight%E7%9A%84%E8%A1%A8%E7%A4%BA%E9%97%AE%E9%A2%98.html">这篇</a>。<br>注意下这里的z是<strong>输出值</strong>，之前一直在说hidden layer中的值是a——激活值，其实a就是将z放到一个<strong>激活函数</strong>（activation function）中得到的一个值，这个激活函数是随用户挑选的，如果不能理解激活函数是什么，就暂时理解为激活函数自己想设成什么就设成什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg" alt="神经网络架构图"></p>
<h1 id="神经网络中的输出是怎么计算的"><a href="#神经网络中的输出是怎么计算的" class="headerlink" title="神经网络中的输出是怎么计算的"></a>神经网络中的输出是怎么计算的</h1><h2 id="以一个样本为例"><a href="#以一个样本为例" class="headerlink" title="以一个样本为例"></a>以一个样本为例</h2><p>第0层是输入层，所以是不需要计算的，x我本来就有，我还计算什么？对吧。从hidden layer1开始到output layer每一层都需要计算一连串的值，下面给出第一层的计算公式：</p>
<script type="math/tex; mode=display">
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1，a^1_1 = \sigma(z^1_1)\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2，a^1_2 = \sigma(z^1_2)\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3，a^1_3 = \sigma(z^1_3)\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4，a^1_3 = \sigma(z^1_4)\\
\end{cases}</script><p>这里的<script type="math/tex">\sigma(z)</script>函数其实就是上面说的<strong>激活函数</strong>，一般来讲<script type="math/tex">\sigma</script>这个符号特指sigmoid function: <script type="math/tex">\frac{1}{1+e^{-z}}</script>。<br>这4行公式其实在上面已经给出部分，每一行包含两个公式，也就是说一个神经元中实际上先得到了z，然后再通过激活函数将z转为a。这里可能会有疑惑，已经得到z了为什么还要用一个函数将z转为a呢？这样不是毫无意义？下面有一部分会具体解释，也可以看下面几篇的解释：<br><a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">神经网络激励函数的作用是什么？有没有形象的解释？</a><br><a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a><br>现在回到本文，正如我上面所说，我一共写了四个公式（激活函数现在暂时不看），所以我要分别计算四个公式，也就是说要计算四次。那么有没有办法只计算一次就得到所有结果呢？答案是<strong>向量化</strong>（vectorization），现在开始用向量化来解决这个问题。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    z^1_1\\
    z^1_2\\
    z^1_3\\
    z^1_4\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&w^1_{12}&w^1_{13}\\
    w^1_{21}&w^1_{22}&w^1_{23}\\
    w^1_{31}&w^1_{32}&w^1_{33}\\
    w^1_{41}&w^1_{42}&w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^0_1\\
    a^0_2\\
    a^0_3\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^1 = w^1 * a^0 + b^1</script><p>以下是整个神经网络的计算过程，也就是说只需要下面6行就可以代替上文占据几个屏幕的内容。</p>
<script type="math/tex; mode=display">
\begin{array}{c|}
    z^1 = w^1 * a^0 + b^1\\
    a^1 = \sigma(z^1)\\
    z^2 = w^2 * a^1 + b^2\\ 
    a^2 = \sigma(z^2)\\
    z^3 = w^3 * a^2 + b^3\\
    a^3 = \sigma(z^3)\\ 
\end{array} =>记为P</script><p>最后一个a就是整个神经网络的输出值，也就是预测值（prediction），也可以用<script type="math/tex">\hat{y}</script>表示，自然<script type="math/tex">\hat{y} = a^3</script>。</p>
<h2 id="向量化计算多个样本"><a href="#向量化计算多个样本" class="headerlink" title="向量化计算多个样本"></a>向量化计算多个样本</h2><p>上面我没有特意地说明其实我们只使用了一个样本，我们一直在使用<script type="math/tex">a^0_1\ a^0_2\ a^0_3</script>，但是<script type="math/tex">a^0_1\ a^0_2\ a^0_3</script>实际上只是<strong>一个</strong>样本。<script type="math/tex">a^0</script>代表的是一个样本，<script type="math/tex">a^0_1</script>代表的是样本中的第一个特征，如果不明白我可以举个例子：<script type="math/tex">a^0_1</script>代表天气样本中的第一个特征——温度，<script type="math/tex">a^0_2</script>代表湿度，<script type="math/tex">a^0_3</script>代表PM2.5，<script type="math/tex">a^0</script>代表整一个天气样本。<br>那么如果有成千上万个样本，总不能使用P计算成千上万次吧。这里再次使用向量化进行计算。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    z^{11}_1&z^{12}_1&\cdots\\
    z^{11}_2&z^{12}_2&\cdots\\
    z^{11}_3&z^{12}_3&\cdots\\
    z^{11}_4&z^{12}_4&\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&w^1_{12}&w^1_{13}\\
    w^1_{21}&w^1_{22}&w^1_{23}\\
    w^1_{31}&w^1_{32}&w^1_{33}\\
    w^1_{41}&w^1_{42}&w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{01}_1&a^{01}_1&\cdots\\
    a^{01}_2&a^{02}_1&\cdots\\
    a^{01}_3&a^{03}_1&\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^1 = w^1 * a^0 + b^1</script><script type="math/tex; mode=display">
\begin{pmatrix}
    z^{21}_1&z^{22}_1&\cdots\\
    z^{21}_2&z^{22}_2&\cdots\\
    z^{21}_3&z^{22}_3&\cdots\\
    z^{21}_4&z^{22}_4&\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^2_{11}&w^2_{12}&w^2_{13}&w^2_{14}\\
    w^2_{21}&w^2_{22}&w^2_{23}&w^2_{24}\\
    w^2_{31}&w^2_{32}&w^2_{33}&w^2_{34}\\
    w^2_{41}&w^2_{42}&w^2_{43}&w^2_{44}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{11}_1&a^{11}_1&\cdots\\
    a^{11}_2&a^{12}_1&\cdots\\
    a^{11}_3&a^{13}_1&\cdots\\
    a^{11}_3&a^{14}_1&\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^2_1\\
    b^2_2\\
    b^2_3\\
    b^2_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^2 = w^2 * a^1 + b^2</script><p>省略号代表后面有无数个样本，同理矩阵相乘也可以只用一个字母表示。上标的第二个数字代表是第几个样本，第一个数字依旧是代表所属第几层。</p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>上文中我们一直假设使用sigmoid function作为激活函数。但是事实上还有很多其他选择，甚至其他的激活函数比sigmoid funtion效果要更好。<br>上面讲过<script type="math/tex">\sigma(z)</script>特指sigmoid function，现在我们将表达式改为：<script type="math/tex">a = g(z)</script>，用g来表示激活函数，它可以是线性的，也可以是非线性的。<br>引用吴恩达在深度学习视频中的话：</p>
<blockquote>
<p>有一个函数总是比sigmoid function表现得更好，就是tanh函数或者叫双曲正切函数，公式为：<script type="math/tex">\frac{e^z-e^{-z}}{e^z+e^{-z}}</script>，在数学上实际是<script type="math/tex">\sigma</script>函数平移后的版本。<br>事实证明，如果将<script type="math/tex">g(z)</script>选为tanh函数，效果几乎总比<script type="math/tex">\sigma(z)</script>函数要好。</p>
</blockquote>
<p>有一个例外是output layer，它还是使用sigmoid funtion，因为output layer跟普通的分类问题没什么区别，它要得到0~1之间的一个概率。<br>sigmoid function的值总是位于0~1之间，tanh function的值总是位于-1~1之间。<br>但是不管是<script type="math/tex">\sigma</script>或者tanh函数都一个缺点，那就是当z非常大或者非常小时，函数的斜率（导数的梯度）很小。这样会拖慢梯度下降。在机器学习中还有一个函数，即ReLU函数——Rectified Linear Unit，表达式为<script type="math/tex">max(0, z)</script>。<br>所以在选择激活函数时有一些经验法则：</p>
<ol>
<li>如果你的输出值是0或1，那么<script type="math/tex">\sigma</script>函数很适合做output layer的激活函数，非二元分类的情况下使用tanh函数几乎都比<script type="math/tex">\sigma</script>优越。藏层单元全用ReLU函数，现在ReLU函数已经是隐藏层的默认激活函数了，大多数人都这么做。</li>
<li>还有个叫Leaky ReLU的函数比ReLU稍微好点，但是目前暂时不是很多人用。</li>
</ol>
<h1 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h1><div class="note primary">
            <p>为什么<script type="math/tex">w*x+b=z</script>之后，需要使用一个激活函数<script type="math/tex">a = \sigma(z)</script>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
          </div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。<br>那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。</p>
<script type="math/tex; mode=display">
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w' * a^0 + b' \qquad由于w和b只是一堆常数，所以将w'代替w^2 * w^1，b'同理
    \end{array}
\right.</script><p>通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<script type="math/tex">a^2 = w' * a^0 + b'</script>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。<br>只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降，反向传播算法——backpropagation解析"><a href="#梯度下降，反向传播算法——backpropagation解析" class="headerlink" title="梯度下降，反向传播算法——backpropagation解析"></a>梯度下降，反向传播算法——backpropagation解析</h1><div class="note primary">
            <p>首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？</p>
          </div>
<p>本节的示例均建立在一个样本的情况下，如果是多个样本经过神经网络，可能略微不同。我看了吴恩达老师的深度学习课程，发现多个样本与一个样本的区别，可能只在偏差b那里会有点不同。<br>下图以一个三层神经网络为例，说明正向与反向传播过程。由于神经元之间的链接太多会导致混乱，所以下图只链接了第一个神经元。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="使用导数解释反向传播算法"><br>下图略微简化一下反向传播算法中的导数项，并且完成了最后的权重值优化。值得注意的是：如果cost function不同，下面求导结果会略微不同，本文统一使用<script type="math/tex">cost = \frac{1}{m} * \sum{(\hat{y} - y)^2}</script>，但是神经网络一般是使用<strong>交叉熵</strong>——crossentropy，其公式为：<script type="math/tex">cost = -\frac{1}{m} * (y * log(\hat{y}) + (1 - y) * log(1 - \hat{y}))</script>。使用前者，<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}} = (a^{(3)} - y) * g'(z^{(3)})</script>；如果使用后者，<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}} = a^{(3)} - y</script>。可以看到使用两个不同的代价函数，会有不同的结果，这是因为两个函数求导的结果不一样。而两者对表达式<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}}</script>的结果只差了一个<script type="math/tex">g'(z^{(3)})</script>，这完全是巧合罢了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%AE%80%E5%8C%96%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="简化反向传播算法"><br><strong>另外再提醒一下自己，这里全是以一个样本为例。但是仅仅这样权重已经是一个二维矩阵了，要是如果传入多个样本，权重岂不是是一个三维矩阵？然而不管传入几个样本权重实际上对于不同的样本是没有变化的，所以还是二维矩阵。</strong></p>
<h1 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h1><p>对于逻辑回归可以将<strong>权重</strong>（weight）全部初始化为0，但是对于神经网络来说，将个权重初始化为0，再使用梯度下降会完全无效。实际上将偏差b初始化为0是可以的，但是权重不行。<br>解释起来太麻烦，详情看吴恩达深度学习——01神经网络和深度学习第三周浅层神经网络，3.11随机初始化。吴恩达老师解释地还是很清楚的。<br>可以像以下这样设置weight：<script type="math/tex">w^l = np.random.randn((2, 2)) * 0.01</script>//这可以产生参数为(2, 2)的高斯分布随机变量，后面再成一个很小的数，比如0.01。而对于b，之前说了初始化为0也可以。<br>对于上式的0.01可能会感到很疑惑，为什么要乘这么一个值。因为我们一般将weight初始化为很小的值，如果weight值很大，最终导致z也很大，那么会落在sigmoid function或者tanh function的平缓部分，会使梯度的写了很小，意味着梯度下降算法会非常慢，所以学习得很慢。</p>
<h2 id="初始化补充"><a href="#初始化补充" class="headerlink" title="初始化补充"></a>初始化补充</h2><p>经在作业中做的测试得出如下结论：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Train accuracy</strong></th>
<th><strong>Problem/Comment</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>3-layer NN with <strong>zeros initialization</strong></td>
<td>50%</td>
<td>fails to break symmetry</td>
</tr>
<tr>
<td>3-layer NN with large <strong>random initialization</strong></td>
<td>83%</td>
<td>too large weights</td>
</tr>
<tr>
<td>3-layer NN with <strong>He initialization</strong></td>
<td>99%</td>
<td><strong>recommended method</strong></td>
</tr>
</tbody>
</table>
</div>
<p>其中”He initialization”最近（论文是2015年的）新搞出来得初始化算法，现在推荐使用此算法进行初始化。</p>
<h1 id="核对矩阵维数"><a href="#核对矩阵维数" class="headerlink" title="核对矩阵维数"></a>核对矩阵维数</h1><p>w的维数应该与dw的维数相同。b和db的维数相同</p>
<h1 id="为什么使用深度表示——Why-deep-representations"><a href="#为什么使用深度表示——Why-deep-representations" class="headerlink" title="为什么使用深度表示——Why deep representations"></a>为什么使用深度表示——Why deep representations</h1><p>引用在2017course深度学习课程上吴恩达老师的话</p>
<blockquote>
<p>深度神经网络能解决很多问题，其实并不需要很大的神经网络，但是得有深度。得有比较多的隐藏层。</p>
</blockquote>
<p>为什么深度神经网络会很好用？</p>
<ol>
<li>深度神经网络到底在计算什么？假设现在在做一个人脸识别系统。那么神经网络的第一层会去找照片里的边缘部分；第二层会去识别人类的特征，比如耳朵，鼻子，嘴巴；第三层会去识别不同的人脸。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%9B%B4%E8%A7%82%E6%98%A0%E5%83%8F.jpg" alt="深度表示的直观映像"><br>这种识别模式可能难以理解，但是会在卷积神经网络——Convolutional Neural Network中详细解释。<br>这视频的这一章节有点难以总结，可以看看<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>，总共也就10分钟。</li>
</ol>
<h1 id="深层神经网络块"><a href="#深层神经网络块" class="headerlink" title="深层神经网络块"></a>深层神经网络块</h1><p>此视频中画出了深度神经网络的<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701023" target="_blank" rel="noopener">代码流程</a>。</p>
<h1 id="参数VS超参数"><a href="#参数VS超参数" class="headerlink" title="参数VS超参数"></a>参数VS超参数</h1><p>有如下超参数（hyperparameters）：W, b, lerning rate <script type="math/tex">\alpha</script>, iterations, hidden layer L, hidden units, choice of activatation function.这些超参数都需要自己设置。<br>上面这些都是基础的，实际上还有其他的超参数，稍后会涉及到。 </p>
<h1 id="神经网络和大脑有什么关系？"><a href="#神经网络和大脑有什么关系？" class="headerlink" title="神经网络和大脑有什么关系？"></a>神经网络和大脑有什么关系？</h1><p>计算机视觉、其他深度学习领域或者其他学科在早期可能都受过人类大脑的启发，但是近年来人类将神经网络类比为大脑的次数越来越少，也就是说近年来大家都不怎么认为这二者有关联。</p>
<h1 id="一个Simple-NN的例子"><a href="#一个Simple-NN的例子" class="headerlink" title="一个Simple NN的例子"></a>一个Simple NN的例子</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">Simple Neural Network</a>例子</p>
<hr>
<p>本节以下开始利用算法改善深层神经网络</p>
<hr>
<h1 id="训练-开发-测试集"><a href="#训练-开发-测试集" class="headerlink" title="训练/开发/测试集"></a>训练/开发/测试集</h1><p>训练集——training set<br>开发集/交叉验证集/验证集——dev set/cross validation set/validation set<br>测试集——test set</p>
<p>以前数据量小的时候，比如100个样本、10000个样本。一般将数据按三七分，七份训练集，三份测试集。验证集（以下均称验证集）在训练集中再细分，比如二八分，八份训练集。<br>但是现在进入大数据时代，验证集和测试集已经没有必要占大量比例了。比如现在有100万的样本，那么验证集和测试集只需要各抽取大约10000的样本即可。也就是98/1/1的比例，甚至验证集和测试集可以再降低占比。</p>
<h2 id="训练集和验证集-测试集分布不匹配"><a href="#训练集和验证集-测试集分布不匹配" class="headerlink" title="训练集和验证集/测试集分布不匹配"></a>训练集和验证集/测试集分布不匹配</h2><p>如下图，吴恩达老师建议最好让<strong>验证集</strong>和<strong>测试集</strong>匹配，即来自同一源，要都来自网络高清图，要么都来自手机低像素拍摄。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E5%BC%80%E5%8F%91%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%86%E5%B8%83%E4%B8%8D%E5%8C%B9%E9%85%8D.jpg" alt="训练集和验证集测试集分布不匹配"><br>如果直接不设置测试集也是可以的。</p>
<h1 id="偏差-方差"><a href="#偏差-方差" class="headerlink" title="偏差/方差"></a>偏差/方差</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%86%B3%E7%AD%96%E7%95%8C%E9%99%90.jpg" alt="欠拟合和过拟合的决策界限"></p>
<p>下图讲述了什么是<strong>过拟合</strong>，什么是<strong>欠拟合</strong>，如图所示，该神经网络用于判断一张图片是猫还是狗。<br>左边。训练样本中的误差为1%，这个值已经很小了，但是在验证集上的误差有11%。这就代表了过拟合，试想一下，在训练集上误差很小是因为你的决策界限划分的很好，在上图中的最后一个例子，整条决策界限画的十分完美，但是我们要知道在验证集中，这样一条完美的线肯定不能再拟合的很好。因为训练集和验证集即使来源于同一份数据，他们之间的分布也是不一样的，你训练出一条完美的曲线，在另一份数据集上肯定是过于完美了。所以导致了下图中验证集上的误差有11%。我们称这种情况为<strong>高方差</strong>——high variance。<br>中间。训练样本中的误差为15%，这已经不需要再看验证集上的误差了。因为训练集上的误差那么大，肯定是没有拟合好，所以这就是欠拟合，我们称为<strong>高偏差</strong>——high bais。<br>右边。如果训练集中的误差很高，验证集上的误差更高，那么可以判断为同时具有高方差和高偏差。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88.jpg" alt="欠拟合和过拟合"><br>如果训练集上的误差为0.5%，验证集上的误差为1%。那这就是低方差和低偏差，这是很好结果。<br>最后一点，以上均建立在人眼判断的误差为0%上以及训练集和验证集来自相同分布。如果人眼判断的误差也高达15%，那么中间的例子也算是可以的结果一般来说<strong>最优误差</strong>也被称为<strong>贝叶斯误差</strong>。<br>关于上图同时高方差和高方差，就如同下图紫色线条的决策界限一般。过渡拟合了数据，但是拟合的数据其实狗屁不通。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E5%90%8C%E6%97%B6%E9%AB%98%E6%96%B9%E5%B7%AE%E5%92%8C%E9%AB%98%E5%81%8F%E5%B7%AE%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84.jpg" alt="同时高方差和高偏差是怎么样的"></p>
<h1 id="机器学习遇到偏差或方差的解决办法"><a href="#机器学习遇到偏差或方差的解决办法" class="headerlink" title="机器学习遇到偏差或方差的解决办法"></a>机器学习遇到偏差或方差的解决办法</h1><div class="note info">
    <p>笔记中都记了，懒得再写一遍了。补充一点，遇到偏差或方差都可以更换神经网络架构，比如换成CNN或者RNN，如果是高偏差还可以使用更大的神经网络。</p>
</div>

<p>可以看这个6分半中的小视频，<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702115" target="_blank" rel="noopener">机器学习基础</a>。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>如果出现了过拟合，即高方差的情况，第一件想到的事是<strong>正则化</strong>——regularization。当然也可以增加数据，不过有时候数据不是那么容易获取的。可以对W使用<a href="https://www.baidu.com/s?wd=L2%E8%8C%83%E6%95%B0" target="_blank" rel="noopener">L2范数</a>进行正则化，当然对b也可以进行L2范数正则化，不过一般不加。L2范数的公式为<script type="math/tex">||w||^2_2 = \sum_{j=1}^n w^2_j = W^T * W</script><br>因此代价函数修改为<script type="math/tex">cost = \frac{1}{m} \sum^m_{i=1} g(\hat{y}^i, y^i) + \frac{\lambda}{2m}||w||^2_2</script>，<script type="math/tex">\lambda</script>是正则化的超参数。这里的w实际上是一个二维矩阵，所以L2范数需要把里面的每一个值的平方都加起来。<br>如果加入了正则化项，那么在计算dW时有点变化。将会变为：<script type="math/tex">dW = dZ * A\_prev + \frac{\lambda}{m} w ^ l</script></p>
<h2 id="为什么正则化可以防止过拟合"><a href="#为什么正则化可以防止过拟合" class="headerlink" title="为什么正则化可以防止过拟合"></a>为什么正则化可以防止过拟合</h2><p>略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702116" target="_blank" rel="noopener">1.5 为什么正则化可以减少过拟合？</a></p>
<h1 id="另一个正则化方法——dropout"><a href="#另一个正则化方法——dropout" class="headerlink" title="另一个正则化方法——dropout"></a>另一个正则化方法——dropout</h1><div class="note primary">
            <p>那么问题又来了，dropout背后的原理是什么？</p>
          </div>
<p>dropout，中文翻译为<strong>随机失活</strong>。<br>先将神经网络复制一遍，然后dropout会遍历神经网络的每一层，并设置消除神经网络中结点的概率，比如设置0.5。下图的带X的结点就是准备消除的。另外每一层的概率都可以是不同的，如果在某一层不担心会过拟合可以将概率设为1.0，比如输出层。如果觉得某些层比其他层更容易过拟合，可以把那些层的keep-prob设置的更低。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/dropout%E5%BE%85%E5%88%A0%E9%99%A4%E7%BB%93%E7%82%B9.jpg" alt="dropout待删除结点"><br>下图则是消除后的神经网络。将结点的进出的链接全部删除。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8dropout%E5%90%8E%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg" alt="使用dropout后的神经网络"><br>dropout使用之后，就让一个样本进入神经网络进行训练。而对于其他样本也如法炮制，需要再进行复制一遍神经网络，并进行dropout。<br>以上均是逻辑上的做法，接下来讲实际编码该怎么做。</p>
<ol>
<li>设置一个结点保留的概率——keep-prob，假设为0.8。<script type="math/tex">d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) < keep-prob</script>，这样会得到一个True和False的数组，但是python中Ture等于1，False等于0。</li>
<li>让<script type="math/tex">a^3</script>乘上这个向量。<script type="math/tex">a^3 = np.multiply(a^3, d^3)</script>。由于False等于0，所以变相地将<script type="math/tex">a^3</script>中的值失活了。</li>
<li>最后一步看起来有点奇怪，<script type="math/tex">a^3 /= keep-prob</script>。<br>完整代码如下：<script type="math/tex; mode=display">
\begin{cases}
 d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) < keep-prob\\
 a^3 = np.multiply(a^3, d^3)\\
 a^3 /= keep-prob\\
\end{cases}</script></li>
</ol>
<p>对于最后一步，由于<script type="math/tex">Z^4 = W^4 * A^3 + b^4</script>，由于<script type="math/tex">A^3</script>被dropout减少0.2，为了使得<script type="math/tex">Z^4</script>不受影响，所以对<script type="math/tex">A^3</script>除0.8，来保证<script type="math/tex">A^3</script>的值不变。由于早期的版本没有除于keep-prob，使得测试阶段，平均值越来越复杂。<br>最后，从技术上来讲，输入值也可以使用dropout，但是基本不这么做，直接把keep-prob设为1.0即可，当然0.9也可以。不过太低的值一般不会去设置。<br>以上的步骤被称为<strong>Inverted dropout</strong>——<strong>反向随机失活</strong>。<br>dropout在计算机视觉中用的非常多，甚至成了标配。但要记住一点，dropout是一种正则化方法，为了预防过拟合。所以除非算法过拟合，不然不会使用dropout。由于计算机视觉的特殊性，他们才经常用dropout。<br>dropout的缺点是使我们失去了代价函数这一调试功能。我们经常使用代价函数得到误差，从而画出曲线图。但是使用dropout之后，这样的曲线图就不再准确了。</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>在测试阶段不再使用dropout，因为我们不希望输出结果是随机的，如果使用dropout预测会受到干扰。</p>
<h1 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h1><div class="note primary">
    <p>略。有点晦涩。</p>
</div>

<p>看<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a>。。</p>
<h1 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h1><ol>
<li>Data augment——数据增强。如果拟合猫咪图片分类器，可以对原图片做一些处理，来增加数据，比如翻转、旋转、随机裁剪等。</li>
<li>Early stopping。在训练时画出代价的曲线图，x轴为迭代次数，再绘制验证时的误差。然后选择验证误差曲线图中最低点的迭代次数，下次训练时就改用这个迭代次数，或者也可以在程序中写一个条件判断。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/early%20stopping.jpg" alt="early stopping"></li>
</ol>
<h1 id="均值归一化输入"><a href="#均值归一化输入" class="headerlink" title="均值归一化输入"></a>均值归一化输入</h1><div class="note info">
    <p>略。其实很简单。</p>
</div>

<h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702118&amp;cid=2001699114" target="_blank" rel="noopener">视频</a><br><a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另外一个参考视频</a>，08:37开始。<br><a href="https://www.bilibili.com/video/av10590361/?p=37" target="_blank" rel="noopener">另一个</a>13:50~18左右</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701047" target="_blank" rel="noopener">视频</a></p>
<h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>Gradient checking(Grad check).<br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701048" target="_blank" rel="noopener">原理视频</a><br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702119" target="_blank" rel="noopener">实战视频</a><br>梯度检验可以帮助我们发现神经网络中的一些bug。具体原理是，通过数学上导数的定义来确认反向传播算法是否正确。如果学过高数就会知道，使用导数的定义求解和直接使用公式求解，两者结果十分接近或者一模一样。如果二者不一样说明肯定是求错了。<br>对应于神经网络，那就肯定是代码写错了。具体操作可在视频中看见，每个视频都不超过10分钟。</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果梯度检验确实发现问题，要检查每一项，看看是哪个i的w和b有问题。</li>
<li>记得正则化项，它也被包含在w的梯度中。</li>
<li>梯度检验不能和dropout一起用。</li>
<li><del>在随机初始化时就运行一遍梯度检验；或许在训练一会后可以再运行一遍梯度检验。当W和b接近于0时，梯度下降正确执行在现实中几乎不太可能。</del>吴恩达老师说这条他在现实中几乎不会这么做，并且第五条的翻译，个人感觉翻得有问题，然后看了英文原文后，感觉原文表达得也不是很好，我看不太懂，所以这条就不算进注意事项了。</li>
</ol>
<h1 id="Mini-batch梯度下降"><a href="#Mini-batch梯度下降" class="headerlink" title="Mini-batch梯度下降"></a>Mini-batch梯度下降</h1><div class="note primary">
            <p>为什么Mini-batch比普通的梯度下降快？</p>
          </div>
<p>普通的梯度下降——vanilla gradient descent，是将整个数据集同时做运算，而Mini-batch梯度下降算法是以一组为单位，分别进行梯度下降，所有组执行完毕后再进行下一次迭代。<br>假设现在有m个样本。</p>
<script type="math/tex; mode=display">
X = \begin{pmatrix}x^1&x^2&x^3&\cdots&x^m\end{pmatrix}\\
Y = \begin{pmatrix}y^1&y^2&x^3&\cdots&y^m\end{pmatrix}\\</script><p>使用Mini-batch，假设每1000个样本为一组：</p>
<script type="math/tex; mode=display">
X = \begin{pmatrix}\underbrace{x^1\cdots x^{1000}}_{X^{\{1\}}} & \underbrace{x^{1001}\cdots x^{2000}}_{X^{\{2\}}} & \cdots&\underbrace{\cdots x^m}_{X^{\{t\}}}\end{pmatrix}\\
Y = \begin{pmatrix}\underbrace{y^1\cdots y^{1000}}_{Y^{\{1\}}} & \underbrace{y^{1001}\cdots y^{2000}}_{Y^{\{2\}}} & \cdots&\underbrace{\cdots y^m}_{Y^{\{t\}}}\end{pmatrix}\\</script><p>如果使用代码实现就是类似下面这样的伪代码：<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>, ..., t</span><br><span class="line">	forwardprop <span class="keyword">on</span> X^&#123;t&#125;</span><br><span class="line">	compute cost</span><br><span class="line">	backprop <span class="keyword">to</span> compute grads</span><br><span class="line">	update weights <span class="keyword">and</span> bais</span><br></pre></td></tr></table></figure></p>
<p>for循环完成之后就完成了神经网络的第一次迭代。</p>
<h2 id="理解mini-batch"><a href="#理解mini-batch" class="headerlink" title="理解mini-batch"></a>理解mini-batch</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/Mini-batch%E5%92%8C%E6%99%AE%E9%80%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="Mini-batch和普通梯度下降的区别"></p>
<ol>
<li>如果将batch设为m，那它就是普通的梯度下降算法。</li>
<li>如果将batch设为1，就叫做随机梯度下降——SGD</li>
<li>batch在1到m之间就是mini-batch</li>
</ol>
<p>SGD和普通梯度下降的区别，“+”代表代价最小点。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/SGD%E5%92%8Cvanilla%20gradient%20descent%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="SGD和vanilla gradient descent的区别"><br>SGD和mini-batch的区别。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/SGD%E5%92%8Cmini-batch%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="SGD和mini-batch的区别"></p>
<p><strong>应该记住的有：</strong></p>
<ol>
<li>普通梯度下降、mini-batch和SGD之间的区别就是执行一次参数更新所需的样本数量不同。</li>
<li>你需要自己调整学习速率<script type="math/tex">\alpha</script>。</li>
<li>当mini-batch的量调整良好时，它通常优于普通梯度下降和SGD（尤其是在训练集特别大时）。</li>
</ol>
<h2 id="mini-bacth实现步骤"><a href="#mini-bacth实现步骤" class="headerlink" title="mini-bacth实现步骤"></a>mini-bacth实现步骤</h2><ol>
<li>打乱数据。创建一个打乱数据之后的副本，其中X和Y的每一列都代表一个训练样本。注意X和Y是同步地随机打乱样本，即X中第<script type="math/tex">i^{th}</script>个样本和Y中第<script type="math/tex">i^{th}</script>标签在打乱之后还是是对应的。此步骤确保样本被随机地分割到不同的mini-batches中。下图是步骤示意图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E7%AC%AC%E4%B8%80%E6%AD%A5%E6%89%93%E4%B9%B1%E6%95%B0%E6%8D%AE.jpg" alt="mini-batch第一步打乱数据"></li>
<li>切分。将打乱数据后的XY切分进<code>mini_batch_size</code>大小（下图是64）的mini-batches中。不过注意训练样本的数量并不总能被<code>mini_batch_size</code>整除。最后的mini-batch可能要小点，但是不需要担心这点。使用<code>math.floor()</code>向上取整即可。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E7%AC%AC%E4%BA%8C%E6%AD%A5%E5%88%87%E5%88%86.png" alt="mini-batch第二步切分"></li>
</ol>
<h2 id="一些经验"><a href="#一些经验" class="headerlink" title="一些经验"></a>一些经验</h2><p>如果小数据量（大约小于2000）的话，<strong>只</strong>执行步骤1即可；如果样本数目较大，执行步骤1和步骤2，一般将batch设置在64~512之间，考虑到电脑的内存设置和使用方式，batch的大小设置为2的次方，代码的运行速度会比较快；</p>
<h1 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h1><p>为了更好地理解其他优化算法，需要使用到指数加权平均。这章介绍一下它。<br>Exponentially weighted averages，在统计学中被称为指数加权移动平均——Exponentially weighted moving averages。<br>指数加权平均有一个公式：<script type="math/tex">V_t = \beta * V_{t-1} + (1- \beta) * \theta_t</script>，<script type="math/tex">V_0 = 0</script>，其目的是使用<script type="math/tex">V_t</script>代替<script type="math/tex">\theta_t</script>。<script type="math/tex">V_t</script>可视为<strong>约等于</strong><script type="math/tex">\frac{1}{1 - \beta}</script>个样本的平均值。这里可能会有疑问，为什么<script type="math/tex">V_t</script>可视为约等于<script type="math/tex">\frac{1}{1 - \beta}</script>个样本的平均值？其实我也不知道，也不想知道，我又不是学统计学或者数学的。<br>下图中的数据为伦敦一年之间的温度，来源于吴恩达的深度学习视频，可以看到其中的数据十分杂乱，也就是常在网络上看到别人所说的“噪点”多。我们可以使用<strong>指数加权平均</strong>来画出一条线，就是下图的红线，来代表温度变化的趋势，这样会使得更容易让人类理解和观察。<br>下图中的<script type="math/tex">\beta</script>为0.9。而<script type="math/tex">\frac{1}{1 - 0.9} = 10</script>，所以<script type="math/tex">V_t</script>代表过去<em>十天</em>内的平均温度。如果<script type="math/tex">\beta</script>为0.98，那么<script type="math/tex">V_t</script>代表过去<em>五十</em>天内的平均温度<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E4%BE%8B%E5%AD%90.jpg" alt="指数加权平均例子"><br>下图是不同<script type="math/tex">\beta</script>值的对比。注意到一点，绿色（<script type="math/tex">\beta</script>=0.98）的线比红色的线要平坦一点，这是因为你多平均了几天的温度，所以这根线波动更新、更平坦。但是缺点是曲线进一步向右移，拟合的不是很好。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%B8%8D%E5%90%8Cbeta%E5%80%BC%E7%9A%84%E5%AF%B9%E6%AF%94.jpg" alt="不同beta值的对比"><br>现在看到了平均了10天和50天温度的曲线，现在试试<script type="math/tex">\beta=0.5</script>，也就是只平均两天的温度。由于只平均了两天的温度，数据太少，所以曲线有更多的噪声，更有可能出现异常值。但是这个曲线能更快适应温度变化。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/beta%E5%80%BC%E7%AD%89%E4%BA%8E0.5%E7%9A%84%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87.jpg" alt="beta值等于0.5的指数加权平均"></p>
<h2 id="理解其作用"><a href="#理解其作用" class="headerlink" title="理解其作用"></a>理解其作用</h2><div class="note primary">
            <p>略。至今看不懂。</p>
          </div>
<h2 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h2><p>之前的曲线其实都是理想状态下的，回想绿色的曲线是50天内的温度平均值。但是其实绿色曲线会是紫色曲线那样的轨迹。初始化<script type="math/tex">V_0=0</script>，原数据中<script type="math/tex">\theta_0 = 40</script>，所以其实<script type="math/tex">V_1 = 0.02 * 40 = 8</script>，从而绿色曲线的起点实际上很低。因为起点并没有计算50天内的温度平均，我们默认将<script type="math/tex">V_0</script>初始化为0。<br>我们可以用下图右边的公式将其修正。算出<script type="math/tex">V_t</script>后再做如下计算：<script type="math/tex">\frac{V_t}{1 - \beta^t}</script>，其中<script type="math/tex">\beta</script>的上标t是指<strong>t次方</strong>。<br>另外由于t越大，<script type="math/tex">\beta^t</script>的值越接近0，所以对后面的值几乎没影响。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%81%8F%E5%B7%AE%E4%BF%AE%E6%AD%A3.jpg" alt="指数加权平均偏差修正"></p>
<h1 id="动量梯度下降——Momentum"><a href="#动量梯度下降——Momentum" class="headerlink" title="动量梯度下降——Momentum"></a>动量梯度下降——Momentum</h1><p><div class="note primary">
    <p>算法的意图是明白了，可是算法的原理还是没搞明白。</p>
</div><br>普通的梯度下降，不管是mini-batch、SGD还是其他的什么，都是通过<script type="math/tex">W -= \alpha * dW</script>来更新权重。<br>但是在动量梯度下降中，使用到了<strong>指数加权平均</strong>。尤其是针对mini-batch算法，因为mini-batch算法抖动过大，上面的章节介绍了mini-batch的梯度下降误差曲线，指数加权平均正好可以解决。<br>可以观察下图发现，梯度下降的波动比较大，也就是噪点较多，我们可以使用指数加权平均来减少噪点。下面的公式就是用其减少了梯度dW和db。下式中还对指数加权平均进行了优化，使用了<strong>偏差修正</strong>。</p>
<script type="math/tex; mode=display">
    \begin{cases}
        compute\ dW, db\\
        V_{dW} = \beta * V_{dW} + (1 - \beta) * dW,\quad V_{db} = \beta * V_{db} + (1 - \beta) * db\\
        V^{corrected}_{dW} = \frac{V_{dW}}{1 - \beta^t},\quad V^{corrected}_{db} = \frac{V_{db}}{1 - \beta^t} \\
        W -= \alpha * V^{corrected}_{dW}\\
        b -= \alpha * V^{corrected}_{db}\\
    \end{cases}</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="梯度下降示意图"><br>在梯度下降时的这种波动减慢了下降的速度，无法使用更大的学习速率。因为梯度已经很大了，如果使用更大的学习速率，可能梯度直接爆炸了，直接无法收敛。为了避免摆动过大需要使用较小的学习速率。</p>
<p><div class="note primary">
    <p>
        <blockquote class="blockquote-center"><p>在梯度下降时的这种波动减慢了下降的速度</p>
</blockquote>
        为什么？暂时将结果记住。
    </p>
</div><br>还可以从另一种角度看待。我们希望在纵轴上学习的慢点，我们希望摆动小点，不就是希望纵轴小点吗。而在横轴上我们又希望学习的快点，因为我们希望越快接近中心越好。<br>这个<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">视频</a>讲的直观一点，可以参考一下，从36::00开始看，虽然讲的是RMSprop但是讲的原理跟Momentum的原理一样。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>在课后练习中有更详细的说明，在此补充一下。</p>
<blockquote>
<p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.</p>
</blockquote>
<p>大致意思就是使用Momentum可以使得mini-batch的振荡更小，观察下图。。。说实话我并没有观察出什么，不知道Coursera是怎么想的。我把此图的提示贴出来：</p>
<blockquote>
<p>Figure 3: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence  v  and then take a step in the direction of  v .</p>
</blockquote>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E4%BD%BF%E7%94%A8Momentum%E5%90%8E.png" alt="mini-batch使用Momentum后"></p>
<blockquote>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable  v . Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of  v  as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.</p>
</blockquote>
<p>Momentum考虑到了之前的梯度，从而用其来缓和参数更新。我们将前一次梯度的“方向”存进变量v。后续不翻译了。</p>
<h1 id="均方根传播——RMSprop"><a href="#均方根传播——RMSprop" class="headerlink" title="均方根传播——RMSprop"></a>均方根传播——RMSprop</h1><p>Root mean square prop.<br>一个类似Momentum的算法，没必要死记公式，略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d#/learn/content?type=detail&amp;id=2001702124" target="_blank" rel="noopener">视频地址</a>。<br>或者<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另一个参考视频</a>，36:00开始。<br><strong>RMSprop 没有使用偏差修正。</strong>但是在 Adam 中的 RMSprop 使用了偏差修正。</p>
<script type="math/tex; mode=display">
    \begin{cases}
        compute\ dW, db\\
        S_{dW} = \beta_2 * S_{dW} + (1 - \beta_2) * (dW)^2,\quad S_{db} = \beta_2 * S_{db} + (1 - \beta_2) * (db)^2\\
        W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}\\
        b -= \alpha * \frac{db}{\sqrt{S_{db}} + \epsilon}\\
    \end{cases}</script><p>在更新 W 和 b 时的算法与之前的 Momentum 算法略微不同。另外为了防止 dW 和 db 等于 0，导致分母为 0，所以在分母加了一个极小值<script type="math/tex">\epsilon</script>，在 Keras 中取了 1e-7， 吴恩达老师说 1e-8 是个不错的选择。<br><strong>RMSprop 算法也是使用了指数加权平均算法。</strong>并且还结合了 Adagrad。<br><div class="note info">
            <p>对于理解 RMSprop。<strong>可以观察出 RMSprop 和 Momentum 长得有点像，但是这两个算法的具体关系暂时不清楚</strong>。并且 RMSprop 其实还有简化版的算法，叫做 Adagrad。之前对这些优化算法（Momentum, RMSprop, Adam 等）的理解都是<em>改变 W 和 b 的大小从而使得梯度下降更快</em>。但是又今天看了一遍<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">李宏毅老师的视频</a>，发现还有其他的理解。其实这些算法都在<strong>改变学习速率的大小</strong>。<br>比如 RMSprop 算法，观察<script type="math/tex">W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}</script>，我们可以改写成<script type="math/tex">W = W - \frac{\alpha}{\sqrt{S_{dW}} + \epsilon} * dW</script>。看dW之前的那项<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>实际上就是对学习速率<script type="math/tex">\alpha</script>乘上了<script type="math/tex">\frac{1}{\sqrt{S_{dW}} + \epsilon}</script>。<br>所以对RMSprop的理解是：<strong>如果梯度过大<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>就会相对减小，如果梯度过小<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>就会相对增大</strong>。因为其实<script type="math/tex">S_{dW}</script>就是 dW 算出来的，而梯度过大就是 dW 过大，dW过大就是<script type="math/tex">S_{dW}</script>过大。一个很大的数取倒数，这个数就变很小了。梯度过小同理。<br>而为什么梯度过大就要是学习速率<script type="math/tex">\alpha</script>变小呢？因为梯度过大就是说梯度较为陡峭，可以想象一座陡峭的山，如果跨一大步是不是直接掉下去了？而掉在哪是未知的，很有可能掉到最低点的前面，这样大概率是回不到最低点的（或者是极小值点）。而如果<script type="math/tex">\alpha</script>小点就很好了，因为可以一小步一小步的走，最终可能会走到极小值点（或者最小值点）。梯度过小同理。平原地方肯定要大跨步走，你小步伐走要走到什么时候才能走到极小值点？<br><strong>另外 RMSprop 可以算是 Adagrad 算法的改进版，但是这二者的具体关系未知。</strong></p>
          </div></p>
<h1 id="优化算法历史介绍"><a href="#优化算法历史介绍" class="headerlink" title="优化算法历史介绍"></a>优化算法历史介绍</h1><blockquote>
<p>在深度学习的历史中，有不少学者，包括许多知名学者，提出了优化算法并解决了一些问题。但之后这些算法被指出并不能一般化，并不能适用于多种神经网络。<br>时间久了，深度学习圈子里的人开始多少有点质疑全新的优化算法。<br>但是RMSprop和Adam是少有的经受住人们考验的两种算法。已被证明适用于不同的深度学习结构。</p>
</blockquote>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>全称：Adaptive Moment Estimation<br>这里的 RMSprop 使用了偏差修正。<br><strong>Adam 算法是 Momentum 和 RMSprop 结合起来的算法。</strong>Momentum算法解决算法在纵轴上波动过大的问题，它可以使用类似于物理中的动量来累积梯度。而RMSprop可以在横轴上收敛速度更快同时使得波动的幅度更小。所以将两种算法结合起来表现可能会更好。<br><div class="note primary">
            <p>我的理解是 RMSprop 算法也算是在累计梯度。所以我感觉只使用 RMSprop 和使用 Adam 差不多。</p>
          </div></p>
<script type="math/tex; mode=display">
\begin{array}{l}
    compute\ dW, db\\
    V_{dW} = \beta_1 * V_{dW} + (1 - \beta_1) * dW,\quad V_{db} = \beta_1 * V_{db} + (1 - \beta_1) * db\\
    S_{dW} = \beta_2 * S_{dW} + (1 - \beta_2) * (dW)^2,\quad S_{db} = \beta_2 * S_{db} + (1 - \beta_2) * (db)^2\\
    V^{corrected}_{dW} = \frac{V_{dW}}{1 - \beta_1},\quad V^{corrected}_{db} = \frac{V_{db}}{1 - \beta_1}\\
    S^{corrected}_{dW} = \frac{S_{dW}}{1 - \beta_2},\quad S^{corrected}_{db} = \frac{S_{db}}{1 - \beta_2}\\
    W -= \alpha * \frac{V^{corrected}_{dW}}{\sqrt{S^{corrected}_{dW}} + \epsilon}\\
    b -= \alpha * \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}} + \epsilon}\\
\end{array}</script><p><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">adam paper</a>在这。</p>
<h2 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h2><ol>
<li><script type="math/tex">\alpha</script>需要自行调整。</li>
<li><script type="math/tex">\beta_1</script>一般设置为0.9，计算<script type="math/tex">dW</script>。</li>
<li><script type="math/tex">\beta_2</script>Adam的作者推荐0.999，计算<script type="math/tex">(dW)^2</script>。</li>
<li><script type="math/tex">\epsilon</script>其实不是很重要，但是Adam作者推荐设置为<script type="math/tex">10^{-8}</script>。其实不设置也可以，并不会影响算法的性能。</li>
</ol>
<p>所以在该算法中其实只要调整<script type="math/tex">\alpha</script>就够了，其他的参数也可以调整，但是一般不调整。</p>
<h1 id="学习速率衰减——Learning-rate-decay"><a href="#学习速率衰减——Learning-rate-decay" class="headerlink" title="学习速率衰减——Learning rate decay"></a>学习速率衰减——Learning rate decay</h1><p>减小学习速率的方法有多种，李宏毅老师讲解过一个 Adagrad。<br><a href="https://www.bilibili.com/video/av10590361/?p=6" target="_blank" rel="noopener">李宏毅 Adagrad 参考视频</a>，从06:30开始。<br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d#/learn/content?type=detail&amp;id=2001702125" target="_blank" rel="noopener">吴恩达深度学习——学习速率衰减</a></p>
<h1 id="优化算法总结"><a href="#优化算法总结" class="headerlink" title="优化算法总结"></a>优化算法总结</h1><div class="table-container">
<table>
<thead>
<tr>
<th><strong>optimization method</strong></th>
<th><strong>accuracy</strong></th>
<th><strong>cost shape</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient descent</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Momentum</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Adam</td>
<td>94%</td>
<td>smoother</td>
</tr>
</tbody>
</table>
</div>
<h1 id="如何为超参数选择范围"><a href="#如何为超参数选择范围" class="headerlink" title="如何为超参数选择范围"></a>如何为超参数选择范围</h1><p>上面说了那么多算法，其中包括了许多超参数，那么应该怎么为超参数选择值呢？</p>
<h2 id="超参数的重要程度"><a href="#超参数的重要程度" class="headerlink" title="超参数的重要程度"></a>超参数的重要程度</h2><p>按照吴恩达老师的排序，超参数的重要程度如下：</p>
<ol>
<li>learning rate<script type="math/tex">\alpha</script></li>
<li>Momentum的<script type="math/tex">\beta</script>, hidden layer units, mini-batch size</li>
<li>layer的数量，learning rate decay</li>
<li>Adam中的<script type="math/tex">\beta_1\quad \beta_2\quad \epsilon</script>不是很重要，一般按<script type="math/tex">0.9\quad 0.99\quad 10^{-8}</script>设置</li>
</ol>
<h2 id="超参数的取值"><a href="#超参数的取值" class="headerlink" title="超参数的取值"></a>超参数的取值</h2><ol>
<li>随机取值</li>
<li>从粗糙到精细的策略。首先进行随机取值，发现某个点的效果很好，并且附近的点也很好，然后放大这块区域，进行更密集地取值。下图被圈出来的蓝点就是效果不错的，然后被方框画出一大块区域进行密集地取值或者也可以在这块区域随机取值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BB%8E%E7%B2%97%E7%B3%99%E5%88%B0%E7%B2%BE%E7%BB%86%E7%9A%84%E5%8F%96%E5%80%BC%E7%AD%96%E7%95%A5.jpg" alt="从粗糙到精细的取值策略"></li>
</ol>
<h2 id="选择合适的范围"><a href="#选择合适的范围" class="headerlink" title="选择合适的范围"></a>选择合适的范围</h2><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701053" target="_blank" rel="noopener">视频1</a><br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701054" target="_blank" rel="noopener">视频2</a></p>
<h1 id="batch-normalization——对激活值均值归一化"><a href="#batch-normalization——对激活值均值归一化" class="headerlink" title="batch normalization——对激活值均值归一化"></a>batch normalization——对激活值均值归一化</h1><p>第25章写了均值归一化，它对输入值进行了均值归一，更易于算法优化。而batch normalization对激活值进行了均值归一化，说白了是一个东西。<br>但是。。。我看不懂。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036#/learn/content?type=detail&amp;id=2001701055&amp;cid=2001693088" target="_blank" rel="noopener">视频地址</a><br>这几个视频都看不太懂，可能是没有实战的原因。因为上面的大部分算法在以前学机器学习的时候，我都有实战过。</p>
<h1 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h1><p>Sogmoid函数<script type="math/tex">\sigma = \frac{1}{1 + e^{-z}}</script>适用于二元分类，那么碰到多元分类怎么么办呢？Softmax函数就可以解决这个问题。<br>Softmax函数计算步骤如下，假设是n元分类：</p>
<script type="math/tex; mode=display">
Z^L = W^L * A^{L-1} + b^L\\
t = e^{Z^L}\\
A^L = \frac{e^{Z^L}}{\sum^n_{i=1}t_i},\quad A^L_i = \frac{t_i}{\sum^n_{i=1}t_i}\\</script><p>多元分类中每一个神经元代表对应标签的概率是多少，并且将概率相加等于1。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/softmax%E4%BE%8B%E5%AD%90.jpg" alt="softmax例子"></p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>代价函数为<script type="math/tex">cost(\hat{y}, y) = - \sum^n_{j=1} y_j * log(\hat{y_j})</script>。<br><div class="note primary">
            <p>但是这里可能会有点奇怪。因为二元分类的代价函数是<script type="math/tex">cost(\hat{y}, y) = - \sum^n_{j=1} (y_j * log(\hat{y_j}) + (1 - y_j) * log(\hat{1-y_j}))</script>。怎么多元分类的表达式那么短？</p>
          </div></p>
<h1 id="选择深度学习框架"><a href="#选择深度学习框架" class="headerlink" title="选择深度学习框架"></a>选择深度学习框架</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E9%80%89%E6%8B%A9.jpg" alt="深度学习框架选择"></p>
<h1 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型.html">传送门</a></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/对神经网络整体的理解.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/next主题中使用Mathjax.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/next主题中使用Mathjax.html" class="post-title-link" itemprop="url">next主题中使用Mathjax</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-08 13:27:03" itemprop="dateCreated datePublished" datetime="2019-04-08T13:27:03+08:00">2019-04-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-18 14:28:27" itemprop="dateModified" datetime="2019-04-18T14:28:27+08:00">2019-04-18</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/hexo/" itemprop="url" rel="index"><span itemprop="name">hexo</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>百度了一圈最后还是没有解决问题，因为其他的博客都说要去<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code>文件修改源代码，但是我压根没有这个文件，hexo-renderer-kramed这个文件夹不存在。</p>
<p>最后还是自己试出来了，步骤如下：</p>
<ol>
<li><p>安装kramed。hexo 默认的渲染引擎是 marked，但是 marked 不支持 mathjax。所以要卸载它。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">uninstall</span> hexo-renderer-marked <span class="comment">--save</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-renderer-kramed <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>停用hexo-math，然后安装 hexo-renderer-mathjax 包。其实我也不知道怎么判断自己有没有在使用hexo-math。总而言之卸载就行了，管它存不存在。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">uninstall</span> hexo-math <span class="comment">--save</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-renderer-mathjax <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>开启mathjax，在next主题文件夹里，具体路径：/themes/next/_config.yml<br>你只需要把enable属性改为true即可</p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># Math Equations Render Support</span><br><span class="line">math:</span><br><span class="line">  enable: <span class="keyword">true</span></span><br><span class="line"></span><br><span class="line">  # <span class="keyword">Default</span>(<span class="keyword">true</span>) will load mathjax/katex script <span class="keyword">on</span> demand</span><br><span class="line">  # That <span class="keyword">is</span> it only render those page who <span class="keyword">has</span> `mathjax: <span class="keyword">true</span>` <span class="keyword">in</span> Front Matter.</span><br><span class="line">  # <span class="keyword">If</span> you <span class="keyword">set</span> it <span class="keyword">to</span> <span class="keyword">false</span>, it will load mathjax/katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: <span class="keyword">true</span></span><br><span class="line"></span><br><span class="line">  engine: mathjax</span><br><span class="line">  #engine: katex</span><br><span class="line"></span><br><span class="line">  # hexo-rendering-pandoc (<span class="keyword">or</span> hexo-renderer-kramed) needed <span class="keyword">to</span> full MathJax support.</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: <span class="keyword">true</span></span><br><span class="line">    # Use <span class="number">2.7</span>.<span class="number">1</span> <span class="keyword">as</span> <span class="keyword">default</span>, jsdelivr <span class="keyword">as</span> <span class="keyword">default</span> CDN, works everywhere even <span class="keyword">in</span> China</span><br><span class="line">    cdn: <span class="comment">//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span></span><br><span class="line">    # <span class="keyword">For</span> direct link <span class="keyword">to</span> MathJax.js <span class="keyword">with</span> CloudFlare CDN (cdnjs.cloudflare.com)</span><br><span class="line">    #cdn: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br><span class="line"></span><br><span class="line">    # See: https:<span class="comment">//mhchem.github.io/MathJax-mhchem/</span></span><br><span class="line">    #mhchem: <span class="comment">//cdn.jsdelivr.net/npm/mathjax-mhchem@3</span></span><br><span class="line">    #mhchem: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0</span></span><br><span class="line"></span><br><span class="line">  # hexo-renderer-markdown-it-plus (<span class="keyword">or</span> hexo-renderer-markdown-it <span class="keyword">with</span> markdown-it-katex plugin) needed <span class="keyword">to</span> full Katex support.</span><br><span class="line">  katex:</span><br><span class="line">    # Use <span class="number">0.7</span>.<span class="number">1</span> <span class="keyword">as</span> <span class="keyword">default</span>, jsdelivr <span class="keyword">as</span> <span class="keyword">default</span> CDN, works everywhere even <span class="keyword">in</span> China</span><br><span class="line">    cdn: <span class="comment">//cdn.jsdelivr.net/npm/katex@0.7.1/dist/katex.min.css</span></span><br><span class="line">    # CDNJS, provided <span class="keyword">by</span> cloudflare, maybe the best CDN, but <span class="keyword">not</span> works <span class="keyword">in</span> China</span><br><span class="line">    #cdn: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css</span></span><br><span class="line"></span><br><span class="line">    copy_tex:</span><br><span class="line">      # See: https:<span class="comment">//github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex</span></span><br><span class="line">      enable: <span class="keyword">false</span></span><br><span class="line">      copy_tex_js: <span class="comment">//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js</span></span><br><span class="line">      copy_tex_css: <span class="comment">//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>最后一步，修改/themes/next/layout/_layou.swig文件<br>将如下代码添加到该文件&lt;/body&gt;标签的上面一行</p>
<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="params">&lt;script type="text/x-mathjax-config"&gt;</span></span><br><span class="line">MathJax.Hub.Config(&#123;</span><br><span class="line">	<span class="comment">//下面的HTML-CSS和SVG用于在小屏幕上，数学公式可以自动换行，我测试过后发现无效，但是貌似有人可以。</span></span><br><span class="line">	<span class="string">"HTML-CSS"</span>: &#123; </span><br><span class="line"><span class="symbol">		linebreaks:</span> &#123; </span><br><span class="line"><span class="symbol">			automatic:</span> true </span><br><span class="line">		&#125; </span><br><span class="line">	&#125;,SVG: &#123;</span><br><span class="line"><span class="symbol">		linebreaks:</span> &#123; </span><br><span class="line"><span class="symbol">			automatic:</span> true </span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,menuSettings: &#123;</span><br><span class="line"><span class="symbol">		zoom:</span> <span class="string">"None"</span></span><br><span class="line">	&#125;,</span><br><span class="line"><span class="symbol">	showMathMenu:</span> false,</span><br><span class="line"><span class="symbol">	jax:</span> [<span class="string">"input/TeX"</span>,<span class="string">"output/CommonHTML"</span>],</span><br><span class="line"><span class="symbol">	extensions:</span> [<span class="string">"tex2jax.js"</span>],</span><br><span class="line"><span class="symbol">	TeX:</span> &#123;</span><br><span class="line"><span class="symbol">		extensions:</span> [<span class="string">"AMSmath.js"</span>,<span class="string">"AMSsymbols.js"</span>],</span><br><span class="line"><span class="symbol">		equationNumbers:</span> &#123;</span><br><span class="line"><span class="symbol">			autoNumber:</span> <span class="string">"AMS"</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,tex2jax: &#123;</span><br><span class="line"><span class="symbol">		inlineMath:</span> [[<span class="string">"\\("</span>, <span class="string">"\\)"</span>]],</span><br><span class="line"><span class="symbol">		displayMath:</span> [[<span class="string">"\\["</span>, <span class="string">"\\]"</span>]]</span><br><span class="line">	&#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="params">&lt;/script&gt;</span></span><br><span class="line"><span class="params">&lt;script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/<span class="number">2.7</span><span class="number">.1</span>/MathJax.js?config=TeX-MML-AM_CHTML"&gt;</span><span class="params">&lt;/script&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>1-3步参考<a href="https://www.jianshu.com/p/523e806d6681" target="_blank" rel="noopener">如何在hexo中支持Mathjax</a><br>4步参考<a href="http://npm.taobao.org/package/hexo-renderer-kramed" target="_blank" rel="noopener">hexo-renderer-kramed</a><br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/next主题中使用Mathjax.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">
    
  		
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html" class="post-title-link" itemprop="url">《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-08 12:32:53" itemprop="dateCreated datePublished" datetime="2019-04-08T12:32:53+08:00">2019-04-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:18:47" itemprop="dateModified" datetime="2019-06-04T13:18:47+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><a href="https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html" target="_blank" rel="noopener">本文中文章节地址</a><br><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">本文英文章节地址</a></p>
<h3 id="神经网络中一些符号的定义"><a href="#神经网络中一些符号的定义" class="headerlink" title="神经网络中一些符号的定义"></a>神经网络中一些符号的定义</h3><p>引用自<a href="https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html#%E7%83%AD%E8%BA%AB%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E8%AE%A1%E7%AE%97%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%82%E7%82%B9" target="_blank" rel="noopener">原文中文翻译</a></p>
<blockquote>
<p>我们首先给出网络中权重的清晰定义。我们使用<script type="math/tex">w^l_{jk}</script>表示从 <script type="math/tex">(l−1)^{th}</script> 层的 <script type="math/tex">k^{th}</script> 个神经元到 <script type="math/tex">(l)^{th}</script> 层的 <script type="math/tex">j^{th}</script> （<font style="color:red">注意：这个地方中文文章中写错了</font>，他写成了<script type="math/tex">l^{th}</script>）个神经元的链接上的权重。例如，下图给出了第二隐藏层的第四个神经元到第三隐藏层的第二个神经元的链接上的权重：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E9%93%BE%E6%8E%A5%E4%B8%8A%E7%9A%84%E6%9D%83%E9%87%8D%E8%A1%A8%E7%A4%BA%E5%9B%BE%E8%A7%A3.png" alt="神经网络中的神经元链接上的权重表示图解"><br>我们对网络偏差和激活值也会使用类似的表示。显式地，我们使用 <script type="math/tex">b^l_J</script> 表示在 <script type="math/tex">l^{th}</script> 层 <script type="math/tex">j^{th}</script> 个神经元的偏差，使用 <script type="math/tex">a^l_j</script> 表示 <script type="math/tex">l^{th}</script> 层 <script type="math/tex">j^{th}</script> 个神经元的激活值。下面的图清楚地解释了这样表示的含义：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE%E5%92%8C%E6%BF%80%E6%B4%BB%E5%80%BC%E5%9B%BE%E8%A7%A3.png" alt="神经网络中的偏差和激活值图解"></p>
</blockquote>
<p>说白了就是<strong>前一层的</strong>的神经元到后一层的神经元上的链接的权重。</p>
<p>这里解释一下，我在<a href="https://www.bilibili.com/video/av9770302" target="_blank" rel="noopener">李宏毅深度学习的视频</a>中，也听到他讲过。<script type="math/tex">w^l_{jk}</script>中的j和k在这个公式中是写反的，可以想想：上文说到<script type="math/tex">w^l_{jk}</script>是代表第k个神经元到第j个神经元链接上的权重，按逻辑来说，似乎写成<script type="math/tex">w^l_{kj}</script>更合理。因为我们说话写字都是按顺序的，没有说是反着来的。<br>之所以这么反着写，是因为后面在对权重（weight）、激活值（activations）进行向量化时，不需要加一个转置。接下来解释一下为什么不用加：<br>以上图为例，有如下权重：<script type="math/tex">w^3_{21}</script> <script type="math/tex">w^3_{22}</script> <script type="math/tex">w^3_{23}</script> <script type="math/tex">w^3_{24}</script> 现在将上标3移除组成一个向量——<script type="math/tex">\begin{pmatrix}w_{21}&w_{22}&w_{23}&w_{24}\end{pmatrix}</script>，大家已经发现了吧，我将这个向量横着写的。众所周知，矩阵的下标第一个数字代表矩阵的行，第二个数字代表矩阵的列。所以如果将w这个矩阵补全就是下面这样。</p>
<script type="math/tex; mode=display">\begin{pmatrix}
w_{11}&w_{12}&w_{13}&w_{14}\\
w_{21}&w_{22}&w_{23}&w_{24}\\
\end{pmatrix}</script><p>这是一个2行4列的矩阵，因为第三层layer只有两个神经元，自然只有两个权重向量。先给出公式：<script type="math/tex">z^3_2</script> = <script type="math/tex">w^3_2</script> * <script type="math/tex">a^2</script>，这里原本应该还要加上偏差b，但是由于改起来太麻烦以下都不加上b了，该公式就是<script type="math/tex">\begin{pmatrix}w_{21}&w_{22}&w_{23}&w_{24}\end{pmatrix}</script>乘<script type="math/tex">a^2</script>，<script type="math/tex">a^2</script>就是第二层的激活值的<strong>列</strong>向量表现形式，这很好理解就不解释了。运用考研线性代数的知识可以知道，行向量乘以列向量结果是一个<strong>数值</strong>，这样就得到了<script type="math/tex">z^3_2</script>的值。<br>我们再将第二层到第三层的其他向量补上就是如下的矩阵运算公式，为了方便查看，我将上标又加上了。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
z^3_1\\
z^3_2\\
\end{pmatrix} = 
\begin{pmatrix}
w^3_{11}&w^3_{12}&w^3_{13}&w^3_{14}\\
w^3_{21}&w^3_{22}&w^3_{23}&w^3_{24}\\
\end{pmatrix} * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix}</script><p>上式可以简化为<script type="math/tex">a^l = \sigma(z^l)</script>，其中<script type="math/tex">z^l</script> = <script type="math/tex">w * a^{l-1}</script>。合并之后写为<script type="math/tex">a^l = \sigma(w * a^{l-1})</script>，<script type="math/tex">\sigma</script>只是代表某个函数而已。</p>
<h4 id="如果不交换jk的位置"><a href="#如果不交换jk的位置" class="headerlink" title="如果不交换jk的位置"></a>如果不交换jk的位置</h4><p>绕了这么一大圈，终于说完了为什么jk交换位置要好，因为w被向量化后被表示成行向量了。行乘列直接就可以乘，不需要再对w转置。如果我们不交换jk的位置，那么w表示成如下形式：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix}</script><p>下面这样是乘不了的。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix} * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix} = 
个屁</script><p>如果下面这样就可以。T代表将矩阵转置。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix}^T * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix} = 
\begin{pmatrix}
z^3_1\\
z^3_2\\
\end{pmatrix}</script><p>简化之后就是<script type="math/tex">\sigma(w^T</script> * <script type="math/tex">a^{l-1})</script> = <script type="math/tex">a^l</script>。所以绕了那么大一圈，其实只是为了少加一个T。。。</p>
<h4 id="w的下标总结"><a href="#w的下标总结" class="headerlink" title="w的下标总结"></a>w的下标总结</h4><p>说实话我还是希望加上T的，因为上面说了那么一大堆，说实话是很绕的，还不如直接不交换jk的位置。从各个方面来说都是极为通顺的，不妥的仅仅是多加了T。<br>最后还是无法理解别人为什么将jk交换位置的，可以试着不去想神经网络，完全去思考数学中的矩阵。我给点提示，对于元素<script type="math/tex">w^3_{24}</script>首先3和2是定死的，同理得到<script type="math/tex">w^3_{21}</script> <script type="math/tex">w^3_{22}</script> <script type="math/tex">w^3_{23}</script>，现在开始不要想关于神经网络的事，想想向量<script type="math/tex">\begin{pmatrix}w^3_{21}&w^3_{22}&w^3_{23}&w^3_{24}\end{pmatrix}</script>是不是行向量？<br>再想想下面的向量是不是列向量</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{12}\\
w^3_{22}\\
w^3_{32}\\
w^3_{42}\\
\end{pmatrix}</script><p>我仅仅是交换了jk的位置对吧，虽然说数值没有任何变化，但是从数学角度讲，从行向量转为了列向量。</p>
<p>然而我觉得这样徒增了学习成本。我佛他们。</p>
<h3 id="计算输出的公式"><a href="#计算输出的公式" class="headerlink" title="计算输出的公式"></a>计算输出的公式</h3><p>公式比较简单，就是<script type="math/tex">a^l = \sigma(z^l)</script>，<script type="math/tex">z^l</script> = <script type="math/tex">w^T</script> * <script type="math/tex">a^{l-1}</script> + <script type="math/tex">b^l</script></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/FutureWarning-Conversion-of-the-second-argument-of-issubdtype-from-float-to-np-floating-is-deprecated-In-future-it-will-be-treated-as-np-float64-np-dtype-float-type.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/FutureWarning-Conversion-of-the-second-argument-of-issubdtype-from-float-to-np-floating-is-deprecated-In-future-it-will-be-treated-as-np-float64-np-dtype-float-type.html" class="post-title-link" itemprop="url">FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-07 18:19:34" itemprop="dateCreated datePublished" datetime="2019-04-07T18:19:34+08:00">2019-04-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-12 16:18:56" itemprop="dateModified" datetime="2019-04-12T16:18:56+08:00">2019-04-12</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>网上很多人说要修改h5py的版本，但是我压根没装这个库。<br><a href="https://blog.csdn.net/u013092293/article/details/80447201" target="_blank" rel="noopener">参考文章</a><br>将numpy版本降低即可，我原先好像是1.16，记不清了，我没留意到。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install numpy==<span class="number">1.13</span><span class="number">.0</span></span><br></pre></td></tr></table></figure></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/FutureWarning-Conversion-of-the-second-argument-of-issubdtype-from-float-to-np-floating-is-deprecated-In-future-it-will-be-treated-as-np-float64-np-dtype-float-type.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/机器学习算法（六）：SVM.html">
    
  		
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/机器学习算法（六）：SVM.html" class="post-title-link" itemprop="url">机器学习算法（六）：SVM</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-04 19:32:28" itemprop="dateCreated datePublished" datetime="2019-04-04T19:32:28+08:00">2019-04-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 14:02:03" itemprop="dateModified" datetime="2019-06-04T14:02:03+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>看《机器学习实战》这本书的 SVM 部分，感觉始终无法理解，因为书里把数学公式的推导直接省略了。所以在b站找了视频学习 SVM。<br>本文<a href="https://zhuanlan.zhihu.com/p/24638007" target="_blank" rel="noopener">参考文章</a><br>本文<a href="https://www.bilibili.com/video/av37947862/?p=75" target="_blank" rel="noopener">参考视频</a></p>
<p>首先分割超平面（separating hyperplane）的函数表达式是<script type="math/tex">w^T * x + b = 0</script>，而它上下两条间隔最远的超平面的表达式分别为</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 1 \\
    w^T * x + b = & -1 \\
\end{align}</script><p>至于为什么正好等于 1 和 -1，其实是为了方便计算。实际上可以等于任何值。看以下推导，先让其等于连个随机的值，比如 2 和 -3：</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 2 \\
    w^T * x + b = & -3 \\
\end{align}</script><p>解一个不等式</p>
<script type="math/tex; mode=display">
\begin{align}
    2u + v = & 1 \\
    -3u + v = & -1 \\
\end{align}</script><p>解得 u = <script type="math/tex">\frac{2}{5}</script>， v = <script type="math/tex">\frac{1}{5}</script><br>于是使</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 2 \\
    w^T * x + b = & -3 \\
\end{align}</script><p>左右分别乘上<script type="math/tex">\frac{2}{5}</script>再加上<script type="math/tex">\frac{1}{5}</script>，即可得到</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 1 \\
    w^T * x + b = & -1 \\
\end{align}</script><p>而 W 和 b 只不过是一个表示的符号而已，虽然经过运算，两个 W 和两个 b 已经不是同一个了，但是这么表示没太大问题。<br>将连个表达式更进一步表示为</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x_1 + b = & 1 \\
    w^T * x_2 + b = & -1 \\
\end{align}</script><p>两者相减得</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * (x_1 - x_2) = 2 \\
\end{align}</script><p>也即</p>
<script type="math/tex; mode=display">
\begin{align}
    ||w^T|| * ||(x_1 - x_2)|| cos\theta = 2 \\
\end{align}</script><p>两条竖线代表，向量的模长，<script type="math/tex">\theta</script>代表 <script type="math/tex">W^T</script>和<script type="math/tex">x_1 - x_2</script>之间的夹角。<br>由于初中知识，我们知道：<script type="math/tex">cos\alpha</script>=邻边比斜边，即邻边=斜边*<script type="math/tex">cos\alpha</script>。所以</p>
<script type="math/tex; mode=display">
\begin{align}
    ||w^T|| * d = & 2 \\
    d = \frac{2}{||w^T||}
\end{align}</script>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/机器学习算法（六）：SVM.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/机器学习算法（五）：PCA.html">
    
  		
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录学习时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/机器学习算法（五）：PCA.html" class="post-title-link" itemprop="url">机器学习算法（五）：PCA</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-04 19:13:25" itemprop="dateCreated datePublished" datetime="2019-04-04T19:13:25+08:00">2019-04-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 13:46:49" itemprop="dateModified" datetime="2019-06-04T13:46:49+08:00">2019-06-04</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">朱冲䶮的博客，记录学习时遇到的问题</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">88</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
            <br>
						<!--同个ip访问本站页面次数-->
						<div class="site-state-item site-state-posts" style="border-left:none;">
								<span class="site-state-item-count" id="busuanzi_value_site_pv"></span>
								<span class="site-state-item-name">浏览量</span>
						</div>
						<!--不同ip访问本站次数-->
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count" id="busuanzi_value_site_uv"></span>
								<span class="site-state-item-name">访客量</span>
						</div>
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count">64.7k</span>
								<span class="site-state-item-name">总字数</span>
						</div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://weibo.com/u/2607724281" title="Weibo &rarr; https://weibo.com/u/2607724281" rel="noopener" target="_blank"><i class="fab fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        








        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

	<script src="/lib/my-utils.js"></script>
	<!--图片缩放插件-->
	<script src="/lib/zoomify/zoomify.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <!--不蒜子统计-->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
  <!-- 背景图片 -->
	<script>
		function generateBG(count){
			var bg_prefix = '/images/background/';
			var bg =new Array();
			for(var i = 0; i < count; i++){
				bg[i] = bg_prefix + i + '.jpg';
			}
			bg.shuffle();
			return bg;
		}
		$("body").backstretch(generateBG(5), { 
			duration:60000,//1min一换
			fade: 1500 
		});
		$('#content img').zoomify({duration: 500, });
		$('#content img').on('zoom-in.zoomify', function () {
			$('#sidebar').css('display', 'none');
		});
		$('#content img').on('zoom-out-complete.zoomify', function () {
			$('#sidebar').css('display', '');
		});
  </script>
</body>
</html>
