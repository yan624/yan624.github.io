<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-simple.min.css?v=1.0.2">























  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


<!--图片缩放插件样式-->
<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">

  <meta name="description" content="记录学习问题，积累做的 leetcode 题目">
<meta name="keywords" content="博客，java，javaWeb，NLP，python，机器学习，深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/page/3/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="记录学习问题，积累做的 leetcode 题目">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="记录学习问题，积累做的 leetcode 题目">






  <link rel="canonical" href="http://yan624.github.io/page/3/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
	<!--加载flower canvas-->
<script>
var pathname = window.location.pathname;
if(pathname == '/flower.html'){
	var body =  document.getElementsByTagName('body')[0];
	var canvas = document.createElement("canvas")
	canvas.setAttribute('id', 'sakura')
	// '<canvas id="sakura"></canvas>'
	body.appendChild(canvas)
}
</script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">低阶炼金术士</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">18</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">22</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">117</span></a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/28、Neural Symbolic Machines：Learning Semantic Parsers on Freebase with Weak Supervision.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/28、Neural Symbolic Machines：Learning Semantic Parsers on Freebase with Weak Supervision.html" class="post-title-link" itemprop="url">Neural Symbolic Machines：Learning Semantic Parsers on Freebase with Weak Supervision</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-09 09:22:03" itemprop="dateCreated datePublished" datetime="2019-07-09T09:22:03+08:00">2019-07-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-25 09:35:17" itemprop="dateModified" datetime="2019-07-25T09:35:17+08:00">2019-07-25</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>本文介绍了一种神经符号机（Neural Symbolic Machine, NSM）</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/28、Neural Symbolic Machines：Learning Semantic Parsers on Freebase with Weak Supervision.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/27、Sequence-to-Action：End-to-End Semantic Graph Generation for Semantic Parsing.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/27、Sequence-to-Action：End-to-End Semantic Graph Generation for Semantic Parsing.html" class="post-title-link" itemprop="url">论文笔记：Sequence-to-Action:End-to-End Semantic Graph Generation for Semantic Parsing</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-08 22:33:46" itemprop="dateCreated datePublished" datetime="2019-07-08T22:33:46+08:00">2019-07-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-08 14:27:12" itemprop="dateModified" datetime="2019-08-08T14:27:12+08:00">2019-08-08</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://mp.weixin.qq.com/s?__biz=MzI2NjkyNDQ3Mw==&amp;mid=2247486979&amp;idx=2&amp;sn=2d95556630820c853f2ca9b2855dd60a&amp;chksm=ea87f6d5ddf07fc3cc8477d3a0cd5142e9191d91ff3161847524c37539b372b306a8f9b700a8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">某篇解析</a>；<a href="http://tongtianta.site/paper/11795" target="_blank" rel="noopener">某篇解析</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1809.00773.pdf" target="_blank" rel="noopener">论文地址</a>。<br>&emsp;&emsp;本文提出一种神经语义分析方法——Sequence-to-Action，将语义分析当做一个端到端的<strong>语义图生成</strong>的过程。我们同时使用了最近语义分析两个有前途的方向，<strong>首先</strong>我们的模型使用了一个语义图来表示一个句子的含义，该语义图与知识库紧密相关（博主注：<strong>即可以将语义图看作是知识库的一个子图</strong>）。<strong>其次</strong>，利用神经网络强大的表示学习和预测能力，提出一种 RNN 模型，能够有效的将句子映射到动作序列，从而生成语义图（博主注：<strong>此动作序列就是指生成语义图的动作，将这些动作看作是一个序列</strong>）。实验表明该方法在 OVERNIGHT 数据集上展现了一流的性能，在 GEO 以及 ATIS 数据集上得到了有一定竞争力的性能。<br>&emsp;&emsp;语义分析旨在<strong>将自然语言句子映射为逻辑形式</strong>（Zelle andMooney, 1996; Zettlemoyer and Collins, 2005;Wong and Mooney, 2007; Lu et al., 2008;Kwiatkowski et al., 2013）。例如“Which states border Texas?”将会被映射为 <em>answer (A, (state (A),nextto (A, stateid ( texas ))))</em>。<br>&emsp;&emsp;语义分析器需要两个函数，一个处理结构预测，另一个处理语义基础。传统的语义解析器通常基于复合语法，如 CCG（Zettlemoyer and Collins, <a href="https://arxiv.org/pdf/1207.1420" target="_blank" rel="noopener">2005</a>, <a href="https://www.aclweb.org/anthology/D07-1071" target="_blank" rel="noopener">2007</a>），DCS（<a href="https://www.aclweb.org/anthology/P11-1060" target="_blank" rel="noopener">Liang et al., 2011</a>）等。不幸的是，设计语法和学习精确的词汇仍是一个挑战，特别是在开放域。而且设计有效的特性往往很困难，它的学习过程也不是端到端的。为了解决上述问题，本文提出了两种有前途的研究方向：<strong>基于语义图</strong>的方法和<strong>基于 seq2seq</strong> 方法。<br>&emsp;&emsp;基于语义图的方法(Reddy et al.,2014, 2016; Bast and Haussmann, 2015; Yih et al.,2015)将句子的含义表示为语义图（即知识库的子图，参考图 1 中的例子）并<strong>将语义分析视为语义图匹配/生成过程</strong>。<strong>与逻辑形式相比，语义图与知识库有着紧密的关系</strong>(Yih et al., 2015), ，与句法结构有许多共性（Reddy et al.,2014）。基于语义图的句法分析的主要挑战是如何有效地构造句子的语义图，目前语义图是通过与模式匹配（Bast and Haussmann, 2015），从依赖树转换（Reddy et al., 2014, 2016），或者通过 staged heuristic search algorithm（Yih et al.,2015）构建的。这些方法都是基于人工设计的构造过程，它们很难处理开放/复杂的情况。<br>&emsp;&emsp;近年来，得益于 RNN 模型有较强的表示能力和预测能力，其在 Seq2Seq 模型上取得了成功，比如机器翻译。许多 Seq2Seq 模型也用于语义分析（Xiaoet al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016），不需要高质量的词典、人工构建的语法和特性。这些模型通过端到端的训练，利用注意力机制（Bahdanauet al., 2014; Luong et al., 2015）学习句子和逻辑形式之间的软对齐。<br>&emsp;&emsp;本文提出了一种新的神经语义分析框架——Sequence-to-Action。它可以同时利用语义图表示的优点和 seq2seq 模型强大的预测能力。具体来说，我们将语义分析建模为一个端到端的语义图生成过程。例如，在图 1 中，我们的模型将通过生成一系列变量[add variable:a，addtype:state，…]来解析“which states border Texas”这句话。为了实现上述目标，我们首先设计了一个动作集，对语义图的生成过程进行编码（包括节点动作：add variable,add entity,add type，边动作：add edg 以及操作动作：argmin,argmax,count,sum 等）然后我们设计了一个 RNN 模型，该模型可以生成一个动作序列来构造句子的语义图。最后，我们在解码过程中合并结构和语义约束来进一步增强解析。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/27、Sequence-to-Action：End-to-End Semantic Graph Generation for Semantic Parsing.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/26、Language to Logical Form with Neural Attention.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/26、Language to Logical Form with Neural Attention.html" class="post-title-link" itemprop="url">论文笔记：Language to Logical Form with Neural Attention</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-08 18:33:15" itemprop="dateCreated datePublished" datetime="2019-07-08T18:33:15+08:00">2019-07-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-11 13:50:36" itemprop="dateModified" datetime="2019-08-11T13:50:36+08:00">2019-08-11</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1601.01280.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2016 年。<br>&emsp;&emsp;语义分析的目的是将自然语言映射到机器可解释的有意义表示。传统的方法依赖于高质量的词汇、人工构建的模板以及特定领域或表示的语言特性，本文提出了一种注意力增强的 encoder-decoder 通用模型。将输入的话表示为向量形式，并通过调节输出序列或者树生成逻辑形式（总结来说，就是<strong>将话语转为逻辑形式</strong>，详情请看图 1）。<br>&emsp;&emsp;下图将一句话转为了逻辑形式，不同于以前的方法，它是通过神经网络生成的，而以前的方法依赖于手写的规则。图片取自 <a href="https://www.aclweb.org/anthology/W00-1317" title="Automated construction ofdatabase interfaces: Intergrating statistical and rela-tional learning for semantic parsing" target="_blank" rel="noopener">Tang and Mooney200</a>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/语句转为逻辑形式.jpg" alt="语句转为逻辑形式"></p>
<p>&emsp;&emsp;基于 RNN 的 encoder-decoder 已成功应用于各种 NLP 任务，图 1 中使用了 LSTM，我们的做法是提出了两个变体模型。<strong>第一个模型</strong>将语义解析视为普通的序列转换任务，<strong>第二个模型</strong>配备了层次树解码器，该解码器明确地捕获逻辑形式的组合结构。我们还引入了<strong>注意力机制</strong>，并提出一个识别步骤来<strong>识别很少提到的实体</strong>和<strong>数字</strong>。<br>&emsp;&emsp;对<strong>四个数据集</strong>的实验结果表明，我们的方法在不使用人工设计特征的情况下具有竞争力，并且易于迁移。<br>&emsp;&emsp;我们的工作综合了两种标准研究，即<strong>语义分析</strong>和 <strong>encoder-decoder 架构的神经网络</strong>。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;学习语义解析器的问题引起了广泛的关注，可以追溯到 Woods（1973年）。。。。</p>
<h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>&emsp;&emsp;我们的目标是学习一个模型，将<u><strong>自然语言输入 <script type="math/tex">q = x_1 \dots x_{|q|}</script></strong></u> 映射为其含义的<u><strong>逻辑形式（logical form）表示 <script type="math/tex">a = y_1 \dots y_{|a|}</script></strong></u>。条件概率被分解为：</p>
<script type="math/tex; mode=display">
\begin{align}
    p(a|q) & = \prod^{|a|}_{t=1} p(y_t|y_{<t},q) \tag 1\\
    y_{<t} & = y_1 \dots y_{t-1}
\end{align}</script><p>&emsp;&emsp;我们的模型包含一个编码器和一个解码器，编码器负责将输入的自然语言 q 编码成向量，解码器负责生成 <script type="math/tex">y_1 \dots y_{|a|}</script>。下面将仔细描述。</p>
<h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>&emsp;&emsp;对于普通的 Seq2Seq 任务，使用 LSTM 来计算，如下图所示。<script type="math/tex">h^l_t</script> 代表第 l 层的第 t 个时间步的隐藏层，公式为：</p>
<script type="math/tex; mode=display">
\begin{align}
    h^l_t = \text{LSTM}(h^l_{t-1},h^{l-1}_t) \tag 2
\end{align}</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/Seq2Seq.jpg" alt="Seq2Seq"><br>&emsp;&emsp;在实验中，遵循 <a href="https://arxiv.org/pdf/1409.2329.pdf" title="RECURRENT NEURAL NETWORK REGULARIZATION" target="_blank" rel="noopener">Zaremba et al. 2015</a> 提出的架构。不过，使用其他类型的门控激活函数也是可以的（例如<a href="https://arxiv.org/pdf/1406.1078.pdf" title="Learning phrase representations using RNN encoder-decoder for statistical machine translation" target="_blank" rel="noopener">Cho et al. 2014</a>）。<strong>对于 encoder</strong>，<script type="math/tex">h^0_t = W_qe(x_t)</script>（注：此公式是第 0 层的运算步骤，即输入层）是 RNN 中输入的词向量，<script type="math/tex">W_q \in \mathbb{R}^{n \times |V_q|}</script> 代表输入层的权重值矩阵，e(·) 代表对应 token 的索引。<strong>对于 decoder</strong>，<script type="math/tex">h^0_t = W_ae(y_{t-1})</script> 代表前一个预测词的词向量，其中 <script type="math/tex">W_a \in \mathbb{R}^{n \times |V_a|}</script>。接下来，最后的 LSTM <script type="math/tex">h^L_t</script> 被用于预测 <script type="math/tex">t</script>-th 输出 token，计算公式为：</p>
<script type="math/tex; mode=display">
\begin{align}
    p(y_t|y_t,q) = softmax(W_oh^L_t)^T e(y_t) \tag 3
\end{align}</script><p>&emsp;&emsp;<strong>该公式用于预测每一个 token</strong>。另外补充一点，增加了 “start-of-sequence” <code>&lt;s&gt;</code> 和 “end-of-sequence” <code>&lt;/s&gt;</code>。<br>&emsp;&emsp;该模型总的来说，就是 LSTM 的计算方法，也没什么好说的。</p>
<h2 id="Seq2Tree"><a href="#Seq2Tree" class="headerlink" title="Seq2Tree"></a>Seq2Tree</h2><p>&emsp;&emsp;Seq2Seq 模型有一个<strong>缺点</strong>就是它<strong>忽略了逻辑形式的层次结构</strong>。所以，要改良的话，它需要记住各种辅助信息（比如括号对），以此生成格式良好的输出。如下图 3 所示，是一个层次树 decoder：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/Seq2Tree模型.jpg" alt="Seq2Tree模型"></p>
<p>&emsp;&emsp;Seq2Tree 与 Seq2Seq 的编码器类似，不同的是解码器。Seq2Tree 以自上而下的方式生成逻辑，为了定义树结构，我们定义了一个表示子树的 “nonterminal” <code>&lt;n&gt;</code> 标记。如图 3 所示，将<strong><em>逻辑形式 “lambda $0 e (and (&gt;(departure_time $0) 1600:ti) (from $0 dallas:ci))”</em></strong>预处理为树，方法是<strong>用 nonterminal 替换括号对之间的标记</strong>（token）。<strong>特殊记号 <code>&lt;s&gt;</code> 和 <code>&lt;(&gt;</code> 分别表示序列和 nonterminal 序列的开头</strong>（由于缺少空间，图3中省略了），<strong>记号 <code>&lt;/s&gt;</code> 代表序列结束</strong>。具体步骤是：</p>
<ol>
<li>编码输入值 q；</li>
<li>层次树解码器使用 RNN 在逻辑形式 a（在<strong>任务定义</strong>中已经说明了 q 和 a 的含义）的对应部分的子树中生成 tokens（注意这里的 token 带了 s）；</li>
<li>如果预测的 token 为 <code>&lt;n&gt;</code>，则通过调节 nonterminal 的隐藏向量来解码序列。（博主注：举个例子理解一下：看图 3 的第一层，先是使用 encoder 进行编码，接着开始对逻辑形式进行解码，逻辑形式就是上面的斜体部分。接下来预测到了 token<code>&lt;n&gt;</code> 于是调用 nonterminal 的隐藏向量来进行解码，即生成一棵子树。以此类推，碰到 toekn <code>&lt;n&gt;</code> 就开始解码）</li>
<li>与 Seq2Seq 解码器不同，当前的隐藏状态不仅仅取决于上一个时间步，为了更好地利用 parent nonterminal 的信息，我们引入了一个 parent-feeding 的连接，其中 parent nonterminal 的隐藏向量与输入连接（concatenated）并喂入 LSTM。</li>
</ol>
<p>&emsp;&emsp;再举个例子帮助理解一下，如图 4 所示。逻辑形式为 <strong><em>A B (C)</em></strong>，其中 <script type="math/tex">y_1 \dots y_6</script> 代表不同的时间步，<strong><em>(C)</em></strong> 对应子树。解码一共有<strong>两个步骤</strong>：一旦输入值 q 被编码，首先在深度为 1 处生成 <script type="math/tex">y_1 \dots y_4</script>，直到 token <code>&lt;/s&gt;</code> 被预测到；接下来通过调节 nonterminal <script type="math/tex">t_3</script> 的隐藏向量来生成 <script type="math/tex">y_5, y_6</script>，<script type="math/tex">p(a|q)</script> 的概率是这<strong>两个序列解码步骤</strong>的乘积：</p>
<script type="math/tex; mode=display">
p(a|q) = p(y_1 y_2 y_3 y_4 | q) p(y_5 y_6 | y_{\leq 3},q)</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/一个简单的Seq2Tree的例子.jpg" alt="一个简单的Seq2Tree的例子"></p>
<h2 id="Attention-机制"><a href="#Attention-机制" class="headerlink" title="Attention 机制"></a>Attention 机制</h2><p>&emsp;&emsp;<strong><em>Attention 的原理</em></strong>。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>&emsp;&emsp;我们的目标是<strong>最大化</strong>由自然语言语句作为输入时产生的逻辑形式的可能性，所以目标函数为：</p>
<script type="math/tex; mode=display">
\text{minimize} - \sum_{(q,a) \in D} logp(a|q)</script><p>&emsp;&emsp;其中 <script type="math/tex">D</script> 是所有自然语言逻辑形式训练对的集合，<script type="math/tex">p(a|q)</script> 按式（1）计算。采用 <strong>RMSProp</strong> 算法解决了这一非凸优化问题。此外，使用 <strong>Dropout</strong> 进行正则化。</p>
<h2 id="推论"><a href="#推论" class="headerlink" title="推论"></a>推论</h2><p>&emsp;&emsp;暂时略。</p>
<h2 id="参数识别"><a href="#参数识别" class="headerlink" title="参数识别"></a>参数识别</h2><p>&emsp;&emsp;大多数的语义分析数据集都是为问答开发的。在经典的系统中，问题被映射乘逻辑形式，并在知识库中获取答案。由于问答任务的性质，许多自然语言的语句都包含实体或数字，它们通常被解析为逻辑形式的参数。其中不可避免地会有一些罕见或者根本不会出现在数据集中的实体或数字（对于小规模数据集尤其如此）。传统的序列编码器只是简单地用一个特殊的位置单词符号替换稀有单词（<a href="https://arxiv.org/pdf/1410.8206.pdf" title="Addressing the Rare Word Problem in Neural Machine Translation" target="_blank" rel="noopener">Luong et al. 2015a</a>; <a href="https://arxiv.org/pdf/1412.2007.pdf" title="On Using Very Large Target Vocabulary for Neural Machine Translation" target="_blank" rel="noopener">Jean et al. 2015</a>），这对语义分析是有害的。<br>&emsp;&emsp;为此开发了一个简单的参数识别程序。具体来说就是在输入的问题中标识实体和数字，并用它们的<strong>类型</strong>和<strong>唯一 id</strong> 替换它们。例如，将训练样本“<em>jobs with a salary of 40000</em>”及其逻辑形式“job(ANS), salary_greater_than(ANS,40000, year)”预处理为“jobs with a salary of <em><script type="math/tex">num_0</script></em>”和“job(ANS), salary_greater_than(<em>ANS</em>,<em><script type="math/tex">num_0</script></em>,<em>year</em>)”。一旦解码完毕，后处理步骤就会将所有标记 <script type="math/tex">type_i</script> 恢复到它们以前的实体或数字。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;我们将我们的方法在四个数据集上分别与以前的多个系统进行比较，下面将描述这些数据集。代码可在此处获得<a href="https://github.com/donglixp/lang2logic" target="_blank" rel="noopener">https://github.com/donglixp/lang2logic</a>（lua 版，官方），<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch" target="_blank" rel="noopener">https://github.com/Alex-Fabbri/lang2logic-PyTorch</a>（python 版，非官方）。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>&emsp;&emsp;<strong>JOBS</strong>    工作<br>&emsp;&emsp;<strong>GEO</strong>        Gene Expression Omnibus（基因表达综合）<br>&emsp;&emsp;<strong>ATIS</strong>    Airline Travel Information System（航空旅行信息系统）<br>&emsp;&emsp;<strong>IFTTT</strong>    if this then that（<a href="https://ifttt.com/" target="_blank" rel="noopener">地址</a> <a href="https://baike.baidu.com/item/ifttt/8378533" target="_blank" rel="noopener">百度百科介绍</a>），<a href="https://www.aclweb.org/anthology/P15-1085" target="_blank" rel="noopener">Quirk et al.2015</a> 从 IFTTT 网站提取大量的 if-this-then-that 来创建此数据库<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/数据集介绍.jpg" alt="数据集介绍"></p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>&emsp;&emsp;自然语言语句是小写的，并且使用基于维基百科的常见拼写错误列表来纠正拼写错误。使用 NLTK 来限制词汇（[Bird et al.2009] Natural Language Processing with Python. O’Reilly Media.），对于 IFTTT 过滤了在训练集中出现少于五次的 token，channels 和 functions。对于其他数据集，过滤了在训练集中至少两次没有出现的输入词，但保留了逻辑形式中的所有 token。并且使用了<strong>参数识别</strong>，当然也可以使用更复杂的办法。<br>&emsp;&emsp;超参数在 JOBS 和 GEO 上使用了交叉验证，使用了 ATIS 和 IFTTT 作为标准开发集（就是验证集，不同的叫法而已 development/validation）。</p>
<ul>
<li><strong>RMSProp</strong>：batch size = 20；parameter = 0.95；</li>
<li><strong>梯度修剪</strong>为 5 以缓解梯度爆炸（<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">Pascanu et al.2013</a>）；</li>
<li><strong>参数</strong>从均匀分布 <script type="math/tex">U(-0.08, 0.08)</script> 中随机初始化；</li>
<li>两层 <strong>LSTM</strong> 用于 IFTTT，单层 LSTM 用于其他数据集；</li>
<li><strong>dropout</strong> <script type="math/tex">\in</script> {0.2,0.3,0.4,0.5}；</li>
<li>隐藏向量和词嵌入<strong>维度</strong>从 {150, 200, 250} 选择；</li>
<li><strong>early stopping</strong>；</li>
<li>输入句子在进入编码器之前被反转（<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever et al.2014</a>）；</li>
<li>贪婪搜索生成逻辑形式；</li>
<li><strong>softmax</strong> 用于分类。</li>
</ul>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>&emsp;&emsp;Attention 机制可以提高性能，对于小数据集<strong>参数识别</strong>至关重要。</p>
<h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><h1 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h1><p>&emsp;&emsp;由此篇<a href="http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf" title="Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars" target="_blank" rel="noopener">论文</a>创建。<br>&emsp;&emsp;在<a href="https://github.com/yuxuan1995liu/Semantic-Parsing-Data-Pre-Processing" target="_blank" rel="noopener">此处</a>可找到全部数据，但是这里面的格式不是 lambda-calculus。我有点搞不懂他提供的数据到底是什么意思，这篇<a href="https://arxiv.org/pdf/1805.04793" title="Coarse-to-Fine Decoding for Neural Semantic Parsing" target="_blank" rel="noopener">论文</a>是作者对此篇论文的改进版，以后去那看看有么有详细说明。</p>
<h1 id="此论文的数据格式"><a href="#此论文的数据格式" class="headerlink" title="此论文的数据格式"></a>此论文的数据格式</h1><p>&emsp;&emsp;数据的格式在原论文 4.1 Datasets 中提及了，使用另一个人的论文的格式。lambda calculus 的<a href="https://www.aclweb.org/anthology/D11-1140" target="_blank" rel="noopener">论文地址</a>我直接给出吧。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/26、Language to Logical Form with Neural Attention.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/25、NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/25、NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.html" class="post-title-link" itemprop="url">论文笔记：NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-08 17:00:41" itemprop="dateCreated datePublished" datetime="2019-07-08T17:00:41+08:00">2019-07-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-25 09:34:59" itemprop="dateModified" datetime="2019-07-25T09:34:59+08:00">2019-07-25</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2016 年，首次发表于 2015 年。注：<strong>此篇论文是首次使用 attention 机制到工作的论文，其实跟问答系统没什么关系，只是因为 PPT</strong>（此 PPT 是 CCF ADL100 上的 PPT）<strong>中后续的论文都是用到了 attention 机制，所以可能 PPT 中提了一下</strong>。<br>&emsp;&emsp;神经机器翻译（Neural machine translation）是最近提出的机器翻译方法。不同于统计机器翻译。神经机器翻译的目的是建立一个可以自己调整参数最后最大化翻译性能的神经网络。最近提出的神经机器翻译模型大都是 encoder-dedcoder 家族，编码器将句子编码成固定的向量，解码器从该向量中生成译文。在本文中，我们猜测对于改善基于 encoder-decoder 架构的性能，使用一个定长的向量是否是一个瓶颈。我们提议扩展该模型，使其能够<strong>自动地在原始句子中搜索与预测目标单词相关的部分</strong>。通过使用这种新颖的翻译方法，在英法翻译任务上，我们发现该方法与现存的基于短语的一流系统有可比性。此外，分析表明，模型中这种<strong>对齐（alignment）</strong>的概念与我们的直觉相吻合。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/25、NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/5、Large scale Simple Question Answering with Memory Network.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/5、Large scale Simple Question Answering with Memory Network.html" class="post-title-link" itemprop="url">论文笔记：Large-scale Simple Question Answering with Memory Network</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-07 18:49:45" itemprop="dateCreated datePublished" datetime="2019-07-07T18:49:45+08:00">2019-07-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-25 09:35:31" itemprop="dateModified" datetime="2019-07-25T09:35:31+08:00">2019-07-25</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1506.02075v1.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2015 年。<br>&emsp;&emsp;开放域问答系统的目的是在不受域限制的情况下，为用自然语言表达的问题提供准确的答案。问答系统有很长的历史，它们搜索文本文档或在网络上提取答案（see e.g.(Voorhees and Tice, 2000; Dumais et al., 2002)）。最近在公开的大型知识库（KBS）方面也取得了进展，如 Freebase 知识库。然而，尽管最近大家都在关注设计一个具有推理能力的系统，它可以检索并使用 KB 中的<strong>多重事实</strong>进行问答。但是其实只涉及 <strong>KB 中单个事实的简单问答</strong>都还没被解决，本论文中将其称为 Simple Question Answering。<br>&emsp;&emsp;本文贡献有二：</p>
<ul>
<li>其一，为了<strong>研究现有系统</strong>以及<strong>通过多任务学习在不同数据源上同时训练</strong>成为可能，我们收集了第一个基于知识库的大规模的问答数据集，称为 SimpleQuestions。包含了人类编写和 Freebase facts 相关的超过 10 万个问题，另外现有的基准数据集 WebQuestions 包含的问题少于 6 千个，这些问题是使用 google suggest api 自动创建的。</li>
<li>其二，提出了一种基于词嵌入的问答系统，在 Memory Networks (MemNNs)（<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Weston et al., 2015</a>;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">Sukhbaatar et al., 2015</a>） 框架下开发而成。</li>
</ul>
<p>&emsp;&emsp;虽然我们的模型与之前的 QA 嵌入模型（<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a</a>;<a href="https://arxiv.org/pdf/1404.4326.pdf" target="_blank" rel="noopener">Bordes et al., 2014b</a>）相似，但使用 MemNNs 的框架为未来工作中更复杂的推理方案提供了思路，因为 MemNNs 在复杂的推理问答任务上表现了很好的性能（<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Weston et al., 2015</a>）。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><ol>
<li>Sections 3, 4：介绍了基于词嵌入的问答系统；</li>
<li>Section 5：相关工作；</li>
<li>Section 6：实验结果。</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/5、Large scale Simple Question Answering with Memory Network.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/24、An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/24、An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge.html" class="post-title-link" itemprop="url">论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-06 16:52:07" itemprop="dateCreated datePublished" datetime="2019-07-06T16:52:07+08:00">2019-07-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-06 12:04:15" itemprop="dateModified" datetime="2019-08-06T12:04:15+08:00">2019-08-06</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P17-1021" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;随着知识库数量的增加，人们越来越希望寻找到一些有效的方法来获取这些资源。现在有几种专门为<strong>查询 KBs</strong> 设计的<strong>语言</strong>：SPARQL（<a href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/" target="_blank" rel="noopener">rudhommeaux and Seaborne, 2008</a>）。但要使用这些语言，用户不仅需要熟悉它们，还要了解 KBs 的体系结构。相比之下，<strong>以自然语言为查询语言</strong>的 KB-QA 是一种更友好的方案，近年来已成为研究热点。这项任务<strong>以前</strong>有两个主流的研究方向：</p>
<ol>
<li>基于语义解析（semantic parsing-base, SP-based）</li>
<li>基于信息检索（information  retrieval-based, IR-based）</li>
</ol>
<p>&emsp;&emsp;<strong>现在</strong>随着神经网络方法的发展，基于神经网络的 KB-QA 已经取得了令人瞩目的成果。其中至关重要的步骤就是计算<strong>问题和候选答案</strong>之间的相似性分数，这一步骤的<strong>关键一点</strong>就是学习它们的表示。然而以往的研究更注重<strong>答案的学习表示</strong>。例如，<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al. 2014a</a> 考虑候选答案子图的重要性，<a href="https://www.aclweb.org/anthology/P15-1026" target="_blank" rel="noopener">Dong et al. 2015</a>利用上下文和答案的类型。无论如何，<strong>问题的表示</strong>终究还是表达不全。现有的方法 <a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a,</a> <a href="https://arxiv.org/pdf/1404.4326.pdf" target="_blank" rel="noopener">b</a> 使用 bag-of-word 模型将问题表示为一个向量，但是这样<strong>问题与答案的关联性</strong>还是被忽视了。我们认为一个问题应该根据回答时不同的侧重面来表示（注：<em>其实就是想用注意力机制</em>，回答的侧重面可以是答案实体本身、答案类型、答案上下文等）。<br>&emsp;&emsp;因此本文提出了一个端到端的神经网络模型，通过 <strong>cross-attention</strong> 机制，根据不同的候选答案动态地表示问题及对应的分数。此外还利用了 KB 中的全部知识，旨在将 KB 中丰富的知识集成到答案中，以此缓解 out-of-vocabulary(<strong>OOV</strong>) 的问题，从而帮助 cross-attention 更精确地表示问题。最后实验结果表明了该方法确实有效。<br>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P15-1026" title="Question Answering over Freebase wit hMulti-Column Convolutional Neural Networks" target="_blank" rel="noopener">论文</a>（<a href="https://yan624.github.io/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html">论文笔记地址</a>）中的方法很有启发性，但是由于简单地选择三个独立的 CNN ，因此过于机械化。所以我们使用了基于 cross-attention 的神经网络模型。<br>&emsp;&emsp;模型架构如下，步骤与之前的论文的步骤类似。<strong>1)</strong>先找到问题的主题（main entity/topic entity）；<strong>2)</strong>然后在知识库中找到主题相连的节点作为候选答案，<strong>3)</strong>最后送入 score layer 进行评分，排序分数选出分数最高的候选答案作为正确答案。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge/MCCNN总览.jpg" alt="MCCNN总览"></p>
<p>&emsp;&emsp;为了方便描述，我们将任何一种基本元素称为资源（resource），无论是实体还是关系。比如 (/m/0f8l9c,location.country.capital,/m/05qtj) 的描述是法国的首都是巴黎，其中的 <em>/m/0f8l9c</em> 和 <em>/m/05qtj</em> 分别代表法国和巴黎，<em>location.country.capital</em> 是一种关系。</p>
<h1 id="我们的方法"><a href="#我们的方法" class="headerlink" title="我们的方法"></a>我们的方法</h1><h2 id="候选者生成"><a href="#候选者生成" class="headerlink" title="候选者生成"></a>候选者生成</h2><p>&emsp;&emsp;略，我已经写过无数遍了。使用 Freebase API 构建的。</p>
<h2 id="The-Neural-Cross-Attention-Model"><a href="#The-Neural-Cross-Attention-Model" class="headerlink" title="The Neural Cross-Attention Model"></a>The Neural Cross-Attention Model</h2><p>&emsp;&emsp;下图是模型的架构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge/MCCNN架构.jpg" alt="MCCNN架构"></p>
<ul>
<li>问题表示（图 2 中左侧部分显示了处理步骤）<ol>
<li>使用向量表示问题中的每个单词，这跟其他 NLP 任务差不多，不过它是随机初始化的词嵌入矩阵 <script type="math/tex">E_w \in \mathbb{R}^{d \text{x} v_w}</script>，然后取出对应单词的词向量。d 代表词向量的维度，<script type="math/tex">v_w</script> 代表词表的大小。</li>
<li>将词向量送入 LSTM，值得注意的是我们没有使用单向 LSTM，因为这样一个单词表示只会捕获到之前的单词的信息而不会包含之后的单词。为此我们使用了双向 LSTM 外加 Bahdanau（<a href="https://arxiv.org/pdf/1409.0473.pdf" title="Neural machine translation by jointly learning to align and translate" target="_blank" rel="noopener">Bahdanau, 2014</a>） attention 的处理；</li>
<li>这样就会获得两个表示 <script type="math/tex">(\overrightarrow{h_1}, \overrightarrow{h_2}, \dots, \overrightarrow{h_n})</script> 以及 <script type="math/tex">(\overleftarrow{h_1}, \overleftarrow{h_2}, \dots, \overleftarrow{h_n})</script>，然后将两个表示拼接起来组成 [<script type="math/tex">\overrightarrow{h_i};\overleftarrow{h_i}</script>]，正反向 LSTM 单元的大小都是 <script type="math/tex">\frac{d}{2}</script>。</li>
</ol>
</li>
<li>回答的不同侧面表示（图 2 中右侧下方部分）<ol>
<li>直接使用 KB 的嵌入矩阵 <script type="math/tex">E_k \in \mathbb{R}^{d \text{x} v_k}</script>，其中 <script type="math/tex">v_k</script> 代表知识库中资源的大小，该嵌入矩阵随机初始化并在训练时学习表示，使用全局信息对表示的进一步提高将在 3.3 节 Combining Global Knowledge（原论文）描述。具体来说我们使用回答的四个方面：问答实体 <script type="math/tex">a_e</script>，回答关系 <script type="math/tex">a_r</script>，回答类型 <script type="math/tex">a_t</script>，回答上下文 <script type="math/tex">a_c</script>。它们的嵌入被分别表示为 <script type="math/tex">e_e</script>, <script type="math/tex">e_r</script>, <script type="math/tex">e_t</script>, <script type="math/tex">e_c</script>；</li>
<li>值得注意的是问答上下文由多个 KB 资源组成，我们将它们定义为 (<script type="math/tex">c_1, c_2, \dots, c_m</script>)，首先获得它们的嵌入 (<script type="math/tex">e_{c_1}, e_{c_2}, \dots, e_{c_m}</script>)，然后计算它们的平均值 <script type="math/tex">e_c = \frac{1}{m} \sum^m_{i=1} e_{c_i}</script></li>
</ol>
</li>
<li>Cross-Attention model（图 2 中右侧上方部分以及最上方部分），详见 3.2.3 Cross-Attention model</li>
</ul>
<h2 id="Combining-Global-Knowledge"><a href="#Combining-Global-Knowledge" class="headerlink" title="Combining Global Knowledge"></a>Combining Global Knowledge</h2><p>&emsp;&emsp;Combining Global Knowledg，利用TransE得到knowledge embedding。</p>
<h1 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h1><ol>
<li>使用了 Bahdanau Attention 处理；</li>
<li>使用了双向 LSTM，会得到两个向量，最后将这两个向量拼接在一起，就是 BiLSTM 这层的最终向量。另外正反的 LSTM 的长度都是 <script type="math/tex">\frac{d}{2}</script>；</li>
<li>回答通过问答实体 <script type="math/tex">a_e</script>，回答关系 <script type="math/tex">a_r</script>，回答类型 <script type="math/tex">a_t</script>，回答上下文 <script type="math/tex">a_c</script> 四个方面来表示，其中 ac 是所有词向量的平均值。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li><a href="https://arxiv.org/pdf/1404.4326" title="Open question answering with weakly supervised embedding models" target="_blank" rel="noopener">Antoine Bordes 等 2014b</a>；</li>
<li><a href="https://arxiv.org/pdf/1406.3676" title="Question Answering with Subgraph Embeddings" target="_blank" rel="noopener">Antoine Bordes 等 2014a</a>；</li>
<li><a href="https://www.aclweb.org/anthology/P14-2105" title="Semantic Parsing for Single-Relation Question Answering" target="_blank" rel="noopener">Yih W 等 2014</a>，实际上是基于语义解析的，但是用了词向量；</li>
<li><a href="https://www.aclweb.org/anthology/D14-1071" title="Joint relational embeddings for knowledge-based question answering" target="_blank" rel="noopener">Min-Chul Yang 等 2014</a>，实际上是基于语义解析的但是用了词向量；</li>
<li><a href="https://www.aclweb.org/anthology/P15-1026" title="Question Answering over Freebase with Multi-Column Convolutional Neural Networks" target="_blank" rel="noopener">Dong 等 2015</a>；</li>
<li><a href="https://www.aclweb.org/anthology/C16-1226" title="Hybrid Question Answering over Knowledge Base and Free Text" target="_blank" rel="noopener">Kun Xu 等 2016b</a>；<a href="https://arxiv.org/pdf/1603.00957.pdf" title="Question Answering on Freebase via Relation Extraction and Textual Evidence" target="_blank" rel="noopener">Xu K 等 2016a</a>。</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/24、An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html" class="post-title-link" itemprop="url">论文笔记：Question Answering over Freebase with Multi-Column Convolutional Neural Networks</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-06 14:48:24" itemprop="dateCreated datePublished" datetime="2019-07-06T14:48:24+08:00">2019-07-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-06 10:24:20" itemprop="dateModified" datetime="2019-08-06T10:24:20+08:00">2019-08-06</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P15-1026" target="_blank" rel="noopener">论文地址</a>，发表于 2015 年。<br>&emsp;&emsp;大多数现有的系统通常依靠人工制作的特性和规则来进行<em>问题理解</em>以及<em>答案排序</em>。此外，一些方法（<a href="https://arxiv.org/pdf/1406.3676.pdf" title="Question Answering with Subgraph Embeddings" target="_blank" rel="noopener">Bordes et al., 2014a</a>; <a href="https://arxiv.org/pdf/1404.4326.pdf" title="Open Question Answering with Weakly Supervised Embedding Models" target="_blank" rel="noopener">Bordeset al., 2014b</a>）使用问题的词嵌入的总和来表示问题，但是这忽略了<strong>词序信息</strong>，无法处理复杂问题，例如 who killed A 和 who A killed 两个问题的表示是一样的。本文介绍了 multi-column convolutional neural networks (MCCNNs)，从三个方面（<strong>回答路径（Answer Type），回答上下文（Answer Context），回答类型（Answer Path）</strong>）理解问题。使用 Freebase 作为知识库，在 WebQuestions 数据集上进行了广泛的实验。最终表明，此方法拥有更好的性能。<br>&emsp;&emsp;神经网络训练步骤：</p>
<ol>
<li>MCCNNs 从输入的问题中使用不同 column networks 去提取<strong>回答路径，回答上下文，回答类型</strong>。跟 Bordes 的论文一样，该论文知识库（本文就是 FreeBase）中的实体和关系也由向量表示。</li>
<li>然后评分层（score layer）根据问题和候选答案的表示进行排序（点积）。</li>
</ol>
<h1 id="处理步骤"><a href="#处理步骤" class="headerlink" title="处理步骤"></a>处理步骤</h1><p>&emsp;&emsp;给定一个自然语言问题 <script type="math/tex">q = w_1 \dots w_n</script>，从 FreeBase 中检索相应的实体和属性，然后将它们作为候选答案 <script type="math/tex">C_q</script>。比如，问题 <em>when did Avatar release in UK</em> （阿凡达在英国的发行时间）的答案是 <em>2009-12-17</em>。需要注意的是对于该问题也许有一系列的正确答案。以下数据将被使用到：<strong>WebQuestions</strong>，<strong>FreeBase</strong>，<strong>WikiAnswers</strong>。<br>&emsp;&emsp;MCCNN 概览如图 1 所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Question Answering over Freebase with Multi-Column Convolutional Neural Networks/MCCNN概览.jpg" alt="MCCNN概览"></p>
<p>&emsp;&emsp;比如说，对于问题 whendid Avatar release in UK，从 FreeBase 中查询 <strong>Avatar</strong>（可以称为 <strong>main entity</strong> 或者 <strong>topic entity</strong>） 的<strong>相连节点</strong>（related nodes），这些相连节点被认为是候选答案（<script type="math/tex">C_q</script>）。然后对于每个候选答案 a，模型将会预测一个分数 S(q,a) 以判断 a 是否为正确答案。<br>&emsp;&emsp;对于问题的三个侧面的向量表示分别以 <script type="math/tex">f_1(q)</script> <script type="math/tex">f_2(q)</script> <script type="math/tex">f_3(q)</script> 表示，同理答案的三个侧面分别以 <script type="math/tex">g_1(a)</script> <script type="math/tex">g_2(a)</script> <script type="math/tex">g_3(a)</script> 表示。<script type="math/tex">f_i(q)</script> 和 <script type="math/tex">g_i(a)</script>拥有相同的维度。使用这些问答的表示，我们可以计算问答对 (q,a) 的分数。具体来说，评分函数 S(q,a) 定义为（如图 1 所示，评分层计算分数并将其加起来）：</p>
<script type="math/tex; mode=display">
S(q,a) = \underbrace{f_1(q)^Tg_1(a)}_{\text{answer path}} + \underbrace{f_2(q)^Tg_2(a)}_{\text{answer context}} + \underbrace{f_3(q)^Tg_3(a)}_{\text{answer type}}</script><h2 id="候选者生成"><a href="#候选者生成" class="headerlink" title="候选者生成"></a>候选者生成</h2><p>&emsp;&emsp;训练神经网络的<strong>第一步是</strong>从 FreeBase 中为问题检索候选答案。用户提出的问题应该包含一个<strong>可识别</strong>的实体，该实体与知识库相连。我们使用 <strong>Freebase Search API</strong>（<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf" title="Freebase: a collaboratively created graph database for structuringhuman knowledge" target="_blank" rel="noopener">Bollacker et al., 2008)</a>） 查询问题中的命名体。如果没有任何命名体，则查询名词短语，我们使用调用 API 返回的列表中的第一个实体。这个实体解决办法也被 <a href="https://www.aclweb.org/anthology/P14-1090" title="Information Extraction over Structured Data: Question Answering with Freebase" target="_blank" rel="noopener">Yao and Van Durme, 2014)</a> 使用，还可以研发更好的办法，但不是本论文的关注点。<strong>最后关联实体的所有 2-hops（应该是周围的意思，我没有查到是什么意思，但是在<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html#论文总结">博客笔记</a>中有所总结） 节点被认为是候选答案</strong>。并把问题 q 的候选答案集合称为 <script type="math/tex">C_q</script>。</p>
<h2 id="MCCNNs-for-Question-Understanding"><a href="#MCCNNs-for-Question-Understanding" class="headerlink" title="MCCNNs for Question Understanding"></a>MCCNNs for Question Understanding</h2><p>&emsp;&emsp;MCCNNs 使用多列（<strong>列</strong>指的是图 1 中左侧那三片）卷积网络从字嵌入中学习不同方面。使用 <a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf" title="Natural Language Processing (Almost) from Scratch" target="_blank" rel="noopener">Collobert R 等 2011</a> 的方法解决语言长度不一的问题。具体的做法可参考原论文 <strong>4.2 MCCNNs for Question Understanding</strong>。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&emsp;&emsp;未来的探索方向：</p>
<ol>
<li>整合更多的外部知识源，如clueweb；</li>
<li>以多任务学习方式训练MCCNN；</li>
<li>由于我们的模型能够检测到问题中最重要的单词，因此使用结果挖掘有效的问题模式将是非常有趣的。</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/22、Joint Relational Embeddings for Knowledge based Question Answering.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/22、Joint Relational Embeddings for Knowledge based Question Answering.html" class="post-title-link" itemprop="url">论文笔记：Joint Relational Embeddings for Knowledge-based Question Answering</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-06 13:37:44" itemprop="dateCreated datePublished" datetime="2019-07-06T13:37:44+08:00">2019-07-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-17 14:13:19" itemprop="dateModified" datetime="2019-07-17T14:13:19+08:00">2019-07-17</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/D14-1071" target="_blank" rel="noopener">论文地址</a>，发表于 2014 年。<br>&emsp;&emsp;将自然语言（natural language，NL）问题转换为对应的逻辑形式（logical form，LF）是基于知识库问答（KB-QA）任务的核心任务，转换问题也被称作语义分析。<del>在 KB-QA 任务领域，与以往（Mooney, 2007; Liang et al., 2011;Cai and Yates, 2013; Fader et al., 2013; Berant etal., 2013; Bao et al., 2014）<u>基于词汇化短语（lexicalized phrases）和逻辑谓语（logical predicates）之间的映射作为词汇触发器（lexical trigger）来执行语义分析中的转换任务</u>不同（其中 Fader 2013 提出的论文在<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html">论文笔记1</a>和<a href="https://yan624.github.io/论文/21、Question Answering with Subgraph Embeddings.html">论文笔记2</a>中具有提及，ctrl f 之后搜索 <em>Paraphrase-Driven Learning for Open Question Answering</em> 或者 <em>Fader</em> 即可找到对应位置）</del>，本论文进一步提出了一种<strong>将 NL 问题映射到 LFs 中</strong>的新的<strong>嵌入式</strong>方法，其利用<strong>词汇表达</strong>与 <strong>KB 中的属性</strong>在隐含空间中的语义关联来实现。实验表明，在两个公开的 QA 数据集上，该方法优于其他三种 KB-QA 的基线方法。<br>&emsp;&emsp;先前工作必须处理以下两种限制：</p>
<ol>
<li>由于逻辑谓语的含义通常具有不同的自然语言表达（natural language expression，NLE）形式，因此从谓语提取的词汇触发器可能有时会受到大小限制；</li>
<li>由于命名体识别（named entity recognition，NER）组件检测到的实体将用于与逻辑谓语一起组成逻辑形式，因此它们的类型也应该与谓语一致。然而，现有的 KB-QA 系统使用的 NER 组件大都独立于 NLE 到谓语的映射步骤。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;一如既往地（我为什么要说一如既往？因为前两篇论文笔记都记录了）说明<strong>语义分析</strong>有多糟糕，需要使用大量的人力，继而只能被限制在特定的领域（以后关于这些劣势都不写了）。<br>&emsp;&emsp;一如既往地描述了 FreeBase。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/22、Joint Relational Embeddings for Knowledge based Question Answering.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/21、Question Answering with Subgraph Embeddings.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/21、Question Answering with Subgraph Embeddings.html" class="post-title-link" itemprop="url">论文笔记：Question Answering with Subgraph Embeddings</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-05 15:58:26" itemprop="dateCreated datePublished" datetime="2019-07-05T15:58:26+08:00">2019-07-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-06 10:41:31" itemprop="dateModified" datetime="2019-08-06T10:41:31+08:00">2019-08-06</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1406.3676" target="_blank" rel="noopener">论文地址</a>，发表于 2014 年。<br>&emsp;&emsp;本文的作者在同年发表了另一篇论文，将上一篇论文称为 A，此论文称为 B，对于 A 论文我也做了<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html">论文笔记</a>，本论文是上一篇论文的改进版。A 只是对简单问题进行研究，B 研究如何改进模型并回答更复杂的问题。<br>&emsp;&emsp;开放域问答中的一流技术大致可以分为两大类：1)基于信息检索；2)基于语义解析。<strong>信息检索</strong>系统首先通过 KBs 的搜索 API（转换方式估计是手写模版，论文中未细说） <strong>将问题转换为有效的查询语句</strong>（比如 neo4j 数据库的 CQL）以此检索到大量的候选答案，然后再仔细地识别准确的答案（<a href="https://www.sciencedirect.com/science/article/pii/S0020025511003860" title="A survey on question answering technology from an information retrieval perspective" target="_blank" rel="noopener">Kolomiyets O 等 2011</a>，<a href="https://www2012.universite-lyon.fr/proceedings/proceedings/p639.pdf" title="Template-based Question Answering over RDF Data" target="_blank" rel="noopener">Unger C 等 2012</a>，<a href="https://www.aclweb.org/anthology/P14-1090" title="Information Extraction over Structured Data: Question Answering with Freebase" target="_blank" rel="noopener">Yao X 等 2014</a>）。<strong>语义解析</strong>旨在通过语义分析系统正确<strong>解释</strong>问题的含义，<strong>解释步骤</strong>的做法是把问题转换为数据库查询语句（这里的查询语句应该是逻辑形式，比如<strong>组合范畴法</strong>），以此查询到正确的答案。尽管这两种方法有能力去处理大规模知识库，但是需要专家手动的创建词汇、语法以及 KB 协议才能有所成效。且<strong>没有通用性</strong>，<strong>无法方便地扩展到</strong>具有其他模式、更广泛词汇或英语以外语言的<strong>新数据库</strong>。<br>&emsp;&emsp;相反，<a href="https://www.aclweb.org/anthology/P13-1158" target="_blank" rel="noopener">Paraphrase-Driven Learning for Open Question Answering</a> 提出了一个几乎不需要人工注释的开放域 QA 框架，虽然这是一种有趣的方法，但是它被其他方法超越了。即第二段提到的论文 A。<br>&emsp;&emsp;相比于论文 A，作者作出了以下几点<strong>改进</strong>：1）对于候选答案，考虑更多更长的路径（之前只考虑了 main entity 周围的节点）；2）对候选答案进行更有意义的表示：答案的表示包含问答路径以及周围的子图。</p>
<h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>&emsp;&emsp;假设所有潜在的答案都是 KB 中的实体，当 KB 中不存在该实体时，可以使用一些方法解决（论文中具体没说，只是说了一种极简单的方式：<em>When this entity is not given, plain string matching is used to perform entity resolution</em>）。<br>&emsp;&emsp;此外 N 代表词典的大小，其中 <script type="math/tex">N = N_W + N_S</script>，<script type="math/tex">N_W</script> 代表词嵌入的大小，<script type="math/tex">N_S</script> 代表实体和关系的数量。</p>
<h1 id="改进：考虑多维度的信息"><a href="#改进：考虑多维度的信息" class="headerlink" title="改进：考虑多维度的信息"></a>改进：考虑多维度的信息</h1><p>&emsp;&emsp;以下描述一个候选答案的特征表示，论文将以三个角度进行表示：</p>
<ol>
<li>Single Entity：此表示方式与上一篇论文一样，没什么讲究。就是 Freebase 中的一个实体，<script type="math/tex">\psi(a)</script> 代表答案的 1-of-<script type="math/tex">N_S</script>（one hot）表示；</li>
<li>Path Representation：答案被认为是一条 path，该 path 从<strong>问题中被提及的实体</strong>到<strong>答案实体</strong>。此实验中，考虑 1-hop 或者 2-hops 级别的 path。比如，(barack obama, people.person.place of birth, honolulu) 是 1-hop path，(barack obama, people.person.place of birth, location. location.containedby, hawaii) 是 2-hop path。这导致了 <script type="math/tex">\psi(a)</script> 代表 3-of-<script type="math/tex">N_S</script> 或者 4-of-<script type="math/tex">N_S</script> 的向量，至于为什么是 *-of-<script type="math/tex">N_S</script>，显而易见。</li>
<li>Subgraph Representation：我们将 2 中的 <strong>Path</strong> 和连接候选答案的整个<strong>子图</strong>进行编码。<em>具体看论文，写的有点看不懂</em>。</li>
</ol>
<p>&emsp;&emsp;我们的假想是将所有的信息都编码进表示以提高结果，但是这不大可能。所以还是采用将子图编码进表示的方法。下图即为实验的模型，右下角显示了编码方式。<br><img src="https://img-blog.csdn.net/20171101002818501?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTEFXXzEzMDYyNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="架构图" title="架构图"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;与论文 A 差不多，多了一个多任务训练，其他的细枝末节没仔细看。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/21、Question Answering with Subgraph Embeddings.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/20、Open Question Answering with Weakly supervised Embedding Models.html" class="post-title-link" itemprop="url">论文笔记：Open Question Answering with Weakly supervised Embedding Models</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-05 10:54:15" itemprop="dateCreated datePublished" datetime="2019-07-05T10:54:15+08:00">2019-07-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-05 21:38:57" itemprop="dateModified" datetime="2019-08-05T21:38:57+08:00">2019-08-05</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
<span class="post-meta-divider">|</span>
	浏览量：<span id="busuanzi_value_site_uv"></span>人次
</span>
				
          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1404.4326" target="_blank" rel="noopener">论文地址</a>，论文发表于 2014 年。<br>&emsp;&emsp;建立一个能够回答任何问题的计算机是人工智能的一个长期目标。这一领域一个重要的发展时大规模知识库的建立，如 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Freebase</a> 和 <a href="https://content.iospress.com/download/semantic-web/sw134?id=semantic-web%2Fsw134" target="_blank" rel="noopener">DBPedia</a>，它们存储了大量的通用信息。它们由三元组的形式构成一个数据库，通过各种关系和格式连接成实体对。那么回答问题被定义为<strong>给定一个用自然语言表达的查询语句</strong>（一个查询语句的例子：中国的首都在哪？）<strong>从知识库中检索正确的实体或实体集的任务</strong>。<br>&emsp;&emsp;最近，通过将问题映射为<strong>逻辑形式</strong>或者类似<strong>数据库查询</strong>的方法取得了富有希望的进展。虽然这种方法可能有效，但是缺点是要采用大量的人为标记的数据或者需要工作人员定义词汇表和语法。<br>&emsp;&emsp;本文采用一种激进的学习方式，将问题映射为向量（无法人为解释）的特征表示。并且将重点放在回答一些基于比较宽泛的主题的简单事实性问题。这项任务的难点来自词汇的多样性，而不是句法的复杂性。<br>&emsp;&emsp;该方法采用随机梯度下降，然后使用 fine-tuning 进行训练。经验表明该模型能够捕获一些有意义的信号，且这是唯一一种能够在弱标记数据上训练的方法。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><ol>
<li>Section 2：讨论了之前的工作；</li>
<li>Section 3：介绍了开放域问答的问题；</li>
<li>Section 4：给出了模型；</li>
<li>Section 5：实验结果。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li>大规模的问答历史悠久，主要由 TREC tracks（<a href="https://arxiv.org/pdf/cs/0110053.pdf" target="_blank" rel="noopener">Voorhees 2000</a>） 发起，这是第一个成功地<strong>将问题转换为查询</strong>的问答系统。将问题转换为查询之后，又<strong>将查询提供给 web 搜索引擎</strong>，然后<strong>从返回的页面或片段中取出答案</strong>（<a href="http://aiweb.cs.washington.edu/research/projects/ai3/mulder/mulder-www10.pdf" target="_blank" rel="noopener">Kwok 2001</a>, <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-06/SS02-06-002.pdf" target="_blank" rel="noopener">Banko 2002</a>）。这种方法需要大量的人工操作来处理查询，然后解析和搜索结果。</li>
<li>大型 KBs 的出现，如 FreeBase 和 DBPedia，改变了上述状况，但是也带来巨大的挑战。语言的多样性以及 KBs 规模的庞大，使得需要通过监督学习来处理大量的<strong>带标签</strong>的数据。最早的方法是基于手写模板的 KBs 开放问答，然而对于日新月异 KBs（增加/删除三元组和实体） 还不够健壮。于是在 KBs 出现之后开始尝试使用较少的监督情况下<strong>学习 KBs 和自然语言之间的联系</strong>，但是这项工作实际上在解决<strong>信息提取</strong>的问题（<a href="https://www.aclweb.org/anthology/P09-1113" title="Distant supervision for relation extraction without labeled data" target="_blank" rel="noopener">Mintz M 等 2009</a>，<a href="https://www.aclweb.org/anthology/P11-1055" title="Knowledge-Based Weak Supervision for Information Extractionof Overlapping Relation" target="_blank" rel="noopener">Hoffmann R 等 2011</a>，<a href="https://www.aclweb.org/anthology/D12-1093" title="Reading The Web with Learned Syntactic-Semantic Inference Rules" target="_blank" rel="noopener">Lao N 等 2012</a>，<a href="https://www.aclweb.org/anthology/N13-1008" title="Relation Extraction with Matrix Factorization and Universal Schemas" target="_blank" rel="noopener">Riedel S 等 2013</a>）。</li>
<li>近年来，有一种基于语义解析器（<a href="https://www.aclweb.org/anthology/P13-1042" title="Large-scale Semantic Parsing via Schema Matching and Lexicon Extension" target="_blank" rel="noopener">Cai Q 等 2013</a>，<a href="https://www.aclweb.org/anthology/D13-1160" title="Semantic Parsing on Freebase from Question-Answer Pairs" target="_blank" rel="noopener">Berant J 等 2013</a>，<a href="https://www.aclweb.org/anthology/D13-1161" title="Scaling Semantic Parsers with On-the-fly Ontology Matching" target="_blank" rel="noopener">Kwiatkowski T 等 2013</a>）的新的问答系统被提出，它只具有少量标记数据。但仍需要耗费大量精力去仔细设计词汇，语法和知识库。</li>
<li>所以本文（2014 年）提出了基于嵌入式的问答模型。据我们所知，这是以前从未尝试过的。</li>
</ol>
<h1 id="开放域问答"><a href="#开放域问答" class="headerlink" title="开放域问答"></a>开放域问答</h1><p>&emsp;&emsp;本文使用 <a href="https://www.aclweb.org/anthology/P13-1158" target="_blank" rel="noopener">Fader 2013</a> 的问答框架，并使用了相同的数据。</p>
<h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>&emsp;&emsp;我们将回答问题的任务看作为：给定一个问题 q，对应的答案由 KB 中的三元组 t 给出。这意味着我们的问题由<strong>一组三元组 t</strong> 提供对问题及其答案的解释，例如：</p>
<blockquote>
<p>q: What environment does a dodo live in?（渡渡鸟生活在什么样的环境中？）<br>t: (dodo.e, live-in.r, makassar.e)<br>q: What are the symbols for Hannukah?（光明节的象征是什么？）<br>t: (menorah.e, be-for.r, hannukah.e)<br>q: What is a laser used for?（极光可以用来做什么？）<br>t: (hologram.e,be-produce-with.r,laser.e)</p>
</blockquote>
<p>&emsp;&emsp;这里每个问题我们只给出一个 t，但是实际上它可以有很多，所以上文说是一组三元组。本文其余部分，<strong>使用 <script type="math/tex">\kappa</script>（读作 kappa） 代表 KB ，使用 <script type="math/tex">\epsilon</script> 代表 KB 中的实体或者关系。问题的词表用 V 表示，<script type="math/tex">n_V</script> <script type="math/tex">n_{\epsilon}</script>分别表示 V 和 <script type="math/tex">\epsilon</script> 的大小</strong>。<br>&emsp;&emsp;我们的模型在于<strong>函数 S(·)</strong>，它可以为 question-answer triple pairs (q,t) 打分。因此，找到问题 q 的 top-ranked 的答案 <script type="math/tex">\hat{t}</script>(q) 直接由以下公式得出：</p>
<script type="math/tex; mode=display">
\hat{t}(q) = arg \max_{t' \in \kappa}S(q, t')</script><p>&emsp;&emsp;为了处理多个答案，我们将结果呈现为排完序的列表并对其评分，而不是直接采用最前面的预测结果。<br>&emsp;&emsp;使用评分函数可以直接查询 KB，而不需要在<strong>语义分析系统</strong>中一样为问题定义一个中间的结构化逻辑表示。我们的目标是学习 S(·)，余下将讲述用于训练的数据的创建步骤。</p>
<h2 id="用于训练的数据"><a href="#用于训练的数据" class="headerlink" title="用于训练的数据"></a>用于训练的数据</h2><div class="note info">
            <p>待续</p>
          </div>
<h1 id="Embedding-based-model"><a href="#Embedding-based-model" class="headerlink" title="Embedding-based model"></a>Embedding-based model</h1><p>&emsp;&emsp;模型使用了词嵌入（2019 年了，应该谁都知道了，不做解释）。</p>
<h2 id="Question-KB-Triple-Scoring"><a href="#Question-KB-Triple-Scoring" class="headerlink" title="Question-KB Triple Scoring"></a>Question-KB Triple Scoring</h2><p>&emsp;&emsp;我们的框架关注的是函数 S(q,t) 的学习，该函数的目的是对一个<strong>问题 q</strong> 和 一个<strong>来自 <script type="math/tex">\kappa</script> 的三元组 t</strong> 进行打分。该评分方法受到了先前工作 labeling images withwords 的启发（<a href="https://link.springer.com/content/pdf/10.1007/s10994-010-5198-3.pdf" target="_blank" rel="noopener">Weston 2013</a>），我们采用该方法将图片和标签替换成了问题和三元组。直观来讲就是：<br>有点难翻译，故给出原文：</p>
<blockquote>
<p>&emsp;&emsp;Intuitively, it consists of projecting questions, treated as a bag of words(and possibly n-grams as well), on the one hand, and triples on the other hand,into a shared embedding space and then computing a similarity measure (the dot  product  in  this  paper)  between  both  projections.<br>&emsp;&emsp;大致意思，将问题和三元组使用词袋模型（也可以是 n-gram 模型）投射到共享的嵌入空间，然后计算二者的相似度（本文使用点积的方式）。</p>
</blockquote>
<p>&emsp;&emsp;那么评分函数为:</p>
<script type="math/tex; mode=display">
S(q,t) = f(q)^Tg(t)</script><p>&emsp;&emsp;<strong>其中 f(·) 将问题中的单词映射到 <script type="math/tex">\mathbb{R}^{\kappa}</script>，<script type="math/tex">f(q) = V^T \Theta(q)</script>。V 是关于 <script type="math/tex">\mathbb{R}^{n_v \times \kappa}</script> 包含所有词嵌入 v 的矩阵。<script type="math/tex">\Theta(q)</script>是 q（<script type="math/tex">\in \{0,1\}^{n_v}</script>） 的二进制（稀疏）表示。同样，g(·) 将 KB 三元组中的实体和关系映射到 <script type="math/tex">\mathbb{R}^{\kappa}</script>，<script type="math/tex">g(t) = W^T\Psi(t)</script>，W 是关于 <script type="math/tex">\mathbb{R}^{n_e \times \kappa}</script> 包含所有实体和关系的嵌入 w 的矩阵，<script type="math/tex">\Psi(t)</script> 是 t（<script type="math/tex">\in \{0,1\}^{n_e}</script>） 的二进制（稀疏）表示。</strong><br><div class="note info">
            <p>&emsp;&emsp;注：上一段太长了，解释一下。f(q) 就是词向量，g(t) 就是实体和关系的向量（下一段原文写到 g(t) 是将三元组中的嵌入全部相加）。</p>
          </div></p>
<p>&emsp;&emsp;将单词表示为词袋模型似乎有一点局限性，但是由于我们特定的设置，语法都很简单，因此含有的信息十分有限，所以词袋模型应该也能带来不错的性能。当然也有反例，比如 <em>What are cats afraid of ?vs.What are afraid of cats ?</em> 这将会有不同的答案。不过这种情况十分罕见。未来考虑将 parse tree features 或者 semantic role labels 作为输入放入嵌入模型中。<br>&emsp;&emsp;与以前的工作（<a href="https://arxiv.org/pdf/1307.7973" target="_blank" rel="noopener">Weston 2013</a>）不同的是，在我们的模型中，实体出现三元组的不同侧面（左右侧）时，实体并非拥有相同的嵌入。KB 中的关系并不是对称的，所以会出现三元组中左侧和右侧的实体是不同的情况。<strong>由于 g(·) 是将三元组中的所有成分相加，所以每一个实体我们都需要两个嵌入</strong>。<br>&emsp;&emsp;这样就可以很容易地对任何三元组进行评分：</p>
<script type="math/tex; mode=display">
\hat{t}(q) = arg \max_{t' \in \kappa}S(q, t') = arg \max_{t' \in \kappa}(f(q)^Tg(t'))</script><p>&emsp;&emsp;接下来花了好几段讲怎么训练。</p>
<h2 id="Fine-tuning-the-Similarity-between-Embeddings"><a href="#Fine-tuning-the-Similarity-between-Embeddings" class="headerlink" title="Fine-tuning the Similarity between Embeddings"></a>Fine-tuning the Similarity between Embeddings</h2><p>&emsp;&emsp;由于受到数据大小的限制，需要使用微调来改进性能。</p>
<h1 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h1><p>&emsp;&emsp;通读论文之后还是有点搞不清论文是怎么训练的，后来看了一下 CCF ADL100 刘康老师的 PPT ，感觉有点理解了，以下是训练步骤：</p>
<ol>
<li>输入自然语言表达的问题，比如：姚明的老婆的是哪里人？</li>
<li>使用 entity linking（论文中貌似没有这步，我在看 PPT 时也是一知半解，好在前几天我刚好在一篇论文中看到了这个 entity linking！<a href="https://yan624.github.io/论文/19、Semantic Parsing via Staged Query Graph Generation：Question Answering with Knowledge Base.html#链接主题实体">博客地址</a>，entity linking 源于<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）找到 main entity，main entity 周围的 entity 均是候选 entity。如下图，姚明是 main entity，姚明周围的实体都算作候选 entity，比如叶莉、火箭队、上海等。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Open Question Answering with Weakly supervised Embedding Models/姚明的老婆是谁的知识图谱的子图.jpg" alt="姚明的老婆是谁的知识图谱的子图"></li>
<li>计算问题和候选 entity 的相似度，其中问题由词向量表示，候选 entity 是一个三元组的形式，难以直接用词向量表示，方法是将三元组中的三个对象分别用词向量表示，然后将三个词向量相加。这样就得到了问题的词向量和 entity 的词向量，点乘获得相似度。</li>
<li>由于候选 entity 不一定只有一个，所以可以获得多个相似度。进行排序即可获得最相似的候选 entity。</li>
</ol>
<div class="note danger">
            <p>&emsp;&emsp;以上的训练步骤并不是论文中的训练步骤，只是我为了给自己加深映像写的，具体的训练步骤在原论文第 4 节，具体在 4.1。</p>
          </div>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/20、Open Question Answering with Weakly supervised Embedding Models.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">记录学习问题，积累做的 leetcode 题目</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">117</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">18</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
            <br>
						<!--同个ip访问本站页面次数-->
						<div class="site-state-item site-state-posts" style="border-left:none;">
								<span class="site-state-item-count" id="busuanzi_value_site_pv"></span>
								<span class="site-state-item-name">浏览量</span>
						</div>
						<!--不同ip访问本站次数-->
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count" id="busuanzi_value_site_uv"></span>
								<span class="site-state-item-name">访客量</span>
						</div>
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count">101k</span>
								<span class="site-state-item-name">总字数</span>
						</div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        








        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

	<script src="/lib/my-utils.js"></script>
<!--图片缩放插件-->
<script src="/lib/zoomify/zoomify.min.js"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!--不蒜子统计-->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(6), {
		duration:90000,//1min半一换
		fade: 1500
	});
</script>
<!-- 图片缩放 -->
<script>
$('#content img').zoomify({duration: 500, });
  $('#content img').on('zoom-in.zoomify', function () {
    $('#sidebar').css('display', 'none');
  });
  $('#content img').on('zoom-out-complete.zoomify', function () {
    $('#sidebar').css('display', '');
  });
</script>

	

</body>
</html>
