<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2">























  

<link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.10.2/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


<!--图片缩放插件样式-->
<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">

  <meta name="description" content="记录学习问题，积累做的 leetcode 题目">
<meta name="keywords" content="博客，java，javaWeb，NLP，python，机器学习，深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/page/2/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="记录学习问题，积累做的 leetcode 题目">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="记录学习问题，积累做的 leetcode 题目">






  <link rel="canonical" href="http://yan624.github.io/page/2/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
	<!--加载flower canvas-->
<script>
var pathname = window.location.pathname;
if(pathname == '/flower.html'){
	var body =  document.getElementsByTagName('body')[0];
	var canvas = document.createElement("canvas")
	canvas.setAttribute('id', 'sakura')
	// '<canvas id="sakura"></canvas>'
	body.appendChild(canvas)
}
</script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">低阶炼金术士</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">19</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">23</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">124</span></a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/【NLP算法】（三）条件随机场CRF.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/【NLP算法】（三）条件随机场CRF.html" class="post-title-link" itemprop="url">【NLP算法】（三）条件随机场CRF</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-09-07 17:24:43" itemprop="dateCreated datePublished" datetime="2019-09-07T17:24:43+08:00">2019-09-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-11 21:18:39" itemprop="dateModified" datetime="2019-09-11T21:18:39+08:00">2019-09-11</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>&emsp;&emsp;<a href="https://zhuanlan.zhihu.com/p/44042528" target="_blank" rel="noopener">一篇不错的 CRF 入门文章</a>，但是后面求所有路径的分数的算法，说得有点模糊，推荐看<a href="https://www.bilibili.com/video/av52626653/?p=4" target="_blank" rel="noopener">HMM与CRF隐形马尔可夫链与条件随机场</a>，虽然这个视频播放量不高，但足以看懂 CRF 最后的那个算法。<br>&emsp;&emsp;<a href="https://zhuanlan.zhihu.com/p/27338210" target="_blank" rel="noopener">Bi-LSTM-CRF for Sequence Labeling</a><br>&emsp;&emsp;<a href="https://blog.csdn.net/cuihuijun1hao/article/details/79405740" target="_blank" rel="noopener">Pytorch Bi-LSTM + CRF 代码详解</a></p>
<h1 id="马尔科夫链——Markov-Chain"><a href="#马尔科夫链——Markov-Chain" class="headerlink" title="马尔科夫链——Markov Chain"></a>马尔科夫链——Markov Chain</h1><p>&emsp;&emsp;有时也称为显马尔科夫模型（observed Markov model）。<br>&emsp;&emsp;是一个随机过程模型，它表述了一系列可能的事件，在这个系列当中每一个事件的概率仅依赖于前一个事件。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/【NLP算法】（三）条件随机场CRF/马尔科夫链.jpg" alt="马尔科夫链"></p>
<h1 id="隐马尔科夫模型——HMM"><a href="#隐马尔科夫模型——HMM" class="headerlink" title="隐马尔科夫模型——HMM"></a>隐马尔科夫模型——HMM</h1><h2 id="概率模型"><a href="#概率模型" class="headerlink" title="概率模型"></a>概率模型</h2><p>&emsp;&emsp;概率模型就是将学习任务归结于计算变量的概率分布的模型。<br>&emsp;&emsp;在生活中，我们经常会根据一些已经观察到的现象来推测和估计未知的东西——这种需求，恰恰是概率模型的推断（Inference）行为所作的事情。<br>&emsp;&emsp;推断的本质是：利用可观测变量来预测未知变量的条件分布。<br>&emsp;&emsp;概率模型可以分为两类：生成模型（generative model）和判别模型（discriminative model）。<br>&emsp;&emsp;我们将可观测变量的集合命名为 O，我们感兴趣的未知变量的集合命名为 Y。<br>&emsp;&emsp;生成模型学习出来的是 O 与 Y 的联合概率分布 P(O,Y)，而判别模型学习的是条件概率分布 P(Y|O)。<br>&emsp;&emsp;例如朴素贝叶斯模型就是生成模型，而逻辑回归就是判别模型。</p>
<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><p>&emsp;&emsp;<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/【NLP算法】（三）条件随机场CRF.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/35、Seq2sql：Generating structured queries from natural language using.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/35、Seq2sql：Generating structured queries from natural language using.html" class="post-title-link" itemprop="url">论文笔记：Seq2sql: Generating structured queries from natural language using reinforcement learning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-31 21:04:02" itemprop="dateCreated datePublished" datetime="2019-08-31T21:04:02+08:00">2019-08-31</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-11-04 16:22:07" itemprop="dateModified" datetime="2019-11-04T16:22:07+08:00">2019-11-04</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1709.00103.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。<br>&emsp;&emsp;这篇论文发布了 wikisql 数据集，同时提出了 seq2sql 任务。</p>
          </div>
<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;本文主要做出两项贡献：1）提出 Seq2SQL，将自然语言问题翻译为其对应的 SQL queries。2）发布 WikiSQL 语料库，其包含 80654 个人工标注的自然语言问题实例， SQL queries 以及从 24241 张 HTML 网页中提取的 SQL 表（网页来自 Wikipedia）。<em>WikiSQL 比以前提供给 logical forms 和自然语句的语义分析数据集大一个数量级</em>。发布 WikiSQL 的同时，我们还发布了一个此数据库的查询引擎（query execution engine）<br><div class="note primary">
            <p>&emsp;&emsp;本论文将自然语言转为 SQL，关系型数据库。而知识图谱是非关系型数据库存储的。</p>
          </div><br>&emsp;&emsp;关系型数据库存储了大量的信息并用此构建了许多应用，但是访问关系型数据库需要使用 sql 语句并且很难精通它。于是 Natural language interfaces(NLI) 寻求一条路径使得人类和计算机交互成为可能，即将自然语言翻译为 sql 语句。<br>&emsp;&emsp;balabala…<br>&emsp;&emsp;在 wikisql 数据集上，seq2sql 比先前 Dong &amp; Lapata(2016) 做的语义解析模型效果要好。</p>
<h1 id="2-Model"><a href="#2-Model" class="headerlink" title="2 Model"></a>2 Model</h1><p>&emsp;&emsp;我们的基准模型是 Dong &amp; Lapata(2016) 做的 seq2seq + attention 的模型，它在未使用人工语法的语义解析数据集上实现了最高的性能。<strong>但是这个 seq2seq 模型的 softmax 的输出空间对于这个任务太大了</strong>。（博主注：生成 sql 语句时，并不需要在整个字典中找。sql 语句在某些地方是固定的。比如 select balabala from balabala，格式都是固定的，比如 select，count 等）因此我们可以将生成序列的输出空间限制为 <strong>table schema, question utterance, and SQL key words的并集</strong>。最终模型类似于加入了 augmented inputs 的 <strong>pointer network</strong>。我们</p>
<ol>
<li>首先描述 augmented pointer network model；</li>
<li>其次说明我们定义 seq2sql 的局限性，特别是在生成<em>无序查询条件</em>方面。</li>
</ol>
<h2 id="augmented-pointer-network-model"><a href="#augmented-pointer-network-model" class="headerlink" title="augmented pointer network model"></a>augmented pointer network model</h2><h2 id="seq2sql"><a href="#seq2sql" class="headerlink" title="seq2sql"></a>seq2sql</h2><h1 id="博主注"><a href="#博主注" class="headerlink" title="博主注"></a>博主注</h1><p>&emsp;&emsp;论文提出了 seq2sql 模型，为后面的工作铺垫了基础。基线模型是 Dong 2016 年提出的 seq2seq + attention 模型，seq2sql 为第二个模型。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/35、Seq2sql：Generating structured queries from natural language using.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文代码/论文复现：Language to Logical Form with Neural Attention.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文代码/论文复现：Language to Logical Form with Neural Attention.html" class="post-title-link" itemprop="url">论文复现：Language to Logical Form with Neural Attention</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-26 23:32:16" itemprop="dateCreated datePublished" datetime="2019-08-26T23:32:16+08:00">2019-08-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-01 12:56:18" itemprop="dateModified" datetime="2019-09-01T12:56:18+08:00">2019-09-01</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><p>&emsp;&emsp;论文的地址<a href="https://arxiv.org/pdf/1601.01280.pdf" target="_blank" rel="noopener">在此</a>，作者使用了 Lua 语言实现，代码地址<a href="https://github.com/donglixp/lang2logic" target="_blank" rel="noopener">在这</a>，然而我不会 Lua 语言，于是找了找是否有 Python 的实现版本。还真有，Python 版本代码地址<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch" target="_blank" rel="noopener">在这</a>。<br>&emsp;&emsp;但是 Python 版本的代码<strong>篇幅太长</strong>，且<strong>几乎没有注释</strong>，于是我将其重写了一遍，一些工具类是直接复制别人的，但是核心代码我改写了一下，并添加了一些注释。<br><div class="note info">
            <p>&emsp;&emsp;我将里面的数据获取模块移除了。</p>
          </div></p>
<h1 id="论文实现"><a href="#论文实现" class="headerlink" title="论文实现"></a>论文实现</h1><p>&emsp;&emsp;论文共用了两个办法：1）普通 seq2seq 模型；2）作者自创的 seq2tree 模型。其中每个模型又分别有 <strong>lstm 实现</strong>和 <strong>lstm + attention 实现</strong>两种版本。虽然两个版本使用的技术不同，但是说到底也只是同一个模型。以下讲解原理。</p>
<h2 id="seq2seq-模型"><a href="#seq2seq-模型" class="headerlink" title="seq2seq 模型"></a>seq2seq 模型</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>&emsp;&emsp;论文中使用 LSTM 实现 seq2seq 模型，训练之后，accuracy 大约在 70%。</p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>&emsp;&emsp;我自己用了 Transformer 改写了一下，并且调了几天的参数，发现效果出奇的差，accuracy 最好只有 18%。然后我还发现，对于短句子几乎是百分比预测正确，对于长句子百分比预测错误。所以<strong>我怀疑是否是位置编码那产生的问题，考虑到一个逻辑形式它并不是纯粹的线性结构，它的内部是由很多括号的</strong>。<br>&emsp;&emsp;经调参后得到最好的一组参数如下：</p>
<ol>
<li>learning rate: 0.001</li>
<li>dim_feedforward: 随意（我设置为 256）</li>
<li>h_model: 256</li>
<li>nhead: 4</li>
<li>encoder_layer/decoder_layer: 1</li>
<li>dropout: 0.4</li>
<li>batch_size: 32（16 的效果可能更好）</li>
<li>epoch: 95（epoch 可以进一步修改）</li>
<li>src_mask: False</li>
<li>tgt_mask: True</li>
<li>memory_mask: False</li>
</ol>
<h2 id="seq2tree-模型"><a href="#seq2tree-模型" class="headerlink" title="seq2tree 模型"></a>seq2tree 模型</h2><p>&emsp;&emsp;</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文代码/论文复现：Language to Logical Form with Neural Attention.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/pytorch踩坑总结.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/pytorch踩坑总结.html" class="post-title-link" itemprop="url">pytorch踩坑总结</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-14 15:04:17 / 修改时间：15:22:56" itemprop="dateCreated datePublished" datetime="2019-08-14T15:04:17+08:00">2019-08-14</time>
            

            
              

              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <ol>
<li><code>tensor.view()</code> 相当于numpy中resize（）的功能，但是用法可能不太一样。</li>
<li>关于 nn.LSTM 的用法：如果不想手动初始化隐藏状态，而是想让 pytorch 帮你初始化，那么就只需要填入输入值即可。举个例子，下面的代码输入向量所对应的自然语句是 [‘how are you’, ‘i’m fine’]。<br>其中输入值 x 的形状为 (seq_len, batch_size, input_size)，比如上面的例子就是 (3, 2, 300)，代表序列长度为 3（因为上面的两句话的最大长度为 3），批量大小为 2，词向量维度为 300。注意一点：<strong>pytorch 会根据你输入的序列长度</strong>（输入的张量必须是一样长的，参差不齐的张量无法输入，当然了，你也无法创建出这样的张量）<strong>自动帮你做时间步上的计算，比如你输入的序列长度为 20，那么它会自动在神经网络的第一层计算 20 次，再返回结果给你。</strong>如果看不懂这里是什么意思，说明你不会 LSTM。可以再看看 LSTM 是怎么计算的。<figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cell</span> = nn<span class="variable">.LSTM</span>(input_size, hidden_size, num_layers)</span><br><span class="line"><span class="keyword">output</span>, (a, m) = <span class="keyword">cell</span>(x)</span><br></pre></td></tr></table></figure>
</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/pytorch踩坑总结.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/机器学习的下一步.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/机器学习的下一步.html" class="post-title-link" itemprop="url">机器学习的下一步</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-13 16:56:12 / 修改时间：17:23:16" itemprop="dateCreated datePublished" datetime="2019-08-13T16:56:12+08:00">2019-08-13</time>
            

            
              

              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <ul>
<li>机器学习能不能知道“我不知道”<br>机器学习的 classifier 可以判断一张图片是不是猫，但是能不能判断出“我不知道这是什么”？这项技术叫做 <strong>Anomaly Detection</strong>。</li>
<li>机器说出为什么“我知道”<ul>
<li>神马汉斯的例子</li>
<li>马辨识器的例子。机器只是辨识了英文字母<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/机器学习的下一步/说出为什么“我知道”.jpg" alt="说出为什么“我知道”"></li>
</ul>
</li>
<li>机器的错觉？<ul>
<li>adversarial attack。感觉是 CV 里的技术<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/机器学习的下一步/机器的错觉.jpg" alt="机器的错觉"></li>
</ul>
</li>
<li>终身学习（Life-long Learning）<br>机器能否终身学习。现在模型一般只能对应一个任务，如果让一个模型去学习下围棋，之后再让它去学习玩星海。那么它就不会下围棋了。这被为 <strong>Catastrophic Forgetting</strong>。</li>
<li>学习如何学习<br>如何写一个<strong>能够写出具有学习能力的程序</strong>的程序。这被称为 <strong>Meta-learning/Learn to learn</strong>。</li>
<li>一定需要很多训练数据吗？<ul>
<li>Few-shot learning</li>
<li>Zero-shot learning</li>
</ul>
</li>
<li>Reinforcement learning</li>
<li>神经网络压缩（Network Compression）<ul>
<li>把大神经网络路缩小</li>
<li>参数二元化<ul>
<li>所有的参数都变成 +1 或 -1</li>
</ul>
</li>
</ul>
</li>
<li>如果训练数据和测试数据长得不一样<ul>
<li>对于 CV 来说，训练数据和测试数据长得差不多，比如手写体识别。但是如果在真实场景中，测试数据是彩色的，可能会出现准确率骤降的情况。那么如何解决呢？
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/机器学习的下一步.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </li></ul></li></ul></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/【机器学习算法】半监督学习.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/【机器学习算法】半监督学习.html" class="post-title-link" itemprop="url">【机器学习算法】半监督学习</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-13 16:22:06 / 修改时间：16:56:44" itemprop="dateCreated datePublished" datetime="2019-08-13T16:22:06+08:00">2019-08-13</time>
            

            
              

              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>&emsp;&emsp;半监督学习就是在已有的带标签的数据之后，还有一组不带标签的数据。一般来说，在做无监督学习时，unlabeled data 远大于 labeled data。<br>&emsp;&emsp;半监督学习一般分为两种：</p>
<ul>
<li>Transductive learning：unlabeled data 就是你的 testing sdata</li>
<li>Inductive learning：unlabeled data 不是你的 testing sdata</li>
</ul>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/【机器学习算法】半监督学习/导读.jpg" alt="导读"></p>
<h2 id="为什么做半监督学习"><a href="#为什么做半监督学习" class="headerlink" title="为什么做半监督学习"></a>为什么做半监督学习</h2><p>&emsp;&emsp;有人说机器学习训练数据很少，其实不完全对。因为只是 labeled data 少，unlabeled data 随处可见。所以如果能将这些 unlabeled data 运用进去就好了。原因如下：</p>
<ul>
<li>搜集数据很简单，但是搜集 labeled data 代价昂贵</li>
<li>生活中，我们自己也在做半监督学习</li>
</ul>
<h1 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h1><h1 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h1><h1 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h1><h1 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h1>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/34、Coarse-to-Fine Decoding for Neural Semantic Parsing.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/34、Coarse-to-Fine Decoding for Neural Semantic Parsing.html" class="post-title-link" itemprop="url">论文笔记：Coarse-to-Fine Decoding for Neural Semantic Parsing</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-10 14:32:43" itemprop="dateCreated datePublished" datetime="2019-08-10T14:32:43+08:00">2019-08-10</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-16 16:30:57" itemprop="dateModified" datetime="2019-10-16T16:30:57+08:00">2019-10-16</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;提出一个结构感知的神经架构，将语义解析过程分解为如下两个步骤：给定一个输入语句，1）首先生成它含义的粗略草图（a rough sketch of its meaning），其中低级信息<strong>被掩盖</strong>（如<strong>变量名和参数</strong>）。2）然后考虑输入本身和草图来填充丢失的细节。<br>&emsp;&emsp;RNN 在多种 NLP 任务中的成功应用对 seq2seq 的语义解析产生了强大的冲击力，如<a href="https://arxiv.org/pdf/1606.03622.pdf" title="Data recombination for neural semantic parsing" target="_blank" rel="noopener">Jia and Liang, 2016</a>; Dong and Lapata, 2016; <a href="https://arxiv.org/pdf/1603.06744.pdf" title="Latent predictor networks for code generation" target="_blank" rel="noopener">Ling et al., 2016</a>。<br>&emsp;&emsp;我们认为，这种方法至少有三个优点。首先，分解步骤<strong>将高级语义信息与低级语义信息分离开来</strong>，使译码器能够在不同的粒度级别对语义进行建模。其次，模型可以明确地为具有相同草图（即基本含义）的示例共享粗糙结构的知识，即使它们的实际含义表示不同（例如，由于不同的细节）。第三，在生成草图后，解码器知道语句的基本含义是什么，<strong>模型可以将其作为全局上下文来改进对最终细节的预测</strong>。<br>&emsp;&emsp;使用如下数据集：</p>
<ol>
<li>GEO</li>
<li>ATIS</li>
<li>DJANGO</li>
<li>WikiSQL</li>
</ol>
<h1 id="问题阐释"><a href="#问题阐释" class="headerlink" title="问题阐释"></a>问题阐释</h1><p>&emsp;&emsp;定义 <script type="math/tex">x = x_1 \dots x_{|x|}</script> 为自然语句，<script type="math/tex">y = y_1 \dots y_{|y|}</script>为意义表示，<script type="math/tex">a = a_1 \dots a_{|a|}</script>为 sketch 表示。<del>注意 <strong>sketch 的定义为</strong>：一个中间变量，如将自然语句转化为 Logical Form，Source Code，SQL，noSQL，SPARQL等表示，这些表示都算是一个 sketch。</del>下图论文架构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Coarse-to-Fine Decoding for Neural Semantic Parsing/Coarse2Fine架构.jpg" alt="Coarse2Fine架构"></p>
<h2 id="Sketch-Generation"><a href="#Sketch-Generation" class="headerlink" title="Sketch Generation"></a>Sketch Generation</h2><p>&emsp;&emsp;encoder 将自然语句编码为向量，decoder 去计算 <script type="math/tex">p(a|x)</script> 从而通过encoding 向量 生成 sketch a。具体来讲，<strong>Input Decoder</strong> 将字转为词向量，并使用 Bi-LSTM 训练。<strong>Coarse Meaning Decoder</strong> 生成 sketch a，也使用 LSTM 并且加上 attention 机制。</p>
<h2 id="Meaning-Representation-Generation"><a href="#Meaning-Representation-Generation" class="headerlink" title="Meaning Representation Generation"></a>Meaning Representation Generation</h2><p>&emsp;&emsp;Meaning representation 由输入 x 以及生成的 sketch a 预测产生，具体就是计算 <script type="math/tex">p(y|x,a)</script>。<strong>Sketch Encoder</strong> 与 Input Decoder 类似，使用 Bi-LSTM 并将 sketch a 映射为词向量。<strong>Fine Meaning Decoder</strong> 与 Coarse Meaning Decoder 类似。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;总的来说就是先用自然语句生成 coarse sketch，然后再用 coarse sketch 生成 fine sketch。一共使用了两个 encoder-deocder 模型，但是将这两个模型连起来用。</p>
<h1 id="三个语义分析任务"><a href="#三个语义分析任务" class="headerlink" title="三个语义分析任务"></a>三个语义分析任务</h1><p>&emsp;&emsp;为了证明我们的框架适用于跨域和意义表示，我们为三个任务开发了模型，即将自然语言解析为逻辑形式、Python 源代码和 SQL 查询。对于每一个任务，我们都描述了使用的数据集以及 sketch 提取的提取步骤。</p>
<h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><p>&emsp;&emsp;</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/34、Coarse-to-Fine Decoding for Neural Semantic Parsing.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/深度学习算法（四）：Transformer.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/深度学习算法（四）：Transformer.html" class="post-title-link" itemprop="url">深度学习算法（四）：Transformer</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-07 10:55:15" itemprop="dateCreated datePublished" datetime="2019-08-07T10:55:15+08:00">2019-08-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-29 19:15:36" itemprop="dateModified" datetime="2019-08-29T19:15:36+08:00">2019-08-29</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="Transformer位置信息"><a href="#Transformer位置信息" class="headerlink" title="Transformer位置信息"></a>Transformer位置信息</h1><p>&emsp;&emsp;引用自<a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></p>
<blockquote>
<p>&emsp;&emsp;NLP 句子中单词之间的相对位置是包含很多信息的，上面提过，RNN 因为结构就是线性序列的，所以天然会将位置信息编码进模型；而CNN的卷积层其实也是保留了位置相对信息的，所以什么也不做问题也不大。但是对于 Transformer 来说，为了能够保留输入句子单词之间的相对位置信息，必须要做点什么。为啥它必须要做点什么呢？因为输入的第一层网络是 Muli-head self attention 层，我们知道，Self attention 会让当前输入单词和句子中任意单词发生关系，然后集成到一个 embedding 向量里，但是当所有信息到了 embedding 后，位置信息并没有被编码进去。所以，Transformer 不像 RNN 或 CNN，必须明确的在输入端将 Positon 信息编码，Transformer 是用<strong>位置函数</strong>来进行位置编码的，而 Bert 等模型则给每个单词一个 <strong>Position embedding</strong>。将单词 embedding 和单词对应的 position embedding 加起来形成单词的输入 embedding，类似上文讲的 ConvS2S 的做法。</p>
</blockquote>
<h1 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h1><p>&emsp;&emsp;从<a href="https://zhuanlan.zhihu.com/p/59629215" target="_blank" rel="noopener">文章</a>做的总结，此文为另一篇英文文章的翻译版，英文原文地址为 <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>。此外另一篇<a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">文章</a>也做了翻译。<br>&emsp;&emsp;Transformer 不同于 RNN，它的输入是独立计算的，输入序列的某个时间步并不依赖其它的时间步。也就是说它可以<strong>并行运算</strong>。</p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>&emsp;&emsp;在第一节中讲到，Transformer 天然地不具有句子的<strong>位置</strong>属性，所以我们需要使用一种办法让它拥有句子的位置属性。<br>&emsp;&emsp;为解决这个问题，Transformer 为每个输入的词嵌入增加了一个向量。<br><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png" alt="To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern."></p>
<p>&emsp;&emsp;如果假定词嵌入维度为4，那真实的位置编码如下：<br><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png" alt="A real example of positional encoding with a toy embedding size of 4"></p>
<p>&emsp;&emsp;位置编码的生成方法在原论文的 section 3.5 中有描述。你可以在 <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py" target="_blank" rel="noopener">get_timing_signal_1d()</a> 函数中看到用于生成位置编码的代码。这并不是生成位置编码的唯一方式。然而，它的优点在于可以扩展到看不见的序列长度（eg. 如果要翻译的句子的长度远长于训练集中最长的句子）。</p>
<h2 id="self-Attention"><a href="#self-Attention" class="headerlink" title="self-Attention"></a>self-Attention</h2><p><img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png" alt="Transformer计算机制"></p>
<blockquote>
<p>注意，这些新创建的向量的维度小于词嵌入向量(embedding vector)。它们（新创建的向量）的维度是 <strong>64</strong>，而词嵌入和编码器的输入输出向量的维度是 <strong>512</strong>。它们不必更小，这是一种架构选择，可以使多头注意力(multiheaded attention)计算不变。 </p>
</blockquote>
<ol>
<li>需要从每个编码器的输入向量创建三个向量，即 <strong>Query</strong> 向量，<strong>Key</strong> 向量和 <strong>Value</strong> 向量；<ul>
<li>那么，究竟什么是“query”，“key”，“value”向量呢？（以下为自己的猜测）<ul>
<li>q 代表当前需要计算 score 的向量，此向量查询 key 中的权重，使得查询到自己需要的 score</li>
<li>k 代表当前单词的权重</li>
<li>v 代表单词的词向量</li>
</ul>
</li>
</ul>
</li>
<li>计算得分（score 权重）。这里的分数是通过将 query 向量与我们正在评分的单词的 key 向量做点积来得到。所以如果我们计算位置 #1 处的单词的 self-attention，第一个得分就是就是 <script type="math/tex">q_1</script> 和 <script type="math/tex">k_1</script> 的点积。第二个得分是 <script type="math/tex">q_1</script> 和 <script type="math/tex">k_2</script> 的点积。<br><img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png" alt="计算 score"></li>
<li>将分数除以8（论文中使用 Key 向量维数的平方根—-64。这可以有更稳定的梯度。实际上还可以有其他可能的值，这里使用默认值）<br><img src="https://jalammar.github.io/images/t/self-attention_softmax.png" alt="计算 score"></li>
<li>然后经过一个softmax操作后输出结果。Softmax可以将分数归一化，这样使得结果都是正数并且加起来等于1<br><img src="https://jalammar.github.io/images/t/self-attention-output.png" alt=""></li>
<li>将每个 value 向量乘以 softmax 得分（准备将他们相加）</li>
<li>对加权值向量求和，这样就产生了在这个位置的self-attention的输出（对于第一个单词）</li>
</ol>
<p>&emsp;&emsp;上述为 self-attention 单个单词的计算步骤，其实它可以并行计算。<br><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" alt="并行计算第一步"></p>
<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" alt="并行计算第二步"></p>
<h2 id="multi-headed-attention"><a href="#multi-headed-attention" class="headerlink" title="multi-headed attention"></a>multi-headed attention</h2><p>&emsp;&emsp;看英文名感觉“高大上”，其实很简单。就是将上述的 self-attention 做多次，具体做几次你可以自行选择，Transformer 选择了 8 次。这样就产生了 8 个向量，但是我们训练时其实只要一个向量就够了，所以我们将这 8 个向量<strong>拼接</strong>起来，这样就形成了一个巨长无比的向量。那我们怎么得到我们所需长度的向量呢？很简单，只需要再乘一个权重矩阵就行了。<br><img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png" alt=""></p>
<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png" alt=""></p>
<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png" alt=""></p>
<p>&emsp;&emsp;全过程：<br><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="全过程"></p>
<h2 id="The-Residuals-与-layer-normalization"><a href="#The-Residuals-与-layer-normalization" class="headerlink" title="The Residuals 与 layer-normalization"></a>The Residuals 与 layer-normalization</h2><p>&emsp;&emsp;残差连接说白了就是跳过一层，输入到下一层（比较直白，不一定对）。<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png" alt=""></p>
<p>&emsp;&emsp;layer-normalization：<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" alt=""></p>
<p>&emsp;&emsp;此步骤在解码层中是类似的操作。将内部件画全，就是如下所示一般（图中的 encoder、decoder 各有两个）：<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" alt=""></p>
<p>&emsp;&emsp;在 self_attention 之后要做一次 normalization，此均值归一化步骤的具体算法，详见：</p>
<ul>
<li><a href="https://www.cnblogs.com/hellcat/p/9735041.html#_label3_0" target="_blank" rel="noopener">『计算机视觉』各种Normalization层辨析</a> </li>
<li><a href="https://www.jianshu.com/p/c357c5717a60" target="_blank" rel="noopener">layer normalization 简单总结</a></li>
</ul>
<h1 id="decodedr"><a href="#decodedr" class="headerlink" title="decodedr"></a>decodedr</h1><p>&emsp;&emsp;在讲解 decoder 之前，看一张动图，了解一下 Transformer 是如何运作的。首先下图中生成了第一个字母 I。<br><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" alt=""></p>
<p>&emsp;&emsp;编码器开始处理输入序列，然后将顶部编码器的输出变换为一组注意力向量 <strong>K</strong> 和 <strong>V</strong>，这些将在每个解码器的“encoder-decoder attention” 层使用，这有助于解码器集中注意力在输入序列的合适位置。解释一下 <strong>K</strong> 和 <strong>V</strong> 的具体用法，<strong>在 encoder 中的 self-attention 会使用到 <script type="math/tex">Q_{self-attention}</script>、<script type="math/tex">K_{self-attention}</script>、<script type="math/tex">V_{self-attention}</script> 三个向量，但是它们实际上是输入值 x 的三份拷贝再乘上各自不同的权重矩阵得来。对于 decoder 的 self-attention 与 encoder 的如出一撤，但是对于 encoder-decoder attention 却有点不一样。<script type="math/tex">Q_{encoder-decoder \, attention}</script> 还是 x 的拷贝乘上一个权重矩阵，但是 <script type="math/tex">K_{encoder-decoder \, attention}</script> 和 <script type="math/tex">V_{encoder-decoder \, attention}</script> 分别是 K 和 V 乘上各自的权重矩阵。</strong>以上所有向量所乘的权重矩阵均可以由你自己随机初始化产生。<br>&emsp;&emsp;接着下面的 gif 完成了余下的步骤。Transformer 将之前输出的 I 当做下一个时间步的输入，又走了一遍 decoder。以此循环往复，直到输出一个结束标记 <code>&lt;EOS&gt;</code> 才结束循环。<strong>博主注</strong>：这样一来似乎对于 decoder 来说并不能实现并行运算。<strong>博主注2</strong>：后来发现其实也可以。<br><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt=""></p>
<p>&emsp;&emsp;其中 decoder 中的 attention 与 encoder 的 attenntion 有所区别，具体的区别就是上上段黑体的内容。除了那个区别之外，没有其他区别了。</p>
<h1 id="Generator-The-Final-Linear-and-Softmax-Layer"><a href="#Generator-The-Final-Linear-and-Softmax-Layer" class="headerlink" title="Generator:The Final Linear and Softmax Layer"></a>Generator:The Final Linear and Softmax Layer</h1><p>&emsp;&emsp;如何将其转换为一个单词？这就是最后 Softmax 层和线性层的工作了。<br>&emsp;&emsp;线性层是一个简单的全连接神经网络，它将解码器堆叠(decoder stack)产生的向量映射到一个巨大的向量（词汇表的大小，原来的向量大小是词嵌入的大小）中去，这个向量称为 logits 向量。<br>&emsp;&emsp;Softmax 层将这些分数转化为概率(全部为正数，加起来为 1.0)。选择具有最高概率的单元，并将与其相关的单词作为本时间步的输出。</p>
<h1 id="padding-mask-sequence-mask"><a href="#padding-mask-sequence-mask" class="headerlink" title="padding mask/sequence mask"></a>padding mask/sequence mask</h1><p>&emsp;&emsp;无论 encoder 还是 decoder 都要做 mask（很多对 Transformer 的总结文章都只提到了 decoder 部分的 Masked Multihead Self-Attention，实际上 encoder 也要做一次）。mask 一共分为两种，寻常所见的 decoder 中的 mask 指的是 <strong>sequence mask</strong>，encoder 中的 mask 指的是 <strong>padding mask</strong>。详见：</p>
<ul>
<li><a href="https://luozhouyang.github.io/transformer/" target="_blank" rel="noopener">Transformer模型的PyTorch实现</a></li>
<li><a href="https://www.jianshu.com/p/405bc8d041e0" target="_blank" rel="noopener">The Transformer</a></li>
</ul>
<p>&emsp;&emsp;<strong>对于 padding mask</strong>：在 encoder 中的每次 scaled dot-product 都要做一次 sequence mask。由于我们要让序列的长度相等以便做向量化操作，所以必不可少地需要对输入序列进行<strong>截断</strong>或<strong>补零</strong>操作。所以 sequence mask 的<strong>主要目的</strong>是使得我们的 self-attention 不要过多的关注向量中的 0。<strong>具体操作是</strong>：将序列中补零位置的值置为 -INF，使得序列经过 scaled dot-product 后的 softmax 层时，对应位置会得到<strong>概率 0</strong>。<br>&emsp;&emsp;<strong>对于 sequence mask</strong>：使得 decoder 无法看见未来的信息，decoder 的 attention 只能关注解码单词之前的输出单词，而不能依赖后面未解码出来的单词。</p>
<h1 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h1><p>&emsp;&emsp;在最后的 softmax 层直接输出了概率最大的位置的单词，这叫做<strong>贪婪解码——greedy decoding</strong>。<br>&emsp;&emsp;另一种更合理的解码方式叫做 <strong>Beam Search</strong>。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/深度学习算法（四）：Transformer.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/【NLP算法】（零）NLP基础算法.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/【NLP算法】（零）NLP基础算法.html" class="post-title-link" itemprop="url">【NLP算法】（零）NLP基础算法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-05 13:34:55" itemprop="dateCreated datePublished" datetime="2019-08-05T13:34:55+08:00">2019-08-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-07 17:26:00" itemprop="dateModified" datetime="2019-09-07T17:26:00+08:00">2019-09-07</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="one-hot"><a href="#one-hot" class="headerlink" title="one hot"></a>one hot</h1><h1 id="N-gram-算法"><a href="#N-gram-算法" class="headerlink" title="N-gram 算法"></a>N-gram 算法</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/baimafujinji/article/details/51281816" target="_blank" rel="noopener">参考文章</a></p>
<ul>
<li><strong>基本算法</strong><br>有句子：老王吃过了吗？<br>该句子可表示为 <script type="math/tex">S = w_1w_2w_3w_4w_5w_6</script><br>那么该句子的概率为 p(S)=p(w1w2w3w4w5w6) = <script type="math/tex">\prod</script>p(wt|w1w2…wt-1) = p(w1)p(w2|w1)p(w3|w1w2)…<br>由于该算法太复杂，所以就产生了其他的算法。马尔科夫模型，该模型认为，一个词的出现仅仅依赖于它前面出现的几个词。可理解为是n-gram。</li>
<li><strong>n-gram</strong><br>n-gram只根据前n-1个单词来预测第n个单词的概率。<br>比如n=2时，也被称为bi-gram。公式为： p(wt|wt-1) = p(wtwt-1)/p(wt)<br>意思就是计算wt-1后面出现wt的概率<br>比如t等于1，则就是计算“老”后面出现“王”的概率<br><strong>剩下的问题就是如何计算一个字出现的概率</strong>。</li>
<li><strong>如何计算概率</strong><br>可以做一个统计，将所拥有的数据计算出每个字出现的次数。并且再统计两个字连在一起的次数。比如：p(王|老) = p(老王)/p(王) = 600/10000 = 6%<br>在神经网络模型中，计算概率是通过 softmax 函数计算的，即将 one hot 编码的向量经过矩阵运算后传入 softmax 函数中。从而得到一个类似概率的值。</li>
<li><strong>用途</strong><br><a href="http://blog.sciencenet.cn/blog-713101-797384.html" target="_blank" rel="noopener">参考</a><br>20世纪80年代至90年代初,n-gram技术被广泛地用来进行文本压缩,检查拼写错误,加速字符串查找,文献语种识别。90年代,该技术又在自然语言处理自动化领域得到新的应用,如自动分类,自动索引,超链的自动生成,文献检索,无分隔符语言文本的切分等。<br>目前N-gram最为有用的就是自然语言的自动分类功能。基于n-gram的自动分类方法有两大类,一类是人工干预的分类(Classification),又称分类;一类是无人工干预的分类(Clustering),又称聚类。</li>
</ul>
<h1 id="词袋模型——BOW"><a href="#词袋模型——BOW" class="headerlink" title="词袋模型——BOW"></a>词袋模型——BOW</h1><p>&emsp;&emsp;例如，有文档：你好棒棒</p>
<ol>
<li>建立字典<br>{‘你’:1, ‘好’:2, ‘棒’:3}</li>
<li>bow模型<br>(1, 1)(2, 1)(3, 2)<br>其中第一个元素代表该单词在字典中的id，第二个元素代表该单词在所在文档中出现的次数。</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/【知识图谱】（一）从概念到实战.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/【知识图谱】（一）从概念到实战.html" class="post-title-link" itemprop="url">【知识图谱】（一）从概念到实战</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-03 15:28:06" itemprop="dateCreated datePublished" datetime="2019-08-03T15:28:06+08:00">2019-08-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-11-05 16:50:22" itemprop="dateModified" datetime="2019-11-05T16:50:22+08:00">2019-11-05</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/AI/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="知识图谱描述"><a href="#知识图谱描述" class="headerlink" title="知识图谱描述"></a>知识图谱描述</h1><p>&emsp;&emsp;知识图谱是一种新型的<strong>数据库</strong>，是一种基于图的数据结构。每个节点表示现实世界中存在的“实体”，每条边为实体与实体之间的“关系”。以下为知识图谱的几点作用：</p>
<ul>
<li>从“关系”分析问题</li>
<li>把不同种类的信息连接在一起</li>
<li>一个关系网络</li>
</ul>
<p>&emsp;&emsp;学习知识图谱首先得掌握以下几种技能：</p>
<ol>
<li><strong>基础知识</strong>：自然语言处理、图数据库操作知识、基本编程能力：Python、SQL；</li>
<li><strong>领域知识</strong>：知识图谱构建方法、知识图谱推理方法；</li>
<li><strong>行业知识</strong></li>
</ol>
<h1 id="知识图谱的构建步骤"><a href="#知识图谱的构建步骤" class="headerlink" title="知识图谱的构建步骤"></a>知识图谱的构建步骤</h1><ol>
<li>数据收集(持续收集与更新)（<strong>关键词抽取</strong>、<strong>命名体识别</strong>、<strong>关系抽取</strong>、<strong>事件抽取</strong>）<ol>
<li>原始数据，通常可能是一篇文章<ol>
<li>爬虫技术<ol>
<li>垂直爬虫</li>
<li>搜索引擎相关的爬虫</li>
</ol>
</li>
</ol>
</li>
<li>语料数据，通常词库，词典，同义词</li>
<li>开源的第三方知识图谱，例如搜狗人物关系图</li>
<li>开源的训练好的词向量(word2vec)模型,tfidf</li>
</ol>
</li>
<li>图谱设计<ol>
<li>实体定义(本体)<br>实体：实体类型<ol>
<li>属性<br>例如,手(长度，面积)，类别：身体器官</li>
</ol>
</li>
<li>属性定义</li>
<li>关系定义<ol>
<li>关系也需要定义类别</li>
<li>需要评估关系可以覆盖的数据量，一般服从28 原则，20%的关系，覆盖80%数据</li>
</ol>
</li>
</ol>
</li>
<li>知识清洗<ol>
<li><strong>实体消歧</strong></li>
<li><strong>实体统一</strong></li>
</ol>
</li>
<li>知识融合(实体链接)<ol>
<li>实体与关系的融合</li>
<li>实体扩充(融合外部知识图谱或者数据)（<strong>知识合并</strong>）</li>
</ol>
</li>
<li><strong>知识存储</strong>-图数据库</li>
</ol>
<h2 id="知识图谱的架构与设计"><a href="#知识图谱的架构与设计" class="headerlink" title="知识图谱的架构与设计"></a>知识图谱的架构与设计</h2><p>&emsp;&emsp;略</p>
<h2 id="知识源数据的获取"><a href="#知识源数据的获取" class="headerlink" title="知识源数据的获取"></a>知识源数据的获取</h2><p>&emsp;&emsp;略。可以使用爬虫等技术，或者直接网上搜现成的数据。</p>
<h2 id="信息抽取"><a href="#信息抽取" class="headerlink" title="信息抽取"></a>信息抽取</h2><p>&emsp;&emsp;包括关键词抽取、命名体识别、关系抽取，事件抽取等技术。</p>
<h3 id="关键词抽取"><a href="#关键词抽取" class="headerlink" title="关键词抽取"></a>关键词抽取</h3><h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>&emsp;&emsp;分类算法中的流程：<br>&emsp;&emsp;分词—&gt;(自然语言处理,与,知识,图谱,知识图谱)—&gt;去停词—&gt;(自然语言处理,知识,图谱,知识图谱)—&gt;建立索引—&gt;(1,2,3,432,66)—&gt;one hot—&gt;word2vec—&gt;</p>
<h4 id="语料库"><a href="#语料库" class="headerlink" title="语料库"></a>语料库</h4><p>&emsp;&emsp;jieba 分词同时基于一些<strong>语料库</strong>和手写的<strong>规则</strong>（如隐马尔科夫模型）。<br>&emsp;&emsp;如果想要加入自己的语料库可以使用下面的代码，语料库的格式可在 github jieba 上找到。<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">'/home/python/dictionary.txt'</span>)</span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span>(<span class="string">' '</span>.<span class="keyword">join</span>(seg_list))</span><br></pre></td></tr></table></figure></p>
<ol>
<li>词库<ul>
<li>医药知识图谱<ul>
<li>语料库（网上有现成的，不用自己爬，如：医药行业专业词典）<ul>
<li>医院的名称</li>
<li>疾病的名称</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h4><p>&emsp;&emsp;文本数据的表示模型：</p>
<ul>
<li>布尔模型（boolean model）</li>
<li>向量空间模型（vector space model）</li>
<li>概率模型（probabilistic model）</li>
<li>图空间模型（graph space model）等</li>
</ul>
<p>&emsp;&emsp;以下为几种主要的模型，它们的目标都是：建立文档的向量（矩阵）模型。加粗代表是现在常用的模型</p>
<ol>
<li><strong>TF-IDF</strong></li>
<li>LDA</li>
<li>LSA/LSI</li>
<li><strong>Word2Vec</strong></li>
<li>one-hot</li>
<li>BERT</li>
<li>…</li>
</ol>
<h5 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h5><p>&emsp;&emsp;TF：词频<br>&emsp;&emsp;IDF：逆文档频率。<br>&emsp;&emsp;权重 = TF * IDF<br>&emsp;&emsp;TF-IDF 可能会漏掉一些词。比如一篇文章只出现一次“周杰伦”，但是它已经表示了这篇文章的主旨。可是 TF-IDF 无法为该词分配较高的权重。<br>&emsp;&emsp;另外 jieba 中其实可以直接使用 TF-IDF。导入<code>jieba.analyse</code>即可使用。（TF-IDF 其实就是提取句子的标签）<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> ja</span><br><span class="line">ja.extract_tags(sentence, topK=<span class="number">3</span>,withWeight=<span class="literal">False</span>, allowPOS=())</span><br></pre></td></tr></table></figure></p>
<h5 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h5><p>&emsp;&emsp;TF-IDF 只考虑单个文字，忽略了句子中的上下文信息。而word2vec 考虑了上下文，输入值为某个单词的前几个单词、后几个单词和其本身。<br>&emsp;&emsp;word2vec 现成的工具包有：1)gensim；2)tensorflow；3)keras。<br>&emsp;&emsp;另外 QA 系统等应用可能不适合使用 word2vec 训练出来的单词。因为它训练出来的词向量没有捕获到<strong>太多</strong>的上下文信息。众所众知，QA 系统和对话系统等应用需要经常使用到很多上下文信息。</p>
<h3 id="命名体识别——NER"><a href="#命名体识别——NER" class="headerlink" title="命名体识别——NER"></a>命名体识别——NER</h3><p>&emsp;&emsp;所谓的命名体（named entity）就是人名、机构名、地名以及其他所有以名称为标识的实体。更广泛的实体还包括数字、日期、货币、地址等等。<br>&emsp;&emsp;难点：1)<strong>同义词、歧义词等</strong>；2)<strong>未登录词判定</strong>。<br>&emsp;&emsp;一般流程：1)<strong>基于规则的方法</strong>；2)<strong>基于模型的方法</strong>，常见的序列标注模型包括 <strong>HMM</strong>（Hidden Markov Model）、<strong>CRF</strong>（Conditional random field）、<strong>RNN</strong>。不过虽然基于模型的方法技术比较新颖，但是由于太过复杂以及太难解释，所以公司还是用基于规则的方法比较多。</p>
<h4 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h4><ul>
<li>基于HMM</li>
<li>基于CRF<div class="note danger">
            <p>&emsp;&emsp;上课的时候没听明白。</p>
          </div></li>
<li>基于RNN<br>&emsp;&emsp;要做命名体识别，首先要做序列标注的任务。目前有以下几种公认的标注体系：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/标注体系.png" alt="标注体系"></li>
</ul>
<h3 id="关系抽取（特征工程）"><a href="#关系抽取（特征工程）" class="headerlink" title="关系抽取（特征工程）"></a>关系抽取（特征工程）</h3><ol>
<li><strong>文本特征提取</strong>，采用 tf-idf</li>
<li><strong>关键字抽取</strong>，比如转让，收购，整合等等</li>
<li><strong>句法特征提取</strong>，主要是与核心词之间的关系，包括企业实体本身和前后词与核心词之间的关系，距离等。即抽取（实体，关系，实体）<strong>三要素</strong>特征<ul>
<li>依存句法分析<ul>
<li>依存树，<a href="http://ltp.ai/demo.html" target="_blank" rel="noopener">demo</a></li>
<li>CCG</li>
</ul>
</li>
<li>分类器</li>
</ul>
</li>
<li>如果使用 NN 训练，可以拼接三要素和 tf-idf 特征</li>
</ol>
<h3 id="事件抽取"><a href="#事件抽取" class="headerlink" title="事件抽取"></a>事件抽取</h3><p>&emsp;&emsp;略，培训中未提到，估计跟关系抽取差不多。</p>
<h2 id="知识融合"><a href="#知识融合" class="headerlink" title="知识融合"></a>知识融合</h2><h3 id="实体链接"><a href="#实体链接" class="headerlink" title="实体链接"></a>实体链接</h3><h4 id="实体统一-实体对齐"><a href="#实体统一-实体对齐" class="headerlink" title="实体统一/实体对齐"></a>实体统一/实体对齐</h4><p>&emsp;&emsp;<strong>注：另一种说法是实体统一和实体对齐并不是同一件事。此处姑且当它们是同一件事。</strong><br>&emsp;&emsp;对同一实体具有多个名称的情况进行实体统一，将多个名称统一替换成一个命名实体。比如，“河北银行股份有限公司”和“河北银行”可以统一成“河北银行”。<br>&emsp;&emsp;大致来说这个应用是使用规则来做实体统一。目前（2019 年 7 月）来说，基于规则的做法大概能解决 70% 左右的问题。还可以使用余弦相似度，分类等算法进行融合使用。</p>
<ul>
<li>分离出地名，比如河北，北京</li>
<li>去除后缀，比如有限公司，集团</li>
<li>提取经营范围，比如医疗，化学</li>
<li>剩余部分为中间字段</li>
<li>最后选择以上四个部分的某些部分进行拼接，成为一个唯一的命名实体，如果有中间字段，则仅使用中间字段即可，并对某些特殊的经营范围做补充，比如银行；否则，优先使用地名加经营范围，其次是地名加后缀。</li>
</ul>
<p>&emsp;&emsp;<strong>更新命名体：在做完实体统一之后，将原数据中的实体进行替换即可</strong>。</p>
<h4 id="实体消歧"><a href="#实体消歧" class="headerlink" title="实体消歧"></a>实体消歧</h4><p>&emsp;&emsp;与实体统一不同。实体统一是将两个不一样名称的实体统一起来，而实体消歧是将同一个名称的实体在不同语境下区分开来，比如：苹果在不同的语境下分别有水果和手机的意思。<br>&emsp;&emsp;中文的不怎么好做，主要运用规则。</p>
<h3 id="知识合并"><a href="#知识合并" class="headerlink" title="知识合并"></a>知识合并</h3><p>&emsp;&emsp;<a href="https://102.alibaba.com/downloadFile.do?file=1518508273059/CoLink An Unsupervised Framework for User Identity Linkage.pdf" target="_blank" rel="noopener">阿里巴巴实体合并框架</a></p>
<h2 id="知识加工"><a href="#知识加工" class="headerlink" title="知识加工"></a>知识加工</h2><h2 id="知识存储与检索"><a href="#知识存储与检索" class="headerlink" title="知识存储与检索"></a>知识存储与检索</h2><h2 id="知识应用"><a href="#知识应用" class="headerlink" title="知识应用"></a>知识应用</h2><h1 id="汉语处理的难点"><a href="#汉语处理的难点" class="headerlink" title="汉语处理的难点"></a>汉语处理的难点</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/汉语处理的难点.jpg" alt="汉语处理的难点"></p>
<h1 id="NLP-工具包"><a href="#NLP-工具包" class="headerlink" title="NLP 工具包"></a>NLP 工具包</h1><p>&emsp;&emsp;略。详见此<a href="https://yan624.github.io/2019 普开培训.html#NLP-工具包">博客</a></p>
<h1 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h1><p>&emsp;&emsp;以上为知识图谱的大致概述，以下以几个例子大致地将构建步骤串联起来。首先给出知识图谱的总结<strong>思维导图</strong>，可以按照图中的内容自行对应查找知识点。思维导图的阅读顺序是<strong>从上至下</strong>，<strong>从右至左</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/知识图谱总结.png" alt="知识图谱总结"></p>
<h2 id="医疗命名体识别"><a href="#医疗命名体识别" class="headerlink" title="医疗命名体识别"></a>医疗命名体识别</h2><p>&emsp;&emsp;<a href="https://github.com/liuhuanyong/MedicalNamedEntityRecognition" target="_blank" rel="noopener">项目地址</a>，使用了基于字向量的<strong>四层双向 LSTM</strong> 与 <strong>CRF 模型</strong>的网络。<br>&emsp;&emsp;本项目大致使用了<strong>信息抽取</strong>-&gt;<strong>命名体识别</strong>的技术。项目中有一个名为 data_origin 的文件夹，其结构为：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data_origin</span><br><span class="line">  ├─一般项目</span><br><span class="line">  │  ├─一般项目-<span class="number">1</span>.txt</span><br><span class="line">  │  ├─一般项目-<span class="number">1</span><span class="selector-class">.txtoriginal</span><span class="selector-class">.txt</span></span><br><span class="line">  │  └─。。。</span><br><span class="line">  ├─出院情况</span><br><span class="line">  ├─病史特点</span><br><span class="line">  └─诊疗经过</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;<em>一般项目-1.txt</em> 文件包含了由<strong>人工标注</strong>过的数据，<em>一般项目-1.txtoriginal.txt</em> 包含了原始数据，即未经过任何处理的数据。类似以下的格式。第 2 列和第 3 列代表该命名体在原始数据中的开始和结束的索引。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/原始数据.jpg" alt="原始数据"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/人工标注后的数据.jpg" alt="人工标注后的数据"></p>
<p>&emsp;&emsp;以上的数据为项目的原数据（那个由人工标注过的数据也算原数据），我们需要使用一套标注体系（本项目使用 BIO 体系）来将原数据处理一下，以下是处理结果。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/使用 BIO 标注后的数据.jpg" alt="使用 BIO 标注后的数据"></p>
<p>&emsp;&emsp;你可能会疑惑 DISEASE-* 之类的东西是什么意思，以及它是怎么出来的。其实十分简单，如下所示，都是预先定义好的。以 B 结尾，代表一个命名体的开始，以 I 结尾，代表一个命名体的结束。而产生数据的过程也只是写死的一套逻辑，使用 if else 进行判断罢了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/标签字典.jpg" alt="标签字典"></p>
<h3 id="训练之前的准备工作"><a href="#训练之前的准备工作" class="headerlink" title="训练之前的准备工作"></a>训练之前的准备工作</h3><ol>
<li>定义标签</li>
<li>人工将数据一条一条地标注命名体、起始位置以及标签</li>
<li>选择一套标注体系</li>
<li>将<strong>每一份</strong>原数据使用标注体系处理后，存入<strong>一份</strong>文件</li>
</ol>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>&emsp;&emsp;代码中以 <code>[&#39;。&#39;,&#39;?&#39;,&#39;!&#39;,&#39;！&#39;,&#39;？&#39;]</code> 符号作为一份病历的结束。然后将训练数据重新拆分成多分训练样本。我倒是认为在原数据处理完毕合并时，就做一些处理不行吗？如果以那些符号作为判断条件，可能有些不太准。</p>
<ol>
<li>加载字向量；</li>
<li>以 <code>[&#39;。&#39;,&#39;?&#39;,&#39;!&#39;,&#39;！&#39;,&#39;？&#39;]</code> 符号作为一份病历的结束，重新切分数据为多份训练样本；</li>
<li>每一个字都有一个标注，比如训练样本：[感, 染, 风, 寒]和标注：[CHECK-B, CHECK-I, DISEASE-B, DISEASE-I]—转换为—&gt;[32, 8454, 676, 934]和[7, 8, 10, 9]；</li>
<li>程序定义有 150 个时间步，第一层 BiLSTM 为 128 维，第二层的 BiLSTM 为 64 维，各层之间的 Dropout 取 0.5；</li>
<li>将训练样本输入 RNN，RNN 的输出输入 CRF，CRF 输出一个 11 维的向量，即每一个字都会输出一个 11 维的向量。所以可以看做是一个 11 元分类模型，即判断一个字属于哪一类的标注，也就是序列标注的含义——为字标注属性；</li>
<li>训练结束，就完成了一个序列标注模型。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>命名体识别</strong>的功能，使用了 <strong>LSTM</strong> 以及 <strong>CRF</strong> 的技术，原数据采用了<strong>人工标注</strong>的处理方式，原数据转为训练样本采用了<strong>规则模版</strong>的方式。总的来说，没有太大难度。对于此项目，我们需要理解 LSTM 和 CRF 的算法，整个过程的难点就在人工标注上，费时费力。</p>
<h2 id="中文人物关系知识图谱"><a href="#中文人物关系知识图谱" class="headerlink" title="中文人物关系知识图谱"></a>中文人物关系知识图谱</h2><p>&emsp;&emsp;<a href="https://github.com/liuhuanyong/PersonRelationKnowledgeGraph" target="_blank" rel="noopener">项目地址</a>。此项目代码结构有点复杂，涉及了很多爬虫，我对爬虫不是很了解。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>关系抽取</strong>的功能，具体使用了什么技术<strong>未知</strong>。</p>
<h2 id="判断两个企业实体是否存在投资关系"><a href="#判断两个企业实体是否存在投资关系" class="headerlink" title="判断两个企业实体是否存在投资关系"></a>判断两个企业实体是否存在投资关系</h2><p>&emsp;&emsp;<a href="https://github.com/rlistengr/Entity-relationship-extraction" target="_blank" rel="noopener">项目地址</a>。</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>关系抽取</strong>和<strong>实体统一</strong>的功能，基本上使用了人工模版去判断两个企业是否<strong>统一</strong>。是否存在投资关系也是用规则判断的。</p>
<h2 id="金融问答项目"><a href="#金融问答项目" class="headerlink" title="金融问答项目"></a>金融问答项目</h2><p>&emsp;&emsp;此项目（<strong>实验21-1-FinancialKGQA</strong>）实现了一个简单的金融问答项目，前提项目为<strong>实验19-neo4j构建简单的金融知识图谱</strong>，旨在使用爬虫技术构建一个金融知识图谱。数据和代码已经由 2019.6.27 普开知识图谱培训机构提供。</p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;从下图可以看出，只是简单的关键词匹配。然后通过 neo4j 的 CQL 语句进行查询。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/金融问答示例代码.jpg" alt="金融问答示例代码"></p>
<h2 id="企业经营退出风险预测"><a href="#企业经营退出风险预测" class="headerlink" title="企业经营退出风险预测"></a>企业经营退出风险预测</h2><p>&emsp;&emsp;<a href="https://github.com/xiaorancs/business-exit-risk-forecast" target="_blank" rel="noopener">项目地址</a>，还没研究过。同一个项目，<a href="https://github.com/ShawnyXiao/2017-CCF-BDCI-Enterprise" target="_blank" rel="noopener">另一个人的项目地址</a><br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/【知识图谱】（一）从概念到实战.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">记录学习问题，积累做的 leetcode 题目</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">124</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">23</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
			  
			  <!-- 不蒜子/busuanzi -->
			  <div class="site-state-item site-state-posts">
			  	<span class="site-state-item-count">117.5k</span>
			  	<span class="site-state-item-name">总字数</span>
			  </div>
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

  
  <!-- 自己新增的所有 js 文件 -->
  <script src="/lib/my-utils.js"></script>
<!--图片缩放插件-->
<script src="/lib/zoomify/zoomify.min.js"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- 背景插件 -->
<script src="https://cdn.bootcss.com/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(11), {
		duration:90000,//1min半一换
		fade: 1500
	});
</script>
<!-- 图片缩放 -->
<script>
$('#content img').zoomify({duration: 500, });
  $('#content img').on('zoom-in.zoomify', function () {
    $('#sidebar').css('display', 'none');
  });
  $('#content img').on('zoom-out-complete.zoomify', function () {
    $('#sidebar').css('display', '');
  });
</script>

	

</body>
</html>
