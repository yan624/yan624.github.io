<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2">























  

<link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.10.2/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


<!--图片缩放插件样式-->
<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">

  <meta name="description" content="记录学习问题，积累做的 leetcode 题目">
<meta name="keywords" content="博客，java，javaWeb，NLP，python，机器学习，深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="记录学习问题，积累做的 leetcode 题目">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="记录学习问题，积累做的 leetcode 题目">






  <link rel="canonical" href="http://yan624.github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
	<!--加载flower canvas-->
<script>
var pathname = window.location.pathname;
if(pathname == '/flower.html'){
	var body =  document.getElementsByTagName('body')[0];
	var canvas = document.createElement("canvas")
	canvas.setAttribute('id', 'sakura')
	// '<canvas id="sakura"></canvas>'
	body.appendChild(canvas)
}
</script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">低阶炼金术士</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">19</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">23</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">130</span></a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/tips.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/tips.html" class="post-title-link" itemprop="url">tips</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-24 19:04:25 / 修改时间：19:19:53" itemprop="dateCreated datePublished" datetime="2019-01-24T19:04:25+08:00">2019-01-24</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p><strong>提示：先按住ctrl再点击超链接可以在新窗口打开</strong></p>
</blockquote>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/tips.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/assorted/一些快捷键.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/assorted/一些快捷键.html" class="post-title-link" itemprop="url">一些快捷键</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-12 16:57:05" itemprop="dateCreated datePublished" datetime="2019-03-12T16:57:05+08:00">2019-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-19 11:04:49" itemprop="dateModified" datetime="2019-08-19T11:04:49+08:00">2019-08-19</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>最近老是用到win10的快捷键，但是用过之后过几天就忘了，所以记录下。<br>还有有谁知道怎么用快捷键打开高级系统设置，设置环境变量老是要打开这个，烦得很。</p>
<h1 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h1><p>按 V 进入 Visual Mode，然后可以上下左右选择行数。按 y 复制，d 剪切，p 粘贴。</p>
<h1 id="win10"><a href="#win10" class="headerlink" title="win10"></a>win10</h1><ol>
<li>win+E 打开我的电脑</li>
<li>win+R 打开运行</li>
<li>win+L 锁屏</li>
<li>fn+ESC 打开/关闭功能键。f5是刷新键，但是有时候发现按f5无效，其实是因为功能键打开了，只需要按fn+ESC关闭就可。</li>
<li>四指在触摸板向左/向右滑动，切换桌面。</li>
<li>三指向上滑动将当前的任务以小窗口显示在桌面，三指向下滑动隐藏。</li>
<li>win+Tab类似6</li>
<li>win+d最小化所有打开的窗口</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/assorted/一些快捷键.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/53. Neural Machine Translation of Rare Words with Subword Units.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/53. Neural Machine Translation of Rare Words with Subword Units.html" class="post-title-link" itemprop="url">论文笔记：Neural Machine Translation of Rare Words with Subword Units</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-12-14 23:10:47 / 修改时间：23:11:23" itemprop="dateCreated datePublished" datetime="2019-12-14T23:10:47+08:00">2019-12-14</time>
            

            
              

              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id=""><a href="#" class="headerlink" title=" "></a> </h1><p>&emsp;&emsp;</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/53. Neural Machine Translation of Rare Words with Subword Units.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/52. A Sketch-Based System for Semantic Parsing.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/52. A Sketch-Based System for Semantic Parsing.html" class="post-title-link" itemprop="url">论文笔记：A Sketch-Based System for Semantic Parsing</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-12-12 20:46:40 / 修改时间：20:50:29" itemprop="dateCreated datePublished" datetime="2019-12-12T20:46:40+08:00">2019-12-12</time>
            

            
              

              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.xilesou.top/pdf/1909.00574.pdf" target="_blank" rel="noopener">论文地址</a>，论文作者 Zechang Li 等，发表于 2019 年 9 月。</p>
          </div>
<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><p>&emsp;&emsp;</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/52. A Sketch-Based System for Semantic Parsing.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/51. A Transformer-based Semantic Parser for NLPCC-2019 Shared Task 2.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/51. A Transformer-based Semantic Parser for NLPCC-2019 Shared Task 2.html" class="post-title-link" itemprop="url">论文笔记：A Transformer-based Semantic Parser for NLPCC-2019 Shared Task 2</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-12-02 14:11:43" itemprop="dateCreated datePublished" datetime="2019-12-02T14:11:43+08:00">2019-12-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-12 20:45:34" itemprop="dateModified" datetime="2019-12-12T20:45:34+08:00">2019-12-12</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>&emsp;&emsp;<a href="http://tcci.ccf.org.cn/conference/2019/papers/EV15.pdf" target="_blank" rel="noopener">论文地址</a>，论文作者 D Ge 等，发表于 2019 年。</p>
          </div>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>&emsp;&emsp;使用 BPE、Sharing Vocab、Synthetic Training Instance 极大地提高了准确率。</p>
<h1 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h1><p>&emsp;&emsp;seq2seq 方法将语义解析形式化为一个翻译任务，即将一个句子转换为其对应的 lf（logical form）。然而，在缺少大规模标注过的数据集的情况下，即使在一流的 seq2seq 模型（如 Transformer）中也会遇到数据稀疏的问题。为了解决这个问题，本文探索了<strong>三种广泛应用于<u>神经机器翻译</u>的技术</strong>，以更好地适应 seq2seq 模型的语义分析任务。</p>
<ol>
<li>byte pair encoding (<strong>BPE</strong>): 将单词分割成子词（subword），将稀有单词转换成高频的子词；</li>
<li>我们在 source 和 target 共享词表（<strong>Sharing Vocab</strong>）；</li>
<li>我们定义启发式规则生成合成的实例，以提高训练集的覆盖率（<strong>Synthetic Training Instance</strong>）。</li>
</ol>
<h1 id="建立基准模型"><a href="#建立基准模型" class="headerlink" title="建立基准模型"></a>建立基准模型</h1><p>&emsp;&emsp;使用 Transformer 建立基准模型。</p>
<h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p>&emsp;&emsp;在 MSParS 数据集中的每一个实例都是一个包含 4 个元素的元组，包括 question, its logical form, parameters, and question type。在本论文中我们只使用问题和其对应的 lf 来训练我们的解析模型，忽视 parameters and question type，因为它们没有被评估过。实验步骤是：question —fed-into—&gt; encoder, lf —fed-into—&gt; decoder。<br>&emsp;&emsp;<strong>注意在 lf 中，一个实体被表示一个由多个单词组成并以“_”相连的字符串</strong>。在预处理中，我们将实体分割成它对应的单词和“_”。例如，lf: <code>( lambda ?x ( mso:film.film.art director “ i see you ” from avatar ?x ) )</code>，在后处理中，我们只需要将“ _ ”替换为“_”即可。<br>&emsp;&emsp;我们还尝试将实体类型的字符串拆分为多个片段。例如，<code>mso:film.film.art director</code>被分为<code>mso : film . film . art director</code>。然而，我们的初步实验表明，这对性能有轻微的影响。</p>
<h2 id="seq2seq-model"><a href="#seq2seq-model" class="headerlink" title="seq2seq model"></a>seq2seq model</h2><p>&emsp;&emsp;介绍什么是 Transformer，略。</p>
<h2 id="生成合成的训练实例"><a href="#生成合成的训练实例" class="headerlink" title="生成合成的训练实例"></a>生成合成的训练实例</h2><p>&emsp;&emsp;监督机器学习算法容易出现数据不平衡问题。在 MSParS 数据集中，我们发现实体类型包含<strong>偏态分布</strong>(Skewed distribution)，例如，实体类型 mso:film.actor.film 包含大多数实体实例，共有 1832 个，而实体类型 mso:barball.batting statistics.slugging_pct 只有一个实体实例。<strong>在这样一种数据集上训练的 seq2seq 模型可能会被数量多的实体类型的训练实例所淹没，而数量小的实体类型的参数则没有很好的学习</strong>。由于泛化能力有限，所得到的模型容易在测试集上获得相对较差的性能。<br>&emsp;&emsp;为了解决这一数据不平衡的问题，我们从以下两个角度生成<strong>合成的训练实例</strong>。</p>
<ul>
<li>Entity-based: 给定一个来自原训练集中的一个句子和其对应的 lf，我们选择句子中的一个实体 A，将其替换成一个随机的实体 B，A 与 B 拥有相同的实体类型（<strong>博主注</strong>：原文中是 entity type，但是我认为用 entity type 不恰当，因为在 MSParS 中确实存在着一个 entity type，与前面提到的重名了，我觉得叫 realation 或者 predicate 更合适）。如：<br><strong>Original pair</strong><br>Sentence: movies jim bob duggar has done<br>Logical Form: ( lambda ?x ( mso:film.actor.film jim_bob_duggar ?x ) )<br><strong>Synthetic pair</strong><br>Sentence: movies marisa tomei has done<br>Logical Form: ( lambda ?x ( mso:film.actor.film marisa_tomei ?x ) )</li>
<li>Labeled-based: 选择一个拥有多个实体类型的实体，将其的实体类型替换为其他一个有效的实体。如下所示，<code>&quot;_i_see_you_&quot;_from_avatar</code> 拥有多个实体类型，我们随机的选择另一个实体类型（不能是 film.film.art_director）进行替换。<br><strong>Original pair</strong><br>Sentence: who is film art directors of “ i see you “ from avatar<br>Logical Form: ( lambda ?x ( mso:film.film.art_director “_i_see_you_“_from_avatar ?x ) )<br><strong>Synthetic pair</strong><br>Sentence: who is film art directors of “ i see you “ from avatar<br>Logical Form: ( lambda ?x ( mso:film.film.editor “_i_see_you_“_from_avatar ?x ) )</li>
</ul>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;本节中，1）我们首先介绍使用的<strong>数据集</strong>。2）然后描述了实验中我们<strong>模型的设置</strong>。3）之后，将我们的系统与其他参与的系统进行了<strong>比较研究</strong>。（博主注：数据集的介绍和模型的设置我直接跳过了）</p>
<h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>&emsp;&emsp;测试集未提供给参赛队伍，组织者根据某一标准将测试集分割，选择一个 hard subset。所以每个团队都有两个结果：full set score and hard subset score。<br>&emsp;&emsp;评估标准为 accuracy(ACC)，即生成的逻辑形式与正确的逻辑形式完全吻合。<br>&emsp;&emsp;参数设置略。为了克服数据稀疏的问题，在所有的问题中，我们跟随 <a href="https://www.ijcai.org/proceedings/2019/0691.pdf" target="_blank" rel="noopener">Ge et al</a>，在输入和输出都<strong>共享词表</strong>。为了解决稀有单词的翻译，我们通过 <strong>BPE</strong>（这篇论文暂时找不到） 将单词分割为 subword。我们对最后 20 个模型的参数进行平均，以提高性能。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>&emsp;&emsp;对语义解析来说，这显示了对解决数据稀疏的问题有两个办法。<strong>BPE</strong> 和 <strong>vocabulary sharing</strong>。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td><strong>85.93</strong></td>
</tr>
<tr>
<td>-BPE</td>
<td>54.90</td>
</tr>
<tr>
<td>-Sharing Vocab.</td>
<td>84.00</td>
</tr>
<tr>
<td>-Both</td>
<td>52.47</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;生成合成的训练实例的方法从本质上增加了我们训练集实例的数量。如下表所示，添加生成合成的训练实例的方法后，数量几乎翻了一倍，并且两种方法都取得了相似的性能提升，这表明我们的两种方法在提高训练实例覆盖率方面是有效的。（<strong>博主注</strong>：ACC 几乎与 baseline 相等，训练集翻了 3 倍多，我佛了）然而，这两种方法的覆盖率存在重叠。在一种方法存在的情况下，另一种方法实现有限或无改进。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th># Instances</th>
<th>ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td>Original</td>
<td>63,826</td>
<td>85.93</td>
</tr>
<tr>
<td>+Entity-based</td>
<td>137,198</td>
<td>86.78</td>
</tr>
<tr>
<td>+Label-based</td>
<td>140,485</td>
<td>86.94</td>
</tr>
<tr>
<td>+Both (our final model)</td>
<td>213,857</td>
<td><strong>86.96</strong></td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;我们还将我们的最终系统与表4中其他参与者的系统进行了比较。从结果可以看出，我们的最终系统达到了最高的性能，特别是在 hard subset 上。这说明了我们的基于 seq2seq 的语义分析是可行和有效的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>ACC on full set</th>
<th>ACC on hard subset</th>
</tr>
</thead>
<tbody>
<tr>
<td>Soochow_SP (this paper)</td>
<td>85.68</td>
<td>57.43</td>
</tr>
<tr>
<td>NP-Parser</td>
<td>83.73</td>
<td>51.93</td>
</tr>
<tr>
<td>WLIS</td>
<td>82.53</td>
<td>47.83</td>
</tr>
<tr>
<td>Binbin Deng</td>
<td>68.82</td>
<td>35.41</td>
</tr>
<tr>
<td>kg_nlpca_ai_lr</td>
<td>30.79</td>
<td>14.89</td>
</tr>
<tr>
<td>TriJ</td>
<td>26.77</td>
<td>14.49</td>
</tr>
</tbody>
</table>
</div>
<h1 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h1><p>&emsp;&emsp;分析为什么预测出错。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/51. A Transformer-based Semantic Parser for NLPCC-2019 Shared Task 2.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/50. Enriching Word Vectors with Subword Information.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/50. Enriching Word Vectors with Subword Information.html" class="post-title-link" itemprop="url">论文笔记：Enriching Word Vectors with Subword Information</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-11-30 20:02:03" itemprop="dateCreated datePublished" datetime="2019-11-30T20:02:03+08:00">2019-11-30</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-13 23:36:51" itemprop="dateModified" datetime="2019-12-13T23:36:51+08:00">2019-12-13</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/abs/1607.04606.pdf" target="_blank" rel="noopener">论文地址</a>，作者 Piotr Bojanowski et al.，发表于 2016 年。</p>
          </div>
<h1 id="使用fasttext"><a href="#使用fasttext" class="headerlink" title="使用fasttext"></a>使用fasttext</h1><p>&emsp;&emsp;</p>
<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;现在流行的模型对单词表征的学习忽视了词法（morphology of word），它们直接给单词分配了不同的向量。这有一定的局限性，尤其对大规模词表并且含有大量稀有单词的语言。本论文提出基于 <strong>skipgram model</strong> 的方法，每个单词都被表示为一个 <strong>character n-grams</strong>（博主注：注意是 character，不是 word）词袋。每个 character n-grams 有一个向量，而单词由这些表征相加表示（即 e(where) = e(wh) + e(whe) + e(her) + e(ere) + e(re)，e() 表示 character n-grams 对应的向量）。<br>&emsp;&emsp;模型<strong>快</strong>，且可以计算那些不<strong>在训练数据中的单词表征</strong>（OOV 单词）。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;介绍了很多关于词嵌入的工作，可以参考。</p>
<h2 id="Morphological-word-representations"><a href="#Morphological-word-representations" class="headerlink" title="Morphological word representations"></a>Morphological word representations</h2><h2 id="Character-level-features-for-NLP"><a href="#Character-level-features-for-NLP" class="headerlink" title="Character level features for NLP"></a>Character level features for NLP</h2><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>&emsp;&emsp;首先提出通用模型，阐述我们如何训练词向量。然后提出 subword 模型，最终描述我们如何处理 character n-grams 的词表。</p>
<h2 id="通用模型"><a href="#通用模型" class="headerlink" title="通用模型"></a>通用模型</h2><p>&emsp;&emsp;</p>
<h2 id="Subword模型"><a href="#Subword模型" class="headerlink" title="Subword模型"></a>Subword模型</h2><p>&emsp;&emsp;<strong>对每一个单词赋予一个不同的向量这忽视了单词内在的结构</strong>。为了考虑这一信息，本节提出一个不同的<strong>评分函数</strong>（scoring function） <strong>s</strong>。<br>&emsp;&emsp;<strong>1）</strong>每一个单词 w 都被表示为一个 character n-gram 的词袋。<strong>2）</strong>我们在单词的开始和结尾增加了特殊的边界符号 <code>&lt;</code> 和 <code>&gt;</code>，以便<strong>区分来自其他单词的前缀和后缀</strong>。<strong>3）</strong>我们也将单词 w 本身放入了它的 n-grams 集合，以学习每一个单词（除了 character n-grams）的表征。以单词 <em>where</em> 和 <script type="math/tex">n = 3</script> 为例，它被表示为该 character n-grams：</p>
<script type="math/tex; mode=display">
<wh, whe, her, ere, re></script><p>&emsp;&emsp;以及它的特殊序列：</p>
<script type="math/tex; mode=display">
<where></script><p>&emsp;&emsp;注意序列 <code>&lt;her&gt;</code> 对应于单词 <em>her</em>。单词 <em>her</em> 不同于来自单词 <em>where</em> 的 trigram <em>her</em>。在实践中，我们提取所有 <script type="math/tex">3 <= n <= 6</script> 的 n-grams（<strong>博主注</strong>：如果我没理解错，是提取了所有的 3~6-grams）。这是一种非常简单的方法，并且可以考虑不同的 n-grams 集合（<strong>博主注</strong>：为什么可以考虑不同的 n-grams？因为一个单词他们提取了所有的 3~6-grams），例如取所有前缀和后缀。<br>&emsp;&emsp;假设给出一个 G 大小 n-grams 词表。给定一个单词 w，将其表示为 <script type="math/tex">G_w \subset \{1, \dots, G\}</script>，其中 n-grams 集合 <script type="math/tex">G_w</script> 会出现在 w 中。我们将一个向量表征 <script type="math/tex">\boldsymbol{z_g}</script> 与每一个 n-gram <script type="math/tex">g</script> 关联。而一个单词由其 n-grams 的向量表征相加表示。因此我们获得评分函数（<strong>博主注</strong>：这应该在计算相似度）：</p>
<script type="math/tex; mode=display">
\boldsymbol{
    s(w,c) = \sum_{g \in G_w} z^T_g v_c
}</script><p>&emsp;&emsp;这个简单的模型允许<strong>跨单词共享</strong>表征，从而允许学习<strong>稀有单词</strong>可靠的表征。（<strong>博主注</strong>：由于每个单词都被拆分开来，这样一个稀有单词就有很大概率可以由细碎的 n-grams 表示）<br>&emsp;&emsp;<strong>为了限制我们模型的内存需求</strong>，巴拉巴拉，我没看懂什么意思，大致意思是由于 n-grams 诞生的 token 太多了，所以需要使用 hash 的技巧来缩小存储空间。使用一个 token 来查找其对应的索引非常耗时，而使用 hash 算法就快多了。参考了：</p>
<ul>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-05-3" target="_blank" rel="noopener">fastText，智慧与美貌并重的文本分类及向量化工具</a></li>
</ul>
<h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><ol>
<li>baseline：几乎所有实验的基线都为 C 实现的 <a href="https://code.google.com/archive/p/word2vec" target="_blank" rel="noopener">word2vec 包</a>，除了论文中的 5.3 节；</li>
<li>optimization：对前面（<strong>博主注</strong>：通用模型一节中）提出的负对数似然估计，进行 <strong>SGD</strong> 优化。在基线的 skipgram 模型中，我们使用<strong>基于步长的线性衰减</strong>。给定一个包含 T 个单词的训练集，并且在上传递的次数等于 P，则 t 时刻的步长等于 <script type="math/tex">\gamma_0 (1 - \frac{t}{TP})</script>，其中 <script type="math/tex">\gamma_0</script> 是一个固定的参数（<strong>博主注</strong>：<strong>这部分的线性衰减没看懂什么意思</strong>）。我们通过使用 <strong><a href="http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent" target="_blank" rel="noopener">Hogwild</a></strong>（Recht et al., 2011）并行地执行优化，所有线程以异步方式共享参数和更新向量；</li>
<li>实现细节：</li>
<li>datasets：</li>
</ol>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>&emsp;&emsp;在以下几个方面评估我们的模型：</p>
<ol>
<li>Human similarity judgement</li>
<li>Word analogy tasks</li>
<li>Comparison with morphological representations：与顶尖模型的比较</li>
<li>Effect of the size of the training data</li>
<li>Effect of the size of n-grams</li>
<li>从我们的模型中获取到的词向量在语言模型任务中的评估</li>
</ol>
<h1 id="定性分析"><a href="#定性分析" class="headerlink" title="定性分析"></a>定性分析</h1><p>&emsp;&emsp;</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/50. Enriching Word Vectors with Subword Information.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习意见.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习意见.html" class="post-title-link" itemprop="url">学习意见</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-11-28 17:10:47" itemprop="dateCreated datePublished" datetime="2019-11-28T17:10:47+08:00">2019-11-28</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-12 23:04:29" itemprop="dateModified" datetime="2019-12-12T23:04:29+08:00">2019-12-12</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="推荐的论文"><a href="#推荐的论文" class="headerlink" title="推荐的论文"></a>推荐的论文</h1><div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>技术</th>
</tr>
</thead>
<tbody>
<tr>
<td>tricks</td>
<td></td>
</tr>
<tr>
<td>模型</td>
<td></td>
</tr>
<tr>
<td>优化器</td>
<td></td>
</tr>
<tr>
<td>word embedding</td>
<td>fasttext</td>
</tr>
<tr>
<td>nlp任务模型</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>参考：</p>
<ol>
<li><a href="https://www.zhihu.com/question/31785984" target="_blank" rel="noopener">深度学习入门必看的书和论文？有哪些必备的技能需学习？</a>，直接定位作者：<strong>景略集智</strong>。</li>
</ol>
<h2 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h2><ol>
<li><a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">Improving neural networks by preventing co-adaptation of feature detectors</a>，论文作者 Geoffrey Hinton 等人，Hinton 提出了 <strong>Dropout</strong>。</li>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="noopener">Dropout: a simple way to prevent neural networks from overfitting</a>，论文作者 Srivastava Nitish，对 <strong>dropout</strong> 的理解。</li>
<li><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch normalization: Accelerating deep network training by reducing internal covariate shift</a>，论文作者 Ioffe Sergey 和 Christian Szegedy，提出 <strong>Batch normalization</strong>。</li>
<li><a href="https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&amp;utm_medium=refer&amp;utm_campaign=promote" target="_blank" rel="noopener">Layer normalization</a>，作者 Jamie Ryan Kiros 和 Geoffrey E. Hinton 等人，本篇论文是对 Batch Normalization 的进一步研究成果，提出 <strong>Layer normalization</strong>。</li>
</ol>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ol>
<li><a href="https://www.semanticscholar.org/paper/Binarized-Neural-Networks%3A-Training-Deep-Neural-and-Courbariaux-Hubara/6eecc808d4c74e7d0d7ef6b8a4112c985ced104d" target="_blank" rel="noopener">Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1</a>，论文作者 Courbariaux Matthieu 等，提出了一种速度非常快的新模型。</li>
<li><a href="https://arxiv.org/pdf/1608.05343.pdf" target="_blank" rel="noopener">Decoupled neural interfaces using synthetic gradients</a>，论文作者 Jaderberg Max 等，论文提出了一种非常创新的训练方法。</li>
</ol>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><ol>
<li><a href="http://proceedings.mlr.press/v28/sutskever13.pdf" target="_blank" rel="noopener">On the importance of initialization and momentum in deep learning</a>，作者 Sutskever Ilya 等，论文提出了 <strong>Momentum optimizer</strong>。</li>
<li><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">Adam: A method for stochastic optimization</a>，作者 Kingma Diederik 等，提出 Adam。</li>
</ol>
<h2 id="word-embedding"><a href="#word-embedding" class="headerlink" title="word embedding"></a>word embedding</h2><ol>
<li><a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="noopener">Enriching Word Vectors with Subword Information</a>，作者 Piotr Bojanowski 和 Edouard Grave，提出了 <strong>fasttext</strong>。<a href="https://github.com/facebookresearch/fastText/" target="_blank" rel="noopener">代码地址</a>，<strong><a href="https://yan624.github.io/论文/50. Enriching Word Vectors with Subword Information.html">论文笔记</a></strong>。应用场景：处理 OOV 词或者低频词。</li>
</ol>
<h2 id="nlp任务模型"><a href="#nlp任务模型" class="headerlink" title="nlp任务模型"></a>nlp任务模型</h2><ol>
<li><a href="https://arxiv.org/pdf/1308.0850.pdf" target="_blank" rel="noopener">Generating sequences with recurrent neural networks</a>，论文作者 Graves Alex，使用 RNN 进行生成序列。</li>
<li><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Learning phrase representations using RNN encoder-decoder for statistical machine translation</a>，论文作者 Cho Kyunghyun，第一篇 <strong>seq2seq</strong> 论文。</li>
<li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">Sequence to sequence learning with neural networks</a>，论文作者 Sutskever Ilya 等人。</li>
<li><a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Memory networks</a>，论文作者 Weston Jason 和 Sumit Chopra。</li>
<li><a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">End-to-end memory networks</a>，论文作者 Sukhbaatar Sainbayar 和 Jason Weston。</li>
<li><a href="http://web.eecs.utk.edu/~ielhanan/courses/ECE-692/Bobby_paper1.pdf" target="_blank" rel="noopener">Long short-term memory</a>，LSTM。</li>
<li><a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" target="_blank" rel="noopener">Pointer networks</a>，作者 Vinyals Oriol 和 Meire Fortunato 等。</li>
<li><a href="https://arxiv.xilesou.top/pdf/1409.0473.pdf" target="_blank" rel="noopener"></a></li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li><a href="http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf" target="_blank" rel="noopener">Deep learning</a>，Hinton,lecun,bengio 三巨头写的 deep learning 综述。</li>
<li>《Neural Networks Tricks of the Trade》，调参技巧。</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习意见.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/zcy/特征工程：笔记.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/zcy/特征工程：笔记.html" class="post-title-link" itemprop="url">特征工程：笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-11-16 18:13:50" itemprop="dateCreated datePublished" datetime="2019-11-16T18:13:50+08:00">2019-11-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-11-23 12:33:30" itemprop="dateModified" datetime="2019-11-23T12:33:30+08:00">2019-11-23</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="《特征工程入门与实践》"><a href="#《特征工程入门与实践》" class="headerlink" title="《特征工程入门与实践》"></a>《特征工程入门与实践》</h1><h1 id="3、特征增强：清洗数据"><a href="#3、特征增强：清洗数据" class="headerlink" title="3、特征增强：清洗数据"></a>3、特征增强：清洗数据</h1><h2 id="填充缺失值"><a href="#填充缺失值" class="headerlink" title="填充缺失值"></a>填充缺失值</h2><p>&emsp;&emsp;通常数据集会因为各种原因有所缺失。必须尽可能地了解数据集，以便找到使用<strong>其他符号填充的</strong>确实数据。<strong>公开数据集的文档</strong>里面有可能会提到缺失数据的问题。<br>&emsp;&emsp;如果没有文档，缺失值的常见填充方法有：</p>
<ul>
<li>0（数值型）</li>
<li>unknown 或 Unknown（类别型）</li>
<li>?（类别型）</li>
</ul>
<h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><p>&emsp;&emsp;如果 pandas 将缺失值自动填充了 0（需要自行判断 0 是否缺失值），那么可以先用 python 的 None 填充缺失值，以便使用 pandas 的 fillna, dropna and isnull 等方法。<br>&emsp;&emsp;现在开始介绍方法，处理缺失值的主要办法是：1. 删除缺失值的行；2. 填充缺失值。</p>
<ol>
<li>最常见也是最容易的方法大概是直接删除存在缺失值的行。通过这种操作，我们会留下具有数据的完整数据点。可以使用 pandas 的 dropna 方法获取新的 DataFrame。<br>但是我们可能会丢失大量的原始数据（书中的例子使用的是《皮马印第安人糖尿病预测数据集》，丢失 51% 的行）。从机器学习的角度考虑，尽管数据都有值、很干净，但是我们没有利用尽可能多的数据。经书中使用 pandas 分析，某些数据的均值下降严重，所以<strong>我们应该保留下尽可能多的数据</strong>。<br>然后此书的作者使用去除缺失值的数据集运行了一个 KNN 模型，最终得到最好的结果为：k=7，acc=74.5%。但是如果用到所有的数据，会不会更好？</li>
<li>填充缺失值是一种更复杂的方法。<strong>填充</strong>指用现有的知识/数据来确定缺失的数量值，并填充的行为。<br>我们有几种选择，最常见的是用<strong>此列其余部分的均值</strong>填充缺失值。可以使用 pandas 的 fillna(mean_value) 方法即可填充，但是这有点麻烦，我们可以选用 scikit-learn 预处理类的 Imputer 模块，它更简单。只需指定策略 <script type="math/tex">Imputer = Imputer(strategy='mean')</script>，再调用 <script type="math/tex">pima_imputed = imputer.fit_transform(pima)</script> 即可。<br>那么来验证一下这样填充的 acc 如何。首先全部填充 0 用来充当对照组，发现 KNN 模型的 acc=73.31%，明显低于 74.5%。然后填充均值，发现 acc=65.625%，居然更低了。<br>这里需要解释一点，其实使用上述的方法去填充缺失值是错误的。我们<strong>将整个数据集的一列的均值去填充对应列的缺失值</strong>实际上犯了一个错误，即<strong>当预测测试集的响应值（即 y）时，不能假设我们已经知道了整个数据集的均值</strong>。所以我们应该<strong>使用训练集的均值去填充训练集和测试集的缺失值</strong>。注：我们假设测试集是未知的，所以它并没有均值，并且我们使用了训练集的信息去训练，所以我们需要使用训练集的均值去填充测试集的缺失值。<br><strong>但是</strong>最 sao 的是，此书中用正确的填充方法，最后得到的 acc=73.18%，比直接填充 0 还低。另外使用<strong>中位数</strong>填充得到的 acc=73.57%，始终没有高于<strong>直接删除缺失值</strong>得到的 acc。</li>
</ol>
<h2 id="标准化与归一化"><a href="#标准化与归一化" class="headerlink" title="标准化与归一化"></a>标准化与归一化</h2><p>&emsp;&emsp;仔细观察数据，我们发现数据的大小差别很大。而某些机器学习模型受数据尺度（scale）的影响很大。数据工程师可以选用某种归一化操作。我们将重点关注 3 种归一化方法，<strong>前两个方法特别用于调整特征，第三个方法虽然操作行，但效果与前两个相当</strong>。</p>
<ul>
<li>z 分数标准化</li>
<li>min-max 标准化</li>
<li>行归一化</li>
</ul>
<p>&emsp;&emsp;<strong>z 分数标准化</strong>是最常见的标准化技术，即均值归一化。使得输出会被重新缩放，使<strong>均值为 0、标准差为 1</strong>。公式为：</p>
<script type="math/tex; mode=display">
    z = \frac{x - \mu}{\sigma}</script><ul>
<li>z 是新的值</li>
<li>x 是单元格中原来的值</li>
<li><script type="math/tex">\mu</script> 是该列的均值</li>
<li><script type="math/tex">\sigma</script> 是列的标准差</li>
</ul>
<p>&emsp;&emsp;<strong>min-max 标准化</strong>与 z 分数标准化类似，它也用一个公式替换列中的每个值。它会使得每一列的值都位于 [0,1] 。公式为：</p>
<script type="math/tex; mode=display">
    m = \frac{(x - x_{min})}{x_{max} - x_{min}}</script><ul>
<li>m 是新的值；</li>
<li>x 是单元格原来的值；</li>
<li><script type="math/tex">x_{min}</script> 是该列的最小值；</li>
<li><script type="math/tex">x_{max}</script> 是该列的最大值。</li>
</ul>
<p>&emsp;&emsp;<strong>行归一化</strong>是关于行的，而不是关于列的。它不是计算每列的统计值（均值、最小值、最大值），而是保证每行都有<strong>单位范数</strong>，意味着每行的向量长度相同。计算方式如下所示，即 L2 范数，其他范数方式这里不讨论。</p>
<script type="math/tex; mode=display">
    ||x|| = \sqrt{(x^2_1 + x^2_2 + \dots + x^2_n)}</script><h2 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h2><p>&emsp;&emsp;最后我们将填充缺失值的方法和标准化（或归一化）的方法结合起来用，在 pima 数据集上发现使用<strong>均值填充 + min-max 标准化</strong>的交叉验证准确率最高。</p>
<h1 id="4、特征构建：我能生成新特征吗"><a href="#4、特征构建：我能生成新特征吗" class="headerlink" title="4、特征构建：我能生成新特征吗"></a>4、特征构建：我能生成新特征吗</h1><ol>
<li>填充分类特征</li>
<li>编码分类变量</li>
<li>扩展数值特征：多项式</li>
<li>针对文本：词袋模型、TF-IDF…</li>
</ol>
<h1 id="5、特征选择：对坏属性说不"><a href="#5、特征选择：对坏属性说不" class="headerlink" title="5、特征选择：对坏属性说不"></a>5、特征选择：对坏属性说不</h1><p>&emsp;&emsp;</p>
<h1 id="6、特征转换：数学显神通"><a href="#6、特征转换：数学显神通" class="headerlink" title="6、特征转换：数学显神通"></a>6、特征转换：数学显神通</h1><p>&emsp;&emsp;</p>
<h1 id="7、特征学习：以AI促AI"><a href="#7、特征学习：以AI促AI" class="headerlink" title="7、特征学习：以AI促AI"></a>7、特征学习：以AI促AI</h1><p>&emsp;&emsp;</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/zcy/特征工程：笔记.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/比赛项目总结/多领域seq2lf.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/比赛项目总结/多领域seq2lf.html" class="post-title-link" itemprop="url">多领域seq2lf</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-11-05 22:08:08" itemprop="dateCreated datePublished" datetime="2019-11-05T22:08:08+08:00">2019-11-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-11-22 19:37:43" itemprop="dateModified" datetime="2019-11-22T19:37:43+08:00">2019-11-22</time>
              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/project/" itemprop="url" rel="index"><span itemprop="name">project</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>&emsp;&emsp;本竞赛涉及知识点或技巧如下：</p><ol><li>LSTM</li><li>pytorch loss 的使用以及一些坑</li></ol>
          </div>
<h1 id="总结写在前"><a href="#总结写在前" class="headerlink" title="总结写在前"></a>总结写在前</h1><h2 id="pytorch-loss-function"><a href="#pytorch-loss-function" class="headerlink" title="pytorch loss function"></a>pytorch loss function</h2><p>&emsp;&emsp;pytorch 的多元分类 loss 有 CrossEntropyLoss 和 NLLLoss。NLLLoss 全称 Negative Log Likelihood Loss，说白了就是求对数概率并取负，我们从函数图像就可以理解。模型输出的概率分布在 0-1 之间，log 函数的 0-1 区间正好全是负数，所以要加上一个负号，让 loss 值为正数。显而易见，概率越接近 1，loss 值越小。接下来描述一下这两个函数。</p>
<ol>
<li>CrossEntropyLoss = LogSoftmax + NLLLoss；</li>
<li>CrossEntropyLoss 中已经附带了 log_softmax 操作，所以如果你想省事，那么直接将输出向量输入 CrossEntropyLoss 即可；</li>
<li>如果使用 NLLLoss，那么在使用 NLLLoss 之前，还需要经过一层 LogSoftmax。</li>
</ol>
<p>&emsp;&emsp;需要注意一点，我感觉网上很多人也没有理解什么是 CrossEntropyLoss，导致很多人都被误导了。首先 nll 的公式如下：</p>
<script type="math/tex; mode=display">
nll\_loss = -log(pred)</script><div class="note warning">
            <p>&emsp;&emsp;nll loss 可以有多种表达方式，我把在网上看到的公式都罗列一下。<br>&emsp;&emsp;以下的公式与 crossentropy 一样。（实际上公式就是一样的，只不过在概念上有点不同，由于输出值 y 是 one hot 形式，为 0 时，就相当于没有加，最后的结果就是上面的公式）</p><script type="math/tex; mode=display">nll\_loss = -\sum^n_{i=1} y_i log(pred_i) = -log(pred)</script><p>&emsp;&emsp;这里的 class 就是指第几个标签，它不是 one hot 表示形式。大家会发现这里少了一个 log 函数，实际上 pred 是使用 log_softmax 函数计算之后的结果。</p><script type="math/tex; mode=display">nll\_loss = -pred[class]</script>
          </div>
<p>&emsp;&emsp;CrossEntropyLoss 公式如下：</p>
<script type="math/tex; mode=display">
crossentropy\_loss = -\sum^n_{i=1} y_i log(pred_i)</script><p>&emsp;&emsp;<del>无法理解的原因之一是，在学机器学习的时候，大家都知道啥是 crossentropy，后来在学多元分类时，开始分 binary_crossentropy 和 crossentropy。这点大家都能理解，但是到看到 NLLLoss 时，就开始懵逼了。</del><br>&emsp;&emsp;<del>由于 CrossEntropyLoss = LogSoftmax + NLLLoss，在 crossentropy 的公式中貌似没有出现 softmax（更没有 log_softmax），所以开始懵了，无法理解其中的 LogSoftmax 是干啥的。</del><br>&emsp;&emsp;首先我要解释一点 CrossEntropyLoss 是 LogSoftmax 和 NLLLoss 两个步骤之和，之前说的“+”号，并非是数学意义上的加号。也就是说，CrossEntropyLoss 就比 NLLLoss 多做了一步 LogSoftmax（<strong>博主注</strong>：<em>个人认为实际上只是多做了一步 softmax，说多做了一步 log_softmax，是因为站在 pytorch 框架的角度</em>）。<br>&emsp;&emsp;其次，对于真实输出值 y 来说，无非就是 0 和 1（注意多元分类也只有 0 和 1），并且根据上述 crossentropy 的公式。实际上公式可以化简为以下所示，其中的 m 代表真实值为 1 的索引。</p>
<script type="math/tex; mode=display">
crossentropy\_loss = -\sum^n_{i=1} y_i log(pred_i) = -log(pred_m)</script><p>&emsp;&emsp;请注意这里的 <script type="math/tex">pred_m</script>。我们都知道在进行分类问题时，我们需要将输出结果置于 0-1 之间，对于二元分类我们使用 sigmoid 函数，对于多元分类我们使用 softmax（到这开始有内味了）。由于分类问题都是要这么做的，所以将 softmax 这个函数放到公式 <script type="math/tex">crossentropy\_loss = -log(pred_m)</script> 中，我们惊奇的发现 crossentropy 函数变成了 log_softmax（最前面的负号暂时不看）。即 crossentropy + softmax = -log_softmax。</p>
<div class="note warning">
            <p>&emsp;&emsp;请始终留意，pred 是一个向量通过 softmax/log_softmax 计算之后的值。</p>
          </div>
<p>&emsp;&emsp;最后你会发现这样还是不对。<script type="math/tex">nll\_loss = -log(pred)</script>，之前说 CrossEntropyLoss = <strong>LogSoftmax</strong> + NLLLoss，我把 log_softmax 放到 nll 里，变成了 <script type="math/tex">LogSoftmax + NLLLoss = -log(log(pred))</script>，怎么多了一个 log？实际上 nll 的公式应该以 <script type="math/tex">nll\_loss = -pred[class]</script> 为准，你会发现这个公式中没有 log 函数。这样将 logsoftmax 放入 nll loss 中，就正好是 crossentropy 了。<br>&emsp;&emsp;那么你就会问 nll 明明是 Negative Log Liklihood，log 不见了，这不就是名存实亡了？<br>&emsp;&emsp;<strong>这可能是因为 pytorch 想要简化操作，才这么设置的，别的框架可能并不是这样。简而言之，pytorch 框架中，nll loss 的公式是 -pred。crossentropy 的公式是 logsoftmax + nll loss，即 nll(log_softmax(output))</strong><br>&emsp;&emsp;<strong>也就是说，如果神经网络的最后一层输出是 logsoftmax，那么就使用 nll loss（上一段 nll loss 那个 pred 就是通过 log_softmax 的输出值）。如果最后一层只是输出，偷懒不想写 logsoftmax，那么就使用 crossentropy loss（上一段 crossentropy 中的 output 就是一个普通的神经网络输出）。</strong><br>&emsp;&emsp;参考：</p>
<ol>
<li><a href="http://blog.leanote.com/post/lee-romantic/crossentry" target="_blank" rel="noopener">CrossEntropyLoss和NLLLoss的理解</a></li>
<li><a href="https://www.cnblogs.com/ranjiewen/p/10059490.html" target="_blank" rel="noopener">Pytorch之CrossEntropyLoss() 与 NLLLoss() 的区别</a></li>
<li><a href="https://blog.csdn.net/m0_38133212/article/details/88087206" target="_blank" rel="noopener">CrossEntropyLoss与NLLLoss的总结</a></li>
<li><a href="https://www.cnblogs.com/marsggbo/p/10401215.html" target="_blank" rel="noopener">Pytorch里的CrossEntropyLoss详解</a></li>
</ol>
<h3 id="顺便一提KLDivLoss"><a href="#顺便一提KLDivLoss" class="headerlink" title="顺便一提KLDivLoss"></a>顺便一提KLDivLoss</h3><p>&emsp;&emsp;<a href="https://www.cnblogs.com/charlotte77/p/5392052.html" target="_blank" rel="noopener">【原】浅谈KL散度（相对熵）在用户画像中的应用</a><br>&emsp;&emsp;暂时还没用过这个 loss，简单来说，是用来比较两个概率分布之间的信息熵差异，如 AB 两组群体，有对某一商品的总消费分布 P 和群体人数的分布 Q，可以计算 PQ 之间的信息熵差异，从而获得 AB 两组群体对该商品的偏爱程度。</p>
<h2 id="optimizer-zero-grad"><a href="#optimizer-zero-grad" class="headerlink" title="optimizer.zero_grad()"></a>optimizer.zero_grad()</h2><p>&emsp;&emsp;在 pytorch 中，为什么要在每个循环之初调用这个方法？因为 pytorch 把计算的每个梯度都累加起来，并不会每迭代一次就将梯度清零。这样做看起来令人费解，并反常理。但是实际上这样做可以做更多神奇的操作，比如</p>
<ul>
<li><a href="https://www.zhihu.com/question/303070254" target="_blank" rel="noopener">https://www.zhihu.com/question/303070254</a></li>
</ul>
<p>&emsp;&emsp;还有，试想本来你想运行 batch_size=1024，但是由于电脑太差，只能运行 batch_size=256 的批次数据。那么只需要每循环两次调用一次 zero_grad() 即可。<br>&emsp;&emsp;参考：</p>
<ol>
<li><a href="https://blog.csdn.net/u011959041/article/details/102760868" target="_blank" rel="noopener">pytorch中为什么要用 zero_grad() 将梯度清零</a></li>
</ol>
<h2 id="clip-gradient"><a href="#clip-gradient" class="headerlink" title="clip gradient"></a>clip gradient</h2><p>&emsp;&emsp;<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">此论文</a>提出了 clip gradient（以下称 clipping）。首先看下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/多领域seq2lf/the error surface is rough.jpg" alt="the error surface is rough"></p>
<p>&emsp;&emsp;如果只看图，会发现有一个像峭壁一样的东西，它就是罪魁祸首。当我们将一个小球往前移动时，有时候正好迈过峭壁，小球得以正常移动。但是当小球碰到峭壁时，小球就会被反弹回去，导致 loss 发生剧烈变化。<br>&emsp;&emsp;从数学角度来看，那个峭壁就是梯度。<strong>需要注意的是</strong>，由于 z 轴标注的 total loss，所以第一印象感觉峭壁代表 total loss，但是<strong>峭壁代表的是梯度，而不是 total loss</strong>。根据参数更新公式 <script type="math/tex">w -= \alpha * \Delta w</script>，其中 <script type="math/tex">\Delta w</script> 代表 w 的梯度，所以 w 的更新方向其实与梯度直接相关。<strong>当 w 不幸到达某个值时，遇到梯度极大的情况，那么不管梯度是正还是负，都会将 w 更新到一个相对很大的值，从而 loss 值也会跟着改变。注：这里其实也与 learning rate 有关，因为原本的梯度都很小，所以我们初始设置的 lr 都很大。突然梯度增大，而 lr 没有适应，一个大的梯度乘上一个大的 lr，那就更大了</strong>。<br>&emsp;&emsp;<strong>解决办法</strong>是：当 gradient 大于某个 threshold 时，就不让它大于 threshold（视频中说作者的代码中将 threshold 设置为 15，但是这个 threshold 应该是具体情况具体分析）。<br><div class="note warning">
            <p>&emsp;&emsp;Q：那么是什么导致了出现梯度猛增的现象呢？（ps：梯度消失也会出现）<br>&emsp;&emsp;A：首先不是 sigmoid 的锅。sigmoid 函数会出现 gradient vanishing（梯度消失）的问题，按理说换成 ReLU 就能解决了。但是将 RNN 的激活函数换成 ReLU 并没有解决问题，所以跟激活函数没什么关系。<em><a href="https://yan624.github.io/zcy/对神经网络整体的理解.html#缺陷">sigmoid 的缺陷</a>这一章说明了 sigmoid 具有梯度消失的缺陷</em>。<br>&emsp;&emsp;我们只需要小小地改变一个参数，然后观察 output 的变化，就能测出这个参数的 gradient 的大小。现在假设激活函数是 y = w * x，序列长度为 1000，并且第一个 timestep 的输入为 1，其他的 timestep 皆为 0，则<script type="math/tex">1^{1000} = 1</script>，而 <script type="math/tex">1.01^{1000} \approx 20000</script>。我们可能会想到去减小 learning rate 从而减小梯度，但是 <script type="math/tex">0.99^{1000} \approx 0</script>，你又要调大 learning rate，导致 lr 调起来很麻烦，所以调不调 lr 并没有什么区别。而这里就分别出现了梯度消失和梯度爆炸。这是由于 RNN 是序列模型，它需要处理一连串的序列。前一个的输出是后一个输入，类似于蝴蝶效应，一个很小的值，经过多个函数也能被放的很大。<br>&emsp;&emsp;那么为什么可以通过观察一个参数的变化从而观察 gradient 的变化呢？很简单，例如 y = w * x，用上面的例子，第 1000 个输出会是 <script type="math/tex">y_{1000} = w^{1000} * x</script>，对 w 求导得 <script type="math/tex">\Delta w = 1000 w^{999} x</script>，由于 <script type="math/tex">w^{999}</script> 比 <script type="math/tex">w^{1000}</script> 消不了多少，所以 gradient 很大。<br>&emsp;&emsp;经过上面的例子我们就知道了问题出现的原因。当然一般情况下不会像上面序列长度有 1000 那么离谱。现在设序列长度就 40，当我们遇到一个较大的值时，依旧会出现梯度爆炸的问题。<br>&emsp;&emsp;解决办法：LSTM，clipping 等。<strong>注意：LSTM 只解决了 gradient vanishing 的问题，没有解决 gradient explode。详见下面的《LSTM 解决了 RNN 的什么问题》章节</strong>。同理 clipping 也只能解决 gradient explode 的问题，因为它能将梯度限制在一个点上（<strong>这是博主的思考，视频中并没有给出此结论</strong>）。那么我们可以结合 LSTM 和 clipping。<br>&emsp;&emsp;根据上面的例子得出，在 RNN 中，即使参数在很小一个范围内，梯度的变化也会很大。<br>&emsp;&emsp;最后还有一个问题，就是上面的例子是用 y = w * x 的例子，但是实际上，激活函数是用的 sigmoid, tanh 等，所以每个神经元的输出都永远在 [-1, 1] 中间。那么怎么会造成上面的例子中的情况呢？其实上面的例子只是一个比喻，当然当激活函数为 ReLU 时确实适用。真正的原因是反向传播时做的链式求导，它导致了梯度的连乘。详细推导见 <a href="https://yan624.github.io/zcy/深度学习算法（二）：simple RNN 推导与理解.html#simple-RNN-的缺陷">simple RNN 的缺陷</a> 蓝色提示框。</p>
          </div></p>
<p>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://www.bilibili.com/video/av10590361?p=37" target="_blank" rel="noopener">P37 26: Recurrent Neural Network (Part II)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34203833" target="_blank" rel="noopener">深入理解lstm及其变种gru</a></li>
</ol>
<p>&emsp;&emsp;以下对新闻分类进行测评，将使用了 clipping 与没使用 clipping 的结果进行对比。</p>
<ol>
<li>测评一条新闻的分类：“<strong>广西最美的县城，很多人第一次旅行就是去这里</strong>”。没有使用 clip gradient 的结果是：<strong>news_agriculture</strong>。使用 clip gradient 的结果是：<strong>news_travel</strong>。</li>
<li>测评一条新闻的分类：“<strong>在越南游玩，有漂亮女子问“要不要生菜”，这是什么意思？</strong>”。没有使用 clip gradient 的结果是：<strong>news_military</strong>。使用 clip gradient 的结果是：<strong>news_agriculture</strong>。</li>
<li>测评一条新闻的分类：“<strong>去西藏旅游时，导游反复叮嘱：无论多脏都最好不要洗澡？</strong>”。没有使用 clip gradient 的结果是：<strong>news_agriculture</strong>。使用 clip gradient 的结果是：<strong>news_story</strong>。</li>
</ol>
<p>&emsp;&emsp;使用 clip gradient 之后，分类结果好像有点正确了，第二句里面有菜，所以是农业。第三句跟农业好像没什么关系，但是跟 story 虽然说不上有关系，但是好像也不是完全没关系。另外以上三条新闻的正确分类其实都是<strong>旅行</strong>。需要说明的是这只是随便一个例子，用来说明加了 clip gradient 之后，对网络的预测有点用。示例中所用的神经网络只是一个简单的 LSTM，仅供参考：）。</p>
<h2 id="learning-rate-decay"><a href="#learning-rate-decay" class="headerlink" title="learning rate decay"></a>learning rate decay</h2><p>&emsp;&emsp;在什么时候需要使用 learning rate decay？</p>
<h2 id="LSTM-解决了-RNN-的什么问题"><a href="#LSTM-解决了-RNN-的什么问题" class="headerlink" title="LSTM 解决了 RNN 的什么问题"></a>LSTM 解决了 RNN 的什么问题</h2><p>&emsp;&emsp;解决了两点。仅为个人观点。</p>
<ol>
<li>memory 机制</li>
<li>梯度消失</li>
</ol>
<h3 id="memory机制"><a href="#memory机制" class="headerlink" title="memory机制"></a>memory机制</h3><p>&emsp;&emsp;<strong>第一</strong>，RNN 受到短期记忆的影响。如果序列很长，他们将很难将信息从较早的时间步传送到后面的时间步。LSTM 通过改进 memory，可以更好地保留序列信息。<br>&emsp;&emsp;在每个时间点， RNN 都只用每个 cell 的 output 覆盖 memory 里的值，即每个 tiemstep 中的信息都会被覆盖掉。而在 LSTM 中，它会将 memory 乘上一个权重再加上 input，从而获得新的 memory。它不会每次都 forget memory，除非 forget gate 计算结果等于 0。从公式的角度看就是：</p>
<script type="math/tex; mode=display">
RNN: memory_{new} = cell(memory, input) \\
LSTM: memory_{new} = a * input + b * memory \\</script><p>&emsp;&emsp;cell() 代表一个简单的 sigmoid 函数，memory 与 input 可以做拼接处理，也可以 memory + input，具体自己设计。a 是 input gate 的计算结果，b 是 forget gate 的计算结果，说白了都是一个权重，可以忽略。<br>&emsp;&emsp;这样乍一看好像 RNN 和 LSTM 没什么区别，它们都是会进行一些计算，然后获得一个新的 memory。但是 RNN 的计算方式是将 memory 与 input <strong>一起</strong>输入进 neuron，从而产生一个 output，最后将这个 output 作为新的 memory。你会发现，在 RNN 中，虽然 output 是由 memory 和 input 计算得来的，但是在更新 memory 时不是采用 LSTM 的策略，而是直接用 output 将 memory 覆盖掉，<strong>这既没考虑到原 memory 的值，也没考虑当前 input 的值</strong>。而 LSTM 在覆盖 memory 时，会考虑当前 memory 以及 input 的值。<strong>理解该段的重点是：下面的 1</strong>。</p>
<ol>
<li><strong>个人理解</strong>：RNN 看似用到了原 memory 和 input，但是在实际计算时（即 cell() 函数所做的操作），由它俩 train 出来的权重矩阵只是为了使 cell() 计算的结果尽可能地接近 y，而并非在计算一个好的 memory。这里尤其要注意，cell() 函数的功能跟 memory 没关系，RNN 与 memory 有关的操作仅仅只有一步，即 <script type="math/tex">memory_{new} = output</script>，它只是将以前的 memory 覆盖掉。而 LSTM 不光在更新 memory 时用到了原 memory 和 input，它里面的 3 个 gate 也都需要通过 input 计算，所以 input 对 LSTM 的输出影响很大，对 memory 的更新自然也大。</li>
<li><strong>李宏毅机器学习视频中的说法</strong>：如果 weight 可以影响到 memory 里的值，那么这个影响会一直存在。我觉得李宏毅老师的讲解跟我的应该差不多，重点也是 LSTM 多了一个可以训练 memory 的权重。</li>
<li>memory 本质是在记忆之前所有的 input。</li>
</ol>
<p>&emsp;&emsp;其实 forget gate 还是有几率清空 memory 的，那么为什么不直接取消 forget gate 呢？你不是要充分利用 input 吗？实际上在 LSTM 的第一个版本是没有 forget gate 的，它是后来才加上去的。甚至现在的说法是在训练 LSTM 时，不要给 forget gate 太大的权重，要让它大部分时间都是开着的，即大部分时间都不要清空 memory。如果以后训练 LSTM 时，觉得过拟合严重，可以使用 GRU，GRU 只有两个 gate（无 output gate）。（<em>引用<a href="https://www.bilibili.com/video/av10590361?p=37" target="_blank" rel="noopener">教学视频</a> 22:50 的话</em>）。 </p>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>&emsp;&emsp;<strong>第二</strong>，反向传播时出现的问题，以下 Q 为问题，A 为解释。<br>&emsp;&emsp;引子：如《clip gradient》一章黄框中所说，RNN 很容易出现峭壁和平原。<strong>LSTM 只解决了 gradient vanishing 的问题</strong>，没有解决 gradient explode。LSTM 使得 error surface 不那么崎岖，<strong>消除了训练时的一些平坦的地方</strong>。虽然梯度在有些地方依然崎岖，但是不会有太平坦的地方。<strong>所以在训练时可以放心的将 lr 调小</strong>，不需要担心会出现平坦的地方，导致训练过慢。<br>&emsp;&emsp;如果公司问为什么把 RNN 换成 LSTM？<del>回答 LSTM 比较潮、因为 LSTM 比较复杂。</del>回答 LSTM 可以处理 gradient vanishing 的问题。具体解释如下：<br>&emsp;&emsp;<strong>Q</strong>：为什么 LSTM 可以解决 gradient vanishing 的问题？（解决梯度消失也可以说成避免 gradient 特别小（消除平原））<br>&emsp;&emsp;<strong>A</strong>：在 RNN 中利用 memory 的方式是一种复合函数的结构，所以在反向传播时，需要链式求导，梯度与梯度相乘容易造成<strong>梯度消失</strong>和<strong>梯度爆炸</strong>。关于 RNN 反向传播的求导结果可以参考 <a href="https://yan624.github.io/zcy/深度学习算法（二）：simple RNN 推导与理解.html#simple-RNN-的缺陷">simple RNN 的缺陷</a> 蓝色提示框。<br>&emsp;&emsp;虽然这样的求导大致已经可以解释了梯度消失的问题，但是如果仔细想想就会发现盲点。在此之前，我想先说明 RNN 家族的反向传播路径与其他的神经网络不同，它的 loss 值是每一个 timestep 的真实值 y 与输出值 的 loss 之和。<a href="https://mooc.study.163.com/learn/2001280005?tid=2001391038&amp;_trace_c_p_k2_=72573d316c3441869416d70899cdf382#/learn/content?type=detail&amp;id=2001770031" target="_blank" rel="noopener">此视频</a> 大致讲明白了这个总 loss 值到底是由哪些 loss 相加得到的。<br>&emsp;&emsp;知道了上面的前提条件，就可以很简单的理解接下来的内容了。<strong>参考资料 1 大致解释了这一问题</strong>，这一段可能比较绕，<strong>简单来说就是后面的 timestep（比如下图中 <script type="math/tex">loss_4</script>）在反向传播时，求 <script type="math/tex">\Delta W</script> 会出现梯度消失（注意 RNN 每个 timestep 的 W 都是一样），这是因为在求梯度时，函数已经复合了好几层</strong>。而对 <script type="math/tex">loss_1</script> 求 W 的导数时，由于它本身就在序列的前面，函数还没有复合，所以 <script type="math/tex">\Delta W</script> 的导数还没梯度消失。<strong>最后在计算总的 loss 时，是将各个阶段的梯度加起来</strong>，即使后面的 loss 会得到一个很小的的梯度 <script type="math/tex">\Delta W</script>，但由于 <script type="math/tex">loss_1</script> 的原因，并不会发生梯度消失。<br>&emsp;&emsp;但是事实上是会发生的，那么梯度消失从何而来呢？是因为后面的 loss 在求梯度时，导致前面输入值的梯度很小，从而产生了<strong>信息丢失</strong>。信息丢失就是 RNN 的梯度消失。有一点需要考虑，RNN 在反向传播时，是需要传播到输入值 x 的，即词向量。在计算 <script type="math/tex">loss_4</script> 时，RNN 中 W 的梯度肯定是很小的，但是在更新 <script type="math/tex">x_4</script> 时，这里的梯度 <script type="math/tex">\Delta W</script> 还不是很小，所以信息无问题。但是当反向传播到 <script type="math/tex">x_1</script> 时，梯度已经很小了，由于小梯度导致 <script type="math/tex">x_1</script> 无法得到很好的更新，于是产生了信息丢失。<script type="math/tex">loss_3</script> 以及 <script type="math/tex">loss_2</script> 以此类推，不过对于 <script type="math/tex">loss_1</script> 并无问题，因为它没有复合函数。<br>&emsp;&emsp;注：<strong>上两段，还参考了参考资料 3，个人认为参考资料 1 中内容并不是很完整</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/多领域seq2lf/total loss.jpg" alt="total loss"></p>
<p>&emsp;&emsp;以上的参考资料：</p>
<ol>
<li><a href="https://www.zhihu.com/question/34878706/answer/665429718" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li>
<li><a href="https://www.bilibili.com/video/av10590361?p=37" target="_blank" rel="noopener">李宏毅机器学习</a>)</li>
<li><a href="https://www.bilibili.com/video/av41393758?p=8" target="_blank" rel="noopener">RNN 和语言模式</a>，19.50 开始</li>
</ol>
<p>&emsp;&emsp;而在 LSTM 中，是使用加和的计算方式（博主注：<strong>由于我没有计算过，所以我也不是很肯定</strong>），所以大致解决了梯度消失的问题。注意我没有说解决了梯度爆炸的问题。<br><div class="note success">
            <p>&emsp;&emsp;LSTM 的模型以及参数名参考<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#长短期记忆——Long-Short-term-Memory-LSTM">此处</a>。</p><script type="math/tex; mode=display">\begin{aligned}    LSTM: z_i & = [a_{i - 1}; x_i] \\    memory_{new} & = g(z_i) * input(z_i) + memory * forget(z_i) \\    a_i & = h(g(z_i) * input(z_i) + memory * forget(z_i)) * output(z_i) \\\end{aligned}</script><p>&emsp;&emsp;对 LSTM 的求导结果很复杂，就不写了（实际上算得我自己都乱了）。。。它的复杂结构使得它不会出现一个数被连乘，导致极小。<strong>注意 LSTM 并没有解决梯度爆炸的问题。可以结合 clipping 训练 LSTM</strong>。<br>参考资料：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/28749444" target="_blank" rel="noopener">LSTM如何解决梯度消失问题</a></li><li><a href="https://zhuanlan.zhihu.com/p/36101196" target="_blank" rel="noopener">漫谈LSTM系列的梯度问题</a></li><li><a href="https://www.zhihu.com/question/34878706/answer/192444888" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li><li><a href="https://zhuanlan.zhihu.com/p/83496936" target="_blank" rel="noopener">人人都能看懂的LSTM介绍及反向传播算法推导（非常详细）</a></li></ol>
          </div></p>
<p>&emsp;&emsp;其他一些思考：LSTM 虽然可以永久的记住以前的 input 信息，但是 memory 说白了就是一个权重矩阵，不可能无限制的记住任何信息。所以可以对 memory 进行一些魔改，比如 memory network 将 memory 修改成用数组存储。<br>&emsp;&emsp;那么问题来了，GRU 解决了什么问题呢？为什么过拟合严重，可以使用 GRU？详见下一章。</p>
<h3 id="其他解决梯度消失的办法"><a href="#其他解决梯度消失的办法" class="headerlink" title="其他解决梯度消失的办法"></a>其他解决梯度消失的办法</h3><ol>
<li>Clockwise RNN</li>
<li>Structurally Constrained Recurrent Network(SCRN)</li>
<li></li>
</ol>
<h3 id="LSTM-为什么没有解决梯度爆炸？"><a href="#LSTM-为什么没有解决梯度爆炸？" class="headerlink" title="LSTM 为什么没有解决梯度爆炸？"></a>LSTM 为什么没有解决梯度爆炸？</h3><p>&emsp;&emsp;理论上，梯度爆炸也同样糟糕。但在实践上，其实我们可以直接砍一刀（原话：it turns out we can actually have a hack），这由 Thomas Mikolov 首次提出。在某种程度上是不精确的，比如说“<strong>现在你有一个很大的梯度 100，让我们把它限制在 5 吧</strong>”。这方法就结束了。你只要定义一个临界值，当梯度大于临界值时，就使梯度等于临界值。虽然不是一个数学方法，但结果表明在实践中效果不错。<br>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://www.bilibili.com/video/av41393758?p=8" target="_blank" rel="noopener">RNN 和语言模式</a> 49.06 - 62.38</li>
</ol>
<h2 id="GRU-改进了-LSTM-的啥"><a href="#GRU-改进了-LSTM-的啥" class="headerlink" title="GRU 改进了 LSTM 的啥"></a>GRU 改进了 LSTM 的啥</h2><p>&emsp;&emsp;简化了 Gate，GRU 能够达到 LSTM 相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率。<br>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/32481747" target="_blank" rel="noopener">人人都能看懂的GRU</a></li>
</ol>
<h2 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h2><p>&emsp;&emsp;本人的笔记</p>
<ol>
<li><a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#Attention">吴恩达李宏毅综合学习笔记：RNN入门</a></li>
<li><a href="http://localhost:4000/学习笔记/CS224n学习笔记.html#attention" target="_blank" rel="noopener">CS224n学习笔记</a></li>
<li><strong><a href="http://localhost:4000/zcy/深度学习算法（三）：RNN 各种机制.html#Attention" target="_blank" rel="noopener">RNN 各种机制</a></strong></li>
</ol>
<p>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/67909876" target="_blank" rel="noopener">浅谈Attention注意力机制及其实现</a></li>
</ol>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>&emsp;&emsp;参考资料：<br><a href="https://zhuanlan.zhihu.com/p/38200980" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p>
<h2 id="处理OOV"><a href="#处理OOV" class="headerlink" title="处理OOV"></a>处理OOV</h2><p>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://www.zhihu.com/question/329708785" target="_blank" rel="noopener">word2vec缺少单词怎么办？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/72312668" target="_blank" rel="noopener">香侬读 | 怎样在小数据集下学习OOV词向量？</a></li>
</ol>
<h2 id="Batch-Norm"><a href="#Batch-Norm" class="headerlink" title="Batch Norm"></a>Batch Norm</h2><h2 id="可视化训练结果"><a href="#可视化训练结果" class="headerlink" title="可视化训练结果"></a>可视化训练结果</h2><p>&emsp;&emsp;以前一直用 matplotlib 来画图，现在用了 tensorboardX 之后，感觉人瞬间就爽了，以下为教程。无法启动看 2，启动后网页无法显示看 3，代码不会写看 1。<br>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://tensorboardx.readthedocs.io/en/latest/tutorial.html#what-is-tensorboard-x" target="_blank" rel="noopener">官方文档</a></li>
<li><a href="https://blog.csdn.net/qq_40605167/article/details/95761885" target="_blank" rel="noopener">tensorboard OSError: [Errno 22] Invalid argument错误处理</a></li>
<li><a href="https://blog.csdn.net/weixin_44135282/article/details/86156961" target="_blank" rel="noopener">tensorboard生成的网址打不开的解决方法</a></li>
<li><a href="https://blog.csdn.net/bigbennyguo/article/details/87956434" target="_blank" rel="noopener">详解PyTorch项目使用TensorboardX进行训练可视化</a></li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/比赛项目总结/多领域seq2lf.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/论文/49、Improved-Representation-Learning-for-Question-Answer-Matching.html">


    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="记录学习问题，积累做的 leetcode 题目">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">
				
				
	

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/论文/49、Improved-Representation-Learning-for-Question-Answer-Matching.html" class="post-title-link" itemprop="url">论文笔记：Improved Representation Learning for Question Answer Matching</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-11-04 18:32:52 / 修改时间：23:04:53" itemprop="dateCreated datePublished" datetime="2019-11-04T18:32:52+08:00">2019-11-04</time>
            

            
              

              
            
          </span>

          

            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;自从短文级别（passage-level）的问答匹配需要有效的表征以捕获问题与答案之间复杂的语义关联开始，它就成为一个巨大的挑战。本文我们提出一系列的深度学习模型去解决如何选择短文答案。<br>&emsp;&emsp;将短文答案与符合语义关系的问题相匹配，不同于之前的大多数工作，即只使用一个深度学习结构。我们开发了一个混合模型去处理文本，其中用到了 CNN 和 RNN，结合了两种结构提取语言信息的优点。<br>&emsp;&emsp;此外，还开发了简单而有效的注意力机制。在两个数据集 InsuranceQA and TREC-QA 上显示此模型超出基线。</p>
<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><p>&emsp;&emsp;短文级别的答案选择是 QA 系统的重要组成部分之一。它的定义如下所示：给定一个问题和一群候选短文，挑选出包含候选答案的短文。<br>&emsp;&emsp;一个回答优于另一个回答取决于多种因素。尤其是不同于其他 NLP 对匹配任务，问题与答案之间语言上的相似度对我们的任务既可能有用也可能没用，这取决于问题。此外，虽然一个好的回答必须与问题相关联，但是它们之间没有共通的词汇单元。<br>&emsp;&emsp;因此，与基于深度学习的方法相比，这些挑战使得手工制作的特征变得不那么理想。此外，它们还要求我们的系统学习如何区分有用的片段和不相关的片段，其中更关注前者。</p>
<h1 id="approach"><a href="#approach" class="headerlink" title="approach"></a>approach</h1><p>&emsp;&emsp;作者开发一个模型，同时使用到了 CNN 和 RNN，并且还加上 attention 机制。</p>
<ol>
<li>LSTM<ul>
<li>q 和 a 分别输入 bi-LSTM</li>
<li>拼接 bi-LSTM 的正反向向量</li>
<li>对输出向量做平均（因为序列中有多个词向量，所以需要取平均）</li>
<li>做 max pooling</li>
</ul>
</li>
<li>CNN</li>
<li>Attention LSTM</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/论文/49、Improved-Representation-Learning-for-Question-Answer-Matching.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">记录学习问题，积累做的 leetcode 题目</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">130</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">23</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
			  
			  <!-- 不蒜子/busuanzi -->
			  <div class="site-state-item site-state-posts">
			  	<span class="site-state-item-count">125.4k</span>
			  	<span class="site-state-item-name">总字数</span>
			  </div>
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://lzh0928.gitee.io/" title="https://lzh0928.gitee.io/" rel="noopener" target="_blank">Mr.Liu</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

  
  <!-- 自己新增的所有 js 文件 -->
  <script src="/lib/my-utils.js"></script>
<!--图片缩放插件-->
<script src="/lib/zoomify/zoomify.min.js"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- 背景插件 -->
<script src="https://cdn.bootcss.com/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
<!-- 背景图片 -->
<script>
	function generateBG(count){
		var bg_prefix = '/images/background/';
		var bg =new Array();
		for(var i = 0; i < count; i++){
			bg[i] = bg_prefix + i + '.jpg';
		}
		bg.shuffle();
		return bg;
	}
	$("body").backstretch(generateBG(11), {
		duration:90000,//1min半一换
		fade: 1500
	});
</script>
<!-- 图片缩放 -->
<script>
$('#content img').zoomify({duration: 500, });
  $('#content img').on('zoom-in.zoomify', function () {
    $('#sidebar').css('display', 'none');
  });
  $('#content img').on('zoom-out-complete.zoomify', function () {
    $('#sidebar').css('display', '');
  });
</script>

	

</body>
</html>
