<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-simple.min.css?v=1.0.2">























  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
<meta name="keywords" content="博客，java，javaWeb">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="http://yan624.github.io/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博客">
<meta name="twitter:description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">






  <link rel="canonical" href="http://yan624.github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

	<!--图片缩放插件样式-->
	<link rel="stylesheet" href="/lib/zoomify/zoomify.min.css">
</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
  <!--加载flower canvas-->
  <script>
	var pathname = window.location.pathname;
	if(pathname == '/flower.html'){
		var body =  document.getElementsByTagName('body')[0];
		var canvas = document.createElement("canvas")
		canvas.setAttribute('id', 'sakura')
		// '<canvas id="sakura"></canvas>'
		body.appendChild(canvas)
	}
  </script>
  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">浅度学习中~~</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-常用链接">

    
    
    
      
    

    
      
    

    <a href="/常用链接" rel="section"><i class="menu-item-icon fas fa-fw fa-bookmark"></i> <br>常用链接</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">12</span></a>

  </li>
        
        
        
          
            
            
            
              
              

  
  
    
  
  <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">18</span></a>

  </li>


            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
            
            
            
          
        
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">82</span></a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/tips.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/tips.html" class="post-title-link" itemprop="url">tips</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-24 19:04:25 / 修改时间：19:19:53" itemprop="dateCreated datePublished" datetime="2019-01-24T19:04:25+08:00">2019-01-24</time>
            

            
              

              
            
          </span>
          
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p><strong>提示：先按住ctrl再点击超链接可以在新窗口打开</strong></p>
</blockquote>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/tips.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/win10快捷键.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/win10快捷键.html" class="post-title-link" itemprop="url">win10快捷键</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-12 16:57:05" itemprop="dateCreated datePublished" datetime="2019-03-12T16:57:05+08:00">2019-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-08 22:13:52" itemprop="dateModified" datetime="2019-04-08T22:13:52+08:00">2019-04-08</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/assorted/" itemprop="url" rel="index"><span itemprop="name">assorted</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>最近老是用到win10的快捷键，但是用过之后过几天就忘了，所以记录下。<br>还有有谁知道怎么用快捷键打开高级系统设置，设置环境变量老是要打开这个，烦得很。</p>
<ol>
<li>win+E 打开我的电脑</li>
<li>win+R 打开运行</li>
<li>win+L 锁屏</li>
<li>fn+ESC 打开/关闭功能键。f5是刷新键，但是有时候发现按f5无效，其实是因为功能键打开了，只需要按fn+ESC关闭就可。</li>
<li>四指在触摸板向左/向右滑动，切换桌面。</li>
<li>三指向上滑动将当前的任务以小窗口显示在桌面，三指向下滑动隐藏。</li>
<li>win+Tab类似6</li>
</ol>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/win10快捷键.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/前馈神经网络的正反向推导.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/前馈神经网络的正反向推导.html" class="post-title-link" itemprop="url">前馈神经网络的正反向推导</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-16 19:53:55" itemprop="dateCreated datePublished" datetime="2019-05-16T19:53:55+08:00">2019-05-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-19 10:37:05" itemprop="dateModified" datetime="2019-05-19T10:37:05+08:00">2019-05-19</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <div class="note info">
            <p>本文的公式不存在次方的说法，所以看见上标，不要想成是次方。<br>对于权重的表示问题，请看<a href="https://yan624.github.io/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">博客</a>，但是由于是以前的学习笔记，不保证完全正确。<br>如果想了解为什么梯度下降要对w和b求导，可以看<a href="https://yan624.github.io/学习笔记/梯度下降算法的推导.html">这篇</a>。<br><strong>建议边看边写，否则思维跟不上。</strong></p>
          </div>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>参考<a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">文章</a><br>以如下神经网络架构为例。参考<a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">文章</a>中使用了一个2 2 2的神经网络架构，但是现实中神经网络架构不会这么整整齐齐。所以还是使用了略复杂的架构，此外原文中未对bias（偏差）更新。另外原文也没有实现向量化后的计算。虽然在后面的代码写了，但是由于代码太长了，有一种代码我给出来了，你们自己去看的感觉。说实话没多少注释，都没看的欲望(╬￣皿￣)。然后她所使用的符号让我不太习惯，因为看吴恩达以及李宏毅老师使用的符号都是<script type="math/tex">w^l_{ji}\ a^l_i</script>等等，所以自己重新推导一遍，并且使用了数学公式，而不是截图，更好看一点。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>解释一下最下面的神经元，这个神经元初始化为1，也就是意味着1 * b = b。输入值为1，一个偏差乘1还是偏差本身。</p>
<h2 id="关于函数选用"><a href="#关于函数选用" class="headerlink" title="关于函数选用"></a>关于函数选用</h2><p>本文所有激活函数选择sigmoid函数，代价函数选择binary_crossentropy。</p>
<h2 id="一些约定"><a href="#一些约定" class="headerlink" title="一些约定"></a>一些约定</h2><div class="note info">
            <p>本文所有的输入值，激活值，输出值都是<strong>列向量</strong>。</p>
          </div>
<h1 id="初始化数据以及正向传播"><a href="#初始化数据以及正向传播" class="headerlink" title="初始化数据以及正向传播"></a>初始化数据以及正向传播</h1><h2 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h2><p>此处初始化各层的权重值，偏差。由于是演示，所以顺便把输入层也初始化了。<br>设</p>
<script type="math/tex; mode=display">
\begin{cases}
    x_1 = a^0_1 = 0.55, x_2 = a^0_2 = 0.72\\
    y_1 = 0.60, y_2 = 0.54\\
\end{cases}\\
\begin{cases}
    w^1_{11}=0.4236548, w^1_{12}=0.64589411\quad|\quad w^1_{21}=0.43758721, w^1_{22}=0.891773\quad|\quad w^1_{31}=0.96366276, w^1_{32}=0.38344152\\
    b^1_1=0.79172504, b^1_2=0.52889492, b^1_3=0.56804456\\
    w^2_{11}=0.92559664, w^2_{12}=0.07103606, w^2_{13}=0.0871293\quad|\quad w^2_{21}=0.0202184, w^2_{22}=0.83261985, w^2_{23}=0.77815675\\
    b^2_1=0.87001215, b^2_2=0.97861834\\
\end{cases}</script><p>不用多看，反正也用不到几次。。。</p>
<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>对于正向传播，应该是很熟悉了，所以我直接一次写完，不做过多解释。</p>
<h3 id="输入层到隐藏层"><a href="#输入层到隐藏层" class="headerlink" title="输入层到隐藏层"></a>输入层到隐藏层</h3><script type="math/tex; mode=display">
z^1_1 = w^1_{11} * a^0_1 + w^1_{12} * a^0_2 + 1 * b^1_1\\
z^1_2 = w^1_{21} * a^0_1 + w^1_{22} * a^0_2 + 1 * b^1_2\\
z^1_3 = w^1_{31} * a^0_1 + w^1_{32} * a^0_2 + 1 * b^1_3\\</script><p>带入sigmoid函数中，以下开始省略bias乘的1：</p>
<script type="math/tex; mode=display">
a^1_1 = \sigma{(z^1_1)}\\
a^1_2 = \sigma{(z^1_2)}\\
a^1_3 = \sigma{(z^1_3)}\\</script><h3 id="隐藏层到输出层"><a href="#隐藏层到输出层" class="headerlink" title="隐藏层到输出层"></a>隐藏层到输出层</h3><script type="math/tex; mode=display">
z^2_1 = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1\\
z^2_2 = w^2_{21} * a^1_1 + w^2_{22} * a^1_2 + w^2_{13} * a^1_3 + b^2_2\\</script><p>带入sigmoid函数中：</p>
<script type="math/tex; mode=display">
a^2_1 = \sigma{(z^2_1)}\\
a^2_2 = \sigma{(z^2_2)}\\</script><h3 id="计算代价"><a href="#计算代价" class="headerlink" title="计算代价"></a>计算代价</h3><p>以字母J记为代价函数的名称，最后一个表达式为最简版：</p>
<script type="math/tex; mode=display">
\begin{align}
    J & = -[(y_1 * \log(a^2_1) + (1 - y_1) * \log(1 - a^2_1) + (y_2 * \log(a^2_2) + (1 - y_2) * \log(1 - a^2_2)]\\
    J & = -\Sigma^2_{i = 1}{[(y_i * \log(a^2_i) + (1 - y_i) * \log(1 - a^2_i)]}\\
    J & = -\Sigma{[(y * \log(a^2) + (1 - y) * \log(1 - a^2)]}\\
\end{align}</script><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>上述的表达式全部是一个一个列出来的，如果使用向量来表示乘积那就方便很多。可以看到下面只用了五行就写完了上面<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#正向传播">正向传播</a>的所有步骤。<br><div class="note warning">
            <p>如果无法理解这一步那就是不会线性代数的问题，线性代数不在此文的介绍范围之内。</p>
          </div></p>
<script type="math/tex; mode=display">
\begin{align}
    z^1 & = w^1 * a^0 + b^1 & \text{输入层到隐藏层}\\
    a^1 & = \sigma{(z^1)} & \text{带入隐藏层的激活函数}\\
    z^2 & = w^2 * a^1 + b^2 & \text{隐藏层到输出层}\\
    a^2 & = \sigma{(z^2)} & \text{带入输出层的激活函数}\\
    J & = -\Sigma{[(y * \log(a^2) + (1 - y) * \log(1 - a^2)]} & \text{计算代价}\\
\end{align}</script><h2 id="上述表达式代码实现"><a href="#上述表达式代码实现" class="headerlink" title="上述表达式代码实现"></a>上述表达式代码实现</h2><p>最后几节有神经网络numpy实现的全部代码，可以直接跳过本节看下一节，这里的代码只是给出一个直观的理解，可以自己运行看看。<br>受到keras以及万物皆对象的启发，首先建立一个神经元对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">'sigmoid'</span>)</span>:</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line">        self.hyperparameters = dict()</span><br><span class="line">        <span class="comment"># W, b, A_prev的导数</span></span><br><span class="line">        self.grads = dict()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hyperparameters</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化超参数，在神经网络中权重值不能初始化为0，偏差可以</span></span><br><span class="line"><span class="string">        :param shape: 神经元的形状，(units, input_shape)</span></span><br><span class="line"><span class="string">        :return: hyperparameters，该神经元的超参数。可以通过hyperparameters['W']、hyperparameters['b']取值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 由于是演示，所以使用了随机初始化</span></span><br><span class="line">        W = np.random.rand(*shape)</span><br><span class="line">        b = np.random.rand(shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> W, b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构造神经元，目前只执行初始化超参数的步骤</span></span><br><span class="line"><span class="string">        :param input_shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        W, b = self._init_hyperparameters(shape=(self.units, input_shape))</span><br><span class="line">        self.hyperparameters[<span class="string">'W'</span>] = W</span><br><span class="line">        self.hyperparameters[<span class="string">'b'</span>] = b</span><br></pre></td></tr></table></figure></p>
<p>为方便起见，将大部分的函数都放入Model中，下面给出所有的代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> simple_neural_network.activation_function <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> simple_neural_network.cost_function <span class="keyword">import</span> *</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">'sigmoid'</span>)</span>:</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hyperparameters</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化超参数，在神经网络中权重值不能初始化为0，偏差可以</span></span><br><span class="line"><span class="string">        :param shape: 神经元的形状，(units, input_shape)</span></span><br><span class="line"><span class="string">        :return: hyperparameters，该神经元的超参数。可以通过hyperparameters['W']、hyperparameters['b']取值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 用于是演示，所以使用了随机初始化</span></span><br><span class="line">        hyperparameters = &#123;</span><br><span class="line">                <span class="string">'W'</span>: np.random.rand(*shape),</span><br><span class="line">                <span class="string">'b'</span>: np.random.rand(shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hyperparameters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构造神经元，目前只执行初始化超参数的步骤</span></span><br><span class="line"><span class="string">        :param input_shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.hyperparameters = self._init_hyperparameters(shape=(self.units, input_shape))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 全部的神经元，并且根据神经元数量计算神经网络架构的层数，在计算时需要减1因为输入层不算入神经网络层数</span></span><br><span class="line">        self.neurons = list()</span><br><span class="line">        <span class="comment"># 按顺序缓存A, (Z, W, b)，由于输入层不需要任何缓存，所以放入None填充此位置。方便根据索引取值</span></span><br><span class="line">        self.value_caches = [<span class="keyword">None</span>]</span><br><span class="line">        <span class="comment"># 代价函数</span></span><br><span class="line">        self.cost = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, neuron)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        在模型中添加神经元</span></span><br><span class="line"><span class="string">        :param neuron:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.neurons.append(neuron)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(self, A_prev, W, b)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        正向传播，线性运算：Z = W * A + b</span></span><br><span class="line"><span class="string">        :param A_prev: 前一层的激活值</span></span><br><span class="line"><span class="string">        :param W: 权重值</span></span><br><span class="line"><span class="string">        :param b: 偏差</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        Z: 运算结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        cache = A_prev, (Z, W, b)</span><br><span class="line">        self.value_caches.append(cache)</span><br><span class="line">        <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nonlinear_forward</span><span class="params">(self, A_prev, W, b, activation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        进入激活函数进行非线性计算</span></span><br><span class="line"><span class="string">        :param A_prev:</span></span><br><span class="line"><span class="string">        :param W:</span></span><br><span class="line"><span class="string">        :param b:</span></span><br><span class="line"><span class="string">        :param activation:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Z = self.linear_forward(A_prev, W, b)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'sigmoid'</span>:</span><br><span class="line">            <span class="keyword">return</span> sigmoid(Z)</span><br><span class="line">        <span class="keyword">elif</span> activation == <span class="string">'relu'</span>:</span><br><span class="line">            <span class="keyword">return</span> relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deep_forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param X: 输入值</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        A: 最后一层的运算结果，也就是输出层的激活值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 计算神经网络层数，减1是为了去掉输入层，众所周知输入层不需要进行计算</span></span><br><span class="line">        L = len(self.neurons) - <span class="number">1</span></span><br><span class="line">        A = X</span><br><span class="line">        <span class="comment"># 循环整个神经网络，进行正向传播，从1开始，因为索引0是输入层</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 根据索引获取神经元实例</span></span><br><span class="line">            neuron = self.neurons[l]</span><br><span class="line">            A_prev = A</span><br><span class="line">            W = neuron.hyperparameters[<span class="string">'W'</span>]</span><br><span class="line">            b = neuron.hyperparameters[<span class="string">'b'</span>]</span><br><span class="line">            A = self.nonlinear_forward(A_prev, W, b, neuron.activation)</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compile</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, epochs=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入层的神经元添加进去</span></span><br><span class="line">        self.neurons.insert(<span class="number">0</span>, SimpleNN(len(X)))</span><br><span class="line">        <span class="comment"># 初始化神经元的超参数</span></span><br><span class="line">        <span class="keyword">for</span> i, n <span class="keyword">in</span> enumerate(self.neurons[<span class="number">1</span>:]):</span><br><span class="line">            input_shape = self.neurons[i].units</span><br><span class="line">            n.build(input_shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始反向传播</span></span><br><span class="line">        AL = self.deep_forward(X)</span><br></pre></td></tr></table></figure></p>
<p>激活函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> z * (z &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    dZ = np.array(dA, copy=<span class="keyword">True</span>)  <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line"></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    s = sigmoid(Z)</span><br><span class="line">    <span class="keyword">return</span> dA * s * (<span class="number">1</span> - s)</span><br></pre></td></tr></table></figure></p>
<p>测试一下<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入值的大小</span></span><br><span class="line">input_size = 2</span><br><span class="line"><span class="comment"># 输出值的大小</span></span><br><span class="line">output_size = 2</span><br><span class="line"><span class="comment"># 方便书写，截断小数</span></span><br><span class="line">X0 = np.round(np.random.rand(input_size, 1), 2)</span><br><span class="line">Y0 = np.round(np.random.rand(output_size, 1),2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该模型为2 3 2架构</span></span><br><span class="line">model = Model()</span><br><span class="line">model.add(SimpleNN(3))</span><br><span class="line">model.add(SimpleNN(output_size))</span><br><span class="line">model.compile()</span><br><span class="line">model.fit(X0, Y0)</span><br><span class="line">print(model.value_caches[0])</span><br></pre></td></tr></table></figure></p>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><div class="note primary">
            <p>说是说反向传播，实际上整个流程就是在<strong><a href="https://baike.baidu.com/item/链式法则/3314017?fr=aladdin" target="_blank" rel="noopener">链式求导</a></strong>。如果把这点想通了，整个神经网络的难点就只在向量化上了。一定要理解为什么整个流程只是在做链式求导的问题，在这里我并不是随便一提。</p>
          </div>
<p>为了便于查找，把之前的图再放这。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"></p>
<h2 id="首先更新输出层的权重值（W）以及偏差值（b）"><a href="#首先更新输出层的权重值（W）以及偏差值（b）" class="headerlink" title="首先更新输出层的权重值（W）以及偏差值（b）"></a>首先更新输出层的权重值（W）以及偏差值（b）</h2><p>梯度下降公式大家应该都知道：<script type="math/tex">W = W - \alpha * grad</script>。其中的grad实际上就是W的导数，<a href="https://yan624.github.io/学习笔记/梯度下降算法的推导.html">参考</a>。<br>可以对照上图观察，<strong>输出层</strong>的权重值分别为:</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    w^2_{11}&w^2_{12}&w^2_{13}\\
    w^2_{21}&w^2_{22}&w^2_{23}\\
\end{pmatrix}</script><p>所以我们需要分别求：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} \tag{3.1.1}\label{3.1.1}</script><h3 id="求矩阵中第一个w的导数并更新w"><a href="#求矩阵中第一个w的导数并更新w" class="headerlink" title="求矩阵中第一个w的导数并更新w"></a>求矩阵中第一个w的导数并更新w</h3><p>先求<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>，我们知道这个神经网络的代价的表达式是</p>
<script type="math/tex; mode=display">
J = -[(y_1 * \log(a^2_1) + (1 - y_1) * \log(1 - a^2_1) + (y_2 * \log(a^2_2) + (1 - y_2) * \log(1 - a^2_2)]</script><p><strong>为了方便对照我将隐藏层到输出层的正向传播的步骤</strong>也写在下面：</p>
<script type="math/tex; mode=display">
\begin{align}
    z^2_1 & = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1\\
    z^2_2 & = w^2_{21} * a^1_1 + w^2_{22} * a^1_2 + w^2_{13} * a^1_3 + b^2_2\\
    a^2_1 & = \sigma{(z^2_1)} = \frac{1}{1 - e^{-z^2_1}}\\
    a^2_2 & = \sigma{(z^2_2)} = \frac{1}{1 - e^{-z^2_2}}\\
\end{align}</script><p>根据链式求导法则得：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}}</script><p>我们将其拆解，一步一步地求：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] & \text{首先对a求导，此步如果你不会微积分会有疑惑} \tag{3.1.2}\label{3.1.2}\\
    \frac{\partial a^2_1}{\partial z^2_1} & = (a^2_1) * (1 - a^2_1) & \text{这是对sigmoid函数的求导，百度一下求导过程}\\
    \frac{\partial J}{\partial z^2_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1} & \text{其次对z求导}\\
                                      & = \frac{\partial J}{\partial a^2_1} * (a^2_1) * (1 - a^2_1) \tag{3.1.3}\label{3.1.3}\\
    \frac{\partial z^2_1}{\partial w^2_{11}} & = a^1_1\\
    \frac{\partial J}{\partial w^2_{11}} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}} & \text{最后对w求导} \tag{3.1.4}\label{3.1.4}\\
                                         & = \frac{\partial J}{\partial z^2_1} * a^1_1\\
    \frac{\partial J}{\partial w^2_{11}} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1) * a^1_1 & \text{整合在一起}\\
\end{align}</script><p>其中<script type="math/tex">[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1)</script>实际上是可以化简的，化简为<script type="math/tex">a^2_1 - y_1</script>，同时去掉了负号，所以</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = (a^2_1 - y_1) * a^1_1</script><p>我们将数值带入其中，之前的正向传播已经得到了所有激活值。</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial w^2_{11}} = (0.85220348 - 0.60) * 0.81604509 = 0.20580941153491322</script><p>对<script type="math/tex">w^2_{11}</script>更新，</p>
<script type="math/tex; mode=display">
w^2_{11} = w^2_{11} - \alpha * \frac{\partial J}{\partial w^2_{11}}</script><p>学习速率<script type="math/tex">\alpha</script>选1，经过简单的运算，<script type="math/tex">w^2_{11} = 0.92559664 - 1 * 0.20580941153491322 = 0.7197872284650868</script><br><div class="note info">
            <p>如果细心点就会发现，<script type="math/tex">\frac{\partial J}{\partial w^2_{11}}</script>其实就等于这层的z的导数乘上前一层的激活值a。如果没发现也没关系，下面<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#向量化-1">向量化</a>这节会做一个总结。</p>
          </div></p>
<h3 id="求所有w的导数"><a href="#求所有w的导数" class="headerlink" title="求所有w的导数"></a>求所有w的导数</h3><p>同理可以求出所有的导数</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} \tag{\ref{3.1.1}}</script><h3 id="向量化-1"><a href="#向量化-1" class="headerlink" title="向量化"></a>向量化</h3><div class="note danger">
            <p>上面只求了一个w的导数，虽然其他的w的求导都是类似操作，但是真要算起来，对于自己没去算过的人，可能花一天都没有办法将其用<strong>向量化表示</strong>。<br>求导是十分简单的，但是向量化可能会有点问题。问题的主要来源是<strong>想偷懒</strong>。对于这种问题，最好得到解决办法是暴力破解，即求出所有的w的导数，然后再将其向量化。</p>
          </div>
<p>首先观察上述公式<script type="math/tex">\ref{3.1.4}</script>：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}}</script><p>它由两部分组成，一个是<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>，第二部分是<script type="math/tex">\frac{\partial z^2_1}{\partial w^2_{11}}</script>，如果你自己求过导就会发现其实<script type="math/tex">\frac{\partial z^2_1}{\partial w^2_{11}} = a^1_1</script>。为了方便你们观察，我列出所有式子：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial z^2_1} * a^1_1 \quad \frac{\partial J}{\partial w^2_{12}} = \frac{\partial J}{\partial z^2_1} * a^1_2 \quad \frac{\partial J}{\partial w^2_{13}} = \frac{\partial J}{\partial z^2_1} * a^1_3\\
    \frac{\partial J}{\partial w^2_{21}} = \frac{\partial J}{\partial z^2_2} * a^1_1 \quad \frac{\partial J}{\partial w^2_{22}} = \frac{\partial J}{\partial z^2_2} * a^1_2 \quad \frac{\partial J}{\partial w^2_{23}} = \frac{\partial J}{\partial z^2_2} * a^1_3\\
\end{align}</script><p><strong>可能到这你有点烦躁了，因为表达式实在太多了。没关系，下方蓝色的note会给出总结，直接一步求解完毕。</strong><br>有没有发现，里面有一半是重复的元素？我们可以将它们组成向量得到：</p>
<script type="math/tex; mode=display">
\eqref{3.1.1}
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} = 
\begin{pmatrix}
\frac{\partial J}{\partial z^2_1}\\
\frac{\partial J}{\partial z^2_2}\\
\end{pmatrix} * 
\begin{pmatrix}
a^1_1 & a^1_2 & a^1_3
\end{pmatrix} \tag{3.1.5}</script><p>公式3.1.5和上面那六个表达式实际上计算的东西是一样的。进一步缩写为</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2} = \frac{\partial J}{\partial z^2} * (a^1)^T \tag{3.1.6}</script><p>这里加了一个T代表转置，实际上我们所有的输入值，激活值，输出值都是列向量。<br><div class="note info">
            <p>总结一下，在这里<strong>求<script type="math/tex">\frac{\partial J}{\partial w}</script>的步骤为：求权重值所在层的z的导数<script type="math/tex">\frac{\partial J}{\partial z}</script>再乘上前一层的激活值</strong>。这是对一个w求导所做的运算，而对一整个W矩阵求导那就是公式3.1.6的那个向量化操作。但是观察公式3.1.6发现，其实求一个w和求一个W矩阵并无区别，无非是将数字相乘改为向量（矩阵）相乘。<br>另外，其实这对神经网络中每一层的操作都是一样。如果不信可以自己算一下。所以以后理解的时候，可以用这种方式理解，加快理解速度。</p>
          </div></p>
<h3 id="更新偏差"><a href="#更新偏差" class="headerlink" title="更新偏差"></a>更新偏差</h3><p>偏差比权重简单很多。</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] \tag{\ref{3.1.2}}\\
    \frac{\partial J}{\partial z^2_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1}\\
                                      & = \frac{\partial J}{\partial a^2_1} * (a^2_1) * (1 - a^2_1) \tag{\ref{3.1.3}}\\
    \frac{\partial J}{\partial b^2_1} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial b^2_1} \\
                                      & = \frac{\partial J}{\partial z^2_1}\\
    \frac{\partial J}{\partial b^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1)\\
\end{align}</script><p>可以看到</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial b^2_1} = \frac{\partial J}{\partial z^2_1}</script><p>所以偏差的向量化比较简单：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial b^2_1}\\
    \frac{\partial J}{\partial b^2_2}\\
\end{pmatrix} = 
\begin{pmatrix}
    \frac{\partial J}{\partial z^2_1}\\
    \frac{\partial J}{\partial z^2_2}\\
\end{pmatrix}</script><h3 id="整理"><a href="#整理" class="headerlink" title="整理"></a>整理</h3><p>整理一下上一波的求导过程。目标是求得<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>，但是上面我并没有一步求导到底，相反我将每一步都写出来了，这是有原因的。因为<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>会在前一层对w求导时使用，所以在代码上当然需要保存副本。而<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>已经在这次的反向传播中使用过了，它的价值也算是用完了。<br>在<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#首先更新输出层的权重值（W）以及偏差值（b）">首先更新输出层的权重值（W）以及偏差值（b）</a>中<strong>略有瑕疵</strong>的步骤（也就是上述所有步骤）是：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script></li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script></li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script></li>
</ol>
<div class="note info">
            <p>根据上面三步，我们可以观察出，如果需要求出一层的<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script>（<strong>步骤3</strong>），需要<strong>先</strong>求出<strong>同一层</strong>的<script type="math/tex">\frac{\partial J}{\partial a^2}\ \frac{\partial J}{\partial z^2}</script>（<strong>步骤1和2</strong>）。<strong><em>所以</em></strong>如果我们需要求出<strong>前一层</strong>的<script type="math/tex">\frac{\partial J}{\partial w^1}\ \frac{\partial J}{\partial b^1}</script>，必须先求出<strong>前一层</strong>的a的导数<script type="math/tex">\frac{\partial J}{\partial a^1}\ \frac{\partial J}{\partial z^1}</script>，而由于公式<script type="math/tex">z^2_1 = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1</script>，可以观察到上述<strong>步骤2</strong>对z求导之后其实拥有三个选项：</p><ol><li>求w的导数（<strong>步骤3</strong>）</li><li>求b的导数（<strong>步骤3</strong>）</li><li>求上一层a的导数</li></ol>
          </div>
<p>所以正确的步骤是：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script>（<strong>不变</strong>）</li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script>（<strong>不变</strong>）</li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script>（<strong>不变</strong>）</li>
<li>最后求出前一层的<script type="math/tex">\frac{\partial J}{\partial a^1}</script>，准备下一步的计算。</li>
</ol>
<p><strong>也就是说，我们在一层中进行求导，需要分别求4个参数的导数，即当前层的a，w，b以及前一层的a的导数。</strong></p>
<h2 id="更新隐藏层的权重值以及偏差值"><a href="#更新隐藏层的权重值以及偏差值" class="headerlink" title="更新隐藏层的权重值以及偏差值"></a>更新隐藏层的权重值以及偏差值</h2><p>由于上述步骤太多，来回滑动网页略繁琐，我再次把图放出来，以供参考。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>隐藏层的更新与输出层略微不同，由于看公式不太形象，可以看上面的图。观察发现，隐藏层的某一个神经元链接着输出层的<strong>所有</strong>神经元。所以隐藏层的神经元的误差其实来源于与它相连接的输出层的神经元。<br>根据链式求导法则，我们知道：一个函数对一个变量求导，如果有多条路径可以到达该变量，那么就需要对每条路径都求导，最后将结果相加。转换成数学公式就跟下面公式3.2.1的求导过程一样。</p>
<h3 id="对第一个w求导"><a href="#对第一个w求导" class="headerlink" title="对第一个w求导"></a>对第一个w求导</h3><p>我们按照上一节<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#整理">《整理》</a>的四个步骤来做，先求出a的导数：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^1_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial a^1_1} + \frac{\partial J}{\partial a^2_2} * \frac{\partial a^2_2}{\partial a^1_1} & \text{输出层两个神经元均要求导再相加}\\
                                      & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial a^1_1} + \frac{\partial J}{\partial z^2_2} * \frac{\partial z^2_1}{\partial a^1_1} & \text{之前求过z的导数，为了方便书写用它替换}\\
                                      & = \frac{\partial J}{\partial z^2_1} * w^2_{11} + \frac{\partial J}{\partial z^2_2} * w^2_{21} \tag{3.2.1}\label{3.2.1}\\
\end{align}</script><div class="note info">
            <p>我们可以观察到隐藏层的a的导数<script type="math/tex">\frac{\partial J}{\partial a^1_1}</script>实际上就是<strong>输出层</strong>的z的导数<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>乘上与之相连的<strong>输出层</strong>的神经元的w。<br>一般化之后就是：<strong>除了输出层</strong>，其他所有层的<strong>a的导数</strong>都是<strong>后一层</strong>的<strong>z的导数</strong>乘上<strong>后一层</strong>的w。因为输出层的<strong>a的导数</strong>是通过代价函数求的。</p>
          </div>
<p>所以下一步就是求z的导数：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial z^1_1} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} + \frac{\partial J}{\partial z^2_2} * \frac{\partial z^2_1}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1}\\
                                      & = \frac{\partial J}{\partial z^2_1} * w^2_{11} * \frac{\partial a^1_1}{\partial z^1_1}  + \frac{\partial J}{\partial z^2_2} * w^2_{21} * \frac{\partial a^1_1}{\partial z^1_1}\\
                                      & = \frac{\partial J}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} & \text{这里a的导数参考}\ref{3.2.1}\\
    \frac{\partial a^1_1}{\partial z^1_1} & = a^1_1 * (1 - a^1_1) & \text{对sigmoid函数求导，前面已经说过了}\\
\end{align}</script><p>最后求出w的导数</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial w^1_{11}} & = \frac{\partial J}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} * \frac{\partial z^1_1}{\partial w^1_{11}}\\
                                         & = \frac{\partial J}{\partial z^1_1} * \frac{\partial z^1_1}{\partial w^1_{11}}\\
                                         & = \frac{\partial J}{\partial z^1_1} * a^0_1
\end{align}</script><h3 id="向量化-2"><a href="#向量化-2" class="headerlink" title="向量化"></a>向量化</h3><p>你肯定已经想把它向量化了。先列出所有的表达式。</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^1_{11}} = \frac{\partial J}{\partial z^1_1} * a^0_1\\
\frac{\partial J}{\partial w^1_{12}} = \frac{\partial J}{\partial z^1_1} * a^0_2\\
\frac{\partial J}{\partial w^1_{21}} = \frac{\partial J}{\partial z^1_2} * a^0_1\\
\frac{\partial J}{\partial w^1_{22}} = \frac{\partial J}{\partial z^1_2} * a^0_2\\
\frac{\partial J}{\partial w^1_{31}} = \frac{\partial J}{\partial z^1_3} * a^0_1\\
\frac{\partial J}{\partial w^1_{32}} = \frac{\partial J}{\partial z^1_3} * a^0_2\\</script><p>可以发现这其实跟上面的向量化步骤一模一样：</p>
<script type="math/tex; mode=display">
\begin{align}
    \begin{pmatrix}
        \frac{\partial J}{\partial w^1_{11}} & \frac{\partial J}{\partial w^1_{12}}\\
        \frac{\partial J}{\partial w^1_{21}} & \frac{\partial J}{\partial w^1_{22}}\\
        \frac{\partial J}{\partial w^1_{31}} & \frac{\partial J}{\partial w^1_{32}}\\
    \end{pmatrix} & = 
    \begin{pmatrix}
        \frac{\partial J}{\partial z^1_1}\\
        \frac{\partial J}{\partial z^1_2}\\
        \frac{\partial J}{\partial z^1_3}\\
    \end{pmatrix} * 
    \begin{pmatrix}
        a^0_1 & a^0_2
    \end{pmatrix} \\
    \frac{\partial J}{\partial w^1} & = \frac{\partial J}{\partial z^1} * (a^0)^T
\end{align}</script><h3 id="对偏差求导"><a href="#对偏差求导" class="headerlink" title="对偏差求导"></a>对偏差求导</h3><p>这一步更是简单，直接给结果了。</p>
<script type="math/tex; mode=display">
\begin{align}
    \begin{pmatrix}
        \frac{\partial J}{\partial b^1_1}\\
        \frac{\partial J}{\partial b^1_2}\\
        \frac{\partial J}{\partial b^1_3}\\
    \end{pmatrix} & = 
    \begin{pmatrix}
        \frac{\partial J}{\partial z^1_1}\\
        \frac{\partial J}{\partial z^1_2}\\
        \frac{\partial J}{\partial z^1_3}\\
    \end{pmatrix}\\
    \frac{\partial J}{\partial b^1} & = \frac{\partial J}{\partial z^1}
\end{align}</script><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><p>上述步骤看起来没什么问题，但是在实际编程中会有很大问题。在向量化的时候，我直接使用了<script type="math/tex">\frac{\partial J}{\partial z^1}</script>，但是问题就是<script type="math/tex">\frac{\partial J}{\partial z^1}</script>的向量化我直接跳过了。要向量化<script type="math/tex">\frac{\partial J}{\partial z^1}</script>，实际上得先向量化<script type="math/tex">\frac{\partial J}{\partial a^1}</script>。观察表达式<script type="math/tex">\ref{3.2.1}</script>，先给出所有的式子：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial a^1_1} = \frac{\partial J}{\partial z^2_1} * w^2_{11} + \frac{\partial J}{\partial z^2_2} * w^2_{21}\\
\frac{\partial J}{\partial a^1_2} = \frac{\partial J}{\partial z^2_1} * w^2_{12} + \frac{\partial J}{\partial z^2_2} * w^2_{22}\\
\frac{\partial J}{\partial a^1_3} = \frac{\partial J}{\partial z^2_1} * w^2_{13} + \frac{\partial J}{\partial z^2_2} * w^2_{23}\\</script><h4 id="向量化-3"><a href="#向量化-3" class="headerlink" title="向量化"></a>向量化</h4><script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial a^1_1}\\
    \frac{\partial J}{\partial a^1_2}\\
    \frac{\partial J}{\partial a^1_3}\\
\end{pmatrix} = 
\begin{pmatrix}
w^2_{11} & w^2_{12} & w^2_{13}\\
w^2_{21} & w^2_{22} & w^2_{23}\\
\end{pmatrix}^T * 
\begin{pmatrix}
\frac{\partial J}{\partial z^2_1} & \frac{\partial J}{\partial z^2_2}
\end{pmatrix}</script><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在反向传播中，每一层都只需要重复如下几步：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script></li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script></li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script></li>
<li>最后求出前一层的<script type="math/tex">\frac{\partial J}{\partial a^1}</script>，准备下一步的计算。此步骤的向量化操作在<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#注意点">注意点</a>。</li>
</ol>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">使用numpy实现一个简单的神经网络</a></p>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>在做反向传播代码时，验算了很多遍，发现公式推导没有问题，但是梯度却一直在上升，心态都炸了。<br>最后发现，用了大半年的crossentropy在最前面居然要加上一个“-”号。以前由于是偷懒，在求导的时候一般不加负号，在求完导之后再补上。然后由于写习惯了，导致我忘记crossentropy居然是有负号的。</p>
<h1 id="撒花"><a href="#撒花" class="headerlink" title="撒花"></a>撒花</h1><p>在第二节有一步是初始化数据，但是全篇都没用几个地方用到。是因为数据测试起来太麻烦了，我需要在代码里一步一步分析神经网络的计算过程，从而获得数据。以后再补充吧。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/前馈神经网络的正反向推导.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/linux非root用户配置环境变量.html">
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/linux非root用户配置环境变量.html" class="post-title-link" itemprop="url">linux非root用户配置环境变量</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-15 16:04:44 / 修改时间：16:10:07" itemprop="dateCreated datePublished" datetime="2019-05-15T16:04:44+08:00">2019-05-15</time>
            

            
              

              
            
          </span>
          
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><a href="https://www.cnblogs.com/gstblog/p/10160976.html" target="_blank" rel="noopener">参考文章</a><br>本文以配置anaconda的环境变量为例。</p>
<p>切换到用户目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br></pre></td></tr></table></figure></p>
<p>输入，发现有一个名为<code>.bashrc</code>的文件<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ll</span></span><br></pre></td></tr></table></figure></p>
<p>编辑它<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~<span class="string">/.bashrc</span></span><br></pre></td></tr></table></figure></p>
<p>在最后一行加上如下代码，保存并退出。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=~/anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>
<div class="note warning">
            <p>PATH和=之间不能有空格。由于写java代码习惯了，加上了空格，导致报错。</p>
          </div>
<p>更新配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/linux非root用户配置环境变量.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/梯度下降算法的推导.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/梯度下降算法的推导.html" class="post-title-link" itemprop="url">梯度下降算法的推导</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-10 19:55:24" itemprop="dateCreated datePublished" datetime="2019-05-10T19:55:24+08:00">2019-05-10</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-11 11:57:18" itemprop="dateModified" datetime="2019-05-11T11:57:18+08:00">2019-05-11</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>梯度下降算法大家都知道，公式是<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，其中J是代价函数。但是这个算法具体是怎么来的，可能不太清楚。<br>本文参考<br><a href="https://mp.weixin.qq.com/s/k26Fm0GL3fdVA9VbQIVAuQ" target="_blank" rel="noopener">微信公众号</a><br><a href="https://baike.baidu.com/item/梯度/13014729" target="_blank" rel="noopener">百度百科</a><br>由于没有专业的制图工具，所以只能手画了。。。</p>
<h1 id="梯度下降问题"><a href="#梯度下降问题" class="headerlink" title="梯度下降问题"></a>梯度下降问题</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/梯度下降草图.jpg" alt="梯度下降草图"><br>由图中可以观察到，我们将参数初始化到A点，我们的目标是将点移动到最小值点（或者极小值点）。那么问题就是如何移动了。<br>先给出梯度下降公式：<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，J是代价函数，这个公式应该不陌生。</p>
<h1 id="一阶泰勒展开式"><a href="#一阶泰勒展开式" class="headerlink" title="一阶泰勒展开式"></a>一阶泰勒展开式</h1><p>如果学过高数，应该知道<strong>一阶泰勒展开式</strong>的公式是：<script type="math/tex">f(x) = f(x_0) + (x - x_0) * f'(x_0) + R_n(x)</script>，其中<script type="math/tex">R_n(x)</script>是泰勒公式的余项，可以理解为一个无穷小量。既然是无穷小量那么便可以省略不写，但是即使是无穷小，其实等式的左右边还是有点差距的，所以将等式修改为<strong>约等于号</strong>。</p>
<script type="math/tex; mode=display">
f(x) \approx f(x_0) + (x - x_0) * f'(x_0)</script><p>但是由于我们最小化的代价函数的参数是<script type="math/tex">\theta</script>，所以我们可以将x替换为<script type="math/tex">\theta</script>，即</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script><p>如果不知道泰勒公式，可以看下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/泰勒公式线性近似.webp" alt="泰勒公式线性近似"><br>在点<script type="math/tex">\theta_0</script>处，找一条极短的直线来表示曲线，则直线的斜率为<script type="math/tex">f'(\theta_0)</script>，并且已知<script type="math/tex">\theta_0</script>，那么根据初中数学，可以获得直线公式<script type="math/tex">f(\theta) = f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>（还不懂看这个：<script type="math/tex">y-y_0=k(x-x_0)</script>===&gt;<script type="math/tex">y = y_0 + k(x-x_0)</script>）。<br><div class="note warning">
            <p>如果仔细看到了上一行的推导，你也许要问：为什么直线斜率是<script type="math/tex">f'(\theta_0)</script>。百度。</p>
          </div><br><div class="note warning">
            <p>如果对上式没有问题，可能要问为什么这个红线的箭头要向下，不能向上？我有强迫症，我就要让它向上，并且我还要让<script type="math/tex">\theta</script>在<script type="math/tex">\theta_0</script>右边。这个下面会讲，但是现在假定以下的步骤均围绕上图展开。</p>
          </div><br>至此准备工作完成。</p>
<h1 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h1><p>我们将<script type="math/tex">f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>的<script type="math/tex">\theta - \theta_0</script>用字母<script type="math/tex">\alpha v</script>代替，实际上只用<script type="math/tex">v</script>代替也可以。但是还是使用<script type="math/tex">\alpha v</script>吧。</p>
<script type="math/tex; mode=display">
\theta - \theta_0 = \alpha v</script><p>所以公式被简化为如下形式，并且将导数的表示做一下改变，用<strong>倒三角</strong>表示</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + \alpha v * \nabla f(\theta_0)</script><p>由于我们的目标是使得<script type="math/tex">f(\theta)</script>比<script type="math/tex">f(\theta_0)</script>小，也就是使得<script type="math/tex">f(\theta) - f(\theta_0) < 0</script>。那么将公式转变为</p>
<script type="math/tex; mode=display">
f(\theta) -  f(\theta_0) \approx \alpha v * \nabla f(\theta_0) < 0</script><p>省略一部分</p>
<script type="math/tex; mode=display">
\alpha v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">\alpha</script>一般为正值，所以</p>
<script type="math/tex; mode=display">
v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>实际上都是向量。所以上式就转换为<strong>两个向量相乘在什么时候是小于0的</strong>，并且我们希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好，也就是<script type="math/tex">v * \nabla f(\theta_0)</script>越小越好。那么问题又转化为<strong>两个向量相乘在什么时候是最小的</strong>。<br><div class="note warning">
            <p>问题1：为什么<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>是向量。<br>由于以上都是使用二维的图来描述，所以无法体现是向量。但是实际上<script type="math/tex">\theta</script>不只有一个。<br>问题2：为什么希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好。<br>因为希望<script type="math/tex">f(\theta)</script>这一步迈远一点。</p>
          </div><br>以下为向量乘积的三种形式，由初中的知识可以得知，当向量相反时<script type="math/tex">cos(\alpha)</script>为-1，即cos函数的最小值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/向量的乘积.webp" alt="向量的乘积"><br>由于公式可以转为如下，其中<script type="math/tex">\beta</script>是向量夹角</p>
<script type="math/tex; mode=display">
|v| * |\nabla f(\theta_0)| * cos(\beta) < 0</script><p>所以当<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>正好相反时，<script type="math/tex">cos(\beta) = -1</script>。也就是说当<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>反向，<script type="math/tex">v * \nabla f(\theta_0)</script>最小。<br>众所周知，<script type="math/tex">\nabla f(\theta_0)</script>就是梯度，也就是梯度方向。所以只需要<script type="math/tex">v</script>为<script type="math/tex">\nabla f(\theta_0)</script>的反方向即可。所以</p>
<script type="math/tex; mode=display">
v  = -\frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}\\</script><p>之所以要除以<script type="math/tex">\nabla f(\theta_0)</script>的模，是因为<script type="math/tex">v</script>是单位向量。<br><div class="note warning">
            <p><script type="math/tex">v</script>为什么是单位向量。不太清楚，原博主没说明。</p>
          </div><br>将<script type="math/tex">v</script>带入到<script type="math/tex">\theta - \theta_0 = \alpha * v</script>中</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha * \frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}</script><p>一般地，因为<script type="math/tex">|\nabla f(\theta_0)|</script>是标量，可以并入到中，即简化为：</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha *\nabla f(\theta_0)</script>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/梯度下降算法的推导.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/深度学习中的一些疑问总结.html" class="post-title-link" itemprop="url">深度学习中的一些疑问总结</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-03 13:54:56" itemprop="dateCreated datePublished" datetime="2019-05-03T13:54:56+08:00">2019-05-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 18:36:14" itemprop="dateModified" datetime="2019-05-07T18:36:14+08:00">2019-05-07</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h1><div class="note primary">
            <p>为什么<script type="math/tex">w*x+b=z</script>之后，需要使用一个激活函数<script type="math/tex">a = \sigma(z)</script>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
          </div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。<br>那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。</p>
<script type="math/tex; mode=display">
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w' * a^0 + b' \qquad由于w和b只是一堆常数，所以将w'代替w^2 * w^1，b'同理
    \end{array}
\right.</script><p>通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<script type="math/tex">a^2 = w' * a^0 + b'</script>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。<br>只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降的意义"><a href="#梯度下降的意义" class="headerlink" title="梯度下降的意义"></a>梯度下降的意义</h1><div class="note primary">
            <p>首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？</p>
          </div>
<h1 id="dropout背后的原理"><a href="#dropout背后的原理" class="headerlink" title="dropout背后的原理"></a>dropout背后的原理</h1><div class="note primary">
            <p>dropout背后的原理是什么？</p>
          </div>
<p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a></p>
<h1 id="为什么Mini-batch比普通的梯度下降快？"><a href="#为什么Mini-batch比普通的梯度下降快？" class="headerlink" title="为什么Mini-batch比普通的梯度下降快？"></a>为什么Mini-batch比普通的梯度下降快？</h1><div class="note primary">
            <p>为什么Mini-batch比普通的梯度下降快？</p>
          </div>
<h1 id="指数加权平均的作用"><a href="#指数加权平均的作用" class="headerlink" title="指数加权平均的作用"></a>指数加权平均的作用</h1><div class="note primary">
            <p>指数加权平均的作用</p>
          </div>
<h1 id="为什么要deep"><a href="#为什么要deep" class="headerlink" title="为什么要deep"></a>为什么要deep</h1><div class="note primary">
            <p>学了有一段时间的深度学习，但是有个问题一直没想明白。那就是将hidden layer叠多层的意义是什么？</p>
          </div>
<p>可以观察下图发现，确实越deep代价越小。左边的两列显示了，层数越多代价越小。右边两列显示即使整个神经网络参数类似，但是明显越deep代价越小。同一行代表hidden layer的参数接近。那个size指的是神经网络中的参数。再看最后一行，它显示即使参数暴增到16k，代价也不是很低。图中红框还显示了2层2k的model比1层16k的model好多了。<br>下图是由底下的论文的作者做的实验得出的结论。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/隐藏层层数对cost的影响.jpg" alt="隐藏层层数对cost的影响"><br><strong>那么为什么神经网络越深效果越好呢？</strong><br>这其实归功于modularization——模块化。如下图所示，如果直接写一个model用于将4种人分类，那么可能会出现某类人的数据并不多的情况，比如长发的男生的数据可能并不多。那么分类的准确率可能会不是很高。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/解释why deep的例子.jpg" alt="解释why deep的例子"><br>下图中先将其分类为男女以及长短发，然后再进一步分类。虽然说长发男生的数据比较少，但是男女和长短发的数据有很多，我们可以得到一个很好的模型。之后我们再叠一层用于进一步分类，此时，由于我们已经做了上一步的分类，所以新的一层可以使用上一层的特征。<br>上一层的分类已经把难的事情——辨别男女，辨别长短发等解决了，所以后一层只要使用少量的数据就能进行分类。<br><strong>没有使用模块化</strong>的那个模型，它是用少量的数据硬生生地去识别长发男生。<strong>使用模组化</strong>的模型是先识别男女以及长短发，再通过调用前一步的特征判断。下图4个分类器区别可能只是辨别的方式不同而已，比如权重不同。输入一张图片之后第一层已经可以辨别是男还是女，长发还是短发，然后后一层经过简单的运算就可以确定了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/模块化后.jpg" alt="模块化后"><br>经过上面的解释，可能已经大致理解是什么意思了。但是真要讲清还有点问题，尤其是模块化怎么做。<br>但是李宏毅老师说模块化其实是神经网络从数据中<strong>自动</strong>学到的。</p>
<h2 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h2><p>用数电的逻辑门来举例，但是我没怎么学过数电，所以没有理解。<br>另一个比较贴近生活的例子，就是剪窗花。没有人会一瓣花一瓣花的去剪窗花，都是将纸先折好，然后一步剪完。这就是模块化了。</p>
<p>还有其他领域的人也有过解读，个人理解why deep这个问题可能到现在没有一个官方的回应，可能前辈也是误打误撞才发现deep learning很牛。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/其他领域对为什么要deep的解读.jpg" alt="其他领域对为什么要deep的解读"></p>
<h2 id="吴恩达老师的解释"><a href="#吴恩达老师的解释" class="headerlink" title="吴恩达老师的解释"></a>吴恩达老师的解释</h2><p><a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>也做了解释。</p>
<h1 id="词向量乘上权重以及做梯度下降有什么意义"><a href="#词向量乘上权重以及做梯度下降有什么意义" class="headerlink" title="词向量乘上权重以及做梯度下降有什么意义"></a>词向量乘上权重以及做梯度下降有什么意义</h1><p><a href="https://mooc.study.163.com/learn/2001280005?tid=2001391038&amp;_trace_c_p_k2_=023fecd41c524f0d9485b18d2d773f53#/learn/content?type=detail&amp;id=2001770038" target="_blank" rel="noopener">本文灵感</a><br>本文疑问：</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<h2 id="准备词向量"><a href="#准备词向量" class="headerlink" title="准备词向量"></a>准备词向量</h2><p>假设有这么一句话：I want a glass of orange ___.<br>要做的是估计划线处应该填入什么词。答案是juice。<br>首先我们需要一个词典——vocabulary，每个单词对应一个索引，这是通用步骤。词表大小为10000。<br>然后将上述的句子，从单词转成索引形式。即：<br>I want a glass of orange —-&gt; 4343 9665 1 3852 6163 6257<br>此外每一个单词都会对应一个词向量，而词表中所有单词的词向量就组合一个词嵌入矩阵。词表以及词向量都是可以找一些预训练的，比如<strong>GloVe</strong>。<br>梳理一遍就是：<br>单词:索引<br>索引:词向量<br>所以可以通过单词间接地获取到词向量。关于索引对应词向量，实际上是里面没有索引的因为一个矩阵它本身就有一个属性表示索引，如第0行就是代表第0个单词，第1行就是代表第一个单词。<br>总而言之，我们通过单词获取索引后，就能通过该索引直接获取词向量。伪代码可以表示为：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index = vocabulary.get_index('want') <span class="comment"># 索引为9665</span></span><br><span class="line">word_vector = embedding_matrix[index, :] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p><strong>对于词嵌入矩阵的行代表词向量，还是列代表词向量不必纠结。</strong>你要乐意可以改成<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_vector = embedding_matrix[:, index] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p>如果使用one hot编码来执行上述代码就是将9665转为one hot编码，即除了9665位置为1，其余位置全为0。然后<script type="math/tex">word\_vector = embedding\_matrix^T * word\_one\_hot</script>。这样也能得到词向量，但是由于one hot编码全是0，算起来速度太慢了。<br>现在有了句子“I want a glass of orange”的所有词向量，接下来要做的是将这些词向量从头到尾拼在一起，接成一个更长的向量，也就是6倍长的向量。原词向量是300维，拼接完成后是1800维。然后将这个向量输入一个神经网络中，最后经过softmax函数进行预测，预测范围是在10000个单词中，看谁的概率大。</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>将词向量送入神经网络中当然还需要梯度下降进行迭代。这里会有很多疑问，</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<p>我进行逐一思考，本文仅为自己的理解。<br>首先其实有一件事很多视频没讲，可能他们认为这是一件很平常的事，所以没讲。<br>上述的这个步骤并不是预测步骤，而是在进行迭代，所以是一个训练步骤。人家之所以说<em>我们可以通过这个神经网络预测出单词为juice</em>，是因为逻辑上是这样的。<br>由于是训练步骤，所以我们有一个很重要的数据，最终结果。最终结果我们是知道的，然而我们初学者在考虑整个流程时，没把最终结果算进去，因为老师说<em>我们可以通过这个神经网络预测出单词为juice</em>，由于是<em>预测</em>，那么结果肯定没有啊。这很合乎逻辑。所以就陷入了一个思维的怪圈，<strong>正确的逻辑是：</strong></p>
<ol>
<li>首先我们知道最终结果，所以当第一次迭代时，所有的权重都是随机初始化的，1、2两个问题也就没有意义了。第一次迭代完毕后，结果肯定稀巴烂，所以进行梯度下降。</li>
<li>这里面我们又会碰到一个问题，就是梯度下降到底在算什么？其实这里的疑问来自我们的潜意识始终将句子当做文字在看，自然而然就意识不到梯度下降在干什么。而其实我们在几步之前就已经将文字转为词向量了。<br>词向量说白了就是一堆浮点型数字，而最终结果juice也是一个词向量，所以实际上就是将一个权重矩阵乘上一个1800维的向量，得到一个输出值（may be 激活值），然后将这个输出值和juice的向量放入代价函数中进行计算，接下来的梯度下降其实就是跟正常的步骤一样。</li>
<li><strong>梯度下降就是在寻找一个合适的权重矩阵使得权重矩阵乘1800维向量得到的值接近juice的向量。</strong><br>这里在解释第3个问题时，顺便也解释了第1、2个问题。<strong>权重值实际上就是用来使得预测值和实际结果越接近越好</strong></li>
<li>由于开头就说了我们实际上是知道划线处应该填juice，所以第4个问题压根不需要解答，因为我们本来就知道应该填juice，也没必要预测。之前之所以有这个问题，是因为我们潜意识觉得老师说<em>我们可以通过这个神经网络预测出单词为juice</em>。</li>
<li>至于第5个问题如何预测其他句子。打个比方，现在预测一个新的句子：I want a glass of apple ___.<br>由于我们知道词嵌入矩阵是由很多单词的词向量组成的。而一个单词词向量其实就是一堆特征组成的，对比两个句子，apple和orange的词向量肯定很接近，因为它们都是水果，它们的特征（水多不多，好不好吃，是不是水果，有没有性别特征，是不是动词等）都类似。<br>而我们之前已经训练了一个神经网络，我们得到了所需要的权重值，我们直接把这个权重值乘上新句子的词向量，那么结果肯定和权重值乘上之前句子的词向量的结果很接近。因为它们之间的区别仅仅是apple和orange的词向量不同，并且apple和orange的词向量其实也并不是完全不同，只是略有不同。所以二者的结果自然也差不多。<br>最后经过softmax函数在10000个单词之中预测，输出的结果必然都是juice。</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>这里还会有一个问题，那么如果需要预测的句子单词数不一致怎么办？<br>之前两个例子的句子都是6个单词，预测第7个单词。那要是前面有10个单词，我要预测第11个单词怎么办？<br>其实可以只考虑划线处前几个单词，比如只考虑划线处前4个单词，这样输入的维度就相同了。<br>在本文开头的参考视频里，吴恩达老师讲得很清楚了。由古圣先贤总结，一般来说取前4个单词即可，当然你自己也可以用其他办法。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/深度学习中的一些疑问总结.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/深度学习学习记录大纲.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/深度学习学习记录大纲.html" class="post-title-link" itemprop="url">深度学习学习记录大纲</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-02 13:54:55" itemprop="dateCreated datePublished" datetime="2019-05-02T13:54:55+08:00">2019-05-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-06 21:45:59" itemprop="dateModified" datetime="2019-05-06T21:45:59+08:00">2019-05-06</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/summary/" itemprop="url" rel="index"><span itemprop="name">summary</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题"><a href="#《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题" class="headerlink" title="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题"></a>《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题</h1><p><a href="https://yan624.github.io/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">文章地址</a><br>由于神经网络中参数太多，而有些参数的表现形式太过复杂， 比如文中权重——<script type="math/tex">w^l_{ji}</script>有太多上标下标，所以写了一篇文章记录一下。</p>
<h1 id="对神经网络整体的理解"><a href="#对神经网络整体的理解" class="headerlink" title="对神经网络整体的理解"></a>对神经网络整体的理解</h1><p><a href="https://yan624.github.io/学习笔记/对神经网络整体的理解.html">文章地址</a><br>通常学习深度学习从一个最简单的神经网络开始，但是由于对深度学习时0基础，所以需要同时学习大量算法以及其原理，比如梯度下降，Momentum，Adam，RMSprop，adagrad等等算法。所以写了一篇文章记录一下大部分的算法以及原理。</p>
<h1 id="吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型"><a href="#吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型" class="headerlink" title="吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型"></a>吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型.html">文章地址</a><br>学习完神经网络之后，可以学习其他的神经网络模型。由于本人初步决定学习nlp，所以基本没有看CNN，直接学了RNN。本文就是学习RNN的记录，包括了许多算法。</p>
<h1 id="吴恩达深度学习学习笔记：自然语言处理与词嵌入"><a href="#吴恩达深度学习学习笔记：自然语言处理与词嵌入" class="headerlink" title="吴恩达深度学习学习笔记：自然语言处理与词嵌入"></a>吴恩达深度学习学习笔记：自然语言处理与词嵌入</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html">文章地址</a><br>学习完RNN之后，就可以学习nlp的理念了。</p>
<h1 id="练习Keras-RNN的代码"><a href="#练习Keras-RNN的代码" class="headerlink" title="练习Keras RNN的代码"></a>练习Keras RNN的代码</h1><p><a href="https://yan624.github.io/学习笔记/练习Keras RNN的代码.html">文章地址</a><br>学习完Simple NN，RNN和NLP之后，就可以练习一下了。文中使用Keras框架，写了几个例子练习。</p>
<h1 id="疑问总结"><a href="#疑问总结" class="headerlink" title="疑问总结"></a>疑问总结</h1><p><a href="https://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">文章地址</a><br>深度学习入门后必然有很多疑问待解答，此篇解决疑问。</p>
<h1 id="开始CNN"><a href="#开始CNN" class="headerlink" title="开始CNN"></a>开始CNN</h1><p>待续。。。</p>
<h1 id="开始继续学习机器学习"><a href="#开始继续学习机器学习" class="headerlink" title="开始继续学习机器学习"></a>开始继续学习机器学习</h1><p>待续。。。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/深度学习学习记录大纲.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/练习Keras RNN的代码.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/练习Keras RNN的代码.html" class="post-title-link" itemprop="url">练习Keras RNN的代码</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-27 15:28:20" itemprop="dateCreated datePublished" datetime="2019-04-27T15:28:20+08:00">2019-04-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-02 15:21:03" itemprop="dateModified" datetime="2019-05-02T15:21:03+08:00">2019-05-02</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="《Python深度学习》第6章预测imdb的影评"><a href="#《Python深度学习》第6章预测imdb的影评" class="headerlink" title="《Python深度学习》第6章预测imdb的影评"></a>《Python深度学习》第6章预测imdb的影评</h1><p><a href="https://github.com/yan624/deep-learning-notes/tree/master/RNN_learning/imdb_predication" target="_blank" rel="noopener">imdb影评代码</a></p>
<h1 id="第五课第二周作业：Emojify"><a href="#第五课第二周作业：Emojify" class="headerlink" title="第五课第二周作业：Emojify"></a>第五课第二周作业：Emojify</h1><p>本文实现吴恩达深度学习第五课第二周的Emojify作业。目的是了解一个小型nlp系统的构建流程。<br>首先需要导入以下模块。由于在Jupyter中打开的ipynb文件全是英文，所以本文的大部分标题也用英文，方便ctrl F。<br>Emojifier-V1略。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>加了很多注释，但是代码的顺序我做了很大的改动。下面博客里面的代码，是作业里面的代码，基本没改几个字。<br><a href="https://github.com/yan624/deep-learning-notes/tree/master/RNN_learning/emojify" target="_blank" rel="noopener">emojify_V2代码</a></p>
<h2 id="Emojifier-V2-Using-LSTMs-in-Keras"><a href="#Emojifier-V2-Using-LSTMs-in-Keras" class="headerlink" title="Emojifier-V2: Using LSTMs in Keras"></a>Emojifier-V2: Using LSTMs in Keras</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.seed</span>(<span class="number">0</span>)</span><br><span class="line">from keras<span class="selector-class">.models</span> import Model</span><br><span class="line">from keras<span class="selector-class">.layers</span> import Dense, Input, Dropout, LSTM, Activation</span><br><span class="line">from keras<span class="selector-class">.layers</span><span class="selector-class">.embeddings</span> import Embedding</span><br><span class="line">from keras<span class="selector-class">.preprocessing</span> import sequence</span><br><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.seed</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br></pre></td></tr></table></figure>
<h3 id="Overview-of-the-model"><a href="#Overview-of-the-model" class="headerlink" title="Overview of the model"></a>Overview of the model</h3><h3 id="Keras-and-mini-batching"><a href="#Keras-and-mini-batching" class="headerlink" title="Keras and mini-batching"></a>Keras and mini-batching</h3><p>本练习中，我们使用mini-batch算法训练Kears。大部分深度学习框架要求在相同的mini-batch中所有序列都要等长。这使得可以执行向量化，如果你有一个3个单词的句子和一个4个单词的句子，它们之间的计算会不同（一个需要3个timestep，一个需要4个timestep，也就是说需要的LSTM个数不同），所有同时计算它们是不可能的，即无法向量化。<br>通用的解决办法是使用padding。具体来说，设置一个序列的最大长度，然后使其他的序列都与该长度等长。比如序列的最大长度是20，那么将其他的序列在后面补充0，知道长度等于20。所以句子“I love you”会在“you”后面被补充17个0。即<script type="math/tex">\begin{pmatrix}e_i & e_{love} & e_{you} & \overrightarrow{0} & \overrightarrow{0} & \cdots & \overrightarrow{0}\end{pmatrix}</script>，e代表词向量。如果长度大于20的话会被裁剪。<br>以下代码实现将句子转换为句子中的每个单词转为索引，这个索引是GloVe词嵌入的，每一个单词对应一个id。id就是索引。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sentences_to_indices</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将句子转为索引形式，短于max_len的句子后面补充0，长于max_len的句子直接截断</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape</span></span><br><span class="line">    X_indices = np.zeros((m, max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] =word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure></p>
<p>以下设计Embedding层，使用keras的Embedding类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pretrained_embedding_layer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此方法创建了一个Embedding层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># GloVe的总单词数量</span></span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                  <span class="comment"># adding 1 to fit Keras embedding (requirement)</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    <span class="comment"># 词嵌入矩阵，之前V1压根没用词嵌入矩阵，将这个矩阵放入Embedding层中供keras使用</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable.</span></span><br><span class="line">    <span class="comment"># Use Embedding(...). Make sure to set trainable=False.</span></span><br><span class="line">    embedding_layer = Embedding(vocab_len, emb_dim, trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure></p>
<h3 id="Building-the-Emojifier-V2"><a href="#Building-the-Emojifier-V2" class="headerlink" title="Building the Emojifier-V2"></a>Building the Emojifier-V2</h3><p>以下代码完成Emojify。主要是keras代码。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: Emojify_V2</span></span><br><span class="line"></span><br><span class="line">def Emojify_V2(input_shape, word_to_vec_map, word_to_index):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    <span class="attr">sentence_indices</span> = Input(input_shape, <span class="attr">dtype='int32')</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    <span class="attr">embedding_layer</span> = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    <span class="attr">embeddings</span> = embedding_layer(sentence_indices)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    <span class="comment"># 这个128是神经元的个数</span></span><br><span class="line">    <span class="attr">X</span> = LSTM(<span class="number">128</span>, <span class="attr">return_sequences=True)(embeddings)</span></span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    <span class="attr">X</span> = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    <span class="attr">X</span> = LSTM(<span class="number">128</span>, <span class="attr">return_sequences=False)(X)</span></span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    <span class="attr">X</span> = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    <span class="attr">X</span> = Dense(<span class="number">5</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    <span class="attr">X</span> = Activation('softmax')(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    <span class="attr">model</span> = Model(<span class="attr">inputs=sentence_indices,</span> <span class="attr">outputs=X)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    return model</span><br></pre></td></tr></table></figure></p>
<h2 id="系统整体流程"><a href="#系统整体流程" class="headerlink" title="系统整体流程"></a>系统整体流程</h2><p>RNN</p>
<ol>
<li>读取GloVe文件和训练数据</li>
<li>将训练数据的label转为one hot表示</li>
<li>求出所有训练数据中最长句子的长度，该长度就是LSTM的个数。由于向量化的要求，LSTM的个数需要相同，以最长长度作为LSTM的个数，当然并不需要每个项目都这么设置，完全可以自己选，随便举几个例子比如20,50，100等。</li>
<li>设计Embedding层</li>
<li>建立神经网络模型</li>
<li>将每句话转换为索引表示，如果长度不够就填0，够了就截断。《Python深度学习》中使用了pad_sequences类</li>
<li>使用模型预测，第6条就是输入的训练数据，第2条就是输入的标签</li>
</ol>
<p>Embedding层需要输入一个词嵌入矩阵，就是一个二维数组。每行代表一个单词的特征向量，行数就是单词的索引。而输入的训练数据被处理成单词的索引形式。如一组训练样本，每个单词被替换成唯一的索引。<br>Embedding层太过复杂，故不作详解。第一章的代码中全部有注释。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/练习Keras RNN的代码.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html" class="post-title-link" itemprop="url">Jupyter出现gbk codec cant decode byte 0x93 in position 3136：illegal multibyte sequence</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-27 14:45:05 / 修改时间：14:45:49" itemprop="dateCreated datePublished" datetime="2019-04-27T14:45:05+08:00">2019-04-27</time>
            

            
              

              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>一般来说是open()方法没有加encoding=’utf-8’，但是没用，试了其他办法没一个能用。<br>解决办法：重启Jupyter。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/Jupyter出现gbk-codec-cant-decode-byte-0x93-in-position-3136：illegal-multibyte-sequence.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yan624.github.io/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html">
    
  		
    

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="朱冲䶮">
      <meta itemprop="description" content="朱冲䶮的博客，记录在学习ml、dl时遇到的问题">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html" class="post-title-link" itemprop="url">吴恩达深度学习学习笔记：自然语言处理与词嵌入</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-25 23:58:24" itemprop="dateCreated datePublished" datetime="2019-04-25T23:58:24+08:00">2019-04-25</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-14 15:28:56" itemprop="dateModified" datetime="2019-05-14T15:28:56+08:00">2019-05-14</time>
              
            
          </span>
          
          
          	
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine-learning</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep-learning</span></a></span>

                
                
              
            </span>
          
          
          <!--不蒜子计数-->
          <span id="busuanzi_container_site_uv">
          	<span class="post-meta-divider">|</span>
			浏览量：<span id="busuanzi_value_site_uv"></span>人次
		  </span>

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>我们一直使用<a href="https://yan624.github.io/学习笔记/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型.html#one hot编码">one hot编码</a>，这在之前已经记过笔记。这种表示方法的最大缺点是将每个词孤立起来，并且泛化能力不强。由于每个向量的内积都是0，所以它们之间的距离都是一样的。比如</p>
<ol>
<li>I want a glass of orange juice.</li>
<li>I want a glass of apple <em>_</em>.<br>这两个句子是很常见的句子，所以自然而然的想到划线处应该是juice。但是由于one hot编码，程序并不知道orange和apple之间的关系，也就猜不出来。</li>
</ol>
<h2 id="Featurized-representation：-word-embedding"><a href="#Featurized-representation：-word-embedding" class="headerlink" title="Featurized representation： word embedding"></a>Featurized representation： word embedding</h2><p>既然one hot有问题，那么自然就有人发明了新的算法。<br>使用特征来表示每个词。如果适应特征化来表示，那么最后发现orange和apple的特征差不多，就可以推测出划线处应该填写什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Featurized representation： word embedding.jpg" alt="Featurized representation： word embedding"></p>
<h2 id="Visualizing-word-embedding"><a href="#Visualizing-word-embedding" class="headerlink" title="Visualizing word embedding"></a>Visualizing word embedding</h2><p>可以使用t-SNE算法将数据可视化为二维的图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Visualizing word embedding.jpg" alt="Visualizing word embedding"></p>
<h1 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h1><h2 id="类比"><a href="#类比" class="headerlink" title="类比"></a>类比</h2><p>看下图中的表格，现在已知对应关系man-&gt;woman，能否推出king对应于queen？也就是说king-&gt;<em>_</em>，填空题。<br>解法是：<br>求出man和woman之间的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-1\\
0.01\\
0.03\\
0.09\\
\end{pmatrix} - 
\begin{pmatrix}
1\\
0.02\\
0.02\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>假设计算king和queen的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-0.95\\
0.93\\
0.70\\
0.02\\
\end{pmatrix} - 
\begin{pmatrix}
0.97\\
0.95\\
0.69\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>算法的原理就是找到一个词使得man和woman的差与king和新词的差接近。翻译为代码就是<script type="math/tex">find\ word\ w: argmax\ sim(e_w, e_{king} - e_{man} + e _{woman})</script>。但是算法的准确度只有30%-75%。</p>
<h3 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h3><p>余弦相似度也可以计算相似度。公式为<script type="math/tex">sim(u,v) = \frac{u^Tv}{\parallel u\parallel_2\parallel v\parallel_2}</script></p>
<h1 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h1><p>略。大致意思是一个嵌入矩阵E乘上one hot编码可以得到一个单词的特征向量。E就是全部单词的特征矩阵。</p>
<h1 id="如何train一个词嵌入矩阵"><a href="#如何train一个词嵌入矩阵" class="headerlink" title="如何train一个词嵌入矩阵"></a>如何train一个词嵌入矩阵</h1><p>在早期深度学习的研究人员都是使用比较复杂的算法，但是随着时间的推移，这些复杂的算法被慢慢的简化。以至于现在的新手看到这些简化版的算法时，会疑惑这样简单的算法时怎么工作的。所以现在先介绍一个比较复杂的算法，再慢慢介绍简化版的。<br><div class="note info">
            <p>这节好像是用来讲如何建立神经语言模型的，以后再看。之前讲了嵌入矩阵E，但是E中全部的特征向量是已经假定存在的，那么这些特征从何而来呢？就是这节讲的，去训练得来的。但是其实有已经训练好的，我们可以直接拿来用，网上有很多。</p>
          </div></p>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><h1 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h1><h1 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h1><p>就是词嵌入中可能带有一些偏见，比如男女偏见、种族偏见等。现在的目的就是除去这种偏见。<br>暂且不看，其他的算法都还没学。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="朱冲䶮">
            
              <p class="site-author-name" itemprop="name">朱冲䶮</p>
              <p class="site-description motion-element" itemprop="description">朱冲䶮的博客，记录在学习ml、dl时遇到的问题</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">82</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">18</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
            <br>
						<!--同个ip访问本站页面次数-->
						<div class="site-state-item site-state-posts" style="border-left:none;">
								<span class="site-state-item-count" id="busuanzi_value_site_pv"></span>
								<span class="site-state-item-name">浏览量</span>
						</div>
						<!--不同ip访问本站次数-->
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count" id="busuanzi_value_site_uv"></span>
								<span class="site-state-item-name">访客量</span>
						</div>
						<div class="site-state-item site-state-posts">
								<span class="site-state-item-count">61.4k</span>
								<span class="site-state-item-name">总字数</span>
						</div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897538633@qq.com" title="E-Mail &rarr; mailto:897538633@qq.com" rel="noopener" target="_blank"><i class="fas fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/yan624" title="GitHub &rarr; https://github.com/yan624" rel="noopener" target="_blank"><i class="fab fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://weibo.com/u/2607724281" title="Weibo &rarr; https://weibo.com/u/2607724281" rel="noopener" target="_blank"><i class="fab fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://huaguoguo.gitee.io" title="http://huaguoguo.gitee.io" rel="noopener" target="_blank">葬爱丶华少的天下</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朱冲䶮</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        








        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


  

	<script src="/lib/my-utils.js"></script>
	<!--图片缩放插件-->
	<script src="/lib/zoomify/zoomify.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <!--不蒜子统计-->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
  <!-- 背景图片 -->
	<script>
		function generateBG(count){
			var bg_prefix = '/images/background/';
			var bg =new Array();
			for(var i = 0; i < count; i++){
				bg[i] = bg_prefix + i + '.jpg';
			}
			bg.shuffle();
			return bg;
		}
		$("body").backstretch(generateBG(5), { 
			duration:60000,//1min一换
			fade: 1500 
		});
		$('#content img').zoomify({duration: 500, });
		$('#content img').on('zoom-in.zoomify', function () {
			$('#sidebar').css('display', 'none');
		});
		$('#content img').on('zoom-out-complete.zoomify', function () {
			$('#sidebar').css('display', '');
		});
  </script>
</body>
</html>
