<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2019年度文章总结</title>
    <url>/timeline/2019%E5%B9%B4%E5%BA%A6%E6%96%87%E7%AB%A0%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;2019年写了很多博文或者学习笔记，此文章记录一些比较重要的博文，以供未来查阅。</p>
          </div>
<ul>
<li><a href="https://yan624.github.io/categories/assorted/paper/">论文笔记</a></li>
<li><a href="https://yan624.github.io/categories/algorithm/">数据结构与算法学习</a></li>
<li><a href="https://yan624.github.io/categories/assorted/conference/">参加的会议</a></li>
<li><strong>学习路线</strong><ol>
<li>【2019-04-12】<a href="https://yan624.github.io/·zcy/AI/dl/对神经网络整体的理解.html">对神经网络整体的理解</a><div class="note success">
            <p>通常学习深度学习从一个最简单的神经网络开始，但是由于对深度学习时0基础，所以需要同时学习大量算法以及其原理，比如梯度下降，Momentum，Adam，RMSprop，adagrad等等算法。所以写了一篇文章记录一下大部分的算法以及原理。</p>
          </div></li>
<li><a href="https://yan624.github.io/·学习笔记/AI/dl/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题</a><div class="note success">
            <p>由于神经网络中参数太多，而有些参数的表现形式太过复杂， 比如文中权重 <script type="math/tex">w^l_{ji}</script> 有太多上标下标，所以写了一篇文章记录一下。</p>
          </div></li>
<li><a href="https://yan624.github.io/·学习笔记/AI/ml/梯度下降算法的推导.html">梯度下降算法的推导</a><div class="note success">
            <p>学会了最基本的神经网络之后，开始理解反向传播算法。之前仅仅是在使用，现在想要理解它到底在干什么。所以自己推导了一遍。发现其实就是链式求导。</p>
          </div>
<a id="more"></a></li>
<li><a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">吴恩达李宏毅综合学习笔记：RNN入门</a><div class="note success">
            <p>学习完神经网络之后，可以学习其他的神经网络模型。由于本人初步决定学习 nlp，所以基本没有看 CNN，直接学了 RNN。本文就是学习 RNN 的记录，包括了许多算法以及技术。<br>one hot representation, RNN(双向、深层), GRU, LSTM, RNN反向传播, seq2seq, 计算图, language model, Pointer Network</p>
          </div></li>
<li>深度学习：<ol>
<li><a href="https://yan624.github.io/·zcy/AI/dl/深度学习算法（一）：simple NN（前馈神经网络的正反向推导）.html">深度学习算法（一）：simple NN（前馈神经网络的正反向推导）</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/dl/深度学习算法（二）：simple RNN 推导与理解.html">深度学习算法（二）：simple RNN 推导与理解</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/dl/深度学习算法（三）：RNN 各种机制.html">深度学习算法（三）：RNN 各种机制</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/dl/深度学习算法（四）：Transformer.html">深度学习算法（四）：Transformer</a></li>
</ol>
</li>
<li><a href="https://yan624.github.io/·学习笔记/AI/nlp/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html">吴恩达深度学习学习笔记：自然语言处理与词嵌入</a><div class="note success">
            <p>学习完RNN之后，就可以学习 NLP 的概念了，这里面讲得虽然还是神经网络，但是其实都是 NLP 领域的知识。</p>
          </div></li>
<li>nlp：<ul>
<li><a href="https://yan624.github.io/·zcy/AI/nlp/【NLP算法】（一）word2vec.html">【NLP算法】（一）word2vec</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/nlp/【NLP算法】（零）NLP基础算法.html">【NLP算法】（零）NLP基础算法</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/nlp/【NLP算法】（三）条件随机场CRF.html">【NLP算法】（三）条件随机场CRF</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/KG/【知识图谱】（一）从概念到实战.html">【知识图谱】（一）从概念到实战</a></li>
</ul>
</li>
<li>此篇文章已删除，转到《深度学习500问》笔记中。<div class="note success">
            <p>深度学习入门后必然有很多疑问待解答，此篇解决疑问。</p>
          </div></li>
<li><a href="https://yan624.github.io/·学习笔记/AI/nlp/CS224n学习笔记.html">CS224n学习笔记</a></li>
<li><a href="https://yan624.github.io/·学习笔记/AI/nlp/【读书笔记】：《自然语言处理综论》（第二版）.html">【读书笔记】：《自然语言处理综论》（第二版）</a></li>
<li>【 2019-11-05】<strong><a href="https://yan624.github.io/project/多领域seq2lf.html">多领域seq2lf</a></strong><div class="note danger">
            <p>虽说此篇文章是我做实验的笔记，但是由于是我第一次做实验。所以我在其中记录了大量的笔记/炼丹技巧/原理分析、遇到的问题/bug。例如：</p><ol><li>pytorch 一些值得注意的地方/tensorboardX 可视化</li><li>clipping 分析</li><li>LSTM/GRU/Attention 机制分析</li><li>dropout/early stopping/norm 等使用方法与技巧</li><li>梯度爆炸/消失原理分析</li></ol><p>所以这篇文章可以说是我一整年的精华所在，里面还引用了这一年中写的其他文章。</p>
          </div></li>
</ol>
</li>
<li>学习笔记<ul>
<li><a href="https://yan624.github.io/·学习笔记/git学习记录.html">git学习记录</a></li>
<li><a href="https://yan624.github.io/IT-stuff/python/Python爬虫学习记录（一）：正则表达式.html">Python爬虫学习记录（一）：正则表达式</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/特征工程：笔记.html">特征工程：笔记</a></li>
</ul>
</li>
<li>机器学习<ul>
<li><a href="https://yan624.github.io/·zcy/AI/ml/代价函数.html">代价函数</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习算法（一）：线性回归.html">机器学习算法（一）：线性回归</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习算法（二）：逻辑回归.html">机器学习算法（二）：逻辑回归</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习算法（三）：决策树.html">机器学习算法（三）：决策树</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习算法（四）：K均值（K-means）.html">机器学习算法（四）：K均值（K-means）</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习算法（五）：PCA.html">机器学习算法（五）：PCA</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习算法（六）：SVM.html">机器学习算法（六）：SVM</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习算法（七）：K-NN.html">机器学习算法（七）：K-NN</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习算法（八）：Adaboost.html">机器学习算法（八）：Adaboost</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/【机器学习算法】半监督学习.html">【机器学习算法】半监督学习</a></li>
<li><a href="https://yan624.github.io/·zcy/AI/ml/机器学习的下一步.html">机器学习的下一步</a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>timeline</category>
      </categories>
  </entry>
  <entry>
    <title>各种乘法的表示符号</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%90%84%E7%A7%8D%E4%B9%98%E6%B3%95%E7%9A%84%E8%A1%A8%E7%A4%BA%E7%AC%A6%E5%8F%B7.html</url>
    <content><![CDATA[<p>&emsp;&emsp;设</p>
<script type="math/tex; mode=display">
m = [a_1, a_2, \cdots, a_n] \\
n = [b_1, b_2, \cdots, b_n] \\</script><p>&emsp;&emsp;设 f(x)，g(x) 是 <script type="math/tex">R_1</script> 上的两个可积函数。</p>
<h1 id="向量点积"><a href="#向量点积" class="headerlink" title="向量点积"></a>向量点积</h1><p>&emsp;&emsp;英文中叫做 dot product，又被称为<strong>点乘</strong>、<strong>内积</strong>、<strong>数量积</strong>。<br>&emsp;&emsp;则 <script type="math/tex">m \cdot n = a_1 * b_1 + a_2 * b_2 + \cdots a_n * b_n</script>。<br>&emsp;&emsp;也可以写作<script type="math/tex">m \bullet n = a_1 * b_1 + a_2 * b_2 + \cdots a_n * b_n</script>。<br><a id="more"></a></p>
<h1 id="向量叉乘"><a href="#向量叉乘" class="headerlink" title="向量叉乘"></a>向量叉乘</h1><p>&emsp;&emsp;又叫<strong>向量积</strong>、<strong>外积</strong>、<strong>叉积</strong>。</p>
<script type="math/tex; mode=display">
m \times n =</script><h1 id="矩阵相乘"><a href="#矩阵相乘" class="headerlink" title="矩阵相乘"></a>矩阵相乘</h1><p>&emsp;&emsp;矩阵相乘可以省略乘号：</p>
<script type="math/tex; mode=display">A_{3 \times 4}B_{4 \times 100} = C_{3 \times 100}</script><h1 id="矩阵对应元素相乘"><a href="#矩阵对应元素相乘" class="headerlink" title="矩阵对应元素相乘"></a>矩阵对应元素相乘</h1><p>&emsp;&emsp;英语中叫做 <strong>element-wise multiplication</strong>，也被称为 Hadamard 乘积（Hadamard product）。有以下几种符号表示：</p>
<script type="math/tex; mode=display">
A_{3 \times 1} \circ B_{3 \times 1} = C_{3 \times 1} \\
A_{3 \times 4} \circ B_{3 \times 4} = C_{3 \times 4} \\
A_{3 \times 1} \odot B_{3 \times 1} = C_{3 \times 1} \\</script><h1 id="张量积"><a href="#张量积" class="headerlink" title="张量积"></a>张量积</h1><p>&emsp;&emsp;英语中叫做 tensor product，对于向量而言张量积与外积等价。有以下符号表示：</p>
<script type="math/tex; mode=display">A_{4 \times 1} \otimes B_{1 \times 3} = C_{4 \times 3}</script><h2 id="克罗内克积"><a href="#克罗内克积" class="headerlink" title="克罗内克积"></a>克罗内克积</h2><p>&emsp;&emsp;英语中叫做 Kronecker product，克罗内克积是张量积的特殊形式。表示为：</p>
<script type="math/tex; mode=display">A_{m \times n} \otimes B_{p \times q} = C_{mp \times nq}</script><h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><p>&emsp;&emsp;英语中叫做 convolution。表示为：</p>
<script type="math/tex; mode=display">h(x)=(f \ast g)(x) = \int^{\infty}_{-\infty} f(\tau)g(x - \tau)d\tau</script><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol>
<li><a href="https://blog.csdn.net/jdbc/article/details/82753391" target="_blank" rel="noopener">【收藏】各种乘法的区别 “点积、外积、数乘…等”</a></li>
<li><a href="https://blog.csdn.net/dcrmg/article/details/52416832" target="_blank" rel="noopener">向量点乘（内积）和叉乘（外积、向量积）概念及几何意义解读</a></li>
<li><a href="https://baike.baidu.com/item/%E5%90%91%E9%87%8F%E7%A7%AF?fromtitle=cross+product&amp;fromid=18082286" target="_blank" rel="noopener">向量积</a></li>
<li><a href="https://baike.baidu.com/item/%E5%93%88%E8%BE%BE%E7%8E%9B%E7%A7%AF/18894493?fr=aladdin" target="_blank" rel="noopener">哈达玛积</a></li>
<li><a href="https://baike.baidu.com/item/%E5%BC%A0%E9%87%8F%E7%A7%AF/7540845?fr=aladdin" target="_blank" rel="noopener">张量积</a></li>
<li><a href="https://baike.baidu.com/item/%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%E7%A7%AF/6282573?fr=aladdin" target="_blank" rel="noopener">克罗内克积</a></li>
<li><a href="https://www.zhihu.com/question/22298352/answer/637156871" target="_blank" rel="noopener">如何通俗易懂地解释卷积？</a></li>
</ol>
]]></content>
      <categories>
        <category>assorted</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>一些快捷键</title>
    <url>/assorted/%E4%B8%80%E4%BA%9B%E5%BF%AB%E6%8D%B7%E9%94%AE.html</url>
    <content><![CDATA[<p>最近老是用到win10的快捷键，但是用过之后过几天就忘了，所以记录下。<br>还有有谁知道怎么用快捷键打开高级系统设置，设置环境变量老是要打开这个，烦得很。</p>
<h1 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h1><p>按 V 进入 Visual Mode，然后可以上下左右选择行数。按 y 复制，d 剪切，p 粘贴。</p>
<h1 id="win10"><a href="#win10" class="headerlink" title="win10"></a>win10</h1><ol>
<li>win+E 打开我的电脑</li>
<li>win+R 打开运行</li>
<li>win+L 锁屏</li>
<li>fn+ESC 打开/关闭功能键。f5是刷新键，但是有时候发现按f5无效，其实是因为功能键打开了，只需要按fn+ESC关闭就可。</li>
<li>四指在触摸板向左/向右滑动，切换桌面。</li>
<li>三指向上滑动将当前的任务以小窗口显示在桌面，三指向下滑动隐藏。</li>
<li>win+Tab类似6</li>
<li>win+d最小化所有打开的窗口</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
      </categories>
      <tags>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title>任务完成型对话系统论文</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/nlp/%E4%BB%BB%E5%8A%A1%E5%AE%8C%E6%88%90%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0.html</url>
    <content><![CDATA[<h1 id="任务完成型对话系统"><a href="#任务完成型对话系统" class="headerlink" title="任务完成型对话系统"></a>任务完成型对话系统</h1><a id="more"></a>
<h1 id="SLU"><a href="#SLU" class="headerlink" title="SLU"></a>SLU</h1><p>&emsp;&emsp;口语理解（Speech language Understanding，SLU）共由两个任务组成，即意图检测（Intent detection，ID）/意图识别（Intent Determination，ID）和 slot-filling（SF）。ID 可以视为语句分类任务，SF 可以视为序列标注任务。<br>&emsp;&emsp;以下论文按发表时间（或提交到 arxiv 的时间）排序：</p>
<h2 id="SLU-1"><a href="#SLU-1" class="headerlink" title="SLU"></a>SLU</h2><p>&emsp;&emsp;实现 SLU 有多种途径，也无法判断优劣，以下为几种方式：</p>
<ol>
<li>…</li>
<li>separate models：使用神经网络<strong>分别</strong>对两个任务进行训练</li>
<li>joint model：使用一个 joint model 对两个任务进行<strong>联合</strong>训练，即单个 encoder 编码输入语句，decoder 有两个，一个生成意图，另一个进行序列标注。</li>
</ol>
<p>&emsp;&emsp;已经有人对比过很多论文（不重复造轮子了）：</p>
<ol>
<li><a href="https://blog.csdn.net/black_soil/article/details/90405098" target="_blank" rel="noopener">【综述】对话系统中的口语理解技术</a></li>
</ol>
<h3 id="separate-models"><a href="#separate-models" class="headerlink" title="separate models"></a>separate models</h3><ol>
<li><a href="https://arxiv.xilesou.top/pdf/1812.10235.pdf" target="_blank" rel="noopener">Wang et al., 2018.12: A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling</a> <span class="label success">Bi- or uni-LSTM</span> <span class="label primary">encoder-decoder</span><div class="note info">
            <ul><li><a href="https://yan624.github.io/·论文笔记/dilogue/task-oriented/59. A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling.html">论文笔记</a></li><li>对 ID 和 SF 分别使用了一个模型，但是<strong>共享了两个模型的隐藏状态</strong>。论文中描述到，以前的研究经常把 ID 和 SF 当做两个任务来看，这次这篇论文将这两个任务交互起来当做一个任务看待。但是我认为 joint model 这种做法已经是将两个任务当做一个看了，毕竟它们共享了一个模型提取的信息，只是在解码时使用不同的模型。<ul><li>对于解码而言，论文分别使用了 encoder-decoder 结构和纯粹的 encoder 结构，最后是 encoder-decoder 性能略高。</li></ul></li></ul>
          </div></li>
<li><a href="https://speechlab.sjtu.edu.cn/papers/sz128-zhu-icassp17.pdf" target="_blank" rel="noopener">Zhu et al., 2017: ENCODER-DECODER WITH FOCUS-MECHANISM FOR SEQUENCE LABELLING BASED SPOKEN LANGUAGE UNDERSTANDING</a> <span class="label success">Bi- or uni-LSTM</span> <span class="label primary">encoder-decoder</span> <span class="label info">focus mechanism</span><div class="note warning">
            <ul><li><strong>此论文专注于 SLU 中的 slot-filling 任务</strong></li><li>使用 encoder-decoder 结构，其中 encoder 是 Bi-LSTM，decoder 是 uni-LSTM<ul><li>结合了 attention 机制，发现有局限性<ul><li>序列标注中的输入和输出是对齐的，但是 attention scores 是所有输入单词的总和</li><li>对齐（alignment）可由 attention 学习到，但是在序列标记任务中，很难拟合有限的标注数据（与之不同，机器翻译更容易获得成对数据）。</li></ul></li><li>所以对 encoder-decoder 结构提出了一种新的 <strong>focus</strong> 机制</li></ul></li><li>首先使用 Bi-LSTM 编码，得到 <script type="math/tex">h_i = [\overleftarrow{h_i}, \overrightarrow{h_i}], \, \overleftarrow{h_i} = f_l(\overleftarrow{h_{i+1}}, x_i), \, \overrightarrow{h_i} = f_r(\overrightarrow{h_{i-1}}, x_i)</script></li><li>然后使用 uni-LSTM 进行解码，初始状态 <script type="math/tex">s_0 = \overleftarrow{h_1}</script>，每个时间步都由 uni-LSTM 训练并产生隐藏状态 <script type="math/tex">s_i</script>，同时每个时间步除了输入值（输入值是 label 组成的，即 IOB），还需要输入 Bi-LSTM 对应时间步的隐藏状态 <script type="math/tex">h_i</script>。<ul><li><strong>focus mechanism 实际上就是把对应时间步的隐藏状态 <script type="math/tex">h_i</script> 输入给 decoder</strong>。。。也就是说把 attention 机制稍微改装了一下，当前时间步的 score 是 1，其余时间步是 0。</li><li>decoder 的隐藏状态计算方法是 <script type="math/tex">s_i = f_d(s_{t-1}, y_{t-1}, c_t)</script>，其中 <script type="math/tex">s_{t-1}</script> 代表上一个时间步的隐藏状态，<script type="math/tex">y_{t-1}</script> 代表输入值，<script type="math/tex">c_t</script> 是上下文信息，其实就等于 <script type="math/tex">h_i</script>。</li><li>f 函数均指 LSTM units function</li></ul></li></ul>
          </div>
</li>
</ol>
<h3 id="joint-model"><a href="#joint-model" class="headerlink" title="joint model"></a>joint model</h3><ol>
<li><a href="https://pdfs.semanticscholar.org/1f9e/2d6df1eaaf04aebf428d9fa9a9ffc89e373c.pdf" target="_blank" rel="noopener">Zhang and Wang, 2016: A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding</a> <span class="label success">Bi-GRU</span> <span class="label primary">contextual word embedding</span><div class="note info">
            <ol><li><strong>Embeddings</strong>：<a href="https://research.google/pubs/pub44628.pdf" target="_blank" rel="noopener">Mesnil et al., 2015</a> 发现使用一个 context word window可以提高 RNN 在 SF 上的性能，所以该论文也这么做。获得词向量 <script type="math/tex">e(w_t)</script> 后，使用 <script type="math/tex">x^d_t = [e(w_{t-d}), \cdots, e(w_t), \cdots, e(w_{t+d})]</script> 重新表示词向量，其中 d 代表窗口半径。为了获得更好的性能，在 <script type="math/tex">x^d_t</script> 中还拼接了单词所对应的命名体词向量，比如“New York”的命名体为“B-city I-City”，即 <script type="math/tex">x^d_t = [e(w_{t-d}), \cdots, e(w_t), \cdots, e(w_{t+d}), e\prime(n_{t-c}), \cdots, e\prime(n_t), \cdots, e\prime(n_{t+c})]</script>，其中 <script type="math/tex">e\prime</script> 代表命名体的词向量，矩阵被随机初始化。<ul><li>小声逼逼，这不是 context word embedding 吗？这论文 2015 年的，比 BERT 之流早了不知道多少。</li></ul></li><li>Recurrent Hidden Layers：使用 Bi-GRU 提取句子信息</li><li>Task Specific Layers：解码层（两个）<ul><li>for SF</li><li>for ID</li></ul></li></ol>
          </div></li>
<li><a href="https://arxiv.xilesou.top/pdf/1609.01454.pdf" target="_blank" rel="noopener">Liu and Lane, 2016a: Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling</a> <span class="label success">Bi- or uni-LSTM</span> <span class="label primary">encoder-decoder</span> <span class="label info">attention</span><div class="note info">
            <ul><li>使用 seq2seq + attention 技术</li><li>对于 ID 来说，提高了 0.56%，对于 SF 来说，提高了 0.23%。emmmm，attention 对序列标注可能没多大帮助，可能 seq2seq 模型都是多余的，但是语句分类来说可能有一点点帮助。</li></ul>
          </div>
</li>
</ol>
<h2 id="SLU-DST"><a href="#SLU-DST" class="headerlink" title="SLU + DST"></a>SLU + DST</h2><ol>
<li><a href="https://arxiv.org/abs/1606.03777" target="_blank" rel="noopener">Mrkšić N et al., 2016: Neural Belief Tracker: Data-Driven Dialogue State Tracking</a> <span class="label success">NBT-DNN</span> <span class="label info">NBT-CNN</span> <span class="label warning">Semantic Decoding</span> <span class="label primary">Context Modelling</span> <span class="label primary">二元决策</span> <span class="label primary">Belief State Update Mechanism</span><blockquote class="blockquote-center"><p><a href="https://yan624.github.io/·论文笔记/dilogue/task-oriented/57. Neural Belief Tracker：Data-Driven Dialogue State Tracking.html">论文笔记</a></p>
</blockquote></li>
<li></li>
</ol>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/83825070" target="_blank" rel="noopener">认真的聊一聊对话系统（任务型、检索式、生成式对话论文与工具串讲）</a></li>
<li><a href="https://mp.weixin.qq.com/s/DTcwvTx-JDZJqqOI_FRiIQ" target="_blank" rel="noopener">Awesome Paper List of Dialogue Systems</a></li>
<li><a href="https://www.cnblogs.com/jiangxinyang/p/10789512.html" target="_blank" rel="noopener">任务型对话（一）—— NLU/SLU（意图识别和槽值填充）</a></li>
<li>见每章节提供的论文链接</li>
</ol>
]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>dialogue system</tag>
      </tags>
  </entry>
  <entry>
    <title>【算法设计题目】回溯法</title>
    <url>/algorithm/%E3%80%90%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E9%A2%98%E7%9B%AE%E3%80%91%E5%9B%9E%E6%BA%AF%E6%B3%95.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://leetcode-cn.com/problems/binary-watch/" target="_blank" rel="noopener">401. 二进制手表</a></p>
          </div>
<h1 id="回溯法"><a href="#回溯法" class="headerlink" title="回溯法"></a>回溯法</h1><h2 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h2><p>&emsp;&emsp;比较全的概念：<a href="https://leetcode-cn.com/tag/backtracking/" target="_blank" rel="noopener">leetcode 回溯法概念</a>。<br>&emsp;&emsp;大白话：世界上有许多只能执行穷举法的问题，并且事实上，这些问题解决办法中不存在除穷尽搜索之外的方法。它一般适用于求存在大量潜在解，但解可拆分为部分的问题。<br>&emsp;&emsp;<strong>回溯法基本特征</strong>：</p>
<ol>
<li>节点是用<strong>深度优先搜索</strong>的方法生成的；</li>
<li>不需要存储整棵搜索树（事实上，连树都没必要存储，只需要记录路径即可）</li>
</ol>
<h2 id="例子：三着色问题"><a href="#例子：三着色问题" class="headerlink" title="例子：三着色问题"></a>例子：三着色问题</h2><p>&emsp;&emsp;给定一个无向图 G=(V, E)，需要使用三种颜色之一为 G 的顶点着色，使得没有两个相邻顶点的颜色相同。</p>
<ol>
<li>合法：没有两个相邻顶点的颜色相同；</li>
<li>非法：相邻顶点的颜色相同；</li>
<li>搜索树：一个 n 个顶点的无向图共有 <code>3^n</code> 种可能的着色，所有可能的着色集合可以用一棵完全三叉树表示，即搜索树；</li>
<li>部分：如果某时没有两个相邻顶点的颜色相同，但图的着色还未完成，称此时的解为部分解；</li>
<li>现节点：当前待判断该着色是否合法的节点；</li>
<li>死节点：当前节点的着色已经是非法的，则称此节点为死节点。</li>
</ol>
<h1 id="401-二进制手表"><a href="#401-二进制手表" class="headerlink" title="401. 二进制手表"></a>401. 二进制手表</h1><h2 id="回溯法-1"><a href="#回溯法-1" class="headerlink" title="回溯法"></a>回溯法</h2><p>&emsp;&emsp;分析：</p>
<ol>
<li>问题可转化为： 10（4+6） 个电子灯一次实验取出 n 个，取完之后放回，问共有多少种取法？</li>
<li>显然一次实验中只能取一个电子灯，所以电子灯是不可重复选取的，但是顺序却有关系，这是由于前 4 个电子灯代表小时，后 6 个电子灯代表分钟。所以这是一个排列的问题，共有 <script type="math/tex">\frac{10!}{(10 - k)!}</script> 种。</li>
<li>此问题的<strong>约束函数</strong>是前 4 个电子灯的二进制表示只能在 0~11 之间，后 6 个电子灯的二进制表示只能在 0~59 之间。<ul>
<li>并且电子灯亮着的个数要为 n。</li>
</ul>
</li>
</ol>
<a id="more"></a>
<p>&emsp;&emsp;解决办法为：<br><figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.led_num = <span class="number">10</span></span><br><span class="line">        <span class="comment"># 前四个灯代表时，后六个灯代表分</span></span><br><span class="line">        <span class="keyword">self</span>.leds = [-<span class="number">1</span> <span class="keyword">for</span> <span class="number">_</span> <span class="keyword">in</span> range(<span class="keyword">self</span>.led_num)]</span><br><span class="line">        <span class="comment"># “排列”答案</span></span><br><span class="line">        <span class="keyword">self</span>.permutation = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">readBinaryWatch</span><span class="params">(<span class="keyword">self</span>, <span class="symbol">num:</span> int)</span></span> -&gt; List[str]<span class="symbol">:</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 算法实现</span></span><br><span class="line">        <span class="keyword">while</span> index &gt;= <span class="number">0</span><span class="symbol">:</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">self</span>.leds[index] &lt; <span class="number">1</span><span class="symbol">:</span></span><br><span class="line">                <span class="keyword">self</span>.leds[index] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">self</span>.is_legal(num)<span class="symbol">:</span></span><br><span class="line">                    h, m = <span class="keyword">self</span>.gethm()</span><br><span class="line">                    <span class="keyword">self</span>.permutation.append(<span class="string">'%i:%s'</span> % (h, <span class="string">'0%i'</span> % m <span class="keyword">if</span> m &lt; <span class="number">10</span> <span class="keyword">else</span> str(m)))</span><br><span class="line">                elif <span class="keyword">self</span>.is_partial(num) <span class="keyword">and</span> index &lt; <span class="keyword">self</span>.led_num - <span class="number">1</span><span class="symbol">:</span></span><br><span class="line">                    index += <span class="number">1</span></span><br><span class="line">            <span class="keyword">self</span>.leds[index] = -<span class="number">1</span></span><br><span class="line">            index -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">self</span>.permutation</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 合法条件：亮灯必须等于 num，并且满足约束函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_legal</span><span class="params">(<span class="keyword">self</span>, num)</span></span>: <span class="keyword">return</span> sum([<span class="number">1</span> <span class="keyword">for</span> _b <span class="keyword">in</span> <span class="keyword">self</span>.leds <span class="keyword">if</span> _b == <span class="number">1</span>]) == num <span class="keyword">and</span> <span class="keyword">self</span>.constraint_function()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 部分条件：亮灯必须小于等于 num，并且满足约束函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_partial</span><span class="params">(<span class="keyword">self</span>, num)</span></span>: <span class="keyword">return</span> sum([<span class="number">1</span> <span class="keyword">for</span> _b <span class="keyword">in</span> <span class="keyword">self</span>.leds <span class="keyword">if</span> _b == <span class="number">1</span>]) &lt;= num <span class="keyword">and</span> <span class="keyword">self</span>.constraint_function()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 约束函数为小时的范围为 0~11，分钟的范围为 0~59</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">constraint_function</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        h, m = <span class="keyword">self</span>.gethm()</span><br><span class="line">        <span class="keyword">return</span> h &lt;= <span class="number">11</span> <span class="keyword">and</span> m &lt;= <span class="number">59</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gethm</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        h = <span class="number">0</span></span><br><span class="line">        m = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> index, _b <span class="keyword">in</span> enumerate(<span class="keyword">self</span>.leds)<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">if</span> _b &gt;= <span class="number">0</span><span class="symbol">:</span></span><br><span class="line">                <span class="keyword">if</span> index &lt;= <span class="number">3</span><span class="symbol">:</span></span><br><span class="line">                    h += (<span class="number">2</span> ** index) * _b</span><br><span class="line">                <span class="symbol">else:</span></span><br><span class="line">                    m += (<span class="number">2</span> ** (index - <span class="number">4</span>)) * _b</span><br><span class="line">        <span class="keyword">return</span> h, m</span><br></pre></td></tr></table></figure></p>
<h2 id="其他办法"><a href="#其他办法" class="headerlink" title="其他办法"></a>其他办法</h2><p>&emsp;&emsp;leetcode 上有非回溯法的做法。</p>
<h1 id="784-字母大小写全排列"><a href="#784-字母大小写全排列" class="headerlink" title="784. 字母大小写全排列"></a>784. 字母大小写全排列</h1><p>&emsp;&emsp;一开始是按照回溯法的思想去思考的，但是后来我反应过来了。这题不就是穷举法吗？只不过排除了相同的组合。完全可以使用穷举做出来，只需要用集合的概念将重复的元素剔除即可。</p>
<h2 id="暴力穷举法"><a href="#暴力穷举法" class="headerlink" title="暴力穷举法"></a>暴力穷举法</h2><p>&emsp;&emsp;经过十来分钟的研究，发现穷举法不可行，必须要用递归考虑进每种情况。之前想太简单了，还是用回溯法吧。</p>
<h2 id="回溯法-2"><a href="#回溯法-2" class="headerlink" title="回溯法"></a>回溯法</h2><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>回溯法</tag>
      </tags>
  </entry>
  <entry>
    <title>更新hexo next主题</title>
    <url>/assorted/hexo/%E6%9B%B4%E6%96%B0hexo-next%E4%B8%BB%E9%A2%98.html</url>
    <content><![CDATA[<p>&emsp;&emsp;<a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">hexo-theme-next</a> 主题的 github 页面中提到了如何更新的问题。他们在教程中指出，在更新的过程中可能会遇到 <code>Commit your changes or stash them before you can merge</code> 的问题，其实这就是代表你修改过主题中的源码文件，所以不允许对 next 主题进行更新。这里的源码文件指代比较宽泛，大家最常改的应该是 <code>_config.yml</code> 文件。<br>&emsp;&emsp;以下将介绍如何处理。</p>
<ol>
<li><strong>前置操作：将自己的博客复制一份，以免修改错。</strong></li>
<li>使用 <code>git pull</code> 命令，如果出现问题，那就代表你更新成功了。如果出现了上面的 <code>Commit your changes or stash them before you can merge</code> 问题，那就往下看。</li>
<li>使用 <code>git stash</code>，重置自己的修改，再次使用 <code>git pull</code>，更新完成。</li>
<li><strong>hexo next 主题更新完成后，就代表你之前所有的自定义配置都没了。</strong>所以接下来的内容是将它们改回来。</li>
</ol>
<p>&emsp;&emsp;博客的路径如下所示：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">D:.</span><br><span class="line">├─blog</span><br><span class="line">│  ├─.deploy_git</span><br><span class="line">│  ├─node_modules</span><br><span class="line">│  ├─...</span><br><span class="line">│  ├─source</span><br><span class="line">│  ├─themes</span><br><span class="line">│  │  ├─next</span><br><span class="line">│  │  │  ├─source</span><br><span class="line">│  │  │  ├─...</span><br><span class="line">│  │  │  ├─_config.yml</span><br><span class="line">│  ├─_config.yml</span><br><span class="line">├─blog_bakup</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h1 id="config-yml-的处理"><a href="#config-yml-的处理" class="headerlink" title="_config.yml 的处理"></a>_config.yml 的处理</h1><p>&emsp;&emsp;如果自己改过 <code>_config.yml</code> 文件那就比较麻烦了，首先在  <code>blog/source</code> 中创建 <code>_data</code> 文件，再在其中创建 <code>next.yml</code> 文件。<br>&emsp;&emsp;分别打开 blog 中 next 主题的 <code>_config.yml</code> 文件和 blog_backup 中的 next 主题的 <code>_config.yml</code> 文件。<br>&emsp;&emsp;blog_backup 中的配置文件是你之前自定义的配置，对照两份文件（或者如果自己有印象，可以不用对照）。将自定义的配置移入到 <code>blog/source/next.yml</code> 文件中。注意只需要移入更改过的配置即可，比如如下配置：<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line"><span class="attr">  since:</span> <span class="number">2015</span></span><br><span class="line"><span class="attr">  icon:</span></span><br><span class="line">	<span class="attr">name:</span> <span class="string">user</span></span><br><span class="line">	<span class="attr">animated:</span> <span class="literal">false</span></span><br><span class="line">	<span class="attr">color:</span> <span class="string">"#808080"</span></span><br><span class="line"><span class="attr">creative_commons:</span></span><br><span class="line"><span class="attr">  license:</span> <span class="string">by-nc-sa</span></span><br><span class="line"><span class="attr">  sidebar:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  post:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  language:</span></span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;如果你只更改过 <code>since</code> 的配置，将其改为 2018。那么在 <code>blog/source/next.yml</code> 文件中，应该像下面这么写：<br><figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">footer:</span></span><br><span class="line"><span class="symbol">  since:</span> <span class="number">2018</span></span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;其他的配置不需要更改。<a href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/DATA-FILES.md" target="_blank" rel="noopener">此处</a>是官方文档，还有不懂的，可以去看文档。</p>
<h1 id="出现的bug"><a href="#出现的bug" class="headerlink" title="出现的bug"></a>出现的bug</h1><ol>
<li>更新至最新版（v7.7.1）之后，<code>/categories</code> 路径失效了，我只要在 <code>menu</code> 中添加一条 <code>categories</code>，整个博客就无法运行了。搞了一下午才试出来，只要不取 <code>categories</code> 这个名字就行了，我将其改为 <code>分类</code>。</li>
<li>还有一个 bug 就是在 <code>categories</code> 路径中，最新版 next 主题不再显示二级目录，这应该是代码写的有问题。找到 <code>themes/next/layout/_partials/header/sub-menu.swig</code> 文件，将第三行的 <code>【%- if theme.menu and is_page() %】</code> 改为 <code>【%- if theme.menu and not is_home() and not is_post() %】</code> 即可。<ul>
<li>由于博客本身的限制，请将“【】”替换为“{}”。</li>
</ul>
</li>
</ol>
<h1 id="自定义代码"><a href="#自定义代码" class="headerlink" title="自定义代码"></a>自定义代码</h1><h2 id="更自由地使用Font-Awesome"><a href="#更自由地使用Font-Awesome" class="headerlink" title="更自由地使用Font Awesome"></a>更自由地使用Font Awesome</h2><div class="note danger">
            <p>&emsp;&emsp;慎重修改，更改之后可能导致其他未被考虑到的地方出现无法显示 icons 的问题。</p>
          </div>
<h3 id="修改sidebar的代码"><a href="#修改sidebar的代码" class="headerlink" title="修改sidebar的代码"></a>修改sidebar的代码</h3><p>&emsp;&emsp;在 <code>themes/next/layout/_partials/sidebar/site-overview.swig</code> 中将 social icon 的代码进行替换，大约在 85 行的位置：<code>【%-set sidebarIcon = &#39;&lt;i class=&quot;fa fa-fw fa-&#39; + link.split(&#39;||&#39;)[1] | trim + &#39;&quot;&gt;&lt;/i&gt;&#39; %}</code>，<code>ctrl f</code> 搜索 <code>fa fa-fw fa-</code> 应该就能找到了。将其替换为：<br><figure class="highlight mel"><table><tr><td class="code"><pre><span class="line">【%-set sidebarIcon = <span class="string">'&lt;i class="'</span> + link.split(<span class="string">'||'</span>)[<span class="number">1</span>] | <span class="keyword">trim</span> + <span class="string">'"&gt;&lt;/i&gt;'</span> %&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="修改categories的代码"><a href="#修改categories的代码" class="headerlink" title="修改categories的代码"></a>修改categories的代码</h3><p>&emsp;&emsp;为了更自由地使用 Font Awesome，在 <code>themes/next/layout/_partials/header/menu-item.swig</code> 中，将第 11 行的<br><figure class="highlight mel"><table><tr><td class="code"><pre><span class="line">[%- <span class="keyword">if</span> theme.menu_settings.icons %]</span><br><span class="line">  [%- set menuIcon = <span class="string">'&lt;i class="fa fa-fw fa-'</span> + value.split(<span class="string">'||'</span>)[<span class="number">1</span>] | <span class="keyword">trim</span> + <span class="string">'"&gt;&lt;/i&gt;'</span> %]</span><br><span class="line">[%- endif %]</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;修改为<br><figure class="highlight mel"><table><tr><td class="code"><pre><span class="line">[%- <span class="keyword">if</span> theme.menu_settings.icons %]</span><br><span class="line">  [%- set menuIcon = <span class="string">'&lt;i class="'</span> + value.split(<span class="string">'||'</span>)[<span class="number">1</span>] | <span class="keyword">trim</span> + <span class="string">'"&gt;&lt;/i&gt;'</span> %]</span><br><span class="line">[%- endif %]</span><br></pre></td></tr></table></figure></p>
<h3 id="将fontawesome的版本更至最新"><a href="#将fontawesome的版本更至最新" class="headerlink" title="将fontawesome的版本更至最新"></a>将fontawesome的版本更至最新</h3><p>&emsp;&emsp;由于在后续又发现在其他地方也用到了icon，所以需要大量更改配置。<strong>如果不会写程序的还是别改了。</strong><br>&emsp;&emsp;next的fontawesome默认版本是4.6.2，在写本文时，fontawesome的最新版本是5.8.1。貌似fontawesome在5.0.0版本之后改版了。总之一直出现方框乱码。<br>&emsp;&emsp;后来发现，现在的fontawesome链接已经跟以前不一样了，它现在分为3大类别。<br>&emsp;&emsp;现在的使用方法是：在next主题的_config.xml中搜索fontawesome，并更改属性<code>fontawesome: //cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css</code>，注意里面的文件是all.min.css，而不是font-awesom.min.css。（v7.7.1 可以在 <code>next.yml</code> 中修改 <code>vendors:  fontawesome:</code>）<br>&emsp;&emsp;但是这样更改之后还是会出现方框乱码，原因是next默认使用的fa的类，而有时候我们需要使用fab或其他的类。所以需要修改一下源代码。<br>&emsp;&emsp;找到layout/_macro/menu/menu-item.swig，定位class=”menu-item-icon，将后面的“fa fa-fw fa-”删去。以后再修改icon不能只加一个名字了。可以像我这样修改：<code>assorted: /assorted || fa fa-fw fa-layer-group</code>。<br>&emsp;&emsp;可以看到我将icon的名称补全了。如果想用fab的类，可以像这样修改：<code>python: /python || fab fa-fw fa-python</code>。以此类推。<br>&emsp;&emsp;这样修改以后，如果不想用fontawesome了，想用其他的icon库，改起来也很方便。<br>&emsp;&emsp;layout/_macro/menu/menu-item.swig被layout/_partials/header/sub-menu.swig引用。<br>&emsp;&emsp;另外由于fontawesome版本改动，社交软件的icon也需要更改，在_config.xml中搜索github，将icon改为<code>fab fa-fw fa-github</code>。找到ayout/_macro/siderbar.swig，搜索fa fa-fw fa-，看看定位的地点上面是不是<code>{百分号  if theme.social_icons.enable 百分号}</code>。是的话将fa fa-fw fa-删除。</p>
<h2 id="添加弹窗提示功能"><a href="#添加弹窗提示功能" class="headerlink" title="添加弹窗提示功能"></a>添加弹窗提示功能</h2><p>&emsp;&emsp;在 <code>themes/next/layout/_macro/post.swig</code> 中找到代码：<code>&lt;article itemscope itemtype=&quot;http://schema.org/Article&quot; class=&quot;post-block [% if is_index %]home[% endif %]&quot; lang=&quot;[[ post.lang or post.language or config.language ]]&quot;&gt;</code>，其实就在第 2 行。<br>&emsp;&emsp;在此行下面插入以下代码：<br><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">[% <span class="keyword">for</span> tag <span class="keyword">in</span> post.tags.toArray() %]</span><br><span class="line">	[% <span class="keyword">if</span> tag.name == <span class="string">'学习笔记'</span> <span class="keyword">and</span> <span class="keyword">not</span> is_home() %]</span><br><span class="line">	&lt;link rel="stylesheet" <span class="keyword">type</span>="text/css" href="/lib/spop/spop.min.css"&gt;</span><br><span class="line">	《script <span class="keyword">type</span>="text/javascript" src="/lib/spop/spop.min.js"&gt;&lt;/script&gt;</span><br><span class="line">	&lt;!<span class="comment">--判断该文章是否为学习笔记--&gt;</span></span><br><span class="line">	《script&gt;</span><br><span class="line">	  spop(&#123;</span><br><span class="line">		<span class="keyword">template</span>: <span class="string">'&lt;h4 class="spop-title"&gt;注意&lt;/h4&gt;此文章仅为博主的学习笔记，并非教学，其中可能含有理论错误。'</span>,</span><br><span class="line">		<span class="keyword">group</span>: <span class="string">'tips'</span>,</span><br><span class="line">		position  : <span class="string">'bottom-center'</span>,</span><br><span class="line">		style: <span class="string">'success'</span>,</span><br><span class="line">		autoclose: <span class="number">5500</span>,</span><br><span class="line">		onOpen: <span class="keyword">function</span> () &#123;</span><br><span class="line">		  //这里设置灰色背景色</span><br><span class="line">		&#125;,</span><br><span class="line">		onClose: <span class="keyword">function</span>() &#123;</span><br><span class="line">		  //这里可以取消背景色</span><br><span class="line">		  spop(&#123;</span><br><span class="line">			<span class="keyword">template</span>: <span class="string">'ε = = (づ′▽`)づ'</span>,</span><br><span class="line">			<span class="keyword">group</span>: <span class="string">'tips'</span>,</span><br><span class="line">			position  : <span class="string">'bottom-center'</span>,</span><br><span class="line">			style: <span class="string">'success'</span>,</span><br><span class="line">			autoclose: <span class="number">1500</span></span><br><span class="line">		  &#125;);</span><br><span class="line">		&#125;</span><br><span class="line">	  &#125;);</span><br><span class="line">	&lt;/script&gt;</span><br><span class="line">	[% endif %]</span><br><span class="line">[% endfor %]</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;其中相应的 css/js 文件需要自己去下载，并放入到 <code>themes/next/source/lib/</code> 文件夹下。</p>
<h2 id="为学习笔记添加不同的颜色"><a href="#为学习笔记添加不同的颜色" class="headerlink" title="为学习笔记添加不同的颜色"></a>为学习笔记添加不同的颜色</h2><p>&emsp;&emsp;如果需要为不同类型的博客在博客时间线中显示不同的颜色，可以做如下更改，在 <code>themes/next/layout/_macro/post-collapse.swig</code> 中，找到 <code>&lt;a class=&quot;post-title-link&quot; href=&quot;[[ url_for(post.path) ]]&quot; itemprop=&quot;url&quot;&gt;</code>，在 <code>&lt;a&gt;&lt;/a&gt;</code> 标签下面插入以下代码：<br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 如果博文是学习笔记的话，后面加个小图标 --&gt;</span></span><br><span class="line">[% for tag in post.tags.toArray() %]</span><br><span class="line">	[% if tag.name == '学习笔记'%]</span><br><span class="line">		<span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"notes-tag fas fa-book-reader"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">		《script&gt;</span><br><span class="line">			window.onload = function()&#123;</span><br><span class="line">				var notes = $('.notes-tag');</span><br><span class="line">				//碧螺春绿</span><br><span class="line">				notes.prev().children('span').css('color', '#867018');</span><br><span class="line">			&#125;</span><br><span class="line">		<span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">	[% endif %]</span><br><span class="line">[% endfor %]</span><br></pre></td></tr></table></figure></p>
<h2 id="开启不蒜子"><a href="#开启不蒜子" class="headerlink" title="开启不蒜子"></a>开启不蒜子</h2><p>&emsp;&emsp;开启不蒜子。在 <code>source/_data/next.yml</code> 中设置如下属性，就算开启了：<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">busuanzi_count:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;开启之后，它默认只会显示三个统计数据，即总浏览量、文章浏览量和访客数量。它们的代码在 <code>themes/next/layout/_layout.swig</code> 的 footer 中，文章浏览量的代码在 <code>_macro/post.swig</code> 中。<br>&emsp;&emsp;如果要显示更多，可以在 <code>themes/next/layout/_partials/sidebar/site-overview.swig</code> 中修改代码（这个 <code>site-overview.swig</code> 是由 <code>themes/next/layout/_macro/sidebar.swig</code> 加载）。<br><figure class="highlight cs"><table><tr><td class="code"><pre><span class="line">[<span class="meta">%- if theme.site_state %</span>]</span><br><span class="line">&lt;div <span class="keyword">class</span>=<span class="string">"site-state-wrap motion-element"</span>&gt;</span><br><span class="line">  &lt;nav <span class="keyword">class</span>=<span class="string">"site-state"</span>&gt;</span><br><span class="line">    ...</span><br><span class="line">	&lt;<span class="number">1</span>&gt;</span><br><span class="line">  &lt;/nav&gt;</span><br><span class="line">&lt;div&gt;</span><br><span class="line">[<span class="meta">%- endif %</span>]</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;在文件中找到上面的代码，<code>&lt;nav&gt;&lt;/nav&gt;</code> 标签中有很多代码，将下面的代码添加到 <code>&lt;nav&gt;&lt;/nav&gt;</code> 标签内部的最后一行，即上面代码的 <code>&lt;1&gt;</code> 位置。<br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">[%- if theme.busuanzi_count.enable %]</span><br><span class="line">  <span class="comment">&lt;!-- 不蒜子/busuanzi --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"site-state-item site-state-posts"</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"site-state-item-count"</span>&gt;</span>[[totalcount(site)]]<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"site-state-item-name"</span>&gt;</span>总字数<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">[%- endif %]</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习500问笔记</title>
    <url>/%C2%B7zcy/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0500%E9%97%AE%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<h1 id="第三章-深度学习基础"><a href="#第三章-深度学习基础" class="headerlink" title="第三章 深度学习基础"></a>第三章 深度学习基础</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ol>
<li><strong>神经网络组成？</strong></li>
<li><strong>神经网络有哪些常用模型结构？</strong><ul>
<li><a href="https://www.asimovinstitute.org/neural-network-zoo/" target="_blank" rel="noopener">THE NEURAL NETWORK ZOO</a></li>
<li><a href="https://www.baidu.com/s?ie=UTF-8&amp;wd=a%20mostly%20complete%20chart%20of%20Neural%20Network&amp;" target="_blank" rel="noopener">a mostly complete chart of Neural Network</a></li>
</ul>
</li>
<li><strong>如何选择深度学习开发平台？</strong></li>
<li><strong>为什么使用深层表示？</strong> <div class="note info">
            <p>&emsp;&emsp;一些以前看过的资料：</p><ol><li><a href="https://yan624.github.io/·zcy/AI/dl/对神经网络整体的理解.html#为什么使用深度表示——Why-deep-representations">吴恩达 2017course 深度学习笔记：对神经网络整体的理解#为什么使用深度表示——Why-deep-representations</a></li><li><a href="#为什么要deep">#为什么要deep</a></li></ol>
          </div>
<ol>
<li>深度神经网络是一种<strong>特征递进式</strong>的学习算法，<strong>浅层的神经元</strong>直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而<strong>深层的特征</strong>则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。<div class="note default">
            <p>&emsp;&emsp;如这段所说，浅层的神经元可以从数据中提取出一些低层次的特征。对于 CV 来说，首先提取图片的一部分，然后可能还可以继续通过这一部分图片进一步地提取信息。<br>&emsp;&emsp;但是对于 NLP 来说，第一层提取出的特征是低级的，那么第二层该如何从低级的特征中提取出高级的特征？从低级中提取出高级的东西，这逻辑说不通吧。所以我觉得可能可以加一层残差层（residual layer），将原始文本的特征也并入到低级特征中（可以用一个系数来权衡量级）。-&gt;Transformer</p>
          </div></li>
<li>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。<a id="more"></a></li>
</ol>
</li>
<li><strong>为什么深层神经网络难以训练？</strong> <div class="note info">
            <ol><li><a href="https://yan624.github.io/project/多领域seq2lf.html#clip-gradient">多领域seq2lf#clip-gradient</a><br>虽然讲得是 clip gradient，但是结合了以前的笔记同时讲了<strong>梯度爆炸</strong>和<strong>梯度消失</strong>。</li></ol>
          </div>
<ol>
<li>梯度消失<br>梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。<br>梯度消失的原因受到多种因素影响，例如<strong>学习率的大小</strong>，<strong>网络参数的初始化</strong>，<strong>激活函数的边缘效应</strong>等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率：</li>
<li>梯度爆炸</li>
<li>权重矩阵的退化导致模型的有效自由度减少</li>
</ol>
</li>
<li><strong>深度学习和机器学习有什么不同？</strong></li>
</ol>
<h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><ol>
<li><strong>什么是超参数？</strong></li>
<li><strong>如何寻找超参数的最优值？</strong><br> &emsp;&emsp;在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：<ol>
<li>猜测和检查：根据经验或直觉，选择参数，一直迭代。</li>
<li>网格搜索：让计算机尝试在一定范围内均匀分布的一组值。</li>
<li>随机搜索：让计算机随机挑选一组值。</li>
<li>贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</li>
<li>MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</li>
<li>最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。</li>
</ol>
</li>
<li><strong>超参数搜索一般过程？</strong></li>
</ol>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ol>
<li><p><strong>如何选择激活函数？</strong></p>
 <div class="note info">
            <ol><li><a href="http://localhost:4000/·zcy/AI/dl/对神经网络整体的理解.html#※-激活函数" target="_blank" rel="noopener">对神经网络整体的理解#※-激活函数</a></li></ol>
          </div>
</li>
<li><p><strong>为什么Tanh收敛速度比Sigmoid快？</strong><br>&emsp;&emsp;首先看如下两个函数的求导：</p>
<script type="math/tex; mode=display">
\begin{align}
 tanh^{\prime}(x) = & 1-tanh(x)^{2}\in (0,1) \\
 s^{\prime}(x) = & s(x)*(1-s(x))\in (0,\frac{1}{4}]
\end{align}</script><p>&emsp;&emsp;由上面两个公式可知tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。</p>
<div class="note default">
            <p>&emsp;&emsp;tanh 梯度消失的问题要轻，自然而然参数更新的速度就要快，所以收敛的速度就要快。</p>
          </div>
</li>
</ol>
<h2 id="batch-size"><a href="#batch-size" class="headerlink" title="batch size"></a>batch size</h2><h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><ol>
<li><strong>归一化含义？</strong></li>
<li>为什么要归一化？</li>
<li>为什么归一化能提高求解最优解速度？</li>
<li>3D图解未归一化</li>
<li><strong>归一化有哪些类型？</strong><ul>
<li>线性归一化</li>
<li>标准差标准化</li>
<li>非线性归一化</li>
</ul>
</li>
<li>局部响应归一化作用</li>
<li>理解局部响应归一化</li>
<li><strong>什么是批归一化（Batch Normalization）</strong></li>
<li><strong>批归一化（BN）算法的优点</strong></li>
<li><strong>批归一化（BN）算法流程</strong></li>
<li>批归一化和群组归一化比较</li>
<li><strong>Weight Normalization和Batch Normalization比较</strong></li>
<li><strong>Batch Normalization在什么时候用比较合适？</strong></li>
</ol>
<h2 id="预训练与微调-fine-tuning"><a href="#预训练与微调-fine-tuning" class="headerlink" title="预训练与微调(fine tuning)"></a>预训练与微调(fine tuning)</h2><ol>
<li><strong>为什么无监督预训练可以帮助深度学习？</strong><ul>
<li>深度网络存在问题:<ol>
<li>网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。</li>
<li>多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；</li>
<li>梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。</li>
</ol>
</li>
<li>解决方法：逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。经过预训练最终能得到比较好的局部最优解。</li>
</ul>
</li>
<li><strong>什么是模型微调fine tuning</strong></li>
<li><strong>微调时候网络参数是否更新？</strong></li>
<li><strong>fine-tuning 模型的三种状态</strong><ol>
<li>状态一：只预测，不训练。<br>特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</li>
<li>状态二：训练，但只训练最后分类层。<br>特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</li>
<li>状态三：完全训练，分类层+之前卷积层都训练<br>特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。</li>
</ol>
</li>
</ol>
<h2 id="权重偏差初始化"><a href="#权重偏差初始化" class="headerlink" title="权重偏差初始化"></a>权重偏差初始化</h2><ol>
<li>全都初始化为 0</li>
<li>全都初始化为同样的值</li>
<li>初始化为小的随机数</li>
<li>用 <script type="math/tex">\frac{1}{\sqrt n}</script> 校准方差</li>
<li>稀疏初始化(Sparse Initialazation)</li>
<li>初始化偏差</li>
</ol>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><ol>
<li>学习率的作用</li>
<li><strong>学习率衰减常用参数有哪些</strong></li>
<li>分段常数衰减</li>
<li>指数衰减</li>
<li>自然指数衰减</li>
<li>多项式衰减</li>
<li>余弦衰减</li>
</ol>
<h2 id="Dropout-系列问题"><a href="#Dropout-系列问题" class="headerlink" title="Dropout 系列问题"></a>Dropout 系列问题</h2><ol>
<li><strong>为什么要正则化？</strong><br>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。  </li>
<li><strong>为什么正则化有利于预防过拟合？</strong></li>
<li><strong>理解dropout正则化</strong><br>Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？<br>​    - 直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。<ul>
<li>起平均作用：在不适用 dropout 的条件下，使用 5 个不同的神经网络去训练相同的数据集，那么我们可以使用少数服从多数的方法。比如 3 个神经网络的结果是 b，2 个神经网络的结果是 a，那么结果就是 b。同理，使用 dropout 就相当于训练了不同的神经网络。</li>
</ul>
</li>
<li><strong>dropout率的选择</strong></li>
<li><strong>dropout有什么缺点？</strong><br>dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。所以我们失去了调试工具来绘制这样的图片。</li>
</ol>
<p>&emsp;&emsp;参考文献：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/38200980" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></li>
</ol>
<h1 id="第六章-循环神经网络（RNN）"><a href="#第六章-循环神经网络（RNN）" class="headerlink" title="第六章 循环神经网络（RNN）"></a>第六章 循环神经网络（RNN）</h1><h2 id="为什么需要RNN？"><a href="#为什么需要RNN？" class="headerlink" title="为什么需要RNN？"></a>为什么需要RNN？</h2><h2 id="图解RNN基本结构"><a href="#图解RNN基本结构" class="headerlink" title="图解RNN基本结构"></a>图解RNN基本结构</h2><ol>
<li>vector-to-sequence：例如，输入图像输出一段话</li>
<li>sequence-to-vector：例如，输入一段话判断类别</li>
<li>Encoder-Decoder(seq2seq)：例如机器翻译等</li>
</ol>
<h2 id="RNNs典型特点？"><a href="#RNNs典型特点？" class="headerlink" title="RNNs典型特点？"></a>RNNs典型特点？</h2><ol>
<li>处理序列数据</li>
<li>当前步的输出与之前步的输出有关，因此被称为循环神经网络</li>
<li>隐藏层权重共享</li>
<li><strong>理论上，RNNs能够对任何长度序列数据进行处理。但是在实践中，为了降低复杂度往往假设当前的状态只与之前某几个时刻状态相关。</strong>（这句话是什么意思，基本的 RNN 不都是可以处理任意长的语句？只不过会出现梯度爆炸和梯度消失的问题。）</li>
</ol>
<h2 id="CNN和RNN的区别？"><a href="#CNN和RNN的区别？" class="headerlink" title="CNN和RNN的区别？"></a>CNN和RNN的区别？</h2><div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>特点描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>相同点</td>
<td>1、传统神经网络的扩展。<br>2、前向计算产生结果，反向计算模型更新。<br>3、每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。</td>
</tr>
<tr>
<td>不同点</td>
<td>1、CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算<br>2、RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出</td>
</tr>
</tbody>
</table>
</div>
<h2 id="RNNs和FNNs有什么区别？"><a href="#RNNs和FNNs有什么区别？" class="headerlink" title="RNNs和FNNs有什么区别？"></a>RNNs和FNNs有什么区别？</h2><h2 id="RNNs训练和传统ANN训练异同点？"><a href="#RNNs训练和传统ANN训练异同点？" class="headerlink" title="RNNs训练和传统ANN训练异同点？"></a>RNNs训练和传统ANN训练异同点？</h2><h2 id="为什么RNN训练的时候Loss波动很大"><a href="#为什么RNN训练的时候Loss波动很大" class="headerlink" title="为什么RNN训练的时候Loss波动很大"></a>为什么RNN训练的时候Loss波动很大</h2><p>&emsp;&emsp;由于RNN特有的memory会影响后面 time step 的计算，所以梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏，为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。</p>
<h2 id="标准RNN前向输出流程"><a href="#标准RNN前向输出流程" class="headerlink" title="标准RNN前向输出流程"></a>标准RNN前向输出流程</h2><h2 id="BPTT算法推导"><a href="#BPTT算法推导" class="headerlink" title="BPTT算法推导"></a>BPTT算法推导</h2><h2 id="RNN中为什么会出现梯度消失？"><a href="#RNN中为什么会出现梯度消失？" class="headerlink" title="RNN中为什么会出现梯度消失？"></a>RNN中为什么会出现梯度消失？</h2><div class="note default">
            <ol><li><a href="https://yan624.github.io/project/多领域seq2lf.html#clip-gradient">多领域seq2lf#clip-gradient</a><br> 虽然讲得是 clip gradient，但是结合了以前的笔记同时讲了<strong>梯度爆炸</strong>和<strong>梯度消失</strong>。</li></ol>
          </div>
<h2 id="如何解决RNN中的梯度消失问题？"><a href="#如何解决RNN中的梯度消失问题？" class="headerlink" title="如何解决RNN中的梯度消失问题？"></a>如何解决RNN中的梯度消失问题？</h2><p>&emsp;&emsp;解决办法是将 sigmoid 函数替换为 tanh。注意，在 CNN 中的首选激活函数是 ReLu，但是对于 RNN 来说，首选是 tanh，因为 ReLu 的右半轴是 <code>y=x</code>，会导致 RNN 的 timestep 越往后输出值越大。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><div class="note default">
            <ol><li><a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#长短期记忆——Long-Short-term-Memory-LSTM">吴恩达李宏毅综合学习笔记：RNN入门#长短期记忆——Long-Short-term-Memory-LSTM</a></li><li><a href="https://yan624.github.io/·zcy/AI/dl/深度学习算法（三）：RNN 各种机制.html#LSTM">深度学习算法（三）：RNN 各种机制#LSTM</a></li></ol>
          </div>
<h2 id="LSTMs与GRUs的区别"><a href="#LSTMs与GRUs的区别" class="headerlink" title="LSTMs与GRUs的区别"></a>LSTMs与GRUs的区别</h2><h2 id="RNNs在NLP中典型应用？"><a href="#RNNs在NLP中典型应用？" class="headerlink" title="RNNs在NLP中典型应用？"></a>RNNs在NLP中典型应用？</h2><h2 id="常见的RNNs扩展和改进模型"><a href="#常见的RNNs扩展和改进模型" class="headerlink" title="常见的RNNs扩展和改进模型"></a>常见的RNNs扩展和改进模型</h2><ol>
<li>Simple RNNs(SRNs)</li>
<li>Bidirectional RNNs</li>
<li>Deep RNNs</li>
<li>Echo State Networks（ESNs）</li>
<li>Gated Recurrent Unit Recurrent Neural Networks</li>
<li>Bidirectional LSTMs</li>
<li>Stacked LSTMs</li>
<li>Clockwork RNNs(CW-RNNs)</li>
<li>CNN-LSTMs</li>
</ol>
<h1 id="第十三章-优化算法"><a href="#第十三章-优化算法" class="headerlink" title="第十三章 优化算法"></a>第十三章 优化算法</h1><h1 id="第十四章-超参数调整"><a href="#第十四章-超参数调整" class="headerlink" title="第十四章 超参数调整"></a>第十四章 超参数调整</h1><h1 id="第十六章-自然语言处理（NLP）"><a href="#第十六章-自然语言处理（NLP）" class="headerlink" title="第十六章 自然语言处理（NLP）"></a>第十六章 自然语言处理（NLP）</h1><h1 id="第十七章-模型压缩及移动端部署"><a href="#第十七章-模型压缩及移动端部署" class="headerlink" title="第十七章 模型压缩及移动端部署"></a>第十七章 模型压缩及移动端部署</h1><h1 id="第十八章-后端架构选型、离线及实时计算"><a href="#第十八章-后端架构选型、离线及实时计算" class="headerlink" title="第十八章 后端架构选型、离线及实时计算"></a>第十八章 后端架构选型、离线及实时计算</h1><h1 id="自己的疑问"><a href="#自己的疑问" class="headerlink" title="自己的疑问"></a>自己的疑问</h1><h2 id="为什么Mini-batch比普通的梯度下降快？"><a href="#为什么Mini-batch比普通的梯度下降快？" class="headerlink" title="为什么Mini-batch比普通的梯度下降快？"></a>为什么Mini-batch比普通的梯度下降快？</h2><h2 id="指数加权平均的作用"><a href="#指数加权平均的作用" class="headerlink" title="指数加权平均的作用"></a>指数加权平均的作用</h2><h2 id="为什么要deep"><a href="#为什么要deep" class="headerlink" title="为什么要deep"></a>为什么要deep</h2><div class="note primary">
            <p>学了有一段时间的深度学习，但是有个问题一直没想明白。那就是将hidden layer叠多层的意义是什么？</p>
          </div>
<p>可以观察下图发现，确实越deep代价越小。左边的两列显示了，层数越多代价越小。右边两列显示即使整个神经网络参数类似，但是明显越deep代价越小。同一行代表hidden layer的参数接近。那个size指的是神经网络中的参数。再看最后一行，它显示即使参数暴增到16k，代价也不是很低。图中红框还显示了2层2k的model比1层16k的model好多了。<br>下图是由底下的论文的作者做的实验得出的结论。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/隐藏层层数对cost的影响.jpg" alt="隐藏层层数对cost的影响"><br><strong>那么为什么神经网络越深效果越好呢？</strong><br>这其实归功于modularization——模块化。如下图所示，如果直接写一个model用于将4种人分类，那么可能会出现某类人的数据并不多的情况，比如长发的男生的数据可能并不多。那么分类的准确率可能会不是很高。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/解释why deep的例子.jpg" alt="解释why deep的例子"><br>下图中先将其分类为男女以及长短发，然后再进一步分类。虽然说长发男生的数据比较少，但是男女和长短发的数据有很多，我们可以得到一个很好的模型。之后我们再叠一层用于进一步分类，此时，由于我们已经做了上一步的分类，所以新的一层可以使用上一层的特征。<br>上一层的分类已经把难的事情——辨别男女，辨别长短发等解决了，所以后一层只要使用少量的数据就能进行分类。<br><strong>没有使用模块化</strong>的那个模型，它是用少量的数据硬生生地去识别长发男生。<strong>使用模组化</strong>的模型是先识别男女以及长短发，再通过调用前一步的特征判断。下图4个分类器区别可能只是辨别的方式不同而已，比如权重不同。输入一张图片之后第一层已经可以辨别是男还是女，长发还是短发，然后后一层经过简单的运算就可以确定了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/模块化后.jpg" alt="模块化后"><br>经过上面的解释，可能已经大致理解是什么意思了。但是真要讲清还有点问题，尤其是模块化怎么做。<br>但是李宏毅老师说模块化其实是神经网络从数据中<strong>自动</strong>学到的。</p>
<h3 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h3><p>用数电的逻辑门来举例，但是我没怎么学过数电，所以没有理解。<br>另一个比较贴近生活的例子，就是剪窗花。没有人会一瓣花一瓣花的去剪窗花，都是将纸先折好，然后一步剪完。这就是模块化了。</p>
<p>还有其他领域的人也有过解读，个人理解why deep这个问题可能到现在没有一个官方的回应，可能前辈也是误打误撞才发现deep learning很牛。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/其他领域对为什么要deep的解读.jpg" alt="其他领域对为什么要deep的解读"></p>
<h3 id="吴恩达老师的解释"><a href="#吴恩达老师的解释" class="headerlink" title="吴恩达老师的解释"></a>吴恩达老师的解释</h3><p><a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>也做了解释。</p>
<h2 id="词向量乘上权重以及做梯度下降有什么意义"><a href="#词向量乘上权重以及做梯度下降有什么意义" class="headerlink" title="词向量乘上权重以及做梯度下降有什么意义"></a>词向量乘上权重以及做梯度下降有什么意义</h2><p><a href="https://mooc.study.163.com/learn/2001280005?tid=2001391038&amp;_trace_c_p_k2_=023fecd41c524f0d9485b18d2d773f53#/learn/content?type=detail&amp;id=2001770038" target="_blank" rel="noopener">本文灵感</a><br>本文疑问：</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<h3 id="准备词向量"><a href="#准备词向量" class="headerlink" title="准备词向量"></a>准备词向量</h3><p>假设有这么一句话：I want a glass of orange ___.<br>要做的是估计划线处应该填入什么词。答案是juice。<br>首先我们需要一个词典——vocabulary，每个单词对应一个索引，这是通用步骤。词表大小为10000。<br>然后将上述的句子，从单词转成索引形式。即：<br>I want a glass of orange —-&gt; 4343 9665 1 3852 6163 6257<br>此外每一个单词都会对应一个词向量，而词表中所有单词的词向量就组合一个词嵌入矩阵。词表以及词向量都是可以找一些预训练的，比如<strong>GloVe</strong>。<br>梳理一遍就是：<br>单词:索引<br>索引:词向量<br>所以可以通过单词间接地获取到词向量。关于索引对应词向量，实际上是里面没有索引的因为一个矩阵它本身就有一个属性表示索引，如第0行就是代表第0个单词，第1行就是代表第一个单词。<br>总而言之，我们通过单词获取索引后，就能通过该索引直接获取词向量。伪代码可以表示为：<br><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">index = vocabulary.get_index('want') <span class="comment"># 索引为9665</span></span><br><span class="line">word_vector = embedding_matrix[index, :] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p><strong>对于词嵌入矩阵的行代表词向量，还是列代表词向量不必纠结。</strong>你要乐意可以改成<br><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">word_vector = embedding_matrix[:, index] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p>如果使用one hot编码来执行上述代码就是将9665转为one hot编码，即除了9665位置为1，其余位置全为0。然后<script type="math/tex">word\_vector = embedding\_matrix^T * word\_one\_hot</script>。这样也能得到词向量，但是由于one hot编码全是0，算起来速度太慢了。<br>现在有了句子“I want a glass of orange”的所有词向量，接下来要做的是将这些词向量从头到尾拼在一起，接成一个更长的向量，也就是6倍长的向量。原词向量是300维，拼接完成后是1800维。然后将这个向量输入一个神经网络中，最后经过softmax函数进行预测，预测范围是在10000个单词中，看谁的概率大。</p>
<h3 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h3><p>将词向量送入神经网络中当然还需要梯度下降进行迭代。这里会有很多疑问，</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<p>我进行逐一思考，本文仅为自己的理解。<br>首先其实有一件事很多视频没讲，可能他们认为这是一件很平常的事，所以没讲。<br>上述的这个步骤并不是预测步骤，而是在进行迭代，所以是一个训练步骤。人家之所以说<em>我们可以通过这个神经网络预测出单词为juice</em>，是因为逻辑上是这样的。<br>由于是训练步骤，所以我们有一个很重要的数据，最终结果。最终结果我们是知道的，然而我们初学者在考虑整个流程时，没把最终结果算进去，因为老师说<em>我们可以通过这个神经网络预测出单词为juice</em>，由于是<em>预测</em>，那么结果肯定没有啊。这很合乎逻辑。所以就陷入了一个思维的怪圈，<strong>正确的逻辑是：</strong></p>
<ol>
<li>首先我们知道最终结果，所以当第一次迭代时，所有的权重都是随机初始化的，1、2两个问题也就没有意义了。第一次迭代完毕后，结果肯定稀巴烂，所以进行梯度下降。</li>
<li>这里面我们又会碰到一个问题，就是梯度下降到底在算什么？其实这里的疑问来自我们的潜意识始终将句子当做文字在看，自然而然就意识不到梯度下降在干什么。而其实我们在几步之前就已经将文字转为词向量了。<br>词向量说白了就是一堆浮点型数字，而最终结果juice也是一个词向量，所以实际上就是将一个权重矩阵乘上一个1800维的向量，得到一个输出值（may be 激活值），然后将这个输出值和juice的向量放入代价函数中进行计算，接下来的梯度下降其实就是跟正常的步骤一样。</li>
<li><strong>梯度下降就是在寻找一个合适的权重矩阵使得权重矩阵乘1800维向量得到的值接近juice的向量。</strong><br>这里在解释第3个问题时，顺便也解释了第1、2个问题。<strong>权重值实际上就是用来使得预测值和实际结果越接近越好</strong></li>
<li>由于开头就说了我们实际上是知道划线处应该填juice，所以第4个问题压根不需要解答，因为我们本来就知道应该填juice，也没必要预测。之前之所以有这个问题，是因为我们潜意识觉得老师说<em>我们可以通过这个神经网络预测出单词为juice</em>。</li>
<li>至于第5个问题如何预测其他句子。打个比方，现在预测一个新的句子：I want a glass of apple ___.<br>由于我们知道词嵌入矩阵是由很多单词的词向量组成的。而一个单词词向量其实就是一堆特征组成的，对比两个句子，apple和orange的词向量肯定很接近，因为它们都是水果，它们的特征（水多不多，好不好吃，是不是水果，有没有性别特征，是不是动词等）都类似。<br>而我们之前已经训练了一个神经网络，我们得到了所需要的权重值，我们直接把这个权重值乘上新句子的词向量，那么结果肯定和权重值乘上之前句子的词向量的结果很接近。因为它们之间的区别仅仅是apple和orange的词向量不同，并且apple和orange的词向量其实也并不是完全不同，只是略有不同。所以二者的结果自然也差不多。<br>最后经过softmax函数在10000个单词之中预测，输出的结果必然都是juice。</li>
</ol>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>这里还会有一个问题，那么如果需要预测的句子单词数不一致怎么办？<br>之前两个例子的句子都是6个单词，预测第7个单词。那要是前面有10个单词，我要预测第11个单词怎么办？<br>其实可以只考虑划线处前几个单词，比如只考虑划线处前4个单词，这样输入的维度就相同了。<br>在本文开头的参考视频里，吴恩达老师讲得很清楚了。由古圣先贤总结，一般来说取前4个单词即可，当然你自己也可以用其他办法。</p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Fully Statistical Neural Belief Tracking</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/task-oriented/60.%20Fully%20Statistical%20Neural%20Belief%20Tracking.html</url>
    <content><![CDATA[<h1 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h1><p>&emsp;&emsp;本文提出 NBT 的改进。现有的 NBT 使用人工制作的 belief state update mechanism，每当模型被部署到新的对话领域，就要涉及到代价颇高的手动重新调整。我们证明，这种更新机制其实可以与 NBT 模型的语义解码和上下文建模部分进行联合学习，从而从 DST 框架中消除最后一个基于规则的模块。我们提出了两种不同的统计更新机制，并证明对话动态可以用非常少量的附加模型参数来建模。<br><a id="more"></a></p>
<h1 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h1>]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>NBT</tag>
      </tags>
  </entry>
  <entry>
    <title>A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/task-oriented/59.%20A%20Bi-model%20based%20RNN%20Semantic%20Frame%20Parsing%20Model%20for%20Intent%20Detection%20and%20Slot%20Filling.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.xilesou.top/pdf/1812.10235.pdf" target="_blank" rel="noopener">论文地址</a>，作者是 Yu Wang et.al.，发表于 2018 年 12 月。</p>
          </div>
<h1 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h1><p>&emsp;&emsp;<strong>意图检测和 slot filling 是构建 SLU 的两个主要任务</strong>。现在，多个基于深度学习的模型在这些任务上都取得了良好的效果。最有效的算法是基于 seq2seq（or encoder-decoder）的结构，并使用单独或联合的模型生成<strong>意图</strong>和<strong>语义标记</strong>（<strong>semantic tags</strong>）。然而，以往的研究大多将意图检测和 slot filling 作为两个独立的平行的任务来处理，或者使用 seq2seq 模型来生成语义标记和意图。这些方法大多采用一个（联合的（joint））神经网络模型（包括 encoder-decoder 结构）对两个任务进行建模，因此可能无法充分利用它们之间的交叉影响。本文设计了一种新的基于 <strong>Bi-model</strong> 的 <strong>RNN 语义框架解析神经网络结构</strong>（<strong>RNN semantic frame parsing network structures</strong>），利用两个有关联的双向 LSTMs（<strong>BLSTM</strong>）来将它们之间的相互影响考虑进去，共同完成意图检测和 slot filling 任务。我们的带 decoder 的 Bi-model 结构在 ATIS 上取得了一流的结果，意图准确率提高了0.5%，slot filling 准确率提高了0.9%。<br><a id="more"></a></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>&emsp;&emsp;过去几十年内，SLU 上的研究进展迅速。</p>
<ol>
<li>意图识别做法：regression, SVM, deep NN</li>
<li>slot filling 做法：CRF, RNN</li>
</ol>
<p>&emsp;&emsp;余下章节如下所述：</p>
<ol>
<li>第二节：已知的深度学习方法简要概述（意图检测和 slot filling）</li>
<li>第三节：介绍 Bi-model</li>
<li>第四节：展示不同数据集上的实验</li>
</ol>
<h1 id="Bi-model-RNN-structures-for-joint-semantic-frame-parsing"><a href="#Bi-model-RNN-structures-for-joint-semantic-frame-parsing" class="headerlink" title="Bi-model RNN structures for joint semantic frame parsing"></a>Bi-model RNN structures for joint semantic frame parsing</h1><p>&emsp;&emsp;尽管在这两个任务上使用 seq2seq 获得了成功，但是大多数方法仍旧在每个任务上使用单一的 RNN 模型。他们将意图检测和 slot filling 看作是两个独立的任务。本节，使用 B-model 考虑它俩的关联，从而进一步提高性能。</p>
<h2 id="Bi-model-RNN-Structures"><a href="#Bi-model-RNN-Structures" class="headerlink" title="Bi-model RNN Structures"></a>Bi-model RNN Structures</h2><p>&emsp;&emsp;两个模型（a)Bi-model structure with a decoder; b)Bi-model structure without a decoder）的图例如图 1 所示。两个结构十分相似，图 1a 包含了一个基于 LSTM 的 decoder，因此除了有一个 encoder state <script type="math/tex">h_t</script> 之外，还有一个额外的 decoder state <script type="math/tex">s_t</script>。<br>&emsp;&emsp;<strong>注意：从多个模型/多模态中使用某些信息以实现更好性能的概念已经被广泛应用于深度学习、系统识别以及最近也有被用在强化学习领域之中。除开使用共用的信息，本论文，我们将引入一个全新的方法（通过分享它们之间的内部 state 信息）以异步地训练多个神经网络。</strong></p>
<h3 id="Bi-model-structure-with-a-decoder"><a href="#Bi-model-structure-with-a-decoder" class="headerlink" title="Bi-model structure with a decoder"></a>Bi-model structure with a decoder</h3><p>&emsp;&emsp;Bi-model structure with a decoder 如图 1a 所示，架构中有两个互联的 Bi-LSTMs(BLSTMs)，一个用于意图检测，一个用于 slot filling。每个 BLSTM 分别正向和反向地在输入语句序列（<script type="math/tex">x_1, x_2, \cdots, x_n</script>）上读取信息，并且生成两个隐藏状态序列 <script type="math/tex">hf_t</script> 和 <script type="math/tex">hb_t</script>。然后将 <script type="math/tex">hf_t</script> 和 <script type="math/tex">hb_t</script> 拼接成在第 t 个时间步上的最终 BLSTM state <script type="math/tex">h_t = [hf_t, hb_t]</script>。因此我们的双向 LSTM <script type="math/tex">f_i(\cdot)</script> 生成了一个隐藏状态序列 <script type="math/tex">(h^i_1, h^i_2, \cdots, h^i_n)</script>，其中 i=1 对应神经网络的意图检测任务，i = 2 对应 slot filling 任务。<br>&emsp;&emsp;为了进行意图检测，隐藏状态 <script type="math/tex">h^1_t</script> 与 slot filling 任务中 由<script type="math/tex">f_2(\cdot)</script> 产生的 <script type="math/tex">h^2_t</script> 组合在一起，以生成在第 t 个时间步由 <script type="math/tex">g_1(\cdot)</script> 产生的状态 <script type="math/tex">s^1_t</script>：</p>
<script type="math/tex; mode=display">
s^1_t = \Phi(s^1_{t-1}, h^1_{n-1}, h^2_{n-1})\\
y^1_{intent} = \arg\max_{\hat{y}^1_n} P(\hat{y}^1_n | s^1_{n-1}, h^1_{n-1}, h^2_{n-1})</script><p>&emsp;&emsp;其中 <script type="math/tex">\hat{y}^1_n</script> 包含在最后一个时间步 t 的所有意图标签的预测概率。<br>&emsp;&emsp;对于 slot filling 任务类似。<br><div class="note danger">
            <p>&emsp;&emsp; <script type="math/tex">g_1(\cdot)</script> 和 <script type="math/tex">g_2(\cdot)</script> 实际上就是 <strong>decoder</strong>。</p>
          </div></p>
<h3 id="Bi-Model-structure-without-a-decoder"><a href="#Bi-Model-structure-without-a-decoder" class="headerlink" title="Bi-Model structure without a decoder"></a>Bi-Model structure without a decoder</h3><p>&emsp;&emsp;就是没有 <script type="math/tex">g(\cdot)</script> 产生的 s。</p>
<h3 id="Asynchronous-training"><a href="#Asynchronous-training" class="headerlink" title="Asynchronous training"></a>Asynchronous training</h3>]]></content>
      <categories>
        <category>paper</category>
      </categories>
  </entry>
  <entry>
    <title>uber plato v0.2使用注意点（踩坑）</title>
    <url>/project/uber%20plato%20v0.2%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E7%82%B9%EF%BC%88%E8%B8%A9%E5%9D%91%EF%BC%89.html</url>
    <content><![CDATA[<ol>
<li><code>/plato/controller/basic_controller.py</code>, line 415 将 <code>&#39;/&#39;</code> 改成 <code>os.sep</code>，所有的 controller 都要改。否则会出现 <code>ValueError: Configuration file CamRest_text.yaml not found!</code> 的错误，这是因为 windows 电脑的分隔符是 <code>\\</code> 而不是 <code>&#39;/&#39;</code>。</li>
<li>在运行 LudwigNLU（或自定义的预训练 NLU） 而不是 SlotFillingNLU 的情况下。如果使用 pycharm 运行 plato 而不是命令行，有几点需要注意。<ul>
<li>在包 <code>applications/cambridge_restaurants</code> 中有一个模块为 <code>cambridge_restaurants_agent.py</code>，它可以供人直接运行（只需稍作更改）。但是这会报错 <code>Plato error! Ludwig nlu: Model directory models/camrest_nlu/sys/experiment_run/model not found</code>，这是因为在 <code>applications/cambridge_restaurants</code> 路径下运行程序，那么项目的根路径就变了 <code>plato/applications/cambridge_restaurants</code> 而不是 <code>plato</code>。所以在加载模型时，模型的路径错了。所以可以在 plato 项目的根路径下新建一个 py 文件，然后把 <code>cambridge_restaurants_agent.py</code> 的代码拷过去。</li>
</ul>
</li>
<li>使用 ludwig 训练 NLU/DST/NLG 时，会出现找不到日志文件夹的错误，因为它没有自动创建 log 文件夹的上一级文件夹。个人认为是框架的 bug，所以在训练的时候应该要加上 <code>--skip_save_log</code> 跳过日志记录。<a id="more"></a>
</li>
</ol>
]]></content>
      <categories>
        <category>project</category>
      </categories>
      <tags>
        <tag>plato</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Belief Tracker: Data-Driven Dialogue State Tracking</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/task-oriented/57.%20Neural%20Belief%20Tracker%EF%BC%9AData-Driven%20Dialogue%20State%20Tracking.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/abs/1606.03777" target="_blank" rel="noopener">论文地址</a>，作者是 Nikola Mrkšić，发表于 2016 年。</p>
          </div>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>&emsp;&emsp;现代口语对话系统的核心组成部分之一是 <strong>belief tracker</strong>，它可以在对话的每一步估计用户的目标。然而，目前大多数方法难以扩展到更大、更复杂的对话领域。这是由于他们依赖：a）口语理解（<strong>Spoken Language Understanding</strong>，<strong>SLU</strong>）模型，需要大量注释的训练数据；或者 b）手工制作的词汇表，用于捕捉用户语言中的一些词语变种。我们提出了一个新的 <strong>Neural Belief Tracking</strong>（<strong>NBT</strong>）框架，通过将模型建立在表征学习（最新进展的表征学习，即以前没有人用词向量去做过 DST）上以此克服了这些问题。NBT 模型对预训练的词向量进行推理，学习将它们组合成用户话语（user utterances）和对话上下文（dialogue context）的分布式表示。我们对两个数据集的评估表明……（你懂得，不翻译了）。<br><a id="more"></a></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>&emsp;&emsp;口语对话系统（<strong>Spoken dialogue systems</strong>, <strong>SDS</strong>）允许用户通过交谈的方式与计算机进行交互。</p>
<ul>
<li>基于任务的系统帮助用户实现目标，比如找酒店或者预定航班。SDS 的其中一个模组 <strong>dialogue state tracking</strong> (<strong>DST</strong>) 用于解释用户输入并更新 <strong>belief state</strong>（系统对会话状态的内部表示（Young et al., 2010））。这是下游 <strong>dialogue manager</strong> 用于决定系统下一步应执行操作的 dialogue <strong>states</strong> 的概率分布（Su et al., 2016a,b）；然后 <strong>system action</strong> 由自然语言生成器（<strong>NLG</strong>）描述（Wen et al., 2015a,b; <a href="https://www.aclweb.org/anthology/P15-1044.pdf" target="_blank" rel="noopener">Dušek, Jurcicek, 2015</a>）。</li>
<li><strong>The Dialogue State Tracking Challenge</strong>（<strong>DSTC</strong>）系列公开任务提供了一个带标记数据集的通用评估框架(Williams et al., 2016)。该框架由 domain <strong>ontology</strong> 支持，ontology 定义一系列 <strong>slots</strong> 和 每个 slot 可采用的 <strong>values</strong>。系统必须跟踪用户表达的搜索约束（<strong>goals</strong> or <strong>informable slots</strong>）以及询问用户对搜索结果（<strong>request</strong>）的问题。这需要考虑每个用户的话语（通过语音识别器的输入）和对话上下文（<strong>dialogue context</strong>）（例如，系统刚刚说了什么）。图 1（<strong>博主注</strong>：我一般在论文笔记中不放图，但这张图可以对 DST 有一个直观的了解，推荐去原论文看一下）中给出了一个例子，DST 模块依赖于识别用户话语中的 ontology items。</li>
<li>在一个对话回合内，传统统计方法使用一个单独的 SLU 模组去处理词汇多样性的问题。然而，训练这些模型需要大量特定领域的标注。<ul>
<li>另外，由 Henderson et al. (2014d) 所展示的，<strong>turn-level SLU</strong> 和 <strong>cross-turn DST</strong> 可以合并成单独一个模型以实现更好的  belief tracking 性能。<strong>这样的耦合模型通常依赖于人工构造的语义词典来识别所提到的多样的（词汇上或形态上） ontology items</strong>。图 2 给出了三个 slot-value 对的词典示例。</li>
</ul>
</li>
<li>本文提出了一个新模型——NBT，SLU 和 DST 的组合版</li>
</ul>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>&emsp;&emsp;略。</p>
<h2 id="Separate-SLU"><a href="#Separate-SLU" class="headerlink" title="Separate SLU"></a>Separate SLU</h2><p>&emsp;&emsp;传统的 SDS 管道使用 SLU 解码器去探测在 ASR 输出中的 slot-value pair。然后下游 DST 模型将这一信息与上一轮的对话上下文结合，以此更新 belief state。</p>
<h2 id="Joint-SLU-DST"><a href="#Joint-SLU-DST" class="headerlink" title="Joint SLU/DST"></a>Joint SLU/DST</h2><p>&emsp;&emsp;Joint model 通常依赖一个被称为 delexicalisation 策略，凭此，文本中出现的 slots 和 values 被替换为通用的单词。</p>
<h1 id="Neural-Belief-Tracker"><a href="#Neural-Belief-Tracker" class="headerlink" title="Neural Belief Tracker"></a>Neural Belief Tracker</h1><p>&emsp;&emsp;NBT 是一个模型。</p>
<ul>
<li>功能：在对话过程中，用于检测在给定轮次的情况下的 slot-value 对（即用户的目的）。</li>
<li>输入：它的输入是用户进行输入之前的 <strong>system dialogue acts</strong>，<strong>user utterance</strong> 以及一个候选 <strong>slot-value pair</strong>。</li>
<li>例子：模型也许要决定 <em>‘I’m looking for good pizza’</em> 是否表达出了目的 <code>FOOD=ITALIAN</code>。要执行 belief tracking，NBT 模型要迭代所有的候选 slot-value pairs（由本体定义），并且做出决定刚才是否有一个 pair 被用户表达出来了。</li>
</ul>
<p>&emsp;&emsp;图 3 呈现了模型中的信息流程。NBT 架构的第一层在给定三个输入的情况下执行表征学习（<strong>representation learning</strong>），产生用户话语（user utterance） <strong>r</strong>，当前候选 slot-value <strong>c</strong> 以及 system dialogue acts <script type="math/tex">t_q, t_s, t_v</script> 的向量表征。然后，学习到的向量表征通过 <strong>context modelling</strong> 和 <strong>semantic decoding</strong> 子模组交互，得到中间 interaction summary 向量 <script type="math/tex">d_r, d_c</script> and d。它们被用作最终 <strong>decision-making</strong> 模组的输入，<strong>该模组决定用户是否表达了由候选 slot-value pair 表示的意图（即二元分类）</strong>。</p>
<h2 id="Representation-Learning"><a href="#Representation-Learning" class="headerlink" title="Representation Learning"></a>Representation Learning</h2><p>&emsp;&emsp;训练向量表征，以一个向量表示一句话。</p>
<ul>
<li>NBT-DNN：用 n-grams(1, 2, 3) 向量之和表示 utterance 向量。</li>
<li>NBT-CNN：使用 CNN 训练，得到 utterance 向量。</li>
</ul>
<h2 id="Semantic-Decoding"><a href="#Semantic-Decoding" class="headerlink" title="Semantic Decoding"></a>Semantic Decoding</h2><div class="note warning">
            <p>&emsp;&emsp;<strong>博主注</strong>：Semantic Decoding 指的是用户的 utterance 和 一个 slot-value pair 进行计算，最后得到向量 d。然后再与向量 m 进行计算，得到一个二维的向量，以此来断定该 slot-value pair 是否与 utterance 有关。<br>&emsp;&emsp;其次，不管有无关系，再进行下一次的匹配。再次输入用户的 utterance 和下一个 slot-value pair 进行匹配。<br>&emsp;&emsp;以此反复，直到迭代完毕。最后得到想要的多个/一个/零个 slot-value pair。</p>
          </div>
<p>&emsp;&emsp;图 3 的 NBT 图示显示了 utterance 表征 <strong>r</strong> 和候选 slot-value pair <strong>c</strong> 直接通过 <strong>semantic decoding</strong> 模组交互。<strong>该组件决定用户是否清晰地表达出与当前候选对匹配的意图（即不考虑对话上下文）</strong>。<br>&emsp;&emsp;这种匹配的例子包括 <code>&#39;I want Thai food&#39;</code> 和 <code>&#39;food=Thai&#39;</code>，或者要求更高的食物，如 <code>a pricey restaurant</code> 和 <code>&#39;price=expensive&#39;</code>。<br>&emsp;&emsp;这是使用高质量的预训练词向量发挥的作用：delexicalisation-based model 可以处理前一个例子，但在后一种情况下是无能为力的，除非人类专家提供了一个语义词典来列出 domain ontology 中的每个值的所有可能的替换。<br>&emsp;&emsp;让候选对的 slot name and value 的向量空间表示 由 <script type="math/tex">c_s</script> 和 <script type="math/tex">c_v</script> 给出（多个单词的 slot name/value 向量相加）。NBT 模型学习将这个元组映射成一个向量 <strong>c</strong>，该向量的维数与 utterance 表征 <strong>r</strong> 相同。然后，这两个表征被强制交互，以学习一个相似性度量标准：</p>
<script type="math/tex; mode=display">
c = \sigma(W^s_c(c_s + c_v) + b^s_c) \\
d = r \otimes c</script><p>&emsp;&emsp;其中 <script type="math/tex">\otimes</script> 代表 <strong>element-wise</strong> 相乘（<strong>Hadamard 乘积</strong>）。点积，看起来像更直观的相似性度量，但是它会把 d 中丰富的特性集减少为单个标量（<strong>博主注</strong>：即向量点积的结果是一个值，而不是向量）。元素乘法允许下游网络通过学习 <strong>r</strong> 和 <strong>c</strong> 中特征集之间的非线性交互，更好地利用其参数。</p>
<h2 id="Context-Modelling"><a href="#Context-Modelling" class="headerlink" title="Context Modelling"></a>Context Modelling</h2><p>&emsp;&emsp;这个 decoder 还不足以从人机对话的 utterances 中提取意图，为了理解一些查询，belief tracker 必须注意 context，即 <code>the flow of dialogue leading up to the latest user utterance</code>。虽然所有之前的系统和用户的语句都很重要，但是最相关的一句是上一句系统语句，对话系统可以执行以下两个 system acts（除去其他的行为）中的一个：</p>
<ol>
<li>System Requests：系统询问用户关于一个特定 slot <script type="math/tex">T_q</script> 的值。如果系统的语句为：<code>&#39;what price range whould you like?&#39;</code>，而用户回答了 <code>any</code>，那么模型必须推断出 <code>price range</code> slot，而不是推断出其他的 slots，比如 <code>area</code> 或者 <code>food</code>。</li>
<li>System Confirm：系统询问用户以确认一个特定 slot-value pair(<script type="math/tex">T_s, T_v</script>) 是否是他们所需约束的一部分。例如，如果用户对问题 <code>&#39;how about Turkish food?&#39;</code> 回复 <code>&#39;yes&#39;</code>，模型就必须感知到 system act 以准确更新 belief state。</li>
</ol>
<p>&emsp;&emsp;如果我们使马尔科夫决策只考虑最后一组 system act，我们就可以将 context modelling 纳入 NBT 中。使 <script type="math/tex">t_q</script> 和 (<script type="math/tex">t_s, t_v</script>) 作为 system request 和 confirm acts 的参数的词向量（如果没有，则为零向量）。该模型计算 system acts、候选对（<script type="math/tex">c_s, c_v</script>）和 utterance  表征 <strong>r</strong> 之间的相似性，衡量标准如下所示：</p>
<script type="math/tex; mode=display">
m_r = (c_s \cdot t_q) r \\
m_c = (c_s \cdot t_s) (c_v \cdot t_v) r</script><p>&emsp;&emsp;其中 <script type="math/tex">\cdot</script> 表示 dot product。计算出的相似度项作为 <strong>gating mechanisms</strong>，<strong>只有当系统询问当前的候选 slot 或  slot-value pair 时，该机制才传递 utterance 表征</strong>。<font color="red"><strong>这种类型的交互对于确认 system act 特别有用：如果系统要求用户确认，用户可能不会提到任何 slot values，而只是肯定或否定。这意味着模型必须考虑 utterance，候选 slot-value pair 和系统提供的 slot value pair 三者之间的交互。如果（且仅当）后两者相同，二元决策才认为用户同意。</strong></font><br><div class="note primary">
            <ol><li>比如系统询问：“你觉得中餐怎么样？”，用户回复：“可以。”。那么系统的 slot-value 就是 <code>food=chinese</code>，显而易见只有当候选 slot-value 等于 <code>food=chinese</code> 时 <script type="math/tex">(c_s \cdot t_s) (c_v \cdot t_v)</script> 的积才是最大的，所以语句“可以。”得以传入到二元决策层。在此可能有疑惑，电脑怎么知道“可以。”代表同意呢？实际上就是词向量那一套，在不断的训练之中，“可以。”就含有了同意的意思。</li><li>如果系统询问：“你觉得中餐怎么样？”，用户回复：“不行，我要吃意大利菜。”。逻辑是一样的，当候选 slot-value 等于 <code>food=chinese</code> 时，才允许“不行，我要吃西餐。”通过。但是不要忘了用户的语句也要与候选 slot-value 进行计算。虽然 <script type="math/tex">m_c</script> 蕴含了信息，但是 d 却匹配不通过，d 的 slot-value 必须是 <code>food=italian</code>。因此 <code>food=chinese</code> 二元决策输出“否”。</li></ol>
          </div></p>
<h3 id="Binary-Decision-Maker"><a href="#Binary-Decision-Maker" class="headerlink" title="Binary Decision Maker"></a>Binary Decision Maker</h3><p>&emsp;&emsp;中间表示通过另一个隐藏层，然后合并。如果 <script type="math/tex">\phi_{dim}(x) = \sigma(Wx+b)</script> 是将输入向量 x 映射到 dim 大小向量的层，则输入到最终的 binary softmax（表决策）表示为：</p>
<script type="math/tex; mode=display">y = \Phi_2 (\Phi_{100}(d) + \Phi_{100}(m_r) + \Phi_{100}(m_c))</script><h1 id="Belief-State-Update-Mechanism"><a href="#Belief-State-Update-Mechanism" class="headerlink" title="Belief State Update Mechanism"></a>Belief State Update Mechanism</h1><p>&emsp;&emsp;在 SDS 中，belief tracking 模型在 ASR 上进行操作。尽管语音识别有所改外，但是随着对话系统越来越频繁地在 noisy environments 中使用，目前仍有必要使用不完善的 ASR 系统。<br>&emsp;&emsp;本工作，我们定义了一个简单的 rule-based belief state 更新机制，可以应用到 ASR N-best lists 中。对于 dialogue turn <strong>t</strong>，令 <script type="math/tex">sys^{t-1}</script> 表示之前的系统输出（即上一轮对话中，系统说的话），令 <script type="math/tex">h^t</script> 表示后验概率为 <script type="math/tex">p^t_i</script> 的 N 个 ASR 假设 <script type="math/tex">h^t_i</script> 的列表。对于任意一个假设 <script type="math/tex">h^t_i</script>，slot <strong>s</strong> 以及 slot value v <script type="math/tex">\in V_s</script>，NBT 模型估计 <script type="math/tex">\mathbb{P}(s, v | h^t_i, sys^{t-1})</script> 的概率，这是 (s, v) 在给定假设中表示的（turn-level）概率。对此类 N 个假设的预测如下所示进行相加：</p>
<script type="math/tex; mode=display">\mathbb{P}(s,v | h^t, sys^{t-1}) = \sum^N_{i=1} p^t_i \mathbb{P}(s,v | h^t_i, sys^{t-1})</script><div class="note warning">
            <p>&emsp;&emsp;按理说，到此步为止，就已经将 (s, v) 的概率计算出来了，即 <script type="math/tex">\mathbb{P}(s,v | h^t, sys^{t-1})</script>。<br>&emsp;&emsp;但是上文中其实一直提到一个概念，即 turn-level。我们需要计算 turn level 的 (s, v) 的概率。所以我们还要继续计算，也就是下面的两步。最终结果为 <script type="math/tex">\mathbb{P}(s,v | h^{1:t},sys^{1:t-1})</script>，意思是使用一个系数 <script type="math/tex">\lambda</script> 将以前 turn-level 的概率都加起来。最后的概率才是 (s, v) 的概率。（虽然我不知道原理是什么，但是看起来有点像指数加权平均。此外该论文其实给出了参考文献，该文献貌似讲述了 turn-level 的概念。）</p>
          </div>
<p>&emsp;&emsp;然后，这个 turn-level belief state  estimate 与相加到时间 <code>(t-1)</code> 为止的 (cumulative) belief state 组合，以获得更新过的 belief state  estimate：</p>
<script type="math/tex; mode=display">\mathbb{P}(s,v | h^{1:t},sys^{1:t-1}) = \lambda \mathbb{P}(s,v | h^t, sys^{t-1}) + (1 - \lambda) \mathbb{P}(s,v | h^{1:t-1}, sys^{1:t-2})</script><p>&emsp;&emsp;<script type="math/tex">\lambda</script> 是决定 turn-level 和 前一轮的 belief state estimate 的相对权重的系数。对于 slot <strong>s</strong>，在 turn <strong>t</strong> 时，它的 detected value 集合由下所示：</p>
<script type="math/tex; mode=display">
V^t_s = \{v \in V_s | \mathbb{P}(s,v | h^{1:t},sys^{1:t-1}) \geq 0.5\}</script><div class="note info">
            <p>&emsp;&emsp;上面的公式应该指的是用 NBT 进行二元分类所得到的所有 (s, v)，再一次经过 Belief State Update Mechanism 后，会产生一个新的概率，如果这个概率大于等于 0.5，那么我们就认为它是与用户输入的话相关的 (s,v)。<br>&emsp;&emsp;比如我们经过 NBT 二元分类得到 {(food, chinese), (food, italian), (pricerange, cheap)}，概率分别为 [0.7, 0.59, 0.67]。而经过 Belief State Update Mechanism 之后，概率变为 [0.8, 0.44, 0.53]。那么我们认为 {(food, chinese), (pricerange, cheap)} 就是我们想要的结果（由于 food=italian 小于 0.5 被剔除了）。</p>
          </div>
<p>&emsp;&emsp;对于 informable slots（即 goal-tracking），￥%……&amp;*（&amp;……%￥。对于 requests，<script type="math/tex">V^t_{req}</script> 中的所有 slots 都被视为已被请求。由于 requestable slots 用于模拟 single-turn 用户查询，因此它们在轮次中不需要 belief tracking。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>NBT</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：PLATO DIALOGUE SYSTEM: A FLEXIBLE CONVERSATIONAL AI RESEARCH PLATFORM</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/task-oriented/56.%20PLATO%20DIALOGUE%20SYSTEM%EF%BC%9AA%20FLEXIBLE%20CONVERSATIONAL%20AI%20RESEARCH%20PLATFORM.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/pdf/2001.06463v1.pdf" target="_blank" rel="noopener">论文地址</a>，论文作者 Alexandros Papangelis 等，发表于 2020 年。<br>&emsp;&emsp;注：由于该论文的重点是提出一个工具包，故以下将只翻译我认为比较重要的部分。</p>
          </div>
<h1 id="本文主要内容"><a href="#本文主要内容" class="headerlink" title="本文主要内容"></a>本文主要内容</h1><p>&emsp;&emsp;本论文中，我们提出了 Plato，一个用 Python 编写的灵活的对话 AI 平台，支持任何类型的会话代理架构。包括从标准架构到具有联合训练组件的架构、单方或多方交互，以及任何会话代理组件的离线或在线训练。Plato 被设计成易于理解和调试的形式，并且对训练每个组件的底层学习框架是不可知的（我直接机翻了，我推测大致意思应该是 Plato 支持任意的深度学习框架）。</p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>&emsp;&emsp;传统上，对话式 AI 系统（Conversational AI systems）由语音识别（<strong>Automatic Speech Recognition</strong>, <strong>ASR</strong>），自然语言理解（<strong>Natural Language Understanding</strong>, <strong>NLU</strong>），对话状态追踪（<strong>Dialogue State Tracking</strong>, <strong>DST</strong>），对话管理（<strong>Dialogue Management</strong>, <strong>DM</strong>），自然语言生成（<strong>Natural Language Generation</strong>, <strong>LG</strong>），<strong>Text To Speech</strong>(<strong>TTS</strong>) 组成。<br>&emsp;&emsp;随着某些技术的发展，一系列的平台和工具包被提出，以构建对话式 AI 系统。每个工具包很大程度上都是被设计在某个特定的例子中，<strong>它们由于某些原因，似乎使得尖端研究与其在生产系统中的应用两部分失去关联</strong>。无论如何，对话式 AI 系统变得越来越有用，需要有一个工具包能够在研究与生产之间搭起一个桥梁，并且能够提供一个完整会话体验的快速原型，其需要拥有 ASR、NLU、DST/DM、NLG 和 TTS 功能。目前仅有几个工具包可以完成以上的需要，例如 RASA。<br><a id="more"></a></p>
<h2 id="现存开源对话式-AI-系统"><a href="#现存开源对话式-AI-系统" class="headerlink" title="现存开源对话式 AI 系统"></a>现存开源对话式 AI 系统</h2><p>&emsp;&emsp;一些工具包以及对话式 AI 平台在近期被提出。以下是一些一流、被广泛采用的平台：</p>
<ul>
<li><strong>PyDial</strong>：</li>
</ul>
<h1 id="Plato-架构"><a href="#Plato-架构" class="headerlink" title="Plato 架构"></a>Plato 架构</h1><p>&emsp;&emsp;Plato 可以被用于创建、训练以及评估对话式 AI agents。它主要有四部分组件，即：1）<strong>dialogue</strong>：定义以及实现 dialogue acts 和 dialogue states；2）<strong>domain</strong>：包含对话的本体以及对话系统所需的数据库；3）<strong>controller</strong>：安排对话；4）<strong>agent</strong>：实现每个对话式 agent 的不同组件。四大组件如图 2 所示。<strong>在 Plato 中，每个组件都是使用 YAML 文件中的配置来实例化的</strong>。</p>
<h2 id="Dialogue"><a href="#Dialogue" class="headerlink" title="Dialogue"></a>Dialogue</h2><p>&emsp;&emsp;Plato 通过对话理论中定义明确的概念，如 dialogue states 和 dialogue acts，使 agents 之间的对话更便利。然而，一个 Plato agent 可能需要执行与对话没有直接关系的动作（例如调用一个 API）或使用言语以外的方式传递信息的动作（例如显示一个图像）。因此，Plato 把 Actions 和 States 建模成抽象的容器，以此从中产生 Dialogue Acts 和 Dialogue States。所以如果需要特定的应用（例如 multi-modal conversational agent），就可以有特定于任务的 Dialogue Acts and States。</p>
<h2 id="Domain"><a href="#Domain" class="headerlink" title="Domain"></a>Domain</h2><p>&emsp;&emsp;为了在 Plato 实现一个任务驱动的 slot-filling 对话系统，我们需要规定两种元素，即构成对话系统的 domain：</p>
<ol>
<li>Ontology：在任务驱动的应用中，对于一次会话，ontology 决定了 informable slots（用户提供的信息），requestable slots（用户请求的信息），system requestable slots（系统请求的信息）。</li>
<li>Database：虽然 database 可能已经存在，但是 Plato 提供工具从数据中构建对话系统的 domain 和 database。</li>
</ol>
<p>&emsp;&emsp;在酒店预订的对话 agent 例子中，“口味”被认为是一个 informable slot，database 包含了关于酒店不同口味、价格范围、地址等信息。对于非 slot-fliing 的应用，Plato ontologies 和 database 可以扩展以满足特定任务或领域的需求。</p>
<h3 id="Domain-creation"><a href="#Domain-creation" class="headerlink" title="Domain creation"></a>Domain creation</h3><blockquote>
<p>Plato provides a utility that makes it easy to generate an ontology (a <code>.json</code> file) and a database (SQLite) from a <code>.csv</code> file, with columns representing item attributes and rows representing items (for an example, see <code>plato/example/data/flowershop.csv</code>). The main purpose of this utility is to help quick prototyping of conversational agents. The command <code>plato domain –config &lt;PATH/TO/CONFIG.YAML&gt;</code> calls the utility and generates the appropriate <code>.db</code> and <code>.json</code> files that define the domain. In the YAML configuration file, the user specifies details such as the path to the input <code>.csv</code> file, the columns that represent informable slots, etc.</p>
</blockquote>
<h2 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h2><p>&emsp;&emsp;略</p>
<h2 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h2><p>&emsp;&emsp;Plato 中每个对话应用都有一个或多个 agent。每个 agent 都有一个 role（助手、用户、旅客、导师等）以及一组组件，例如 NLU，DM，DST，dialogue policy，NLG。<br>&emsp;&emsp;每一个组件可以是 rule-based ，也可以是 trained 的。</p>
<h3 id="Rule-based-modules"><a href="#Rule-based-modules" class="headerlink" title="Rule-based modules"></a>Rule-based modules</h3><p>&emsp;&emsp;Plato 提供 slot-filiing 对话 agent 的所有组件（rule-based）：<code>slot_filling_nlu</code>，<code>slot_filling_dst</code>，<code>slot_filling_policy</code>，<code>slot_filling_nlg</code>，以及一个 Agenda-Based User Simulator 的默认版本 <code>agenda_based_us</code>。这些可以用于 <strong>quick prototyping</strong>, <strong>baselines</strong>, or <strong>sanity checks</strong>。具体来说，所有这些组件都遵循基于给定本体的 rules or patterns，有时也遵循给定数据库的 rules or patterns，并且<strong>应该被视为每个组件的最基本版本</strong>。</p>
<h3 id="Trained-modules"><a href="#Trained-modules" class="headerlink" title="Trained modules"></a>Trained modules</h3><p>&emsp;&emsp;Plato 支持以 online（交互期间）或者 offline（从数据中）的形式训练 agent 的组件，可以使用任意的机器学习框架。实际上，只要尊重 Plato 的输入/输出接口，任何模型都可以加载到 Plato 中。例如，如果是一个自定义 NLU 模型，仅需要继承 Plato 的 NLU 抽象类（<code>plato.agent.component.nlu</code>）并且实现必要的方法以及将数据打包/解包到自定义模型中并从中取出的功能即可。</p>
<ul>
<li><strong>Plato internal experience</strong>：为了促进在线学习、调试和评估的能力，</li>
<li><strong>Parsing data with Plato</strong>：Plato 为 DSTC2、MetaLWOZ 和 Taskmaster 数据集提供解析器。……。对于其他数据集，用户应该实现自定义解析器，将数据转换为 Plato 可读格式。……。</li>
<li><strong>Training components of conversational agents</strong>：</li>
<li><strong>Model Training with Plato</strong>：除了监督模型，Plato 还提供了一些强化学习算法的实现</li>
<li><strong>Training with Ludwig</strong>：实际上，在 Plato 中，尽管任何框架都可以为对话式 agent 的不同组件构建和训练深度学习模型，但当目标是 quick prototyping 或出于教育目的时，Ludwig 是一个不错的选择，因为 Ludwig 允许用户在不编写任何代码的情况下训练模型。用户只需要将其数据解析为 <code>.csv</code> 文件，创建描述体系结构的 Ludwig YAML配置文件，然后在终端中运行命令。这允许 Plato 与 Ludwig 模型集成，即加载或保存模型，训练和查询它们。经过训练的模型可以通过配置文件加载到模块中。在 Plato 教程中，我们提供了使用 Ludwig 为 Plato 建立和训练 language understanding, generation, dialogue policy, and dialogue state tracking 模型的示例。</li>
<li><strong>Training with other frameworks</strong>：</li>
</ul>
<h1 id="Plato-Settings"><a href="#Plato-Settings" class="headerlink" title="Plato Settings"></a>Plato Settings</h1><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>plato</tag>
      </tags>
  </entry>
  <entry>
    <title>概率论（叶丙成）</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%A6%82%E7%8E%87%E8%AE%BA%EF%BC%88%E5%8F%B6%E4%B8%99%E6%88%90%EF%BC%89.html</url>
    <content><![CDATA[<h1 id="概率概论"><a href="#概率概论" class="headerlink" title="概率概论"></a>概率概论</h1><p>&emsp;&emsp;概率与统计的差异：</p>
<ul>
<li>概率<ul>
<li>概率模型已知，要学会怎么算某些事件的概率</li>
<li>Ex：已知一骰子为公平骰，看到偶数的概率为多少？</li>
</ul>
</li>
<li>统计<ul>
<li>概率模型未知，要学会怎么从大量的实验结果中去建立概率模型</li>
<li>Ex：不知一骰是否灌铅，欲知各点出现的概率模型？<a id="more"></a>
</li>
</ul>
</li>
</ul>
<h2 id="集合论概念-名词"><a href="#集合论概念-名词" class="headerlink" title="集合论概念/名词"></a>集合论概念/名词</h2><p>&emsp;&emsp;“学生上课不规矩”的概率 = 0.1<br>&emsp;&emsp;p(学生上课不规矩) = 0.1<br>&emsp;&emsp;概率函数的自变量是：事件，而事件是一种集合。<br>&emsp;&emsp;概率论名词复习：</p>
<ul>
<li>元素（Element）<ul>
<li>Ex：小黑、小冀、小湘、小鄂、小美</li>
</ul>
</li>
<li>集合（Set）<ul>
<li>Ex：咸豆腐脑 A = {黑, 冀}</li>
<li>Ex：甜豆腐脑 B = {湘, 鄂}</li>
</ul>
</li>
<li>子集合（Subset）<ul>
<li>嫌咸 C = {湘, 鄂, 美}</li>
<li>B 是 C 的子集，表示为 <script type="math/tex">B \subset C</script></li>
</ul>
</li>
<li>全集（Universal Set）<ul>
<li>Ex：S = {黑, 冀, 湘, 鄂, 美}</li>
</ul>
</li>
<li>空集（Empty Set）<ul>
<li>Ex：<script type="math/tex">\emptyset</script> = <script type="math/tex">\{\}</script></li>
</ul>
</li>
<li>交集（Intersection）<ul>
<li>Ex：喜欢甜豆腐脑<strong>且</strong>喜欢咸豆腐脑的人 = <script type="math/tex">A \cap B = \{\} = \emptyset</script></li>
</ul>
</li>
<li>并集（Union）<ul>
<li>Ex：喜欢甜豆腐脑<strong>或</strong>喜欢咸豆腐脑的人 = <script type="math/tex">A \cup B = \{黑, 冀, 湘, 鄂\}</script></li>
</ul>
</li>
<li>补集（Complement）（绝对补集）（若给定全集 U，<script type="math/tex">A \subseteq U</script>）<ul>
<li>Ex：嫌咸 C = 咸 A 之于补集 <script type="math/tex">C = A^C</script>（<strong>博主注</strong>：叶丙成老师是台湾人，我考研的时候使用的符号好像是 <script type="math/tex">\bar{A}</script>，但是由于 mathjax 好像不是完全支持 bar 符号，所以以下将会混用）</li>
</ul>
</li>
<li>差集（Difference）（相对补集）：X - Y = {在 X 但不在 Y 中的东西}<ul>
<li>嫌咸 - 甜 = {美}</li>
</ul>
</li>
<li>不相交（Disjoint）：如果 <script type="math/tex">X \cap Y = \emptyset</script> =&gt; X，Y 不相交<ul>
<li>Ex：甜 <script type="math/tex">\cap</script> 咸 = <script type="math/tex">\{\}</script></li>
</ul>
</li>
<li>互斥（Mutually Exclusive）：若有一<strong>群</strong>集合 <script type="math/tex">X_1, X_2, \cdots, X_n</script> 中任选两个集合 <script type="math/tex">X_i, X_j</script> 都不相交（<script type="math/tex">\emptyset</script>），则我们称 <script type="math/tex">X_1, X_2, \cdots, X_n</script> 这<strong>群</strong>集合互斥。</li>
</ul>
<p>&emsp;&emsp;De Morgan’s Law 定理：<script type="math/tex">(A \cup B)^C = A^C \cap B^C</script>。证明：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/De Morgan&#39;s Law 定理证明.jpg" alt="De Morgan&#39;s Law 定理证明"></p>
<h2 id="概率名词"><a href="#概率名词" class="headerlink" title="概率名词"></a>概率名词</h2><ul>
<li>实验（Experiment）<ul>
<li>一个概率实验包含了：步骤（procedures）、模型（model）、观察（observations）</li>
<li>Ex：丢两个公平的骰子<ul>
<li>步骤：……拿起两个骰子投入碗中，直到停止为止。</li>
<li>模型：(1, 1)、(1, 2)、……、(6, 6) 发生机会均等</li>
<li>观察：(6, 6)</li>
</ul>
</li>
</ul>
</li>
<li>结果（Outcome）：是实验中可能的结果<ul>
<li>Ex：约心仪店员。成功/失败</li>
<li>Ex：看到华南虎。立体的/平面的</li>
<li>Ex：转幸运之轮</li>
</ul>
</li>
<li>样本空间（Sample Space）：是概率实验所有可能的结果的集合，通常用 S 表示<ul>
<li>Ex：约心仪店员。S = {成功, 失败}</li>
<li>Ex：连丢三次铜板，记录正反面结果（正H，反T）。S = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}</li>
<li>Ex：转幸运之轮一次。S = [0, 1)</li>
<li>Ex：转幸运之轮两次。S = [0, 1) * [0, 1)</li>
</ul>
</li>
<li>事件（Event）<ul>
<li>事件是对于实验结果的某种叙述</li>
<li>概率是在讲实验结果符合某事件叙述的机会有多大</li>
<li>在数学上，“事件”可以看成是“结果”的集合，亦是“样本空间”的子集<ul>
<li>Ex：台大学生上课出席情况<ul>
<li>结果：准时，迟到，旷课</li>
<li>事件1：有出席；E1 = {准时, 迟到}</li>
<li>事件2：没规矩；E2 = {迟到, 旷课}</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>事件空间（Event Space）<ul>
<li>Ex：台大学生上课出席<ul>
<li>S = {准时, 迟到, 旷课}</li>
</ul>
</li>
<li>事件空间 = { {}, {准时}, {迟到}, {旷课}, {准时, 迟到}, {迟到, 旷课}, {准时, 旷课}, {准时, 迟到, 旷课} }</li>
<li>事件空间是包含所有事件的集合</li>
<li>若样本空间有 S = {<script type="math/tex">o_1, o_2, \cdots, o_n</script>} n 个结果，事件空间有 <script type="math/tex">2^n</script> 个</li>
<li>概率是一个函数，其自变量是事件<ul>
<li>p(事件) = 0.6</li>
</ul>
</li>
<li>所有概率可以看成一个映射</li>
</ul>
</li>
</ul>
<h1 id="概率公理、性质"><a href="#概率公理、性质" class="headerlink" title="概率公理、性质"></a>概率公理、性质</h1><ul>
<li><strong>公理</strong>（Axioms）<ul>
<li>近代数学常以数条公理作为整套理论的基石<ul>
<li>Ex：线性代数。8 公理，公理 1：a + b = b + a ……</li>
</ul>
</li>
<li>这样的好处？    头过身就过</li>
<li>公理可否被证明？公理常是能被证明的基本性质</li>
<li>公理为何常被当废话？公理常是非常基本的性质</li>
<li>什么样的数学最厉害？公理越少条、公理越基本，越厉害！</li>
</ul>
</li>
</ul>
<h2 id="概率三公理（Axioms-of-Probability）"><a href="#概率三公理（Axioms-of-Probability）" class="headerlink" title="概率三公理（Axioms of Probability）"></a>概率三公理（Axioms of Probability）</h2><ul>
<li><strong>公理 1</strong>：对任何事件 A 而言，P(A) <script type="math/tex">\geq</script> 0</li>
<li><strong>公理 2</strong>：P(S) = 1</li>
<li><strong>公理 3</strong>：事件 <script type="math/tex">A_1, A_2, \cdots</script> 互斥 =&gt; <script type="math/tex">P(A_1 \cup A_2 \cup A_3 \cdots) = P(A_1) + P(A_2) + P(A_3) + \cdots</script><ul>
<li>公理 3 搭起了集合运算和概率运算的桥梁</li>
</ul>
</li>
</ul>
<h2 id="公理衍生之概率性质"><a href="#公理衍生之概率性质" class="headerlink" title="公理衍生之概率性质"></a>公理衍生之概率<strong>性质</strong></h2><ul>
<li>Ex：从一幅 52 张的扑克牌中抽出一张，结果为 Ace 的概率为多少？<ul>
<li>考虑“抽了一张，结果为 Ace”的事件，如下图所示。由于我们只抽一次，所以不可能一次既抽到黑桃 A，又抽到红桃 A。所以可以按照公理 3 进行计算。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/抽了一张，结果为Ace的事件.jpg" alt="抽了一张，结果为Ace的事件"></li>
</ul>
</li>
<li>若 E = {<script type="math/tex">o_1, o_2, \cdots, o_n</script>}，则 P(E) = P(<script type="math/tex">{o_1}</script>) + P(<script type="math/tex">{o_2}</script>) + <script type="math/tex">\cdots</script> + P(<script type="math/tex">{o_n}</script>)。E：事件，o：outcome。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/公理衍生之概率性质证明.jpg" alt="公理衍生之概率性质证明"></li>
<li>P(<script type="math/tex">\emptyset</script>) = 0<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/空集的概率是0的证明.jpg" alt="空集的概率是0的证明"></li>
<li>P(A) = 1 - P(<script type="math/tex">\bar{A}</script>)<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/P（A）=1-P（A bar）证明.jpg" alt="P(A)=1-P(A bar)证明"></li>
<li>P(A) = P(A - B) + P(A <script type="math/tex">\cap</script> B)<ul>
<li>证明：A = (A - B) <script type="math/tex">\cup</script> (A <script type="math/tex">\cap</script> B) =&gt; P(A) = P(A - B) + P(A <script type="math/tex">\cap</script> B)</li>
</ul>
</li>
<li>P(A <script type="math/tex">\cup</script> B) = P(A) + P(B) - P(A <script type="math/tex">\cap</script> B)</li>
<li>若 <script type="math/tex">C_1, C_2, \cdots, C_n</script> 互斥且 <script type="math/tex">C_1 \cup C_2 \cup \cdots \cup C_n = S</script>，则对任何事件 A：P(A) = P(<script type="math/tex">A \cap C_1</script>) + P(<script type="math/tex">A \cap C_2</script>) + <script type="math/tex">\cdots</script> + P(<script type="math/tex">A \cap C_n</script>)（<strong>切面包定理</strong>）<br>自证：<br>P(A) = P(A <script type="math/tex">\cap</script> S) = <script type="math/tex">P(A \cap (C_1 \cup C_2 \cup \cdots \cup C_n))</script> = <script type="math/tex">P((A \cap C_1) \cup (A \cap C_2) \cup \cdots \cup (A \cap C_n))</script><br><script type="math/tex">\because (A \cap C_1), (A \cap C_2), \cdots, (A \cap C_n)</script> 互斥<br><script type="math/tex">\therefore P((A \cap C_1) \cup (A \cap C_2) \cup \cdots \cup (A \cap C_n))</script> = <script type="math/tex">P(A \cap C_1) + P(A \cap C_2) + \cdots + P(A \cap C_n)</script>.<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/切面包定理例子.jpg" alt="切面包定理例子"></li>
<li>若 <script type="math/tex">A \subset B</script>，则 P(A) &lt; P(B)<ul>
<li>证明：自证</li>
<li>证：<br><script type="math/tex">\because</script> (B - A) <script type="math/tex">\cup</script> A = B 则 P((B - A) <script type="math/tex">\cup</script> A) = P(B)，且 (B - A) 与 A 互斥<br><script type="math/tex">\therefore</script> 根据公理 3 得 P(B - A) + P(A) = P(B)<br><script type="math/tex">\therefore</script> P(A) <script type="math/tex">\leq</script> P(B)<br>又 <script type="math/tex">\because A \subset B</script>，故 B 不为 <script type="math/tex">\emptyset</script>，则 B - A 也不为 <script type="math/tex">\emptyset</script>，即 P(B - A) &gt; 0<br><script type="math/tex">\therefore</script> P(A) &lt; P(B)</li>
</ul>
</li>
<li>对任意 n 个事件 <script type="math/tex">A_1, A_2, \cdots, A_n</script> 而言，<script type="math/tex">P(\bigcup^n_{i=1} A_i) \leq \sum^n_{i=1} P(A_i)</script>。（<script type="math/tex">\bigcup^n_{i=1} A_i</script> 表示 <script type="math/tex">A_1 \cup A_2 \cup \cdots \cup A_n</script>）<ul>
<li>证明：自证</li>
</ul>
</li>
<li>Bonferroni’s 不等式：对任意 n 个事件 <script type="math/tex">A_1, A_2, \cdots, A_n</script> 而言，<script type="math/tex">P(\bigcap^n_{i=1} A_i) \geq 1 - \sum^n_{i=1} P(\bar{A}_i)</script>。<ul>
<li>证明：自证</li>
</ul>
</li>
</ul>
<h1 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h1><ul>
<li>条件概率的表示法：P(X|Y)<ul>
<li>|: given，X: 所关心的事件，Y：条件（观察到的，已发生的事件）</li>
</ul>
</li>
<li>条件概率怎么算？<ul>
<li><a href="https://www.bilibili.com/video/av18918088?p=11" target="_blank" rel="noopener">5:00</a><ul>
<li><strong>延伸：若某实验结果 <script type="math/tex">o_i</script> 与某条件 Y 不相交，则 P(<script type="math/tex">o_i</script>|Y) = 0</strong></li>
</ul>
</li>
<li>延伸：若某条件事件 Y 包含数个实验结果：Y = {<script type="math/tex">o_1, o_2, \cdots, o_n</script>}。<script type="math/tex">P(o_i|Y) = \frac{P(o_i)}{P(o_1) + P(o_2) + \cdots + P(o_n)} = \frac{P(o_i)}{P(Y)}</script><ul>
<li>考虑某事件 X = {<script type="math/tex">o_1, o_2, q_1, q_2</script>}，已知条件事件 Y = {<script type="math/tex">o_1, o_2, o_3</script>} 发生了，则 <script type="math/tex">P(X|Y) = P(o_1|Y) + P(o_2|Y) = \frac{P(o_1)}{P(Y)} + \frac{P(o_2)}{P(Y)} = \frac{P(\{o_1, o_2\})}{P(Y)} = \frac{P(X \cap Y)}{P(Y)}</script></li>
</ul>
</li>
<li>终极延伸：<strong>若已知某条件事件 Y 发生了，则对与任何事件 X，我们可计算其条件概率：P(X|Y) = <script type="math/tex">\frac{P(X \cap Y)}{P(Y)}</script></strong><ul>
<li><script type="math/tex">P(X \cap Y)</script> = P(X|Y) <script type="math/tex">\cdot</script> P(Y)</li>
<li><script type="math/tex">P(X \cap Y)</script> = P(Y|X) <script type="math/tex">\cdot</script> P(X)</li>
</ul>
</li>
</ul>
</li>
<li>条件概率性质：对于任何事件 X 及任何条件事件 Y，我们有：<ul>
<li>性质 1：P(X|Y) = <script type="math/tex">\frac{P(X \cap Y) \geq 0}{P(Y) \geq 0} \geq 0</script></li>
<li>性质 2：P(Y|Y) = <script type="math/tex">\frac{P(Y \cap Y)}{P(Y)} = \frac{P(Y)}{P(Y)} = 1</script></li>
<li>性质 3：A，B 互斥 =&gt; <script type="math/tex">P(A \cup B|Y) = \frac{P(A)}{P(Y)} + \frac{P(B)}{P(Y)}</script> = P(A|Y) + P(B|Y)</li>
</ul>
</li>
<li>Total Probability 定理：若 <script type="math/tex">C_1, C_2, \cdots, C_n</script> 互斥且 <script type="math/tex">C_1 \cup C_2 \cup \cdots \cup C_n = S</script>，则对任意事件 A，有<br>P(A) = P(A|<script type="math/tex">C_1</script>)P(<script type="math/tex">C_1</script>) + P(A|<script type="math/tex">C_2</script>)P(<script type="math/tex">C_2</script>) + <script type="math/tex">\cdots</script> + P(A|<script type="math/tex">C_n</script>)P(<script type="math/tex">C_n</script>)（<strong>切面包定理翻版</strong>）</li>
<li>Bayes’ 定理：若 <script type="math/tex">C_1, C_2, \cdots, C_n</script> 互斥且 <script type="math/tex">C_1 \cup C_2 \cup \cdots \cup C_n = S</script>，则对任意事件 A，有<br>P(<script type="math/tex">C_j|A</script>) = <script type="math/tex">\frac{P(A|C_j)P(C_j)}{P(A|C_1)P(C_1) + P(A|C_2)P(C_2) + \cdots + P(A|C_n)P(C_n)}</script> = <script type="math/tex">\frac{P(A|C_j)P(C_j)}{P(A)}</script><br>（<script type="math/tex">\frac{P(C_j \cap A)}{P(A)} = \frac{P(A|C_j) \cdot P(C_j)}{\sum^n_{i=1} P(A|C_i) \cdot P(C_i)}</script>，<script type="math/tex">P(A|C_j) \cdot P(C_j) = P(A \cap C_j)</script>，P(A) = <script type="math/tex">\sum^n_{i=1} P(A|C_i) \cdot P(C_i)</script>）<ul>
<li>例子：“宪哥的逆袭”。解答：<a href="https://www.cnblogs.com/elaron/p/3408190.html" target="_blank" rel="noopener">宪哥的逆袭 - 叶炳成 -概率论</a></li>
</ul>
</li>
</ul>
<h1 id="概率的独立性"><a href="#概率的独立性" class="headerlink" title="概率的独立性"></a>概率的独立性</h1><ul>
<li>常见定义：若两事件 A、B 的概率满足：<script type="math/tex; mode=display">P(A \cap B) = P(A) \cdot P(B)</script>则 A、B 两事件称为概率上的独立事件<ul>
<li><strong>另一个更好的定义</strong>：若两事件 A、B 的概率满足：<script type="math/tex; mode=display">P(A|B) = P(A)</script>则 A、B 两事件称为概率上的独立事件</li>
</ul>
</li>
<li>在两个定义中，公式其实是一样的。<script type="math/tex">P(A|B) = \frac{P(AB)}{P(B)} = P(A)</script> =&gt; <script type="math/tex">P(AB) = P(A) \cdot P(B)</script></li>
<li>例子：已知学生历史课作业表现与概率课作业表现相互<strong>独立</strong>。若历史课作业未做概率为 0.2，概率课作业未做概率为 0.3。问两科作业同时未做的概率为？答：0.2 <script type="math/tex">\times</script> 0.3 = 0.06</li>
<li>另外一个例子，个人认为这个例子可以比较清楚的理解独立性，并且能够体会到人类从直觉上对概率的感觉比较模糊。如下图所示（<em>古锥姊</em>在台湾是可爱的妹子的意思。图中有个词是“阿辈”，其实是打错了，应该是“阿伯”）：<br>先介绍一下背景：台湾大学的学生骑自行车上学，但是又乱停乱放，所以有阿伯会来处理这些自行车，自行车会被阿伯<strong>回收</strong>，那么在被回收时，如果主人看到了自然会求情。已知阿伯被人求情时，会放行的概率为 0.2。<br>下图中阿伯给古锥姊的<strong>自行车放行</strong>的概率是 0.05（古锥姊在上课未能求情） 和 0.09（古锥姊及时求情），怎么看都很低。但是实际上在古锥姊求情的条件下，阿伯放行的概率高达 90%！乍一看可能无法理解到底为什么。<strong>请注意 P(古锥姊未能及时求情且车放行) = 0.05 求的是事件(古锥姊未求情)和事件(车放行)交集的概率。而(在古锥姊求情的条件下，阿伯放行的概率)是条件概率，两者所求并不相同。</strong><br><strong>那么如何求呢？</strong>首先在事件的概率为独立的情况下，P(古锥姊未能求情且车放行) = P(古锥姊未能求情) <script type="math/tex">\times</script> P(车放行)。但是题目中并没有说两事件独立，所以需要先计算两事件是否独立。<br><strong>下图中已经计算出实际上两事件并不独立！</strong>那么阿宅认为阿伯在给古锥姊放水是否成立呢？只需计算在古锥姊求情的情况下，车放行的概率即可。<br>下图中其实已经计算出 P(古锥姊求情) = P(古锥姊求情且车未放行 <script type="math/tex">\cup</script> 古锥姊求情且车放行) = 0.01 + 0.09 = 0.1。故 P(车放行|古锥姊求情) = <script type="math/tex">\frac{P(古锥姊求情且车放行)}{P(古锥姊求情)} = \frac{0.09}{0.1}</script> = 0.9<br>发现居然如此的高！阿伯在放水！那么为什么呢？感觉人类无法直观的理解这一概念。其实如果无法进行直观的理解，那就别理解了。换种思维，大家看 P(古锥姊求情) = 0.1，是不是特别低？也就是说首先古锥姊不来求情，阿伯不知道车是谁的，自然大概率不会放行。其次古锥姊不经常求情，所以阿伯自然大概率也不放行。这就导致了为何下图中古锥姊放行的概率如此低，是因为样本量太大了！而其实只要古锥姊求情，阿伯 90% 都会放行。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/概率的独立性例子.jpg" alt="概率的独立性例子"></li>
</ul>
<h2 id="多事件独立"><a href="#多事件独立" class="headerlink" title="多事件独立"></a>多事件独立</h2><ul>
<li>若事件 <script type="math/tex">A_1, A_2, \cdots, A_n</script> 满足下列条件，则称此 n 事件独立（n &gt; 2）：从中任选 m 事件 <script type="math/tex">A_{i_1}, A_{i_2}, \cdots, A_{i_m}</script> 均满足<br><script type="math/tex">P(A_{i_1} \cap A_{i_2} \cap \cdots \cap A_{i_m}) = P(A_{i_1})P(A_{i_2}) \cdots P(A_{i_m})</script>，m = 2, 3, <script type="math/tex">\cdots</script>, n.（也就是说每一种组合的事件（一种组合包含 2-n 个事件）都必须使得等式成立，才算多事件独立）<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/多事件独立例子.jpg" alt="多事件独立例子"></li>
</ul>
<h1 id="图解复杂概率"><a href="#图解复杂概率" class="headerlink" title="图解复杂概率"></a>图解复杂概率</h1><p>&emsp;&emsp;当碰到很复杂的概率问题时：</p>
<ul>
<li>先观察这个问题的实验结构</li>
<li>这实验是否能分解成数个子实验</li>
<li>若可以，则利用图解法</li>
</ul>
<p>&emsp;&emsp;下面给出一个例子：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/图解法例子.jpg" alt="图解法例子"></p>
<p>&emsp;&emsp;解：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/图解法例子答案.jpg" alt="图解法例子答案"></p>
<h1 id="数数算概率"><a href="#数数算概率" class="headerlink" title="数数算概率"></a>数数算概率</h1><ul>
<li>古典概率常假设实验结果（outcome）发生概率相同。<ul>
<li>Ex：包子摊肉包、菜包、豆沙包产量相同，外表一致。则 P(咸包子) = <script type="math/tex">\frac{1}{3} \times</script> 2</li>
</ul>
</li>
<li>故计算某事件概率的问题等同于计算此事件包含多少实验结果（outcome）。故计算概率等价于数数问题！</li>
<li><strong>数数基本原则</strong><ul>
<li><strong>若某种实验有 n 种不同结果，而另一种实验有 m 种不同结果。若操作此两实验将有 nm 种不同结果</strong></li>
</ul>
</li>
<li>数数前的重要判断<ul>
<li>所有的物件是否可区分？（Distinguishable）</li>
<li>实验中抽选的物件是否放回供下次抽选（With/Without Replacement？）</li>
<li>实验中被抽选的东西，抽选循序是否有差异？（Order matters or not？）</li>
</ul>
</li>
</ul>
<h2 id="排列（Permutation）"><a href="#排列（Permutation）" class="headerlink" title="排列（Permutation）"></a>排列（Permutation）</h2><p>&emsp;&emsp;礼拜六算一个实验，礼拜天算另一个实验<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/排列（Permutation）.jpg" alt="排列（Permutation）"></p>
<ul>
<li>一般化：若有 n 异物，从中依序取出 k 物共有多少种结果？答：<script type="math/tex">\frac{n!}{(n - k)!}</script>（意味着物品的取出是具有顺序的）<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/排列公式一般化推理.jpg" alt="排列公式一般化推理"></li>
</ul>
<h2 id="重复选取（Choose-with-Replacements）"><a href="#重复选取（Choose-with-Replacements）" class="headerlink" title="重复选取（Choose with Replacements）"></a>重复选取（Choose with Replacements）</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/重复选取（Choose with Replacements）.jpg" alt="重复选取（Choose with Replacements）"></p>
<ul>
<li>一般化：若有 n 异物，从中选取一物，每次取完放回。依序选取 k 次，共有多少种结果？答：<script type="math/tex">n^k</script></li>
</ul>
<h2 id="组合（Combination）"><a href="#组合（Combination）" class="headerlink" title="组合（Combination）"></a>组合（Combination）</h2><p>&emsp;&emsp;先从 3 个人中找一个人，再从 2 个人中找一个人，共有 3 <script type="math/tex">\times</script> 2 种可能，但是由于其中有重复的组合，所以结果不是这个。一共需要选取 2 人，所以重复的组合次数为 2!，所以 3 <script type="math/tex">\times</script> 2 除以 2! 才是最后的次数。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/组合（Combination）.jpg" alt="组合（Combination）"></p>
<ul>
<li>一般化：若有 n 异物，从中选取 k 物共有多少种结果？答：<script type="math/tex">\frac{n!}{(n - k)!k!}</script>（意味着物品的取出是无序的）<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/组合公式一般化推理.jpg" alt="组合公式一般化推理"></li>
<li>※<script type="math/tex">\begin{pmatrix}n\\k\\\end{pmatrix}</script>：二项式系数（binomial coefficients）<ul>
<li>来自<em>二项式定理</em>：<script type="math/tex">(x + y)^n = \sum^n_{k=0} \begin{pmatrix}n\\k\\\end{pmatrix} x^k y^{n-k}</script></li>
</ul>
</li>
</ul>
<h2 id="多项组合（Multinomial）"><a href="#多项组合（Multinomial）" class="headerlink" title="多项组合（Multinomial）"></a>多项组合（Multinomial）</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/多项组合（Multinomial）.jpg" alt="多项组合（Multinomial）"></p>
<ul>
<li>一般化：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/多项组合（Multinomial）一般化.jpg" alt="多项组合（Multinomial）一般化"></li>
</ul>
<h2 id="数数如何应用在算概率上？"><a href="#数数如何应用在算概率上？" class="headerlink" title="数数如何应用在算概率上？"></a>数数如何应用在算概率上？</h2><p>&emsp;&emsp;若一事件包含数个实验结果（outcome）且每个实验结果发生的概率都一样</p>
<ul>
<li>先计算任一实验结果的概率</li>
<li>再计算该事件包含多少个实验结果</li>
<li>两者相乘便得到该事件的概率</li>
</ul>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/多项组合例子（古典概率）.jpg" alt="多项组合例子（古典概率）"></p>
<h1 id="随机变量（Random-Variable-R-V-）"><a href="#随机变量（Random-Variable-R-V-）" class="headerlink" title="随机变量（Random Variable, R.V.）"></a>随机变量（Random Variable, R.V.）</h1><ul>
<li><a href="https://www.bilibili.com/video/av18918088?p=21" target="_blank" rel="noopener">0-04:56</a> 讲了为什么要以 P(X = 1) = 0.3 来表示随机变量的概率</li>
<li>随机变量是一个用来把实验结果数字化的表现方式，目的是可以让概率的推导更数学、更简明。X 就是所谓的随机变量。</li>
<li>探究它的本质：随机变量本质是函数<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/随机变量的本质例子.jpg" alt="随机变量的本质例子"></li>
</ul>
<h2 id="随机变量的种类"><a href="#随机变量的种类" class="headerlink" title="随机变量的种类"></a>随机变量的种类</h2><ul>
<li>离散随机变量（Discrete R.V.）：值是有限个，或是“可数的”无穷多个<ul>
<li>Ex：宅 vs. 店员：X(微笑) = 0, X(不笑) = 1 =&gt; X = 0, X = 1</li>
<li>Ex：小美选男友：X(明) = 0, X(华) = 1, X(袁) = 2 =&gt; X = 0, X = 1, X = 2 </li>
<li>Ex：小明告白多少次才成功：X(0次) = 0, X(1次) = 1, X(2次) = 2,… =&gt; X = 0, X = 1, X = 2,…</li>
</ul>
</li>
<li>连续随机变量（Continuous R.V.）：值有无穷多个，而且是“不可数的”无穷多个<ul>
<li>幸运之轮：X 可以是 0 到 1 之间内的任意数字</li>
</ul>
</li>
</ul>
<h2 id="什么是可数？什么是不可数？"><a href="#什么是可数？什么是不可数？" class="headerlink" title="什么是可数？什么是不可数？"></a>什么是可数？什么是不可数？</h2><ul>
<li>可数：代表它包含的东西是可以被一个一个数的。不管用什么样的方法，它里面的东西总会被数到。比如正偶数集合。</li>
<li>不可数：代表包含的东西是无法被一个一个数的。不管用什么样的方法，它里面一定有一样你没数到。比如 0 - 1 之间所有数字的集合是不可数的。<ul>
<li>证明 0-1 之间的数字集合是不可数的：<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/证明0-1之间的数字集合是不可数的.jpg" alt="证明0-1之间的数字集合是不可数的"></li>
</ul>
</li>
</ul>
<h2 id="随机变量的函数"><a href="#随机变量的函数" class="headerlink" title="随机变量的函数"></a>随机变量的函数</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/随机变量的函数.jpg" alt="随机变量的函数"></p>
<h1 id="累积分布函数-分布函数（Cumulative-Distribution-Function）"><a href="#累积分布函数-分布函数（Cumulative-Distribution-Function）" class="headerlink" title="累积分布函数/分布函数（Cumulative Distribution Function）"></a>累积分布函数/分布函数（Cumulative Distribution Function）</h1><p>&emsp;&emsp;对于任一个随机变量 X，我们定义其 CDF 为函数：</p>
<script type="math/tex; mode=display">F_X(x) = P(X \leq x)</script><p>&emsp;&emsp;函数 <script type="math/tex">F_X(x)</script> 代表随机变量 X 小于等于 x 的概率。<br>&emsp;&emsp;Ex：幸运之轮：<script type="math/tex">F_X(0.5) = P(X \leq 0.5) = \frac{1}{2}</script></p>
<ul>
<li>CDF 有什么用？计算 X 落在某范围内的概率<ul>
<li>P(3 &lt; X <script type="math/tex">\leq</script> 5) = P(-<script type="math/tex">\infty</script> &lt; X <script type="math/tex">\leq</script> 5) - P(-<script type="math/tex">\infty</script> &lt; X <script type="math/tex">\leq</script> 3) = P(X <script type="math/tex">\leq</script> 5) - P(X <script type="math/tex">\leq</script> 3) = <script type="math/tex">F_X(5) - F_X(3)</script></li>
<li>一般化：P(a &lt; X <script type="math/tex">\leq</script> b) = <script type="math/tex">F_X(b) - F_X(a)</script><ul>
<li>P(a <script type="math/tex">\leq</script> X <script type="math/tex">\leq</script> b) = <script type="math/tex">F_X(b) - F_X(a)</script> + P(X = a)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="离散随机变量的-CDF"><a href="#离散随机变量的-CDF" class="headerlink" title="离散随机变量的 CDF"></a>离散随机变量的 CDF</h2><ul>
<li>长什么样？<ul>
<li>Ex：X 为骰子的点数，故 P(X = 1) = P(X = 2) = <script type="math/tex">\cdots</script> = P(X = 6) = <script type="math/tex">\frac{1}{6}</script><ul>
<li>CDF: <script type="math/tex">F_X(x) = P(X \leq x)</script></li>
<li>比如 <script type="math/tex">F_X(0.3) = 0 \quad F_X(0.8) = 0 \quad F_X(1) = \frac{1}{6} \quad F_X(1.9) = \frac{1}{6} \quad F_X(2) = \frac{2}{6} \, \cdots</script></li>
<li>计算 <script type="math/tex">P(3 < X \leq 5) = F_X(5) - F_X(3) = \frac{5}{6} - \frac{3}{6} = \frac{2}{6}</script></li>
<li>计算 <script type="math/tex">P(3 < X < 5) = F_X(5^-) - F_X(3) = F_X(5) - P(X = 5) - F_X(3) = \frac{1}{6}</script></li>
</ul>
</li>
<li>CDF 呈阶梯状</li>
</ul>
</li>
</ul>
<h2 id="连续随机变量的-CDF"><a href="#连续随机变量的-CDF" class="headerlink" title="连续随机变量的 CDF"></a>连续随机变量的 CDF</h2><ul>
<li>长什么样？<ul>
<li>Ex：X 为幸运之轮所停下的数字，X <script type="math/tex">\in</script> [0, 1)<ul>
<li>CDF: <script type="math/tex">F_X(x) = P(X \leq x)</script></li>
<li>比如 <script type="math/tex">F_X(-0.1) = 0 \quad F_X(0.1) = P(0 \leq X \leq 0.1) = 0.1 \quad F_X(1) = 1 \quad F_X(1.7) = 1 \, \cdots</script></li>
<li><script type="math/tex">P(0.3 < X \leq 0.5) = F_X(0.5) - F_X(0.3)</script> = 0.5 - 0.3 = 0.2</li>
<li><script type="math/tex">P(0.3 < X < 0.5) = F_X(0.5^-) - F_X(0.3)</script> = 0.5 - 0.3 = 0.2</li>
</ul>
</li>
<li>CDF 呈连续型</li>
</ul>
</li>
</ul>
<h2 id="CDF-性质"><a href="#CDF-性质" class="headerlink" title="CDF 性质"></a>CDF 性质</h2><ul>
<li>离散随机变量 CDF<script type="math/tex; mode=display">
F_X(x^+) = F_X(x) \\
F_X(x^-) = F_X(x) - P(X = x) \\</script></li>
<li>连续随机变量 CDF<script type="math/tex; mode=display">F_X(x^-) = F_X(x) = F_X(x^+)</script></li>
<li>共同性质<script type="math/tex; mode=display">
F_X(-\infty) = P(X \leq -\infty) = 0 \\
F_X(\infty) = P(X \leq \infty) = 1 \\
0 \leq F_X(x) \leq 1 \\</script></li>
</ul>
<h1 id="概率质量函数（Probability-Mass-Function"><a href="#概率质量函数（Probability-Mass-Function" class="headerlink" title="概率质量函数（Probability Mass Function)"></a>概率质量函数（Probability Mass Function)</h1><ul>
<li>对于一个整数值的离散随机变量 X，我们定义其 PMF 为函数：<script type="math/tex; mode=display">p_X(x) = P(X = x)</script><ul>
<li>Ex：X 为公平骰子的点数，<script type="math/tex">p_X(3) = P(X = 3) = \frac{1}{6}</script></li>
</ul>
</li>
<li>PMF 和 CDF 的关系<script type="math/tex; mode=display">
\begin{align}
  F_X(2.5) = & P(X \leq 2.5)\\
  = & P(X = 2) + P(X = 1) + P(X = 0) + P(X = -1) + \cdots \\
  = & \sum^{2 = \llcorner 2.5 \lrcorner}_{n = -\infty} P(X = n)
\end{align}</script><ul>
<li>对于任何 x：<script type="math/tex; mode=display">F_X(x) = \sum^{\llcorner x \lrcorner}_{n = -\infty} p_X(n)</script></li>
</ul>
</li>
<li>任何一个 PMF 都称作是一种<strong>概率分布</strong></li>
</ul>
<h1 id="离散概率分布"><a href="#离散概率分布" class="headerlink" title="离散概率分布"></a>离散概率分布</h1><h2 id="Bernoulli-概率分布-伯努利分布（0-1分布）"><a href="#Bernoulli-概率分布-伯努利分布（0-1分布）" class="headerlink" title="Bernoulli 概率分布/伯努利分布（0-1分布）"></a>Bernoulli 概率分布/伯努利分布（0-1分布）</h2><p>&emsp;&emsp;<strong>特点</strong>：1 次实验，2 种结果，在意某结果发生与否。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Bernoulli概率分布例子.jpg" alt="Bernoulli概率分布例子"><br>&emsp;&emsp;一般化:<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Bernoulli概率分布一般化.jpg" alt="Bernoulli概率分布一般化"></p>
<h2 id="Binomial-概率分布-二项分布（n-重伯努利分布）"><a href="#Binomial-概率分布-二项分布（n-重伯努利分布）" class="headerlink" title="Binomial 概率分布/二项分布（n 重伯努利分布）"></a>Binomial 概率分布/二项分布（n 重伯努利分布）</h2><p>&emsp;&emsp;<strong>特点</strong>：n 次实验，1 个概率，在意 n 次实验出现某结果 k 次的概率</p>
<ul>
<li>Example<ul>
<li>阿宅鼓起勇气搭讪 10 人，若每次搭讪成功概率为 0.6，10 次成功 8 次的概率为？<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Binomial概率分布.jpg" alt="Binomial概率分布"></li>
<li>一周 5 天午餐在&amp;*&amp;#%汉堡，若每次制作超时的概率为 0.9，5 天中有 3 天制作超时的概率为？</li>
<li>一周有 3 晚会，乱停车 3 此，若每次被阿伯拖走的概率为 0.8，那么 3 次被拖 2 次的概率为？</li>
</ul>
</li>
<li>一般化：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Binomial概率分布一般化.jpg" alt="Binomial概率分布一般化"></li>
</ul>
<h2 id="Uniform-概率分布-均匀分布"><a href="#Uniform-概率分布-均匀分布" class="headerlink" title="Uniform 概率分布/均匀分布"></a>Uniform 概率分布/均匀分布</h2><p>&emsp;&emsp;<strong>特点</strong>：1 次实验，n 种结果，各结果概率均等，在意某结果发生与否。</p>
<ul>
<li>Example<ul>
<li>丢公平骰：1 到 6 各点数出现<strong>概率均等</strong></li>
<li>混哥考试：作答 A，B，C，D <strong>概率均等</strong></li>
<li>狡兔三窟：出现在窟 1，窟 2，窟 3 <strong>概率均等</strong></li>
</ul>
</li>
<li>一般化：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Unifrom概率分布一般化.jpg" alt="Unifrom概率分布一般化"></li>
</ul>
<h2 id="Geometric-概率分布-几何分布"><a href="#Geometric-概率分布-几何分布" class="headerlink" title="Geometric 概率分布/几何分布"></a>Geometric 概率分布/几何分布</h2><p>&emsp;&emsp;<strong>特点</strong>：实验中出现某结果概率已知，重复操作实验至该结果出现为止。在意某结果是在第几次实验才首次出现。</p>
<ul>
<li>Example<ul>
<li>阿宅告白：成功概率为 0.3，不成功誓不休。问第 5 次告白成功的概率为？</li>
<li>孙文革命：已知革命成功的概率为 0.1，不成功誓不休。问第 11 次成功的概率为？</li>
<li>六脉神剑：已知段誉打出六脉神剑的概率为 0.1。他在第 10 次才打出六脉神剑的概率为？<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Geometric概率分布例子.jpg" alt="Geometric概率分布例子"></li>
</ul>
</li>
<li>一般化：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Geometric概率分布一般化.jpg" alt="Geometric概率分布一般化"></li>
<li>有失忆性</li>
</ul>
<h2 id="Pascal-概率分布"><a href="#Pascal-概率分布" class="headerlink" title="Pascal 概率分布"></a>Pascal 概率分布</h2><p>&emsp;&emsp;<strong>特点</strong>：实验中出现某结果概率已知，重复实验至该结果出现第 k 次为止。在意到底第几次实验才结束。（第 n 次成功发生在第 x 次的概率）</p>
<h2 id="Poisson-概率分布"><a href="#Poisson-概率分布" class="headerlink" title="Poisson 概率分布"></a>Poisson 概率分布</h2><p>&emsp;&emsp;未看</p>
<h1 id="概率密度函数（Probability-Density-Function）"><a href="#概率密度函数（Probability-Density-Function）" class="headerlink" title="概率密度函数（Probability Density Function）"></a>概率密度函数（Probability Density Function）</h1><ul>
<li>以幸运之轮为例 <script type="math/tex">X~[0, 1)</script>，<script type="math/tex">p_X(0.7) = ?</script><ul>
<li>[0, 1) 中每个数字发生概率均等，令其为 p</li>
<li>[0, 1) 中有没有超过 <script type="math/tex">10^6</script> 个数字？有！ =&gt; <script type="math/tex">10^6 \times p \leq 1</script> =&gt; <script type="math/tex">p \leq 10^{-6}</script></li>
<li>[0, 1) 中有没有超过 <script type="math/tex">10^8</script> 个数字？有！ =&gt; <script type="math/tex">10^8 \times p \leq 1</script> =&gt; <script type="math/tex">p \leq 10^{-8}</script></li>
<li>所以 <script type="math/tex">p_X(0.7) = p = 0 ?</script></li>
</ul>
</li>
<li>连续随机变量和 PMF 注定没办法在一起，每个数字发生的概率都是 0！那么还是想知道某个数字发生的机会多大，怎么办？<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/PMF-先看一个例子.jpg" alt="PMF-先看一个例子"></li>
<li>对随机变量 X 而言，其概率密度 PDF：<script type="math/tex; mode=display">
\begin{align}
  f_X(x) = & \lim_{\Delta \rightarrow 0} \frac{P(x \leq X \leq x + \Delta x)}{\Delta x} \\
  = & \lim_{\Delta \rightarrow 0}\frac{F_X(x + \Delta x) - F_X(x)}{\Delta x} \\
  = & F^{\prime}_X(x)
\end{align}</script>&emsp;&emsp;所以 CDF 和 PDF 的关系为：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/CDF和PDF的关系.jpg" alt="CDF和PDF的关系"></li>
</ul>
<p>&emsp;&emsp;那么我们如何将它和概率联结呢？</p>
<script type="math/tex; mode=display">
\begin{align}
P(a < X \leq b) = & F_X(b) - F_X(a) \\
= & \int^b_{-\infty} f_X(x)dx - \int^a_{-\infty} f_X(x)dx \\
= & \int^b_a f_X(x)dx
\end{align}</script><h2 id="PDF-性质"><a href="#PDF-性质" class="headerlink" title="PDF 性质"></a>PDF 性质</h2><ul>
<li><script type="math/tex; mode=display">f_X(x) = F^{\prime}_X(x)</script></li>
<li><script type="math/tex; mode=display">F_X(x) = \int^x_{-\infty} f_X(u)du</script></li>
<li><script type="math/tex; mode=display">P(a \leq X \leq b) = \int^b_a f_X(x)dx</script></li>
<li><script type="math/tex; mode=display">\int^{\infty}_{-\infty} f_X(x)dx = 1</script></li>
<li><script type="math/tex; mode=display">f_X(x) \geq 0</script></li>
</ul>
<h1 id="连续概率分布"><a href="#连续概率分布" class="headerlink" title="连续概率分布"></a>连续概率分布</h1><h2 id="Uniform-概率分布"><a href="#Uniform-概率分布" class="headerlink" title="Uniform 概率分布"></a>Uniform 概率分布</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Unifrom连续概率分布.jpg" alt="Unifrom连续概率分布"></p>
<h2 id="Exponential-概率分布"><a href="#Exponential-概率分布" class="headerlink" title="Exponential 概率分布"></a>Exponential 概率分布</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/Exponential概率分布.jpg" alt="Exponential概率分布"></p>
<ul>
<li>有失忆性</li>
</ul>
<h2 id="Erlang-概率分布"><a href="#Erlang-概率分布" class="headerlink" title="Erlang 概率分布"></a>Erlang 概率分布</h2><p>&emsp;&emsp;未看</p>
<h2 id="Normal-概率分布-正态分布"><a href="#Normal-概率分布-正态分布" class="headerlink" title="Normal 概率分布/正态分布"></a>Normal 概率分布/正态分布</h2><ul>
<li>在自然界很常出现：<ul>
<li>Ex：人口身高分布、体重分布（无法证明为什么服从正太分布）</li>
</ul>
</li>
<li>亦常被用作“很多随机变量的总和”的概率模型<ul>
<li>Ex：100 人吃饭时间总和（100 人的吃饭时间都不一样）（可以证明）</li>
<li>原因：来自最后会讲到的“中央极限定理”</li>
</ul>
</li>
<li>也被称为高斯（Gaussian）分布</li>
<li>X <script type="math/tex">\sim</script> Gaussian(<script type="math/tex">\mu, \sigma</script>)<ul>
<li>PDF：<script type="math/tex; mode=display">f_X(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x - \mu)^2}{2\sigma^2}}</script></li>
<li>CDF：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/正态分布CDF.jpg" alt="正态分布CDF"></li>
</ul>
</li>
</ul>
<h3 id="标准正态分布"><a href="#标准正态分布" class="headerlink" title="标准正态分布"></a>标准正态分布</h3><p>&emsp;&emsp;<script type="math/tex">Z \sim Gaussian(0, 1)</script></p>
<script type="math/tex; mode=display">f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}</script><p>&emsp;&emsp;CDF 表示为 <script type="math/tex">\Phi(z) = \int^z_{-\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}} du</script>。<script type="math/tex">\Phi(z)</script> 一般专指标准正态分布。<br>&emsp;&emsp;<script type="math/tex">\Phi(z)</script> 性质：</p>
<script type="math/tex; mode=display">\Phi(-z) = 1 - \Phi(z)</script><p>&emsp;&emsp;任意 <script type="math/tex">\mu</script>，<script type="math/tex">\sigma</script>下的 CDF：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/任意mu,sigma下的CDF.jpg" alt="任意mu,sigma下的CDF"></p>
<h1 id="期望值"><a href="#期望值" class="headerlink" title="期望值"></a>期望值</h1><p>&emsp;&emsp;</p>
<h1 id="联合概率分布"><a href="#联合概率分布" class="headerlink" title="联合概率分布"></a>联合概率分布</h1><p>&emsp;&emsp;什么是联合分布？</p>
<ul>
<li>X：小美 facenook/QQ 离线时间，X~UNIF(8, 12)</li>
<li>Y：小华 facenook/QQ 离线时间，X~UNIF(8, 12)</li>
<li>Z：小袁 facenook/QQ 离线时间，X~UNIF(8, 12)</li>
<li>假设 X,Y,Z 都是离散随机变量</li>
<li>若将小美离线时间 X 与小华离线时间 Z 一起看呢？</li>
<li>画出 P(X=x, Z=z)：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/联合概率分布示意图.jpg" alt="联合概率分布示意图"></li>
<li>若将小美离线时间 X 与小袁离线时间 X 一起看呢？</li>
<li>赫然发现：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/联合概率分布示意图2.jpg" alt="联合概率分布示意图2"></li>
<li>同时将多个随机变量的行为一起拿来看，我们可以看出更多以往看不到的资讯！</li>
</ul>
<h2 id="联合-PMF"><a href="#联合-PMF" class="headerlink" title="联合 PMF"></a>联合 PMF</h2><p>&emsp;&emsp;<script type="math/tex">p_{X, Y}(x, y) = P(X=x 且 Y=y)</script>。假如看上面的例子的 X，Y 变量，那么 <script type="math/tex">P_{X, Y}(9, 10) = 0</script></p>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ul>
<li><script type="math/tex; mode=display">0 \leq p_{X, Y}(x, y) \leq = 1</script></li>
<li><script type="math/tex; mode=display">\sum^{\infty}_{x=-\infty} sum^{\infty}_{y=-\infty} p_{X, Y}(x, y) = 1</script></li>
<li>X，Y 独立。<script type="math/tex; mode=display">
\begin{align}
  P_{X, Y}(x, y) = & P(X=x, Y=y) \\
  = & P(X=x) \cdot P(Y=y) \\
  = & P_X(x)P_Y(y) \\
\end{align}</script></li>
<li>对于任何事件 B：<script type="math/tex">P(B) = \sum_{(x, y) \in B} P_{X, Y}(x, y)</script><ul>
<li>Ex：B：美、华下线时间不晚于 10 点</li>
<li>P(B) = <script type="math/tex">P_{X, Y} = (8, 8) + P_{X, Y} = (9, 9) + P_{X, Y} = (10, 10)</script></li>
<li><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/PMF性质.jpg" alt="PMF性质"></li>
</ul>
</li>
</ul>
<h2 id="联合-CDF"><a href="#联合-CDF" class="headerlink" title="联合 CDF"></a>联合 CDF</h2><script type="math/tex; mode=display">F_{X, Y}(x, y) = P(X \leq x 且 Y \leq y) = P(X \leq x, Y \leq y)</script><p>&emsp;&emsp;那么如何算 <script type="math/tex">F_{X, Y}(10, 10) = ?</script>，其实就是<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/概率论（叶丙成）/PMF性质.jpg" alt="PMF性质"></p>
<h3 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h3><ul>
<li><script type="math/tex; mode=display">0 \leq F_{X, Y}(x, y) \leq 1</script></li>
<li>若 <script type="math/tex">x_1 \leq x_2</script> 且 <script type="math/tex">y_1 \leq y_2</script>，则 <script type="math/tex">F_{X, Y}(x_1, y_1) \leq F_{X, Y}(x_2, y_2)</script></li>
<li><script type="math/tex; mode=display">F_{X, Y}(x, \infty) = P(X \leq x, Y \leq \infty) = P(X \leq x) = F_X(x)</script></li>
<li><script type="math/tex; mode=display">F_{X, Y}(\infty, y) = P(X \leq \infty, Y \leq y) = P(Y \leq y) = F_Y(y)</script></li>
<li><script type="math/tex; mode=display">F_{X, Y}(\infty, \infty) = P(X \leq \infty, Y \leq \infty) = 1</script></li>
<li><script type="math/tex; mode=display">F_{X, Y}(x, -\infty) = P(X \leq x, Y \leq -\infty) \leq P(Y \leq -\infty) = 0</script></li>
<li><script type="math/tex; mode=display">F_{X, Y}(-\infty, y) = P(X \leq -\infty, Y \leq y) \leq P(X \leq -\infty) = 0</script></li>
</ul>
<h1 id="中央极限定理"><a href="#中央极限定理" class="headerlink" title="中央极限定理"></a>中央极限定理</h1>]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习算法（五）：预训练模型</title>
    <url>/%C2%B7zcy/AI/dl/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;视频来自 <a href="https://www.bilibili.com/video/av56235038" target="_blank" rel="noopener">此处</a>，或者 <a href="https://www.bilibili.com/video/av46561029?p=61" target="_blank" rel="noopener">此处</a>。</p><ol><li>4:56 A word can have multiple senses.</li><li>11:24 ELMO</li><li>19:08 BERT</li><li>49:23 GPT(-2)</li></ol>
          </div>
<h1 id="ELMO"><a href="#ELMO" class="headerlink" title="ELMO"></a>ELMO</h1><p>&emsp;&emsp;Embedding from Language Model(ELMO) 可以做到每个 word token 分配不同的 embedding，它是一个 RNN-based model。如下图所示，在提取“潮水退了”这四个字的词向量时，先使用一个 RNN 捕获信息。首先输入单词的词向量，然后将“退了”的 output 作为新的词向量。如果捕获“臣退了”三个字的特征，由于上下文的单词不同，使用 RNN 之后输出的 output 也不同，所以“退了”拥有不同 embeddng。“高烧退了”同理。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/ELMO示意图.jpg" alt="ELMO示意图"></p>
<a id="more"></a>
<p>&emsp;&emsp;但是这样做可能没有捕获到句子的后文的信息，那么只需要再 train 一个反向的 RNN，然后将两个正反向的 embedding 拼起来就得到了单词的词向量。<br>&emsp;&emsp;现在我们 train model 都是 deep 的，那么我们的这个 RNN 也可以是 depp 的。但是当 deep 之后会碰到问题，那就是每一层都有 embedding，我们应该哪一层的 embedding 当做词向量。ELMO 的做法是“我全都要”。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/我全都要.jpg" alt="我全都要.jpg"></p>
<p>&emsp;&emsp;现在每一层都有一个 contextualized embedding，ELMO 的做法是把所有的 embedding 通通加起来变成一个 embedding。但是不是普通的加法，而是 <script type="math/tex">new embedding = \alpha_1 * embed_1 + \alpha_2 * embed_2</script>，其中的 <script type="math/tex">\alpha</script> 是 learn 出来的。但是 <script type="math/tex">\alpha</script> 在不同的任务下实际上选值也不同，详见视频 16:20 或者看论文。</p>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>&emsp;&emsp;Bidirectional Encoder Representations from Transformers(BERT)，BERT 用的是 Transformer 的 encoder。<br>&emsp;&emsp;BERT 实际做的是输入一个句子进去，每个单词都会输出一个 embedding。至于 BERT 内部的架构其实跟 Transformer 的 encoder 是一样的。<br>&emsp;&emsp;BERT 有两种训练方法：</p>
<ul>
<li>Masked LM：输入给 BERT 的句子有 15% 的机会随机被置换成一个特殊的 token [MASK]，现在的任务是 BERT 去猜测这些被盖住的地方是哪一个词汇。在预测时，把输出的 embedding 丢到一个 Linear Multi-class classifier，让它预测被 mask 掉的词汇是哪一个。因为这个 classifier 是一个 linear 的，所以它的能力是很弱的。所以如果要成功预测出正确的词汇，那么 BERT 可能要很深，比如要 24/48 层，我们需要 BERT 抽取出一个很好的表征。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/Masked LM.jpg" alt="Masked LM"></li>
<li>Next Sentence Prediction：比如输入“醒醒吧[SEP]你没有妹妹”，我们需要预测这两句话是否是相接的。另外我们需要在句子的首部加上 token [CLS]，然后将 [CLS] 的 embedding 输入到 Linear Binary Classifier 中去预测两个句子是否相接。但是问题来了，为什么要把 [CLS] 放在句子的开头呢？因为 BERT 的内部是一个 Transformer，所以一个 token 放在句子的开头和末尾影响不大，如果是一个 RNN 结构，那么可能应该放在末尾。<ul>
<li>[SEP]：是两个句子的界限符号</li>
<li>[CLS]：输出分类结果的位置<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/Next Sentence Prediction.jpg" alt="Next Sentence Prediction"></li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;这两个方法是需要同时使用的，这样 BERT 会学得最好。<br>&emsp;&emsp;最后 BERT 也一直被人改进，有许多新版的 BERT 可以分别适用不同的任务，下面的文章介绍了几种新型的 BERT。<br><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650411744&amp;idx=2&amp;sn=1db39446e4e91299f9ba8c1d4eeb5983&amp;chksm=becd94ba89ba1dac221e2092cb1a12ecb1a5a704b6ae5cebd2649adc1a903355b95567ab484e&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1572502447054&amp;sharer_shareid=68f8b84d7a46cc216b0afdc45278d6be&amp;key=8a4bbb55c6c79ce6c9104a6cfe5de2a3d1b8fa801c35e22e74b62948f50b6684c3f06195815e8712080977db6cec80fca5adfc95c9bc6fa848b8e68b41df13d8610e8d6c283ee2392b30de5cdae504bb&amp;ascene=1&amp;uin=MTQxMTUzMzk2MA%3D%3D&amp;devicetype=Windows+10&amp;version=62070152&amp;lang=en&amp;pass_ticket=pZijWLQmmCpNBDcjO4cUImTRWv1ZWLG4JENv1zUqjhXnUnShPGofPjjR%2Bkv1cozV" target="_blank" rel="noopener">BERT 的演进和应用</a></p>
<h2 id="如何使用BERT"><a href="#如何使用BERT" class="headerlink" title="如何使用BERT"></a>如何使用BERT</h2><p>&emsp;&emsp;已经知道了 BERT 的训练方法，那么现在该如何使用呢？可以向 ELMO 一样，把 BERT 当做提取特征的工具，使我们获得词向量。但是 BERT 论文中他不止做了这样的事情，他说可以将 BERT 和你要解的任务一起做训练。<br>&emsp;&emsp;下图介绍了一种例子。如果我们的任务是输入一个句子，输出一个类别，那么我们可以在输入的句子的开头加上一个 [CLS]，然后将 [CLS] 的 embedding 输入到一个 Linear classifier 中就可以预测类别。也就是说我们将 BERT 下下来后，其实只需要定义我们的 classifier，然而调整 classifier 的参数即可，BERT 的参数仅仅需要微调，因为它已经被预训练过了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/How to use BERT - Case 1.jpg" alt="How to use BERT - Case 1"></p>
<p>&emsp;&emsp;第二种例子是输入一个句子，输出每一个单词的类别，比如说 slot filling。这个跟上一个差不多。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/How to use BERT - Case 2.jpg" alt="How to use BERT - Case 2"></p>
<p>&emsp;&emsp;第三种例子是输入两个句子，输出一个类别，比如说 Natural Language Inference。这个也差不多，就是加了一个 [SEP]。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/How to use BERT - Case 3.jpg" alt="How to use BERT - Case 3"></p>
<p>&emsp;&emsp;第四种例子是 QA 问题，而且是 extraction-based QA(E.g.  SQuAD)。即给 model 一篇文章，然后问它一个问题，期望得到一个答案。但是这里放水了，就是说答案中出现的词汇一定会出现在文章中。具体来说就是输入文章 D 和问题 Q，输出 s 和 e，其中 s 和 e 指的是文章 D 中的某个区间中单词。如果 s 和 e 矛盾，即 s &gt; e，那么回答“此题无解”。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/How to use BERT - Case 4.jpg" alt="How to use BERT - Case 4"></p>
<p>&emsp;&emsp;具体做法：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/How to use BERT - Case 4-2.jpg" alt="How to use BERT - Case 4-2"></p>
<h2 id="BERT学到了什么"><a href="#BERT学到了什么" class="headerlink" title="BERT学到了什么"></a>BERT学到了什么</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/深度学习算法（五）：预训练模型/What does BERT learn.jpg" alt="What does BERT learn"></p>
<h2 id="Multilingual-BERT"><a href="#Multilingual-BERT" class="headerlink" title="Multilingual BERT"></a>Multilingual BERT</h2><h1 id="ERNIE"><a href="#ERNIE" class="headerlink" title="ERNIE"></a>ERNIE</h1><p>&emsp;&emsp;Enhanced Representation through Kownledge Integration(ERNIE)，它是特别为中文设计的。</p>
<h1 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h1><p>&emsp;&emsp;Generative Pre-Training(GPT)，GPT 是 transformer 的 decoder。GPT 与 BERT 类似。<br>&emsp;&emsp;56:45 讲了第一个位置的 attention token。</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
        <tag>ELMO</tag>
        <tag>BERT</tag>
        <tag>ERNIE</tag>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅强化学习算法笔记</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/rl/%E6%9D%8E%E5%AE%8F%E6%AF%85%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<h1 id="强化学习导论"><a href="#强化学习导论" class="headerlink" title="强化学习导论"></a>强化学习导论</h1><div class="note info">
            <p>&emsp;&emsp;强化学习导论部分视频来自<a href="https://www.bilibili.com/video/av58458003" target="_blank" rel="noopener">此处</a>。</p>
          </div>
<p>&emsp;&emsp;下图是强化学习的一个场景，Agent 对 environment 进行观察（Observation，其实常见的说法是 State，但是 State 的表述可能不太好，李宏毅老师将其改为观察），然后 Agent 做出 action 去改变环境，最后环境对 action 做出反馈（reward）。比如，现在有一杯水，Agent 将其打翻，environment 得到了 negative reward，因为人告诉它“不要这么做”。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/Scenario of Reinforcement Learning 1.jpg" alt="Scenario of Reinforcement Learning 1" title="Scenario of Reinforcement Learning 1"><br><a id="more"></a></p>
<p>&emsp;&emsp;那么水被打翻之后，又去把它才干净，这就得到了 positive reward。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/Scenario of Reinforcement Learning 2.jpg" alt="Scenario of Reinforcement Learning 2" title="Scenario of Reinforcement Learning 2"></p>
<p>&emsp;&emsp;如果以 Alpha GO 为例。Environment 就是你的对手，你每做出一步 action，你的对手都会有反应。但是其实下围棋还是一个比较复杂的情形，因为在大多数的情况下你得到的 reward 都是 0，只要在输赢时才能得到 reward。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/Alpha GO 为例的强化学习场景.jpg" alt="Alpha GO 为例的强化学习场景"></p>
<p>&emsp;&emsp;Supervised v.s. Reinforcement：</p>
<ul>
<li>Supervised: Learning from teacher</li>
<li>Reinforcement Learning: Learning from experience</li>
</ul>
<p>&emsp;&emsp;但是 Reinforcement Learning 要求机器和对手下棋直到分出胜负才能得到一个 reward，比如要下 3000w 盘才能学习到一个不错结果，但是没人愿意跟机器下 3000w 盘。所以 AlphaGo 用的是 Supervised Learning + Reinforcement Learning。让机器和另一个机器去下。</p>
<h2 id="learning-a-chat-bot"><a href="#learning-a-chat-bot" class="headerlink" title="learning a chat-bot"></a>learning a chat-bot</h2><p>&emsp;&emsp;常用的是 seq2seq model，但是其实 RL 也可以使用在聊天机器人领域。但是显然也要大量的训练才行，如同 AlphaGO 一样。所以我们也需要训练两个机器人让他们互相对话。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/learning a chat-bot -Supervised v.s. Reinforcement.jpg" alt="learning a chat-bot -Supervised v.s. Reinforcement"></p>
<p>&emsp;&emsp;但是跟围棋不一样，chat-bot 领域比较难定义 reward。因为没人能告诉它，它的言语是否正确，而围棋只需要一个简单的程序即可判断胜负。所以现在的方法就是人为定义了一个 reward，详见 <a href="https://arxiv.org/pdf/1606.01541v3.pdf" target="_blank" rel="noopener">论文</a>。</p>
<h2 id="more-applications"><a href="#more-applications" class="headerlink" title="more applications"></a>more applications</h2><p>&emsp;&emsp;14:03-16:10</p>
<h2 id="Playing-Video-Game"><a href="#Playing-Video-Game" class="headerlink" title="Playing Video Game"></a>Playing Video Game</h2><p>&emsp;&emsp;现在在 RL 领域最成功的案例就是打游戏。</p>
<h2 id="RL-outline"><a href="#RL-outline" class="headerlink" title="RL outline"></a>RL outline</h2><p>&emsp;&emsp;强化学习大致分为两大方法：Policy-based 和 Value-based。Value-based 方法先于 Policy-based 方法。下图是 RL 领域中目前最好的方法——A3C的示意图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/outline.jpg" alt="outline"></p>
<p>&emsp;&emsp;如果想要学习更多的 RL 知识，可以参考下图资源。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/学习更多的RL.jpg" alt="学习更多的RL"></p>
<h2 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h2><p>&emsp;&emsp;之后全是 A3C 算法教学。29:40 开始此节。</p>
<h1 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h1><p>&emsp;&emsp;RL 中有三个主要的 component：Actor，Env，Reward Function。下图是两个例子，Video Game 的 Actor 是手柄，Env 是主机，Reward Function 是“杀一只怪得 20 分”。围棋类似。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/basic component.jpg" alt="basic component"></p>
<p>&emsp;&emsp;在 RL 中，Env 和 Reward Function 是不可控制的，它们在开始训练之前就已经被定义了。唯一能做的是调整 Actor 中的 Policy。<br>&emsp;&emsp;Policy <script type="math/tex">\pi</script> 是一个 network，它具有参数 <script type="math/tex">\theta</script>。比如在打电玩，网络的 input 就是游戏的画面（机器观察到的东西被视为一个向量或矩阵），output 就是每个 action 对应的输出层中的每个神经元。比如电玩中的 action 是“left，right，fire”三个神经元。<br>&emsp;&emsp;以 observation <script type="math/tex">s_1</script> 开始，做出的 Action 以 <script type="math/tex">a_1</script> 表示，获得的 reward 以 <script type="math/tex">r_1</script> 表示。以此类推直到许多轮后，游戏结束。将这一过程称之为 episode。那么 <script type="math/tex">Total \  reward = \sum^T_{t=1} r_t</script>。Actor 存在的目的就是想要 maximize total reward。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/example： video game.jpg" alt="example: video game"></p>
<p>&emsp;&emsp;下图是 Actor, Environment, Reward 的关系，Env 和 Actor 可以看成是神经网络（Env 不一定是方程，也可以是 rule-based），那么所有的输入输出可以看成一个 <strong>Trajectory</strong> <script type="math/tex">\tau</script>，我们可以计算它的条件概率 <script type="math/tex">p_{\theta}(\tau)</script>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/Actor, Environment, Reward.jpg" alt="Actor, Environment, Reward"></p>
<p>&emsp;&emsp;最后还剩下 Reward，我们实际计算的是 expected Reward <script type="math/tex">\bar{R_{\theta}}</script>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/学习笔记/李宏毅强化学习算法笔记/Reward.jpg" alt="Reward"></p>
<p>&emsp;&emsp;之前说过我们需要对 Reward 进行最大化，实际上我们是对 expected Reward（reward 期望值）进行最大化。自然而然想到的方法是 gradient ascend。<br>&emsp;&emsp;<strong>21:00</strong>。</p>
<h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><ul>
<li>Add a Baseline。39:35。</li>
<li>Assign Suitable Credit。35:00<ul>
<li>add discount factor</li>
</ul>
</li>
</ul>
<h1 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h1><p>&emsp;&emsp;首先要区分 on-policy 和 off-policy：</p>
<ul>
<li>on-policy: 我们 learn 的 agent 和与环境交互的 agent 是同一个</li>
<li>off-policy: 不是同一个</li>
</ul>
<p>&emsp;&emsp;</p>
<h1 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h1><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>rl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>UML中的组成、聚合等关系</title>
    <url>/assorted/UML%E4%B8%AD%E7%9A%84%E7%BB%84%E6%88%90%E3%80%81%E8%81%9A%E5%90%88%E7%AD%89%E5%85%B3%E7%B3%BB.html</url>
    <content><![CDATA[<p>&emsp;&emsp;个人认为 UML 中的组成（composition）以及聚合（aggregation）的关系十分混乱，比如网上的一些版本。human 和 head 是一个 composition 关系因为 human 不能脱离 head 而独立存在。但是如果不严格来讲，其实 human 是可以脱离 head 存在的，并且现实的业务中并没有如此严格的设定，表示某一样事物不能离开某物存活。<br>&emsp;&emsp;另外认为 car 和 engine 是一个 aggregation 的关系，因为 car 即使脱离了 engine 也能独立存在。但是又不严格的来讲，car 其实脱离了 engine 就无法行驶了，这到底是否还算 car 存在？由于脱离 engine，car 的最主要的功能消失了，这还能算脱离 engine 依旧可以存在吗？<br>&emsp;&emsp;那么现在的问题就是能否独立存在（head 之于 human，engine 之于 car）的概念该如何定义？这是一个很模糊的概念。human 脱离 head 之后，虽然 human 无法存活，但是 human 作为一件实体，它还是会继续存在在这个世界上。难道以是否能存活来区分吗？那么如果下次改为鸡呢？大家知道有只鸡即使已经脱离了头部，它也依旧存活了数月。那么这个概念到底该如何定义呢？我觉可以暂时使用以下的方式：</p>
<ul>
<li>物体拥有多个功能性组件定义为 aggregation<ul>
<li>如果物体只有一个功能性组件定义为 composition。如 PolicyManager 的功能是管理 Policy</li>
<li>如果物体拥有多个功能性组件但是拥有主要（关键）功能，那么将主要功能组件（可多个组件）定义为 composition，其他为 aggregation。如 car 的主要功能是行驶，那么 engine 和 car 肯定是 composition 的关系，而车载音响不过是 aggregation 的关系。</li>
</ul>
</li>
<li>物体的属性定义为 composition。Tree 拥有 Node 属性</li>
</ul>
<a id="more"></a>
]]></content>
      <categories>
        <category>assorted</category>
      </categories>
      <tags>
        <tag>UML</tag>
      </tags>
  </entry>
  <entry>
    <title>分析PyDial toolkit各个包的功能</title>
    <url>/project/%E5%88%86%E6%9E%90PyDial%20toolkit%E5%90%84%E4%B8%AA%E5%8C%85%E7%9A%84%E5%8A%9F%E8%83%BD.html</url>
    <content><![CDATA[<div class="note warning">
            <p>&emsp;&emsp;此篇基于 <a href="https://yan624.github.io/·论文笔记/55. PyDial：A Multi-domain Statistical Dialogue System Toolkit.html">PyDial：A Multi-domain Statistical Dialogue System Toolkit 论文笔记</a> 和 <a href="https://yan624.github.io/·论文笔记/dilogue/task-oriented/54. A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management.html">A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management 论文笔记</a>。<br>&emsp;&emsp;PyDial 的官方文档在 <a href="http://www.camdial.org/pydial/Docs/" target="_blank" rel="noopener">此处</a>。</p>
          </div>
<h1 id="语音对话系统架构"><a href="#语音对话系统架构" class="headerlink" title="语音对话系统架构"></a>语音对话系统架构</h1><p>&emsp;&emsp;PyDial 的论文中已经描述了系统的架构，本文最上面的 alert 中也已经给出了两篇论文笔记。我在这里做个总结：<br>&emsp;&emsp;PyDial 的总体架构如下图所示。其中主要组件被称为 <strong>Agent</strong>，它封装了所有对话系统模块，以实现基于文本的交互。对话系统模块依赖于由一个<strong>本体</strong>（<strong>Ontology</strong>）定义的领域规范。为了与环境交互，PyDial 提供了三个接口：<strong>Dialogue Server</strong>【允许语音型交互】、<strong>Texthub</strong>【允许输入型交互】和 <strong>User Simulation system</strong>。交互的性能由<strong>评估</strong>（<strong>Evaluation</strong>）组件监视。<br>&emsp;&emsp;下图总计<strong>四</strong>大部分，将在下面的小节中分别阐述。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/the general architecture of PyDial.jpg" alt="The general architecture of PyDial" title="The general architecture of PyDial"></p>
<h2 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h2><p>&emsp;&emsp;主要关注 <strong>Agent</strong> 部分。Agent 负责对话交互，因此内部的架构类似于下图中的内容。Agent 还维护 dialogue sessions。因此，可以通过实例化多个 agents 来支持多个对话。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/Architecture of a modular Spoken Dialoug System.jpg" alt="Architecture of a modular Spoken Dialoug System" title="Architecture of a modular Spoken Dialoug System"></p>
<ul>
<li>Semantic Parser/Semantic Decoder：将文本输入转化为一个语义表征。PyDial 提供了一个基于规则（使用正则表达式）的实现，以及一个基于 SVM 的统计模型，即 the Semantic Tuple Classifier。对于后者，只提供了 CamRestaurants 领域的模型</li>
<li>Belief Tracker：负责维护一个被称为 belief state 的对话状态表征，可用 <strong>the rule-based focus tracker</strong> 实现。该实现与领域无关。所有特定领域的信息都是从本体中提取的</li>
<li>Policy：将 belief state 映射到适合的 system dialogue act。有两种实现方式：<strong>1）</strong>人工制定的策略（应该适用于所有领域）；<strong>2）</strong>Gaussian process (GP) reinforcement learning policy。对于多领域对话来说，策略管理器可以像处理所有其他模块一样处理策略。给定每个用户的输入的领域，然后选择相应的领域策略；<br>3）此外，还可以选择 Gasic et al.(2015b) 提出的 Bayesian committee machine(BCM) 处理程序；4）博主注：目前（2020.1）已经不止这三种方法，还有十数种强化学习策略可供选择。</li>
<li>Semantic Output/Language Generator：将 system dialogue act 转化为文本表示。PyDial 提供了两个实现组件：<strong>1）</strong>对于所有领域来说，提供了一个基于模板（定义规则）的语言生成；2）此外 Wen et al. (2015) 提出了基于 LSTM 的语言生成器，里面包含了 CamRestaurants 领域的预训练模型</li>
<li>Topic Tracker：对于多域功能，需要 topic tracker。如果 Topic Tracker 已为某些用户输入标识了领域，那么它将继续使用该领域，直到识别了新的领域。<strong>因此，并非每个用户输入都必须包含相关关键字</strong>。如果 Topic Tracker <strong>无法在开始时识别领域</strong>，那么它将创建与用户的 meta-dialogue，直到确定初始领域或达到最大重试次数。<a id="more"></a>
</li>
</ul>
<h2 id="与环境交互"><a href="#与环境交互" class="headerlink" title="与环境交互"></a>与环境交互</h2><p>&emsp;&emsp;为了与环境交互，PyDial 提供了三个接口：Dialogue Server【允许语音型交互】、Texthub【允许输入型交互】和 User Simulation system。</p>
<ul>
<li>Texthub：只需将 Agent 连接到终端</li>
<li>Dialogue Server：要启用基于语音的对话，Dialogue Server 允许连接到外部语音客户端。此客户端负责使用 ASR 将输入语音信号映射到文本，并使用语音合成将输出文本映射到语音。注意，语音客户端不是 PyDial 的一部分。PyDial 目前连接到 DialPort（Zhao等人，2016）。</li>
<li>User Simulation：支持在语义层面进行对话模拟。使用的是 agenda-based 用户模拟器（Schatzmann et al., 2006）。</li>
</ul>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>&emsp;&emsp;用于计算对话的评估度量（例如 Task Success）。对于基于强化学习的对话模块来说，评估组件还负责提供 reward。</p>
<h2 id="Ontology"><a href="#Ontology" class="headerlink" title="Ontology"></a>Ontology</h2><p>&emsp;&emsp;Ontology 封装了对话域规范以及对后端数据库（例如，一组餐厅及它们的属性）的访问。它被建模为一个全局对象，大多数对话系统模块和用户模拟器使用它来获取用户的相关信息。</p>
<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>&emsp;&emsp;按照如下顺序分析：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">PyDial</span><br><span class="line">├─belieftracking</span><br><span class="line">├─cedm</span><br><span class="line">├─config</span><br><span class="line">├─curiosity</span><br><span class="line">├─Docs</span><br><span class="line">├─evaluation</span><br><span class="line">├─ontology</span><br><span class="line">├─policy</span><br><span class="line">├─resources</span><br><span class="line">├─scripts</span><br><span class="line">├─semanticbelieftracking</span><br><span class="line">├─semi</span><br><span class="line">├─semo</span><br><span class="line">├─tasks</span><br><span class="line">├─tests</span><br><span class="line">├─topictracking</span><br><span class="line">├─Tutorials</span><br><span class="line">├─usersimulator</span><br><span class="line">├─utils</span><br><span class="line">├─__init__.py</span><br><span class="line">├─Agent.py</span><br><span class="line">├─conf.py</span><br><span class="line">├─DialogueServer.py</span><br><span class="line">├─pydial.py</span><br><span class="line">├─Simulate.py</span><br><span class="line">└─Texthub.py</span><br></pre></td></tr></table></figure></p>
<h1 id="semanticbelieftracking"><a href="#semanticbelieftracking" class="headerlink" title="semanticbelieftracking"></a>semanticbelieftracking</h1><p>&emsp;&emsp;如果你仔细看过 PyDial 就会发现里面还有一个 <code>semanticbelieftracking</code> 包，两个包有点关联，所以此处将两个包放在一起讲解。<br>&emsp;&emsp;由于 <code>semanticbelieftracking</code> 包相对于 <code>belieftracking</code> 包只是多了一个 semantic，所以我一度认为 <code>semanticbelieftracking</code> 继承自 <code>belieftracking</code>，但是后来发现并不是。<code>semanticbelieftracking</code> 包将 semantic decode 和 belief track 视为两个独立问题。<strong>即 SemanticBeliefTracking 由 semantic decode 和 belief track 两部分组成。</strong><br>&emsp;&emsp;它们的关系大致如上述所示，但是有一点不太好。PyDial 类与类之间的关系太乱了，明明有一个类可以继承另一个类，非要将这个类重写一遍，导致我在阅读代码时产生了极大的困扰。另外还有一点就是类的命名问题，我真的看不懂。我不知道为啥要叫 <code>semanticbelieftracking</code>。。。<br>&emsp;&emsp;所以两个包的关系大致如下所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/SemanticBeliefTracking.jpg" alt="SemanticBeliefTracking" title="SemanticBeliefTracking.jpg"></p>
<h1 id="cedm"><a href="#cedm" class="headerlink" title="cedm"></a>cedm</h1><p>&emsp;&emsp;cedm 的全称是 The Conversational Entity Dialogue Model，是一个新颖的对话模型，旨在<strong>解决传统</strong>的单个/多个领域对话<strong>模型在</strong>对复杂对话结构（如关系，relations）<strong>建模能力上的限制</strong>。详见 <a href="https://arxiv.xilesou.top/abs/1901.01466" target="_blank" rel="noopener">Ultes et al., 2019.1</a>。<br>&emsp;&emsp;请注意，已被设计出来的 CEDM 原型实现，在某种程度上，是在尽可能地利用 PyDial 的结构和实现。因此，在实现上有一些限制，将在未来的版本中解决。<br>&emsp;&emsp;CEDM 默认是关闭的，使用方法详见文档。</p>
<h1 id="config"><a href="#config" class="headerlink" title="config"></a>config</h1><div class="note warning">
            <p>&emsp;&emsp;我并没有介绍全，其他的配置文件待补充。</p>
          </div>
<p>&emsp;&emsp;一堆配置文件。PyDial 的文档并没有明确地在某个文档中指出 config 文件夹的用法，只介绍了某一个模块的配置文件该如何配置。但是这样写得太杂了，不易理解，接下来写下自己的理解。<br>&emsp;&emsp;Tut 代表 Tutorials，bcm 代表 Bayesian committee machine，gp 代表 Gaussian Process，hdc 代表 Handcrafted。在官方的教程中经常会看到类似 Tut-gp-CamRestaurants.cfg 的配置文件，这些其实是用于教程的配置文件。<br>&emsp;&emsp;PyDial 的基准环境在 config/pydial_benchmarks 下，由 <a href="https://arxiv.xilesou.top/pdf/1711.11023.pdf" target="_blank" rel="noopener">此篇论文</a> 引进。其中一共有 6 个环境，每个环境下又有三个领域。环境：不同的环境拥有不同的训练参数，领域：不同的领域拥有不同场景，如剑桥餐厅、笔记本电脑和洛杉矶餐厅。</p>
<h1 id="evaluation"><a href="#evaluation" class="headerlink" title="evaluation"></a>evaluation</h1><p>&emsp;&emsp;用于评估对话的性能。</p>
<h1 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h1><p>&emsp;&emsp;policy 是 Agent 中的一部分。<br>&emsp;&emsp;policy 包中包含许多 policy 实现，它们都是历年论文的实现。我估计论文 <strong>《A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management》</strong> 使用这些 policy 实现从而提供了 6 个基准测试环境。<br>&emsp;&emsp;policy 包中的各个类的关系大致如下所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/policy.jpg" alt="policy" title="policy.jpg"></p>
<p>&emsp;&emsp;其中 GPPolicy 的关系如下所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/GPPolicy.jpg" alt="GPPolicy" title="GPPolicy.jpg"></p>
<h1 id="semi"><a href="#semi" class="headerlink" title="semi"></a>semi</h1><p>&emsp;&emsp;semi 全名 Semantic input parser。所有的领域都有一个基于规则的 semantic decoder，但是 CamRestaurants 领域有一个基于统计的 decoder，在该包下可以发现一个 <code>SVMSemi.py</code> 文件，所以实际上是基于 SVM 的。<br>&emsp;&emsp;semi 的各类关系大致如下所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/SemI.jpg" alt="SemI" title="Semantic input parser/Semantic Decoder"></p>
<h1 id="semo"><a href="#semo" class="headerlink" title="semo"></a>semo</h1><p>&emsp;&emsp;semo 全名 Semantic Output。为大多数领域提供了基于模版的语言生成，但是为 CamRestaurants 提供了一个基于 LSTM 的生成器。<br>&emsp;&emsp;semo 的各类关系大致如下所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/SemO.jpg" alt="SemO" title="Semantic Output/Language Generator"></p>
<h1 id="topictracking"><a href="#topictracking" class="headerlink" title="topictracking"></a>topictracking</h1><p>&emsp;&emsp;用于在多领域下追踪主题。各类关系大致如下所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/topictracking.jpg" alt="topictracking"></p>
<h1 id="usersimulator"><a href="#usersimulator" class="headerlink" title="usersimulator"></a>usersimulator</h1><p>&emsp;&emsp;对话模拟。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&emsp;&emsp;经过分析发现，所有的模块最终都是由一个 *Manager 的类来管理的。并且在第一节中，我就介绍了 Agent 由 5 个部分组成，即 Semantic Parser/Semantic Decoder(SemI), Belief Tracker, Policy, Semantic Output/Language Generator(SemO), Topic Tracker。在 belieftracking 一节中已经介绍过，SemI 与 belieftracking 已经合并称为 semanticbelieftracking。所以 Agent 实际上只需管理四个 Manager 即可。<br>&emsp;&emsp;正巧，在 pydial 工具包根目录中发现了 Agent.py 模块，那么可想而知该模块就是 Agent 的管理。它管理的 Manager 如下所示：</p>
<ul>
<li>TopicTrackingManager</li>
<li>SemanticBeliefTrackingManager</li>
<li>PolicyManager</li>
<li>SemOManager</li>
<li>EvaluationManager</li>
</ul>
<p>&emsp;&emsp;由于需要评估对话，所以多了一个 EvaluationManager。总得来说，agent 中各类的关系如下所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/分析PyDial toolkit各个包的功能/Agent.jpg" alt="Agent"></p>
<p>&emsp;&emsp;以上已经是 PyDial 所有的功能，观察上图可以发现，所有功能最终的管理者有三类：ConsoleHub、DialogueServer、SimulationSystem。其中 DialogueServer 用于语音输入，SimulationSystem 用于模拟对话，我们暂且不管。<br>&emsp;&emsp;那么最后就只剩下了 ConsoleHub，它用于处理文本型输入。而 ConsoleHub 由 pydial.py 管理，至此 PyDial 的所有模块几乎都已经理清了，还有个别几个模块我没有整理过，但是它们大部分都只有几个类，并且目前暂时用不上，所以先不整理了。</p>
<h1 id="pydial-py"><a href="#pydial-py" class="headerlink" title="pydial.py"></a>pydial.py</h1><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>project</category>
      </categories>
      <tags>
        <tag>pydial</tag>
      </tags>
  </entry>
  <entry>
    <title>A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/task-oriented/54.%20A%20Benchmarking%20Environment%20for%20Reinforcement%20Learning%20Based%20Task%20Oriented%20Dialogue%20Management.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.xilesou.top/pdf/1711.11023.pdf" target="_blank" rel="noopener">论文地址</a>，论文作者 Casanueva I 等，发表于 2017 年。<br>&emsp;&emsp;此论文基于 PyDial，所以可能需要先看 <a href="https://yan624.github.io/·论文笔记/55. PyDial：A Multi-domain Statistical Dialogue System Toolkit.html">这篇</a>博文。</p>
          </div>
<h1 id="本文出现的名词"><a href="#本文出现的名词" class="headerlink" title="本文出现的名词"></a>本文出现的名词</h1><ul>
<li>自动语音识别——Automatic Speech Recognition, ASR</li>
<li>自然语言理解——Natural Language Understanding, NLU</li>
<li>基于开放域聊天的系统（open-domain chat-based systems）：涉及一般性话题的非目标驱动对话</li>
<li>面向任务的对话系统（task-oriented dialogue systems）：将其做成一个小型电子设备附有非常吸引人的界面，旨在通过自然语言帮助用户实现特定目标。</li>
<li>语音对话系统——Spoken Dialogue Systems, SDS</li>
<li>对话管理——Dialogue Management, DM：belief state tracking and policy<ol>
<li>state tracking</li>
<li>策略模型</li>
</ol>
</li>
<li>自然语言生成——Natural Language Generation, NLG</li>
<li>语音合成——speech synthesis</li>
<li>本体（ontology）：本体是系统数据库的结构化表示，定义了 requestable slots, informable slots and database entries（即用户可以与之交互的实体类型及其属性）</li>
<li>强化学习——Reinforcement Learning, RL</li>
<li>试错过程（trial-and-error process）</li>
<li>高斯过程——Gaussian Process, GP</li>
<li>策略梯度（policy gradients）</li>
<li>Q-learning</li>
<li>通用口语对话测试平台（common testbed for spoken dialogue）</li>
<li>基准测试环境（benchmarking environments）</li>
<li>对话状态跟踪挑战——Dialogue State Tracking Challenges, DSTC</li>
<li>马尔科夫决策过程（MDP）</li>
</ul>
<h1 id="论文摘要翻译"><a href="#论文摘要翻译" class="headerlink" title="论文摘要翻译"></a>论文摘要翻译</h1><p>&emsp;&emsp;对话助理正迅速成为不可或缺的日常助手。<br><a id="more"></a></p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p>&emsp;&emsp;近年来，随着自动语音识别（<strong>Automatic Speech Recognition</strong>, <strong>ASR</strong>）、自然语言理解（<strong>Natural Language Understanding</strong>, <strong>NLU</strong>）和机器学习技术的发展，对话系统受到了学术界和工业界的广泛关注。两个方向已经被深入研究：基于开放域聊天的系统（<strong>open-domain chat-based systems</strong>）和面向任务的对话系统（<strong>task-oriented dialogue systems</strong>）。前者涉及一般性话题的非目标驱动对话。后者将它做成一个小型电子设备附有非常吸引人的界面，其旨在通过自然语言帮助用户实现特定的目标。<br>&emsp;&emsp;<strong>在<u>语音驱动</u>的场景下</strong>，语音对话系统（ <strong>Spoken Dialogue Systems</strong>, <strong>SDS</strong>）通常基于模块化架构（图1），由输入处理模块（<strong>ASR</strong> 和 <strong>NLU</strong> 模块）、对话管理（<strong>Dialogue Management</strong>, <strong>DM</strong>）模块（belief state tracking and policy）和输出处理模块（自然语言生成（<strong>Natural Language Generation</strong>, <strong>NLG</strong>）和语音合成（<strong>speech synthesis</strong>））组成。<strong>SDS</strong> 的领域由本体（<strong>ontology</strong>）定义，本体是系统数据库的结构化表示，定义了 <strong>requestable slots</strong>, <strong>informable slots</strong> and <strong>database entries</strong>（即用户可以与之交互的实体类型及其属性）。这种架构中的对话流程的一部分在附录 A 中的图 2 进行解释。<br><div class="note primary">
            <p>&emsp;&emsp;图中并没有画出 DM，所以 DM 是什么？</p>
          </div></p>
<p>&emsp;&emsp;<strong>DM</strong> 模块是 <strong>SDS</strong> 的核心部分，控制对话的会话流程。<strong>传统的方法大多是基于手工构建的决策树</strong>（对应于 PyDial 论文中的描述，<em>PyDial 包含用于所有领域的基于规则的 semantic decoder……</em>），这覆盖所有可能的对话结果。然而，这种方法不能扩展到更大的领域，也无法适应因 ASR 或 NLU 的误差而引起的噪声输入。因此，人们提出了<strong>数据驱动的方法来自动学习策略</strong>，这无论是从对<em>话语料库中</em>学习或者直接从<em>与人类用户的交互中</em>学习都可以。<br><div class="note primary">
            <p>&emsp;&emsp;那么到底是哪一种方法呢？下面介绍一种尚有缺陷的方法。</p>
          </div></p>
<p>&emsp;&emsp;<strong>监督学习</strong>可用于学习对话策略，<strong>训练策略模型</strong>以“模拟”训练语料库中观察到的响应。然而，这种方法有几个缺点。</p>
<ol>
<li>在口语对话场景中，不能保证训练语料库代表最佳行为。如果不考虑<em>采取某个行动会对未来对话进程造成影响</em>，就可能会导致次优行为。</li>
<li>此外，<strong>由于对话状态空间较大，训练数据集可能缺乏足够的覆盖范围</strong>。</li>
</ol>
<div class="note primary">
            <p>&emsp;&emsp;那么如何解决这些缺点呢？使用强化学习。</p>
          </div>
<p>&emsp;&emsp;为了解决上述问题，该任务经常被归结为规划（控制）问题，使用<strong>强化学习</strong>（<strong>Reinforcement Learning</strong>, <strong>RL</strong>）解决。在这个框架中，系统通过一个由 <em>potentially delayed reward signal</em> 控制的试错过程（<em>trial-and-error process</em>）来学习。因此，<strong>DM</strong> 模块学习计划行动以最大化最终结果。基于高斯过程（<strong>Gaussian Process</strong>, <strong>GP</strong>）的 RL 和 deep RL 等方法的最新进展导致了数据驱动对话建模的重大突破（<em>significant progress</em>），这表明策略梯度（<strong>policy gradients</strong>）和 <strong>Q-learning</strong> 等通用算法可以在具有挑战性的对话场景中取得良好的性能。<br><div class="note primary">
            <p>&emsp;&emsp;那么在对话领域是否可行呢？下一段说明了原因，<strong>动机与相关工作</strong>一节进行了进一步的解释。</p>
          </div></p>
<p>&emsp;&emsp;然而，与其他 RL 领域相比，通用口语对话测试平台（<em>common testbed for spoken dialogue</em>）的缺乏使得比较不同的算法变得困难。最近的 RL 进展很大程度上受到基准测试环境（<strong>benchmarking environments</strong>）的发布的影响，<strong>基准测试环境允许对在类似条件下运行的不同 RL 算法进行公平的比较</strong>。<br><div class="note success">
            <p>&emsp;&emsp;就是说其他 RL 领域取得进展很大程度上取决于基准测试环境的发布，但是在对话领域，目前并没有这种基准测试环境。<br>&emsp;&emsp;<strong>而本篇论文的实验产物其实就是开发了这样一个环境</strong>。</p>
          </div></p>
<p>&emsp;&emsp;本着同样的精神，基于最近发布的 <strong><a href="https://www.aclweb.org/anthology/P17-4013/" target="_blank" rel="noopener">PyDial</a></strong> 多领域 <strong>SDS</strong> 工具包，<strong>本文旨在为开发和评估对话模型提供一套测试平台环境</strong>。为了解释不同场景的巨大变化性，这些环境跨越了 <em>different size domains, different user behaviours and different input channel performance</em>。为了提供一些基线，对一组最具代表性的 DM 强化学习算法进行了评估。基准测试（<strong>benchmark</strong>）和环境实现是在线的，允许开发、实施和评估新的算法和任务。</p>
<h1 id="2-动机与相关工作"><a href="#2-动机与相关工作" class="headerlink" title="2 动机与相关工作"></a>2 动机与相关工作</h1><p>&emsp;&emsp;在过去的十年中，一些强化学习算法被应用到对话策略优化的任务中。然而，由于缺乏一个通用的基准环境，这些算法的评估结果很难进行比较。此外，它们通常只在少数环境中进行评估，因此很难评估它们推广到不同环境的潜力。<br>&emsp;&emsp;在其他领域，如视频游戏（<em>video game playing</em>）和持续控制（<em>continuous control</em>），公共基准环境的发布对该领域的研究起到了极大的推动作用，实现了诸如人类水平的游戏操作或在围棋游戏中击败世界冠军等成就。<br><div class="note info">
            <p>&emsp;&emsp;以下指出对话领域<strong>没有通用的试验台的原因</strong>，以及指出<strong>前些年</strong>部分团队/公司<strong>提出的解决方案有些许瑕疵</strong>。</p>
          </div></p>
<p>&emsp;&emsp;回溯历史，对话策略优化任务没有一个通用的试验台，这有几个原因。</p>
<ol>
<li>……</li>
</ol>
<p>&emsp;&emsp;为了解决这些问题，提出了模拟用户和模拟输入处理信道。这些模型近似<strong>真实用户的行为</strong>和<strong>由 ASR 或 NLU 误差引入的输入信道噪声</strong>。然而，<strong>处理模块的开发需要创建一个模拟对话环境，这需要付出巨大的努力</strong>。尽管一些模拟环境是公开可用的，但它们覆盖的对话领域非常小，而且它们之间缺乏一致性，因此无法进行大规模测试。<br><div class="note success">
            <blockquote><p>处理模块的开发需要创建一个模拟对话环境，这需要付出巨大的努力</p></blockquote><p>&emsp;&emsp;上段话的意思是在做科研时，如果我们致力于研究对话系统的处理模块（如研究语义解析），那么我们还需要一个对话环境以此模拟现实场景（因为只有在现实存在的场景下进行语义解析才有意义）。但是问题是当我们做处理模块时，我们一般都是只专精于该领域，所以我们无法提供一个模拟环境。一般来说我们都会去找一个第三方的模拟环境。但是这样就会造成一个问题。如果有很多团队都在做处理模块的研究，而使用的模拟环境又不是统一的，那么不同团队做出的不同模型就无法进行对比。所以我们需要一个统一的平台。</p>
          </div></p>
<p>&emsp;&emsp;对话任务需要一个共同的试验台，这在对话界是一个众所周知的问题，诸如对话状态跟踪挑战（<strong>Dialogue State Tracking Challenges</strong>, <strong>DSTC</strong>）1 至 5 等倡议是最突出的问题。由于有一个明确的评估标准，这些挑战是可能的。最近，BABI 对话任务和 DSTC6（更名为 dialogue Systems Technology Challenge）旨在为基于端到端文本的对话管理创建一个测试平台。然而，这些任务要么集中在端到端的有监督学习，要么集中在基于 RL 的问答任务中，其中奖励信号（<em>reward signal</em>）在时间上仅延迟几步。</p>
<h1 id="3-强化学习的对话管理"><a href="#3-强化学习的对话管理" class="headerlink" title="3 强化学习的对话管理"></a>3 强化学习的对话管理</h1><p>&emsp;&emsp;讲解 RL 算法，略。</p>
<h2 id="3-3-Pydial"><a href="#3-3-Pydial" class="headerlink" title="3.3 Pydial"></a>3.3 Pydial</h2><p>&emsp;&emsp;PyDial 是一个开源的统计语音对话系统工具包，它提供了图 1 所示的所有对话系统模块的域无关实现，同样也有模拟用户和模拟错误模型的功能（<em>as well as simulated users and simulated error models</em>）。因此，该工具包有希望在相同条件下，创建一组基准环境来比较不同 RL 算法。<strong>PyDial 的主要关注的是面向任务的对话，即根据一些约束条件用户必定找到其匹配的实体</strong>。例如，系统需要向用户提供满足特定需求的笔记本电脑（商店中有的）的描述。<strong><font color="#f07c82">本工作中，PyDial 被用于定义不同的环境，当然 PyDial 的论文中提供了指定这些环境的配置文件</font></strong>。<br><div class="note warning">
            <p>&emsp;&emsp;正如本文最前部分的 alert 中所述。关于 PyDial 可以看<a href="https://yan624.github.io/·论文笔记/55. PyDial：A Multi-domain Statistical Dialogue System Toolkit.html">这篇</a>博文</p>
          </div></p>
<h1 id="4-基准测试任务"><a href="#4-基准测试任务" class="headerlink" title="4 基准测试任务"></a>4 基准测试任务</h1><p>&emsp;&emsp;基于 RL 的 DM 研究通常只在单个或非常小的一组环境中进行评估。这样的测试并没有揭示出算法对不同设置的泛化能力，并且可能容易对特定情况过度拟合。为了测试算法在不同环境中的能力，我们定义了一组任务，它们通过许多维度横跨了广泛的环境，环境的多种维度如下所示：</p>
<ul>
<li>Domain：环境中的第一个维度是领域，定义了三个拥有不同大小的数据库的本体，相当于一个信息搜索任务（即剑桥/洛杉矶餐厅、笔记本电脑这三个）。详见表 2。</li>
<li>Input error：</li>
<li>User model：</li>
<li>Masking mechanism：最后，为了测试算法的学习能力，PyDial 中提供的 action mask 机制在两个任务中被禁用。</li>
</ul>
<p>&emsp;&emsp;18 个不同环境的配置详见表 3。</p>
<h1 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5 实验设置"></a>5 实验设置</h1><p>&emsp;&emsp;本节将解释用于运行基准测试任务的实验设置。</p>
<ol>
<li>Simulated user and input channel</li>
<li><strong>Summary actions and action masks</strong></li>
<li>Model hyperparameters</li>
<li>Handcrafted policy</li>
<li>Reward function and performance metrics</li>
</ol>
<h1 id="6-结果与讨论"><a href="#6-结果与讨论" class="headerlink" title="6 结果与讨论"></a>6 结果与讨论</h1><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>pydial</tag>
        <tag>benchmarking environment</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：PyDial: A Multi-domain Statistical Dialogue System Toolkit</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/task-oriented/55.%20PyDial%EF%BC%9AA%20Multi-domain%20Statistical%20Dialogue%20System%20Toolkit.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P17-4013/" target="_blank" rel="noopener">论文地址</a>，论文作者 S Ultes 等，发表于 2017 年。<br>&emsp;&emsp;此文中出现了许多专业名词，可参考 <a href="https://yan624.github.io/·论文笔记/dilogue/task-oriented/54. A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management.html">此处</a> 加以理解。</p>
          </div>
<h1 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h1><p>&emsp;&emsp;统计口语对话系统（<em>Statistical Spoken Dialogue Systems, Statistical SDS, 统计 SDS</em>）已经存在多年了。然而，访问这些系统一直很困难，因为仍然没有公开的端到端系统实现。为了缓解这个问题，我们提出了 PyDial，一个开源的端到端统计语音对话系统工具包，它为<strong>所有对话系统模块</strong>提供<strong>统计方法</strong>的实现。此外，它还被扩展为提供多领域会话功能。它提供了各个对话系统模块的简单配置、易扩展性和域无关（<em>domain-independent</em>）的实现。该工具包可在 Apache2.0 许可下下载。</p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p>&emsp;&emsp;针对机器去设计语音接口（凭此人机交互）是多年来的研究热点。这些语音对话系统（SDSs）通常<strong>基于模块化的架构</strong>，包括<strong>语音识别</strong>（<em>speech recognition</em>）和<strong>语义解码</strong>（<em>semantic decoding</em>）的<strong>输入处理模块</strong>（<em>input processing modules</em>）、<strong>信念跟踪</strong>（<em>belief tracking</em>）和<strong>策略</strong>（<em>policy</em>）的<strong>对话管理模块</strong>（<em>dialogue management modules</em>）、<strong>语言生成</strong>（<em>language generation</em>）和<strong>语音合成</strong>（<em>speech synthesis</em>）的<strong>输出处理模块</strong>（<em>output processing modules</em>）（见图 1）。<br><div class="note success">
            <p>&emsp;&emsp;以上介绍了 SDS 的架构。下一段介绍了一些 SDS 中不同组件的统计方法示例。</p>
          </div></p>
<p>&emsp;&emsp;统计 SDS 是一个语音接口，其中所有 SDS 模块都基于统计模型（从数据中学习）（与人工规则相比）。对话系统中<strong>不同组件的统计方法</strong>例子可以在以下中找到：</p>
<ul>
<li>……（若干论文），详见原论文 Introduction 第二段。<a id="more"></a>
</li>
</ul>
<p>&emsp;&emsp;尽管对统计 SDS 的研究非常丰富，但是仍然没有通用的平台和开放的工具包。其他工具包实现通常集中在单个模块上（例如：<em>略，详见论文</em>）。提供专门针对统计对话系统（<em>statistical dialogue systems</em>）的工具包将使新进入该领域的人员能够更容易地参与进来，结果更容易比较，研究人员能够专注于他们的特定研究问题，而不是重新实现算法（例如，评估（<em>v.</em>）交互功能中的理解或生成组件的性能）。<br>&emsp;&emsp;因此，为了促进研究，以及使人们更容易参与统计口语对话系统的研究，我们提出了一个多领域统计口语对话系统工具包 <strong>PyDial</strong>。PyDial 是用 Python 实现的，剑桥对话系统团队（<em>Cambridge Dialogue Systems Group</em>）正在积极使用它。<br><div class="note default">
            <p>&emsp;&emsp;那么什么是 PyDial 呢？</p>
          </div></p>
<p>&emsp;&emsp;PyDial 支持多领域的应用程序，在这些应用程序中，对话可能涉及多个不同的主题。这引入了许多新的研究课题，包括广义信念跟踪（<em>generalised belief tracking</em>）（论文略），<em>rapid policy adaptation and parallel learning</em> 和自然语言生成（<em>natural language generation</em>）。<br>&emsp;&emsp;论文的余下部分安排如下：</p>
<ol>
<li>第 2 节介绍了 PyDial 的总体架构（并且将 SDS 架构扩展到多个领域）和 PyDial 的主要应用规范；</li>
<li>第 3 节包含各个对话系统模块的实现细节；</li>
<li>第 4 节列出了可用的领域，其中有两个领域用于第5节中的示例交互；</li>
<li>第 6 节总结了该工具包的主要贡献。</li>
</ol>
<h1 id="2-PyDial-架构"><a href="#2-PyDial-架构" class="headerlink" title="2 PyDial 架构"></a>2 PyDial 架构</h1><p>&emsp;&emsp;本节介绍 PyDial 的结构及其与环境的接口方式。随后，描述了在单个领域上功能的扩展，以允许在多个领域上进行对话。最后，我们讨论了 PyDial 设计的三个关键原则。</p>
<h2 id="2-1-总体系统架构"><a href="#2-1-总体系统架构" class="headerlink" title="2.1 总体系统架构"></a>2.1 总体系统架构</h2><p>&emsp;&emsp;PyDial 的总体架构如图 2 所示。其中主要组件被称为 <strong>Agent</strong>，它位于系统的核心。它封装了所有对话系统模块，以实现基于文本的交互，即输入和输出。对话系统模块依赖于由一个<strong>本体</strong>（<strong>Ontology</strong>）定义的领域规范。为了与环境交互（<strong>博主注</strong>：这里的“与环境交互”应该指的是机器与外部环境交互，如机器与餐厅环境交互、与旅游环境交互、与服装商场进行交互等），PyDial 提供了三个接口：<strong>Dialogue Server</strong>【允许语音型交互】、<strong>Texthub</strong>【允许输入型交互】和 <strong>User Simulation system</strong>。交互的性能由<strong>评估</strong>（<strong>Evaluation</strong>）组件监视。</p>
<ul>
<li>Agent：负责对话交互，因此内部的架构类似于图 1 中的内容。<strong>Agent</strong> 还维护 dialogue sessions，即确保每个输入都<u>发送到</u>（<em>is routed to</em>）正确的对话。因此，<strong>可以通过实例化多个 agents 来支持多个对话</strong>。<ol>
<li>semantic parser：将文本输入转化为一个语义表征</li>
<li>belief tracker：负责维护一个被称为 <strong>belief state</strong> 的对话状态表征（<em>dialogue state representation</em>）</li>
<li>policy：将 <strong>belief state</strong> 映射到适合的 <strong>system dialogue act</strong></li>
<li>semantic output：将 <strong>system dialogue act</strong> 转化为文本表示</li>
<li>topic tracker：对于多域功能，需要 <strong>topic tracker</strong>，其功能将在第 2.2 节中解释</li>
</ol>
</li>
<li>User Simulation：支持在语义层面进行对话模拟，即不使用任何语义解析器或语言生成。这是一种广泛应用于训练和评估基于强化学习算法的技术，因为它避免了昂贵的数据收集练习（<em>data collection exercises</em>）和用户试验（<em>user trials</em>）。它当然只提供了一个近似的真实用户行为，所以通过模拟获得的结果应该谨慎看待！</li>
<li>与环境交互：为了使 Agent 能够与其环境进行通信，PyDial 提供了两种模式：语音和文本<ul>
<li>Texthub：只需将 Agent 连接到终端</li>
<li>Dialogue Server：要启用基于语音的对话，<strong>Dialogue Server</strong> 允许连接到外部语音客户端。此客户端负责使用 ASR 将输入语音信号映射到文本，并使用语音合成将输出文本映射到语音（<em>text to speech</em>, <strong>TTS</strong>）。语音客户端通过 HTTP 交换 JSON 消息连接到对话服务器。请注意，语音客户端不是 PyDial 的一部分。基于云的 ASR 和 TTS 服务可以从 <a href="https://cloud.google.com/speech" target="_blank" rel="noopener">Google</a>、<a href="https://www.microsoft.com/cognitive-services/en-us/speech-api" target="_blank" rel="noopener">Microsoft</a>或 <a href="http://www.ibm.com/watson/developercloud/speech-to-text.html" target="_blank" rel="noopener">IBM</a>等提供商处获得。PyDial 目前连接到 DialPort（Zhao等人，2016），其允许基于语音的交互。</li>
</ul>
</li>
<li>Ontology：除了 <strong>Agent</strong> 和接口组件之外，还有 <strong>Ontology</strong>，Ontology 封装了对话域规范以及对后端数据库（例如，一组餐厅及它们的属性）的访问。它被建模为一个全局对象，大多数对话系统模块和用户模拟器使用它来获取有关用户 <em>actions,  slots, slot values, and system actions</em> 的相关信息。</li>
<li>Evaluation：用于计算对话的评估度量（例如 <strong>Task Success</strong>）。对于基于强化学习的对话模块来说，评估组件还负责提供 <em>reward</em></li>
</ul>
<h2 id="2-2-多领域对话系统架构"><a href="#2-2-多领域对话系统架构" class="headerlink" title="2.2 多领域对话系统架构"></a>2.2 多领域对话系统架构</h2><p>&emsp;&emsp;Agent -&gt; <strong>topic tracker</strong></p>
<h2 id="2-3-主要原则"><a href="#2-3-主要原则" class="headerlink" title="2.3 主要原则"></a>2.3 主要原则</h2><p>&emsp;&emsp;为了使 PyDial 能够轻松地应用于新问题，PyDial 体系结构的设计支持三个关键原则：</p>
<ul>
<li>Domain Independence</li>
<li>Easy Configurability</li>
<li>Extensibility</li>
</ul>
<h1 id="3-实现"><a href="#3-实现" class="headerlink" title="3 实现"></a>3 实现</h1><p>&emsp;&emsp;PyDial 工具包是一个不断开发的研究系统。可从Apache2.0许可下的<a href="http://pydial.org" target="_blank" rel="noopener">网站</a>免费下载。在最初的版本中可以使用以下各种系统模块的实现，不过，到时候会有更多的实现。</p>
<ul>
<li><strong>Semantic Decoder</strong>：为了对输入语句（或n个最佳语句列表，or n-best-list of sentences）进行语义解码，PyDial 提供了一个基于规则（使用正则表达式）的实现，以及一个基于 SVM 的统计模型，即 the Semantic Tuple Classifier（Mairess et al., 2009）。对于后者，只提供了 CamRestaurants 领域的模型。</li>
<li><strong>Belief Tracker</strong>：为了跟踪 belief state，可用 <strong>the rule-based focus tracker</strong>（Henderson et al., 2014）。该实现与领域无关。所有特定领域的信息都是从本体中提取的。</li>
<li><strong>Policy</strong>：负责 policy 的决定执行模块有两种实现方式：<strong>1）</strong>人工制定的策略（应该适用于所有领域）；<strong>2）</strong>Gaussian process (GP) reinforcement learning policy（Gasic and Young, 2014）。对于多领域对话，策略管理器可以像处理所有其他模块一样处理策略。给定每个用户的输入的领域，然后选择相应的领域策略。<br>此外，还可以选择 Gasic et al.(2015b) 提出的 Bayesian committee machine(BCM) 处理程序：<strong>当处理一个领域的 belief state 时，参考其他领域的策略来选择最终的系统操作</strong>。为了实现这一点，belief state 被映射到一个抽象表示，然后允许所有策略访问它。在 PyDial 中，经过训练的策略可以在 committee-based handler 和 standard policy manager handler 之间移动。即在 committee 外部（在单域或多域设置中）接受训练的策略可以在 committee 内部使用，反之亦然。</li>
<li><strong>Language Generator</strong>：为了将 semantic system action 映射为文本，PyDial 提供了两个实现组件：<strong>1）</strong>对于所有领域来说，提供了一个基于模板（定义规则）的语言生成；<strong>2）</strong>此外 Wen et al. (2015) 提出了基于 LSTM 的语言生成器，里面包含了 CamRestaurants 领域的预训练模型</li>
<li><strong>Topic Tracker</strong>：PyDial 提供了一个基于关键字的 Topic Tracker 实现。如果 Topic Tracker 已为某些用户输入标识了领域，那么它将继续使用该领域，直到识别了新的领域。<strong>因此，并非每个用户输入都必须包含相关关键字</strong>。如果 Topic Tracker <strong>无法在开始时识别领域</strong>，那么它将创建与用户的 meta-dialogue，直到确定初始领域或达到最大重试次数。</li>
<li><strong>Evaluation</strong>：为了评估对话，目前已经实现了两个 success-based 模块。（下面的话部分看不懂，索性不翻了—）<blockquote>
<p>The objective task success evaluator compares the constraints and requests the system identifies with the true values. The latter may either be derived from the user simulator or, in real dialogues, by specifying a predefined task. For real dialogues, a subjective task success evaluator may also be applied which queries the user about the outcome of the dialogue.</p>
</blockquote>
</li>
<li><strong>User Simulation</strong>：模拟用户的实现使用 agenda-based 用户模拟器（Schatzmann et al., 2006）。该模拟器包含 user model 和 error model，从而创建 a n-best-list of user acts 来模拟噪声语音信道。通过使用一组普遍适用的参数，模拟器可以应用于所有领域。领域特定的信息取自本体。</li>
</ul>
<h1 id="4-一些领域"><a href="#4-一些领域" class="headerlink" title="4 一些领域"></a>4 一些领域</h1><p>&emsp;&emsp;<strong>PyDial 的主要关注的是面向任务的对话，即根据一些约束条件用户必定找到其匹配的实体</strong>。找到实体后，用户可以请求其他信息。为应付这些场景，PyDial 预先加载了总共十个不同复杂度的领域：</p>
<ul>
<li>略</li>
</ul>
<p>&emsp;&emsp;如前所述，所有 <strong>policy</strong> 实现以及 <strong>belief tracker</strong> 和 <strong>user simulator</strong> 的实现都独立于领域之外。因此，在<strong>所有领域</strong>都可以进行交互的模拟。此外，<strong>semantic decoder</strong> 和 <strong>language generator</strong> 在某种程度上依赖于特定领域的实现。PyDial 包含用于<strong>所有领域</strong>的基于规则的 <strong>semantic decoder</strong> 和一个用于 <strong>CamRestaurants</strong> 的统计解码器（<em>statistical decoder</em>）。此外，PyDial 还为<strong>大多数领域</strong>提供了基于模板的语言生成，并为 <strong>CamRestaurants</strong> 提供了基于 LSTM 的生成器。因此，在 CamRestaurants 领域，实现统计对话完全有可能。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>pydial</tag>
      </tags>
  </entry>
  <entry>
    <title>交流</title>
    <url>/%E4%BA%A4%E6%B5%81.html</url>
    <content><![CDATA[<p>&emsp;&emsp;</p>
]]></content>
  </entry>
  <entry>
    <title>在使用完文件后，未调用close()导致数据一直有问题</title>
    <url>/IT-stuff/python/%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%AE%8C%E6%96%87%E4%BB%B6%E5%90%8E%EF%BC%8C%E6%9C%AA%E8%B0%83%E7%94%A8close-%E5%AF%BC%E8%87%B4%E6%95%B0%E6%8D%AE%E4%B8%80%E7%9B%B4%E6%9C%89%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>&emsp;&emsp;在使用完文件后，未调用close()。貌似导致了最后一条数据没有写入文件，我在核对数据的条数时，一直显示少了点，但是不知道哪里有问题，最后加上close()之后发现数据没问题了。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Machine Translation of Rare Words with Subword Units</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/representation%20learning/53.%20Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subword%20Units.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener">论文地址</a>，论文作者 Rico Sennrich 等，发表于 2015 年。<br>&emsp;&emsp;注：本文解决的是机器翻译问题。</p>
          </div>
<h1 id="论文摘要翻译"><a href="#论文摘要翻译" class="headerlink" title="论文摘要翻译"></a>论文摘要翻译</h1><p>&emsp;&emsp;神经机器翻译（NMT）模型通常使用固定的词表，但是翻译是一个开放词表的问题。以前的工作是 <strong>backing off to a dictionary</strong> 来解决 OOV 词的翻译问题。本文介绍了一种新的方法，即 <strong>BPE</strong>。这是<strong>基于这样一种直觉：不同的<a href="http://www.bing.com/dict/word%20classes" target="_blank" rel="noopener">词类</a>（word classes）可以通过使用比该词更小的单元进行翻译，例如姓名（通过字符复制或音译）、复合词（通过构词翻译）以及同源词和外来词（通过语音和形态转换）。</strong><br>&emsp;&emsp;本文讨论了不同的分词技巧的实用性，包括简单的 <strong>character n-gram</strong> 模型（fasttext 就是用的这个）和基于 <strong>BPE</strong> 压缩算法模型。最终实验证明在 WMT 15 英-&gt;德和英-&gt;法 翻译任务上， subword 模型（character n-gram 和 BPE）比 back-off dictionary 各自提高了 1.1 和 1.3 BELU 分数。<br><div class="note warning">
            <p>&emsp;&emsp;接下来的 <strong>Introduction</strong> 和 <strong>Neural Machine Translation</strong> 章节暂且略过。</p>
          </div></p>
<a id="more"></a>
<h1 id="子词翻译"><a href="#子词翻译" class="headerlink" title="子词翻译"></a>子词翻译</h1><p>&emsp;&emsp;很多单词都能够使用单词的 subword 单元进行翻译。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ul>
<li>如果字母表不同，那就音译(Durrani et al., 2014)</li>
<li>Character-based translation(Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012)</li>
<li>……</li>
</ul>
<h2 id="※-Byte-Pair-Encoding-BPE"><a href="#※-Byte-Pair-Encoding-BPE" class="headerlink" title="※ Byte Pair Encoding(BPE)"></a>※ Byte Pair Encoding(BPE)</h2><p>&emsp;&emsp;BPE(Gage, 1994) 是一个简单的数据压缩技巧，其迭代地使用单个、未被使用的 byte 替换在一个序列中出现最频繁的  pair of bytes（<strong>博主注：此为 1994 年 Gage 论文中的算法步骤，无法理解就跳过吧，有兴趣的看<a href="https://blog.csdn.net/qq_27590277/article/details/88343988" target="_blank" rel="noopener">这篇</a></strong>）。我们将这种算法引入进子词中。<strong>Instead of merging frequent pairs of bytes, we merge characters or character sequences</strong>.</p>
<ol>
<li>首先我们使用字符级的词表初始化符号词表，将每个单词表示为一个字符序列（<strong>如 ‘word’ 表示为 ‘w o r d’</strong>），外加一个特殊的单词结尾符号“·”，这允许我们在翻译后恢复原始的分词。</li>
<li>我们迭代地计算所有符号对，并用新的符号“AB”替换最常出现的符号对(‘A’, ‘B’)。每次合并操作都产生一个以 character n-gram 表示的新符号。高频的 character n-grams（或整个单词）最终合并为单个符号，因此 BPE 不需要 shorlist（这里的 shortlist 应该指的是 1994 年论文中的 single and unused byte，即单个未被使用过的 byte）。</li>
<li>最终符号词表的大小等于<strong>最初词表的大小加上合并操作的数量</strong>，后者是算法的唯一超参数。</li>
</ol>
<p>&emsp;&emsp;举个例子，以此辅助理解第一步。如下所示，有一个 vocab 其中包含四个单词 ‘low’, ‘lower’, ‘newest’, ‘widest’，它们的词频分别为 5 2 6 3。<br><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">vocab</span> = &#123;<span class="string">'l o w &lt;/w&gt;'</span> : <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span> : <span class="number">2</span>,<span class="string">'n e w e s t &lt;/w&gt;'</span>:<span class="number">6</span>, <span class="string">'w i d e s t &lt;/w&gt;'</span>:<span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;将单词分割，使得当前单词 <script type="math/tex">w_i</script> 和下一个单词 <script type="math/tex">w_{i+1}</script> 组成一对（pair），并记录 pairs 的词频。其中符号 <code>&lt;/w&gt;</code> 其实就是 <code>·</code>，代表一个单词的结束。<br><figure class="highlight clean"><table><tr><td class="code"><pre><span class="line">&#123;(<span class="string">'l'</span>, <span class="string">'o'</span>): <span class="number">7</span>, (<span class="string">'o'</span>, <span class="string">'w'</span>): <span class="number">7</span>, (<span class="string">'w'</span>, <span class="string">'&lt;/w&gt;'</span>): <span class="number">5</span>, ...&#125;</span><br></pre></td></tr></table></figure></p>
<div class="note info">
            <p>&emsp;&emsp;完整的代码在论文中就有，这里就不加赘述了。代码很短。</p>
          </div>
<p>&emsp;&emsp;为了提高效率，我们并不考虑跨单词边界（cross word boundaries）的 pairs（博主注：这个 cross word boundaries 可能指一个 pair 中的两个符号来源于不同单词）。实际上，我们通过对所有的 pairs 进行索引操作从而提高了效率。</p>
<h2 id="joint-BPE"><a href="#joint-BPE" class="headerlink" title="joint BPE"></a>joint BPE</h2><p>&emsp;&emsp;我们以两种方式应用 BPE：</p>
<ol>
<li>学习两个独立的编码，一个对应 source 词表，一个对应 target 词表；</li>
<li>关联两个词表进行学习，称之为 joint BPE。</li>
</ol>
<p>&emsp;&emsp;<strong>实际上 joint BPE 只是简单地将训练集的 source 词表和 target 词表连接起来，以学习 joint BPE。</strong></p>
<blockquote>
<p>The former has the advantage of being more compact in terms of text and vocabulary size, and having stronger guarantees that each subword unit has been seen in the training text of the respective language, whereas the latter improves consistency between the source and the target segmentation.<br>If we apply BPE independently, the same name may be segmented differently in the two languages, which makes it harder for the neural models to learn a mapping between the subword units. To increase the consistency between English and Russian segmentation despite the differing alphabets, we transliterate the Russian vocabulary into Latin characters with ISO-9 to learn the joint BPE encoding, then transliterate the BPE merge operations back into Cyrillic to apply them to the Russian training text.</p>
</blockquote>
<h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>bpe</tag>
      </tags>
  </entry>
  <entry>
    <title>A Sketch-Based System for Semantic Parsing</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/52.%20A%20Sketch-Based%20System%20for%20Semantic%20Parsing.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.xilesou.top/pdf/1909.00574.pdf" target="_blank" rel="noopener">论文地址</a>，论文作者 Zechang Li 等，发表于 2019 年 9 月。</p>
          </div>
<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><p>&emsp;&emsp;</p>
<a id="more"></a>]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>语义解析KBQA</tag>
        <tag>MSParS</tag>
        <tag>sketch-based</tag>
      </tags>
  </entry>
  <entry>
    <title>A Transformer-based Semantic Parser for NLPCC-2019 Shared Task 2</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/51.%20A%20Transformer-based%20Semantic%20Parser%20for%20NLPCC-2019%20Shared%20Task%202.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="http://tcci.ccf.org.cn/conference/2019/papers/EV15.pdf" target="_blank" rel="noopener">论文地址</a>，论文作者 D Ge 等，发表于 2019 年。</p>
          </div>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>&emsp;&emsp;使用 BPE、Sharing Vocab、Synthetic Training Instance 极大地提高了准确率。</p>
<h1 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h1><p>&emsp;&emsp;seq2seq 方法将语义解析形式化为一个翻译任务，即将一个句子转换为其对应的 lf（logical form）。然而，在缺少大规模标注过的数据集的情况下，即使在一流的 seq2seq 模型（如 Transformer）中也会遇到数据稀疏的问题。为了解决这个问题，本文探索了<strong>三种广泛应用于<u>神经机器翻译</u>的技术</strong>，以更好地适应 seq2seq 模型的语义分析任务。</p>
<ol>
<li>byte pair encoding (<strong>BPE</strong>): 将单词分割成子词（subword），将稀有单词转换成高频的子词；</li>
<li>我们在 source 和 target 共享词表（<strong>Sharing Vocab</strong>）；</li>
<li>我们定义启发式规则生成合成的实例，以提高训练集的覆盖率（<strong>Synthetic Training Instance</strong>）。<a id="more"></a>
</li>
</ol>
<h1 id="建立基准模型"><a href="#建立基准模型" class="headerlink" title="建立基准模型"></a>建立基准模型</h1><p>&emsp;&emsp;使用 Transformer 建立基准模型。</p>
<h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p>&emsp;&emsp;在 MSParS 数据集中的每一个实例都是一个包含 4 个元素的元组，包括 question, its logical form, parameters, and question type。在本论文中我们只使用问题和其对应的 lf 来训练我们的解析模型，忽视 parameters and question type，因为它们没有被评估过。实验步骤是：question —fed-into—&gt; encoder, lf —fed-into—&gt; decoder。<br>&emsp;&emsp;<strong>注意在 lf 中，一个实体被表示一个由多个单词组成并以“_”相连的字符串</strong>。在预处理中，我们将实体分割成它对应的单词和“_”。例如，lf: <code>( lambda ?x ( mso:film.film.art director “ i see you ” from avatar ?x ) )</code>，在后处理中，我们只需要将“ _ ”替换为“_”即可。<br>&emsp;&emsp;我们还尝试将实体类型的字符串拆分为多个片段。例如，<code>mso:film.film.art director</code>被分为<code>mso : film . film . art director</code>。然而，我们的初步实验表明，这对性能有轻微的影响。</p>
<h2 id="seq2seq-model"><a href="#seq2seq-model" class="headerlink" title="seq2seq model"></a>seq2seq model</h2><p>&emsp;&emsp;介绍什么是 Transformer，略。</p>
<h2 id="生成合成的训练实例"><a href="#生成合成的训练实例" class="headerlink" title="生成合成的训练实例"></a>生成合成的训练实例</h2><p>&emsp;&emsp;监督机器学习算法容易出现数据不平衡问题。在 MSParS 数据集中，我们发现实体类型包含<strong>偏态分布</strong>(Skewed distribution)，例如，实体类型 mso:film.actor.film 包含大多数实体实例，共有 1832 个，而实体类型 mso:barball.batting statistics.slugging_pct 只有一个实体实例。<strong>在这样一种数据集上训练的 seq2seq 模型可能会被数量多的实体类型的训练实例所淹没，而数量小的实体类型的参数则没有很好的学习</strong>。由于泛化能力有限，所得到的模型容易在测试集上获得相对较差的性能。<br>&emsp;&emsp;为了解决这一数据不平衡的问题，我们从以下两个角度生成<strong>合成的训练实例</strong>。</p>
<ul>
<li>Entity-based: 给定一个来自原训练集中的一个句子和其对应的 lf，我们选择句子中的一个实体 A，将其替换成一个随机的实体 B，A 与 B 拥有相同的实体类型（<strong>博主注</strong>：原文中是 entity type，但是我认为用 entity type 不恰当，因为在 MSParS 中确实存在着一个 entity type，与前面提到的重名了，我觉得叫 realation 或者 predicate 更合适）。如：<br><strong>Original pair</strong><br>Sentence: movies jim bob duggar has done<br>Logical Form: ( lambda ?x ( mso:film.actor.film jim_bob_duggar ?x ) )<br><strong>Synthetic pair</strong><br>Sentence: movies marisa tomei has done<br>Logical Form: ( lambda ?x ( mso:film.actor.film marisa_tomei ?x ) )</li>
<li>Labeled-based: 选择一个拥有多个实体类型的实体，将其的实体类型替换为其他一个有效的实体。如下所示，<code>&quot;_i_see_you_&quot;_from_avatar</code> 拥有多个实体类型，我们随机的选择另一个实体类型（不能是 film.film.art_director）进行替换。<br><strong>Original pair</strong><br>Sentence: who is film art directors of “ i see you “ from avatar<br>Logical Form: ( lambda ?x ( mso:film.film.art_director “_i_see_you_“_from_avatar ?x ) )<br><strong>Synthetic pair</strong><br>Sentence: who is film art directors of “ i see you “ from avatar<br>Logical Form: ( lambda ?x ( mso:film.film.editor “_i_see_you_“_from_avatar ?x ) )</li>
</ul>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;本节中，1）我们首先介绍使用的<strong>数据集</strong>。2）然后描述了实验中我们<strong>模型的设置</strong>。3）之后，将我们的系统与其他参与的系统进行了<strong>比较研究</strong>。（博主注：数据集的介绍和模型的设置我直接跳过了）</p>
<h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>&emsp;&emsp;测试集未提供给参赛队伍，组织者根据某一标准将测试集分割，选择一个 hard subset。所以每个团队都有两个结果：full set score and hard subset score。<br>&emsp;&emsp;评估标准为 accuracy(ACC)，即生成的逻辑形式与正确的逻辑形式完全吻合。<br>&emsp;&emsp;参数设置略。为了克服数据稀疏的问题，在所有的问题中，我们跟随 <a href="https://www.ijcai.org/proceedings/2019/0691.pdf" target="_blank" rel="noopener">Ge et al</a>，在输入和输出都<strong>共享词表</strong>。为了解决稀有单词的翻译，我们通过 <strong>BPE</strong>（这篇论文暂时找不到） 将单词分割为 subword。我们对最后 20 个模型的参数进行平均，以提高性能。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>&emsp;&emsp;对语义解析来说，这显示了对解决数据稀疏的问题有两个办法。<strong>BPE</strong> 和 <strong>vocabulary sharing</strong>。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td><strong>85.93</strong></td>
</tr>
<tr>
<td>-BPE</td>
<td>54.90</td>
</tr>
<tr>
<td>-Sharing Vocab.</td>
<td>84.00</td>
</tr>
<tr>
<td>-Both</td>
<td>52.47</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;生成合成的训练实例的方法从本质上增加了我们训练集实例的数量。如下表所示，添加生成合成的训练实例的方法后，数量几乎翻了一倍，并且两种方法都取得了相似的性能提升，这表明我们的两种方法在提高训练实例覆盖率方面是有效的。（<strong>博主注</strong>：ACC 几乎与 baseline 相等，训练集翻了 3 倍多，我佛了）然而，这两种方法的覆盖率存在重叠。在一种方法存在的情况下，另一种方法实现有限或无改进。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th># Instances</th>
<th>ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td>Original</td>
<td>63,826</td>
<td>85.93</td>
</tr>
<tr>
<td>+Entity-based</td>
<td>137,198</td>
<td>86.78</td>
</tr>
<tr>
<td>+Label-based</td>
<td>140,485</td>
<td>86.94</td>
</tr>
<tr>
<td>+Both (our final model)</td>
<td>213,857</td>
<td><strong>86.96</strong></td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;我们还将我们的最终系统与表4中其他参与者的系统进行了比较。从结果可以看出，我们的最终系统达到了最高的性能，特别是在 hard subset 上。这说明了我们的基于 seq2seq 的语义分析是可行和有效的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>ACC on full set</th>
<th>ACC on hard subset</th>
</tr>
</thead>
<tbody>
<tr>
<td>Soochow_SP (this paper)</td>
<td>85.68</td>
<td>57.43</td>
</tr>
<tr>
<td>NP-Parser</td>
<td>83.73</td>
<td>51.93</td>
</tr>
<tr>
<td>WLIS</td>
<td>82.53</td>
<td>47.83</td>
</tr>
<tr>
<td>Binbin Deng</td>
<td>68.82</td>
<td>35.41</td>
</tr>
<tr>
<td>kg_nlpca_ai_lr</td>
<td>30.79</td>
<td>14.89</td>
</tr>
<tr>
<td>TriJ</td>
<td>26.77</td>
<td>14.49</td>
</tr>
</tbody>
</table>
</div>
<h1 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h1><p>&emsp;&emsp;分析为什么预测出错。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>语义解析KBQA</tag>
        <tag>MSParS</tag>
      </tags>
  </entry>
  <entry>
    <title>Enriching Word Vectors with Subword Information</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/representation%20learning/50.%20Enriching%20Word%20Vectors%20with%20Subword%20Information.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/abs/1607.04606.pdf" target="_blank" rel="noopener">论文地址</a>，作者 Piotr Bojanowski et al.，发表于 2016 年。</p>
          </div>
<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;现在流行的模型对单词表征的学习忽视了词法（morphology of word），它们直接给单词分配了不同的向量。这有一定的局限性，尤其对大规模词表并且含有大量稀有单词的语言。本论文提出基于 <strong>skipgram model</strong> 的方法，每个单词都被表示为一个 <strong>character n-grams</strong>（博主注：注意是 character，不是 word）词袋。每个 character n-grams 有一个向量，而单词由这些表征相加表示（即 e(where) = e(wh) + e(whe) + e(her) + e(ere) + e(re)，e() 表示 character n-grams 对应的向量）。<br>&emsp;&emsp;模型<strong>快</strong>，且可以计算那些不<strong>在训练数据中的单词表征</strong>（OOV 单词）。<br><a id="more"></a></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;介绍了很多关于词嵌入的工作，可以参考。</p>
<h2 id="Morphological-word-representations"><a href="#Morphological-word-representations" class="headerlink" title="Morphological word representations"></a>Morphological word representations</h2><h2 id="Character-level-features-for-NLP"><a href="#Character-level-features-for-NLP" class="headerlink" title="Character level features for NLP"></a>Character level features for NLP</h2><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>&emsp;&emsp;首先提出通用模型，阐述我们如何训练词向量。然后提出 subword 模型，最终描述我们如何处理 character n-grams 的词表。</p>
<h2 id="通用模型"><a href="#通用模型" class="headerlink" title="通用模型"></a>通用模型</h2><p>&emsp;&emsp;</p>
<h2 id="Subword模型"><a href="#Subword模型" class="headerlink" title="Subword模型"></a>Subword模型</h2><p>&emsp;&emsp;<strong>对每一个单词赋予一个不同的向量这忽视了单词内在的结构</strong>。为了考虑这一信息，本节提出一个不同的<strong>评分函数</strong>（scoring function） <strong>s</strong>。<br>&emsp;&emsp;<strong>1）</strong>每一个单词 w 都被表示为一个 character n-gram 的词袋。<strong>2）</strong>我们在单词的开始和结尾增加了特殊的边界符号 <code>&lt;</code> 和 <code>&gt;</code>，以便<strong>区分来自其他单词的前缀和后缀</strong>。<strong>3）</strong>我们也将单词 w 本身放入了它的 n-grams 集合，以学习每一个单词（除了 character n-grams）的表征。以单词 <em>where</em> 和 <script type="math/tex">n = 3</script> 为例，它被表示为该 character n-grams：</p>
<script type="math/tex; mode=display">
<wh, whe, her, ere, re></script><p>&emsp;&emsp;以及它的特殊序列：</p>
<script type="math/tex; mode=display">
<where></script><p>&emsp;&emsp;注意序列 <code>&lt;her&gt;</code> 对应于单词 <em>her</em>。单词 <em>her</em> 不同于来自单词 <em>where</em> 的 trigram <em>her</em>。在实践中，我们提取所有 <script type="math/tex">3 <= n <= 6</script> 的 n-grams（<strong>博主注</strong>：如果我没理解错，是提取了所有的 3~6-grams）。这是一种非常简单的方法，并且可以考虑不同的 n-grams 集合（<strong>博主注</strong>：为什么可以考虑不同的 n-grams？因为一个单词他们提取了所有的 3~6-grams），例如取所有前缀和后缀。<br>&emsp;&emsp;假设给出一个 G 大小 n-grams 词表。给定一个单词 w，将其表示为 <script type="math/tex">G_w \subset \{1, \dots, G\}</script>，其中 n-grams 集合 <script type="math/tex">G_w</script> 会出现在 w 中。我们将一个向量表征 <script type="math/tex">\boldsymbol{z_g}</script> 与每一个 n-gram <script type="math/tex">g</script> 关联。而一个单词由其 n-grams 的向量表征相加表示。因此我们获得评分函数（<strong>博主注</strong>：这应该在计算相似度）：</p>
<script type="math/tex; mode=display">
\boldsymbol{
    s(w,c) = \sum_{g \in G_w} z^T_g v_c
}</script><p>&emsp;&emsp;这个简单的模型允许<strong>跨单词共享</strong>表征，从而允许学习<strong>稀有单词</strong>可靠的表征。（<strong>博主注</strong>：由于每个单词都被拆分开来，这样一个稀有单词就有很大概率可以由细碎的 n-grams 表示）<br>&emsp;&emsp;<strong>为了限制我们模型的内存需求</strong>，巴拉巴拉，我没看懂什么意思，大致意思是由于 n-grams 诞生的 token 太多了，所以需要使用 hash 的技巧来缩小存储空间。使用一个 token 来查找其对应的索引非常耗时，而使用 hash 算法就快多了。参考了：</p>
<ul>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-05-3" target="_blank" rel="noopener">fastText，智慧与美貌并重的文本分类及向量化工具</a></li>
</ul>
<h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><ol>
<li>baseline：几乎所有实验的基线都为 C 实现的 <a href="https://code.google.com/archive/p/word2vec" target="_blank" rel="noopener">word2vec 包</a>，除了论文中的 5.3 节；</li>
<li>optimization：对前面（<strong>博主注</strong>：通用模型一节中）提出的负对数似然估计，进行 <strong>SGD</strong> 优化。在基线的 skipgram 模型中，我们使用<strong>基于步长的线性衰减</strong>。给定一个包含 T 个单词的训练集，并且在上传递的次数等于 P，则 t 时刻的步长等于 <script type="math/tex">\gamma_0 (1 - \frac{t}{TP})</script>，其中 <script type="math/tex">\gamma_0</script> 是一个固定的参数（<strong>博主注</strong>：<strong>这部分的线性衰减没看懂什么意思</strong>）。我们通过使用 <strong><a href="http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent" target="_blank" rel="noopener">Hogwild</a></strong>（Recht et al., 2011）并行地执行优化，所有线程以异步方式共享参数和更新向量；</li>
<li>实现细节：</li>
<li>datasets：</li>
</ol>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>&emsp;&emsp;在以下几个方面评估我们的模型：</p>
<ol>
<li>Human similarity judgement</li>
<li>Word analogy tasks</li>
<li>Comparison with morphological representations：与顶尖模型的比较</li>
<li>Effect of the size of the training data</li>
<li>Effect of the size of n-grams</li>
<li>从我们的模型中获取到的词向量在语言模型任务中的评估</li>
</ol>
<h1 id="定性分析"><a href="#定性分析" class="headerlink" title="定性分析"></a>定性分析</h1><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>fasttext</tag>
      </tags>
  </entry>
  <entry>
    <title>学习意见</title>
    <url>/%E5%AD%A6%E4%B9%A0%E6%84%8F%E8%A7%81.html</url>
    <content><![CDATA[<h1 id="推荐的论文"><a href="#推荐的论文" class="headerlink" title="推荐的论文"></a>推荐的论文</h1><div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>技术</th>
</tr>
</thead>
<tbody>
<tr>
<td>tricks</td>
<td></td>
</tr>
<tr>
<td>模型</td>
<td></td>
</tr>
<tr>
<td>优化器</td>
<td></td>
</tr>
<tr>
<td>word embedding</td>
<td>fasttext</td>
</tr>
<tr>
<td>nlp任务模型</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>参考：</p>
<ol>
<li><a href="https://www.zhihu.com/question/31785984" target="_blank" rel="noopener">深度学习入门必看的书和论文？有哪些必备的技能需学习？</a>，直接定位作者：<strong>景略集智</strong>。<a id="more"></a>
</li>
</ol>
<h2 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h2><ol>
<li><a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">Improving neural networks by preventing co-adaptation of feature detectors</a>，论文作者 Geoffrey Hinton 等人，Hinton 提出了 <strong>Dropout</strong>。</li>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="noopener">Dropout: a simple way to prevent neural networks from overfitting</a>，论文作者 Srivastava Nitish，对 <strong>dropout</strong> 的理解。</li>
<li><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch normalization: Accelerating deep network training by reducing internal covariate shift</a>，论文作者 Ioffe Sergey 和 Christian Szegedy，提出 <strong>Batch normalization</strong>。</li>
<li><a href="https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&amp;utm_medium=refer&amp;utm_campaign=promote" target="_blank" rel="noopener">Layer normalization</a>，作者 Jamie Ryan Kiros 和 Geoffrey E. Hinton 等人，本篇论文是对 Batch Normalization 的进一步研究成果，提出 <strong>Layer normalization</strong>。</li>
</ol>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ol>
<li><a href="https://www.semanticscholar.org/paper/Binarized-Neural-Networks%3A-Training-Deep-Neural-and-Courbariaux-Hubara/6eecc808d4c74e7d0d7ef6b8a4112c985ced104d" target="_blank" rel="noopener">Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1</a>，论文作者 Courbariaux Matthieu 等，提出了一种速度非常快的新模型。</li>
<li><a href="https://arxiv.org/pdf/1608.05343.pdf" target="_blank" rel="noopener">Decoupled neural interfaces using synthetic gradients</a>，论文作者 Jaderberg Max 等，论文提出了一种非常创新的训练方法。</li>
</ol>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><ol>
<li><a href="http://proceedings.mlr.press/v28/sutskever13.pdf" target="_blank" rel="noopener">On the importance of initialization and momentum in deep learning</a>，作者 Sutskever Ilya 等，论文提出了 <strong>Momentum optimizer</strong>。</li>
<li><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">Adam: A method for stochastic optimization</a>，作者 Kingma Diederik 等，提出 Adam。</li>
</ol>
<h2 id="word-embedding"><a href="#word-embedding" class="headerlink" title="word embedding"></a>word embedding</h2><ol>
<li><a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="noopener">Enriching Word Vectors with Subword Information</a>，作者 Piotr Bojanowski 和 Edouard Grave，提出了 <strong>fasttext</strong>。<a href="https://github.com/facebookresearch/fastText/" target="_blank" rel="noopener">代码地址</a>，<strong><a href="https://yan624.github.io/·论文笔记/representation learning/50. Enriching Word Vectors with Subword Information.html">论文笔记</a></strong>。应用场景：处理 OOV 词或者低频词。</li>
</ol>
<h2 id="nlp任务模型"><a href="#nlp任务模型" class="headerlink" title="nlp任务模型"></a>nlp任务模型</h2><ol>
<li><a href="https://arxiv.org/pdf/1308.0850.pdf" target="_blank" rel="noopener">Generating sequences with recurrent neural networks</a>，论文作者 Graves Alex，使用 RNN 进行生成序列。</li>
<li><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Learning phrase representations using RNN encoder-decoder for statistical machine translation</a>，论文作者 Cho Kyunghyun，第一篇 <strong>seq2seq</strong> 论文。</li>
<li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">Sequence to sequence learning with neural networks</a>，论文作者 Sutskever Ilya 等人。</li>
<li><a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Memory networks</a>，论文作者 Weston Jason 和 Sumit Chopra。</li>
<li><a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">End-to-end memory networks</a>，论文作者 Sukhbaatar Sainbayar 和 Jason Weston。</li>
<li><a href="http://web.eecs.utk.edu/~ielhanan/courses/ECE-692/Bobby_paper1.pdf" target="_blank" rel="noopener">Long short-term memory</a>，LSTM。</li>
<li><a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" target="_blank" rel="noopener">Pointer networks</a>，作者 Vinyals Oriol 和 Meire Fortunato 等。</li>
<li><a href="https://arxiv.xilesou.top/pdf/1409.0473.pdf" target="_blank" rel="noopener"></a></li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li><a href="http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf" target="_blank" rel="noopener">Deep learning</a>，Hinton,lecun,bengio 三巨头写的 deep learning 综述。</li>
<li>《Neural Networks Tricks of the Trade》，调参技巧。</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>特征工程：笔记</title>
    <url>/%C2%B7zcy/AI/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%EF%BC%9A%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<h1 id="《特征工程入门与实践》"><a href="#《特征工程入门与实践》" class="headerlink" title="《特征工程入门与实践》"></a>《特征工程入门与实践》</h1><h1 id="3、特征增强：清洗数据"><a href="#3、特征增强：清洗数据" class="headerlink" title="3、特征增强：清洗数据"></a>3、特征增强：清洗数据</h1><h2 id="填充缺失值"><a href="#填充缺失值" class="headerlink" title="填充缺失值"></a>填充缺失值</h2><p>&emsp;&emsp;通常数据集会因为各种原因有所缺失。必须尽可能地了解数据集，以便找到使用<strong>其他符号填充的</strong>确实数据。<strong>公开数据集的文档</strong>里面有可能会提到缺失数据的问题。<br>&emsp;&emsp;如果没有文档，缺失值的常见填充方法有：</p>
<ul>
<li>0（数值型）</li>
<li>unknown 或 Unknown（类别型）</li>
<li>?（类别型）<a id="more"></a>
</li>
</ul>
<h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><p>&emsp;&emsp;如果 pandas 将缺失值自动填充了 0（需要自行判断 0 是否缺失值），那么可以先用 python 的 None 填充缺失值，以便使用 pandas 的 fillna, dropna and isnull 等方法。<br>&emsp;&emsp;现在开始介绍方法，处理缺失值的主要办法是：1. 删除缺失值的行；2. 填充缺失值。</p>
<ol>
<li>最常见也是最容易的方法大概是直接删除存在缺失值的行。通过这种操作，我们会留下具有数据的完整数据点。可以使用 pandas 的 dropna 方法获取新的 DataFrame。<br>但是我们可能会丢失大量的原始数据（书中的例子使用的是《皮马印第安人糖尿病预测数据集》，丢失 51% 的行）。从机器学习的角度考虑，尽管数据都有值、很干净，但是我们没有利用尽可能多的数据。经书中使用 pandas 分析，某些数据的均值下降严重，所以<strong>我们应该保留下尽可能多的数据</strong>。<br>然后此书的作者使用去除缺失值的数据集运行了一个 KNN 模型，最终得到最好的结果为：k=7，acc=74.5%。但是如果用到所有的数据，会不会更好？</li>
<li>填充缺失值是一种更复杂的方法。<strong>填充</strong>指用现有的知识/数据来确定缺失的数量值，并填充的行为。<br>我们有几种选择，最常见的是用<strong>此列其余部分的均值</strong>填充缺失值。可以使用 pandas 的 fillna(mean_value) 方法即可填充，但是这有点麻烦，我们可以选用 scikit-learn 预处理类的 Imputer 模块，它更简单。只需指定策略 <script type="math/tex">Imputer = Imputer(strategy='mean')</script>，再调用 <script type="math/tex">pima_imputed = imputer.fit_transform(pima)</script> 即可。<br>那么来验证一下这样填充的 acc 如何。首先全部填充 0 用来充当对照组，发现 KNN 模型的 acc=73.31%，明显低于 74.5%。然后填充均值，发现 acc=65.625%，居然更低了。<br>这里需要解释一点，其实使用上述的方法去填充缺失值是错误的。我们<strong>将整个数据集的一列的均值去填充对应列的缺失值</strong>实际上犯了一个错误，即<strong>当预测测试集的响应值（即 y）时，不能假设我们已经知道了整个数据集的均值</strong>。所以我们应该<strong>使用训练集的均值去填充训练集和测试集的缺失值</strong>。注：我们假设测试集是未知的，所以它并没有均值，并且我们使用了训练集的信息去训练，所以我们需要使用训练集的均值去填充测试集的缺失值。<br><strong>但是</strong>最 sao 的是，此书中用正确的填充方法，最后得到的 acc=73.18%，比直接填充 0 还低。另外使用<strong>中位数</strong>填充得到的 acc=73.57%，始终没有高于<strong>直接删除缺失值</strong>得到的 acc。</li>
</ol>
<h2 id="标准化与归一化"><a href="#标准化与归一化" class="headerlink" title="标准化与归一化"></a>标准化与归一化</h2><p>&emsp;&emsp;仔细观察数据，我们发现数据的大小差别很大。而某些机器学习模型受数据尺度（scale）的影响很大。数据工程师可以选用某种归一化操作。我们将重点关注 3 种归一化方法，<strong>前两个方法特别用于调整特征，第三个方法虽然操作行，但效果与前两个相当</strong>。</p>
<ul>
<li>z 分数标准化</li>
<li>min-max 标准化</li>
<li>行归一化</li>
</ul>
<p>&emsp;&emsp;<strong>z 分数标准化</strong>是最常见的标准化技术，即均值归一化。使得输出会被重新缩放，使<strong>均值为 0、标准差为 1</strong>。公式为：</p>
<script type="math/tex; mode=display">
    z = \frac{x - \mu}{\sigma}</script><ul>
<li>z 是新的值</li>
<li>x 是单元格中原来的值</li>
<li><script type="math/tex">\mu</script> 是该列的均值</li>
<li><script type="math/tex">\sigma</script> 是列的标准差</li>
</ul>
<p>&emsp;&emsp;<strong>min-max 标准化</strong>与 z 分数标准化类似，它也用一个公式替换列中的每个值。它会使得每一列的值都位于 [0,1] 。公式为：</p>
<script type="math/tex; mode=display">
    m = \frac{(x - x_{min})}{x_{max} - x_{min}}</script><ul>
<li>m 是新的值；</li>
<li>x 是单元格原来的值；</li>
<li><script type="math/tex">x_{min}</script> 是该列的最小值；</li>
<li><script type="math/tex">x_{max}</script> 是该列的最大值。</li>
</ul>
<p>&emsp;&emsp;<strong>行归一化</strong>是关于行的，而不是关于列的。它不是计算每列的统计值（均值、最小值、最大值），而是保证每行都有<strong>单位范数</strong>，意味着每行的向量长度相同。计算方式如下所示，即 L2 范数，其他范数方式这里不讨论。</p>
<script type="math/tex; mode=display">
    ||x|| = \sqrt{(x^2_1 + x^2_2 + \dots + x^2_n)}</script><h2 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h2><p>&emsp;&emsp;最后我们将填充缺失值的方法和标准化（或归一化）的方法结合起来用，在 pima 数据集上发现使用<strong>均值填充 + min-max 标准化</strong>的交叉验证准确率最高。</p>
<h1 id="4、特征构建：我能生成新特征吗"><a href="#4、特征构建：我能生成新特征吗" class="headerlink" title="4、特征构建：我能生成新特征吗"></a>4、特征构建：我能生成新特征吗</h1><ol>
<li>填充分类特征</li>
<li>编码分类变量</li>
<li>扩展数值特征：多项式</li>
<li>针对文本：词袋模型、TF-IDF…</li>
</ol>
<h1 id="5、特征选择：对坏属性说不"><a href="#5、特征选择：对坏属性说不" class="headerlink" title="5、特征选择：对坏属性说不"></a>5、特征选择：对坏属性说不</h1><p>&emsp;&emsp;</p>
<h1 id="6、特征转换：数学显神通"><a href="#6、特征转换：数学显神通" class="headerlink" title="6、特征转换：数学显神通"></a>6、特征转换：数学显神通</h1><p>&emsp;&emsp;</p>
<h1 id="7、特征学习：以AI促AI"><a href="#7、特征学习：以AI促AI" class="headerlink" title="7、特征学习：以AI促AI"></a>7、特征学习：以AI促AI</h1><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>多领域seq2lf</title>
    <url>/project/%E5%A4%9A%E9%A2%86%E5%9F%9Fseq2lf.html</url>
    <content><![CDATA[<h1 id="pytorch踩坑"><a href="#pytorch踩坑" class="headerlink" title="pytorch踩坑"></a>pytorch踩坑</h1><ol>
<li><code>tensor.view()</code> 相当于numpy中resize（）的功能，但是用法可能不太一样。</li>
<li>关于 nn.LSTM 的用法：如果不想手动初始化隐藏状态，而是想让 pytorch 帮你初始化，那么就只需要填入输入值即可。举个例子，下面的代码输入向量所对应的自然语句是 [‘how are you’, ‘i’m fine’]。<br>其中输入值 x 的形状为 (seq_len, batch_size, input_size)，比如上面的例子就是 (3, 2, 300)，代表序列长度为 3（因为上面的两句话的最大长度为 3），批量大小为 2，词向量维度为 300。注意一点：<strong>pytorch 会根据你输入的序列长度</strong>（输入的张量必须是一样长的，参差不齐的张量无法输入，当然了，你也无法创建出这样的张量）<strong>自动帮你做时间步上的计算，比如你输入的序列长度为 20，那么它会自动在神经网络的第一层计算 20 次，再返回结果给你。</strong>如果看不懂这里是什么意思，说明你不会 LSTM。可以再看看 LSTM 是怎么计算的。<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">cell = nn.<span class="constructor">LSTM(<span class="params">input_size</span>, <span class="params">hidden_size</span>, <span class="params">num_layers</span>)</span></span><br><span class="line">output, (a, m) = cell(x)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="pytorch-loss-function"><a href="#pytorch-loss-function" class="headerlink" title="pytorch loss function"></a>pytorch loss function</h1><p>&emsp;&emsp;pytorch 的多元分类 loss 有 CrossEntropyLoss 和 NLLLoss。NLLLoss 全称 Negative Log Likelihood Loss，说白了就是求对数概率并取负，我们从函数图像就可以理解。模型输出的概率分布在 0-1 之间，log 函数的 0-1 区间正好全是负数，所以要加上一个负号，让 loss 值为正数。显而易见，概率越接近 1，loss 值越小。接下来描述一下这两个函数。</p>
<ol>
<li>CrossEntropyLoss = LogSoftmax + NLLLoss；</li>
<li>CrossEntropyLoss 中已经附带了 log_softmax 操作，所以如果你想省事，那么直接将输出向量输入 CrossEntropyLoss 即可；</li>
<li>如果使用 NLLLoss，那么在使用 NLLLoss 之前，还需要经过一层 LogSoftmax。</li>
</ol>
<p>&emsp;&emsp;需要注意一点，我感觉网上很多人也没有理解什么是 CrossEntropyLoss，导致很多人都被误导了。首先 nll 的公式如下：</p>
<script type="math/tex; mode=display">
nll\_loss = -log(pred)</script><div class="note warning">
            <p>&emsp;&emsp;nll loss 可以有多种表达方式，我把在网上看到的公式都罗列一下。<br>&emsp;&emsp;以下的公式与 crossentropy 一样。（实际上公式就是一样的，只不过在概念上有点不同，由于输出值 y 是 one hot 形式，为 0 时，就相当于没有加，最后的结果就是上面的公式）</p><script type="math/tex; mode=display">nll\_loss = -\sum^n_{i=1} y_i log(pred_i) = -log(pred)</script><p>&emsp;&emsp;这里的 class 就是指第几个标签，它不是 one hot 表示形式。大家会发现这里少了一个 log 函数，实际上 pred 是使用 log_softmax 函数计算之后的结果。</p><script type="math/tex; mode=display">nll\_loss = -pred[class]</script>
          </div>
<p>&emsp;&emsp;CrossEntropyLoss 公式如下：</p>
<script type="math/tex; mode=display">
crossentropy\_loss = -\sum^n_{i=1} y_i log(pred_i)</script><p>&emsp;&emsp;<del>无法理解的原因之一是，在学机器学习的时候，大家都知道啥是 crossentropy，后来在学多元分类时，开始分 binary_crossentropy 和 crossentropy。这点大家都能理解，但是到看到 NLLLoss 时，就开始懵逼了。</del><br>&emsp;&emsp;<del>由于 CrossEntropyLoss = LogSoftmax + NLLLoss，在 crossentropy 的公式中貌似没有出现 softmax（更没有 log_softmax），所以开始懵了，无法理解其中的 LogSoftmax 是干啥的。</del><br>&emsp;&emsp;首先我要解释一点 CrossEntropyLoss 是 LogSoftmax 和 NLLLoss 两个步骤之和，之前说的“+”号，并非是数学意义上的加号。也就是说，CrossEntropyLoss 就比 NLLLoss 多做了一步 LogSoftmax（<strong>博主注</strong>：<em>个人认为实际上只是多做了一步 softmax，说多做了一步 log_softmax，是因为站在 pytorch 框架的角度</em>）。<br>&emsp;&emsp;其次，对于真实输出值 y 来说，无非就是 0 和 1（注意多元分类也只有 0 和 1），并且根据上述 crossentropy 的公式。实际上公式可以化简为以下所示，其中的 m 代表真实值为 1 的索引。</p>
<script type="math/tex; mode=display">
crossentropy\_loss = -\sum^n_{i=1} y_i log(pred_i) = -log(pred_m)</script><p>&emsp;&emsp;请注意这里的 <script type="math/tex">pred_m</script>。我们都知道在进行分类问题时，我们需要将输出结果置于 0-1 之间，对于二元分类我们使用 sigmoid 函数，对于多元分类我们使用 softmax（到这开始有内味了）。由于分类问题都是要这么做的，所以将 softmax 这个函数放到公式 <script type="math/tex">crossentropy\_loss = -log(pred_m)</script> 中，我们惊奇的发现 crossentropy 函数变成了 log_softmax（最前面的负号暂时不看）。即 crossentropy + softmax = -log_softmax。</p>
<div class="note warning">
            <p>&emsp;&emsp;请始终留意，pred 是一个向量通过 softmax/log_softmax 计算之后的值。</p>
          </div>
<p>&emsp;&emsp;最后你会发现这样还是不对。<script type="math/tex">nll\_loss = -log(pred)</script>，之前说 CrossEntropyLoss = <strong>LogSoftmax</strong> + NLLLoss，我把 log_softmax 放到 nll 里，变成了 <script type="math/tex">LogSoftmax + NLLLoss = -log(log(pred))</script>，怎么多了一个 log？实际上 nll 的公式应该以 <script type="math/tex">nll\_loss = -pred[class]</script> 为准，你会发现这个公式中没有 log 函数。这样将 logsoftmax 放入 nll loss 中，就正好是 crossentropy 了。<br>&emsp;&emsp;那么你就会问 nll 明明是 Negative Log Liklihood，log 不见了，这不就是名存实亡了？<br>&emsp;&emsp;<strong>这可能是因为 pytorch 想要简化操作，才这么设置的，别的框架可能并不是这样。简而言之，pytorch 框架中，nll loss 的公式是 -pred。crossentropy 的公式是 logsoftmax + nll loss，即 nll(log_softmax(output))</strong><br>&emsp;&emsp;<strong>也就是说，如果神经网络的最后一层输出是 logsoftmax，那么就使用 nll loss（上一段 nll loss 那个 pred 就是通过 log_softmax 的输出值）。如果最后一层只是输出，偷懒不想写 logsoftmax，那么就使用 crossentropy loss（上一段 crossentropy 中的 output 就是一个普通的神经网络输出）。</strong><br>&emsp;&emsp;参考：</p>
<ol>
<li><a href="http://blog.leanote.com/post/lee-romantic/crossentry" target="_blank" rel="noopener">CrossEntropyLoss和NLLLoss的理解</a></li>
<li><a href="https://www.cnblogs.com/ranjiewen/p/10059490.html" target="_blank" rel="noopener">Pytorch之CrossEntropyLoss() 与 NLLLoss() 的区别</a></li>
<li><a href="https://blog.csdn.net/m0_38133212/article/details/88087206" target="_blank" rel="noopener">CrossEntropyLoss与NLLLoss的总结</a></li>
<li><a href="https://www.cnblogs.com/marsggbo/p/10401215.html" target="_blank" rel="noopener">Pytorch里的CrossEntropyLoss详解</a><a id="more"></a>
</li>
</ol>
<h2 id="顺便一提KLDivLoss"><a href="#顺便一提KLDivLoss" class="headerlink" title="顺便一提KLDivLoss"></a>顺便一提KLDivLoss</h2><p>&emsp;&emsp;<a href="https://www.cnblogs.com/charlotte77/p/5392052.html" target="_blank" rel="noopener">【原】浅谈KL散度（相对熵）在用户画像中的应用</a><br>&emsp;&emsp;暂时还没用过这个 loss，简单来说，是用来比较两个概率分布之间的信息熵差异，如 AB 两组群体，有对某一商品的总消费分布 P 和群体人数的分布 Q，可以计算 PQ 之间的信息熵差异，从而获得 AB 两组群体对该商品的偏爱程度。</p>
<h1 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h1><p>&emsp;&emsp;在将神经网络的参数调整的差不多后，需要应用一些 tricks 使得神经网络获得更好的性能。</p>
<h2 id="optimizer-zero-grad"><a href="#optimizer-zero-grad" class="headerlink" title="optimizer.zero_grad()"></a>optimizer.zero_grad()</h2><p>&emsp;&emsp;在 pytorch 中，为什么要在每个循环之初调用这个方法？因为 pytorch 把计算的每个梯度都累加起来，并不会每迭代一次就将梯度清零。这样做看起来令人费解，并反常理。但是实际上这样做可以做更多神奇的操作，比如</p>
<ul>
<li><a href="https://www.zhihu.com/question/303070254" target="_blank" rel="noopener">https://www.zhihu.com/question/303070254</a></li>
</ul>
<p>&emsp;&emsp;还有，试想本来你想运行 batch_size=1024，但是由于电脑太差，只能运行 batch_size=256 的批次数据。那么只需要每循环两次调用一次 zero_grad() 即可。<br>&emsp;&emsp;参考：</p>
<ol>
<li><a href="https://blog.csdn.net/u011959041/article/details/102760868" target="_blank" rel="noopener">pytorch中为什么要用 zero_grad() 将梯度清零</a></li>
</ol>
<h2 id="clip-gradient"><a href="#clip-gradient" class="headerlink" title="clip gradient"></a>clip gradient</h2><p>&emsp;&emsp;<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">此论文</a>提出了 clip gradient（以下称 clipping）<strong>解决梯度爆炸</strong>。首先看下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/多领域seq2lf/the error surface is rough.jpg" alt="the error surface is rough"></p>
<p>&emsp;&emsp;如果只看图，会发现有一个像峭壁一样的东西，它就是罪魁祸首。当我们将一个小球往前移动时，有时候正好迈过峭壁，小球得以正常移动。但是当小球碰到峭壁时，小球就会被反弹回去，导致 loss 发生剧烈变化。<br>&emsp;&emsp;从数学角度来看，那个峭壁就是梯度。<strong>需要注意的是</strong>，由于图中 z 轴标注的是 total loss，所以第一印象感觉峭壁代表 total loss，但是<strong>峭壁代表的是梯度，而不是 total loss</strong>。根据参数更新公式 <script type="math/tex">w -= \alpha * \Delta w</script>，其中 <script type="math/tex">\Delta w</script> 代表 w 的梯度，所以 w 的更新方向其实与梯度直接相关。<strong>当 w 不幸到达某个值时，遇到梯度极大的情况，那么不管梯度是正还是负，都会将 w 更新到一个相对很大的值，从而 loss 值也会跟着改变。注：这里其实也与 learning rate 有关，因为原本的梯度都很小，所以我们初始设置的 lr 都很大。突然梯度增大，而 lr 没有适应，一个大的梯度乘上一个大的 lr，那就更大了</strong>。<br>&emsp;&emsp;<strong>解决办法</strong>是：当 gradient 大于某个 threshold 时，就不让它大于 threshold（视频中说作者的代码中将 threshold 设置为 15，但是这个 threshold 应该是具体情况具体分析）。<br><div class="note warning">
            <p>&emsp;&emsp;Q：那么是什么导致了出现梯度猛增的现象呢？<br>&emsp;&emsp;A：首先不是 sigmoid 的锅。因为已知 sigmoid 函数只会导致 gradient vanishing（梯度消失）的问题（<em><a href="https://yan624.github.io/·zcy/AI/dl/对神经网络整体的理解.html#缺陷">sigmoid 的缺陷</a>这一章说明了 sigmoid 具有梯度消失的缺陷</em>）。<del>如果是 sigmoid 同时引起梯度爆炸和梯度消失，按理说换成 ReLU 就能同时解决梯度爆炸/消失，但是将 RNN 的激活函数换成 ReLU 并没有解决问题，所以跟激活函数没什么关系</del>（<strong>博主注</strong>：删除内容是从李宏毅老师的视频上记下来的，但是现在看来可能是我理解错了。首先在根本上就错了，sigmoid 会造成梯度消失，但替换品并不是 relu，而是 tanh，其也只是减缓作用而已）。<br>&emsp;&emsp;那么怎么说明是什么导致了梯度爆炸呢？我们只需要小小地改变一个参数，然后观察 output 的变化，就能测出这个参数的 gradient 的大小。现在假设激活函数是 y = w * x，序列长度为 1000，并且第一个 timestep 的输入为 1，其他的 timestep 皆为 0，则<script type="math/tex">1^{1000} = 1</script>，而 <script type="math/tex">1.01^{1000} \approx 20000</script>。既然结果那么大，我们可能会想到去减小 learning rate 从而减小梯度，但是 <script type="math/tex">0.99^{1000} \approx 0</script>，你又要调大 learning rate，导致 lr 调起来很麻烦，所以调不调 lr 并没有什么区别。而这里就分别出现了梯度消失和梯度爆炸。这是由于 RNN 是序列模型，它需要处理一连串的序列。前一个的输出是后一个输入，<strong>类似于蝴蝶效应，一个很小的值，经过多个函数也能被放的很大。</strong><br>&emsp;&emsp;那么为什么可以通过观察一个参数的变化从而观察 gradient 的变化呢？很简单，例如 y = w * x，用上面的例子，第 1000 个输出会是 <script type="math/tex">y_{1000} = w^{1000} * x</script>，对 w 求导得 <script type="math/tex">\Delta w = 1000 w^{999} x</script>，由于 <script type="math/tex">w^{999}</script> 比 <script type="math/tex">w^{1000}</script> 小不了多少，所以 gradient 很大。<br>&emsp;&emsp;经过上面的例子我们就知道了问题出现的原因。当然一般情况下不会像上面序列长度有 1000 那么离谱。现在设序列长度就 40，当我们遇到一个较大的值时（比如 <script type="math/tex">1.1^{40} = 45.26</script>），依旧会出现梯度爆炸的问题。<br>&emsp;&emsp;解决办法：LSTM，clipping 等。<strong>注意：LSTM 只解决了 gradient vanishing 的问题，没有解决 gradient explode。详见下面的《LSTM 解决了 RNN 的什么问题》章节</strong>。同理 clipping 也只能解决 gradient explode 的问题，因为它能将梯度限制在一个点上（<strong>这是博主的思考，视频中并没有给出此结论</strong>）。那么我们可以结合 LSTM 和 clipping，从而同时解决梯度爆炸/消失。<br>&emsp;&emsp;综上所述，在 RNN 中，即使参数在很小一个范围内，梯度的变化也会很大。<br>&emsp;&emsp;最后还有一个问题，就是上面的例子是用 y = w * x 的例子，但是实际上，激活函数是用的 sigmoid, tanh 等，所以每个神经元的输出都永远在 [-1, 1] 中间。那么怎么会造成上面的例子中的情况呢？其实上面的例子只是一个比喻。真正的原因是反向传播时做的链式求导，它导致了梯度的连乘。详细推导见 <a href="https://yan624.github.io/·zcy/AI/dl/深度学习算法（二）：simple RNN 推导与理解.html#simple-RNN-的缺陷">simple RNN 的缺陷</a> 蓝色提示框。</p>
          </div></p>
<p>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://www.bilibili.com/video/av10590361?p=37" target="_blank" rel="noopener">P37 26: Recurrent Neural Network (Part II)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34203833" target="_blank" rel="noopener">深入理解lstm及其变种gru</a></li>
</ol>
<p>&emsp;&emsp;以下对新闻分类进行测评，将使用了 clipping 与没使用 clipping 的结果进行对比。</p>
<ol>
<li>测评一条新闻的分类：“<strong>广西最美的县城，很多人第一次旅行就是去这里</strong>”。没有使用 clip gradient 的结果是：<strong>news_agriculture</strong>。使用 clip gradient 的结果是：<strong>news_travel</strong>。</li>
<li>测评一条新闻的分类：“<strong>在越南游玩，有漂亮女子问“要不要生菜”，这是什么意思？</strong>”。没有使用 clip gradient 的结果是：<strong>news_military</strong>。使用 clip gradient 的结果是：<strong>news_agriculture</strong>。</li>
<li>测评一条新闻的分类：“<strong>去西藏旅游时，导游反复叮嘱：无论多脏都最好不要洗澡？</strong>”。没有使用 clip gradient 的结果是：<strong>news_agriculture</strong>。使用 clip gradient 的结果是：<strong>news_story</strong>。</li>
</ol>
<p>&emsp;&emsp;使用 clip gradient 之后，分类结果好像有点正确了，第二句里面有菜，所以是农业。第三句跟农业好像没什么关系，但是跟 story 虽然说不上有关系，但是好像也不是完全没关系。另外以上三条新闻的正确分类其实都是<strong>旅行</strong>。需要说明的是这只是随便一个例子，用来说明加了 clip gradient 之后，对网络的预测有点用。示例中所用的神经网络只是一个简单的 LSTM，仅供参考：）。</p>
<h2 id="learning-rate-decay"><a href="#learning-rate-decay" class="headerlink" title="learning rate decay"></a>learning rate decay</h2><p>&emsp;&emsp;在什么时候需要使用 learning rate decay？</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/38200980" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></li>
</ol>
<h2 id="处理OOV"><a href="#处理OOV" class="headerlink" title="处理OOV"></a>处理OOV</h2><p>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://www.zhihu.com/question/329708785" target="_blank" rel="noopener">word2vec缺少单词怎么办？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/72312668" target="_blank" rel="noopener">香侬读 | 怎样在小数据集下学习OOV词向量？</a></li>
</ol>
<h1 id="LSTM-解决了-RNN-的什么问题"><a href="#LSTM-解决了-RNN-的什么问题" class="headerlink" title="LSTM 解决了 RNN 的什么问题"></a>LSTM 解决了 RNN 的什么问题</h1><p>&emsp;&emsp;解决了两点。仅为个人观点。</p>
<ol>
<li>memory 机制</li>
<li>梯度消失</li>
</ol>
<h2 id="memory机制"><a href="#memory机制" class="headerlink" title="memory机制"></a>memory机制</h2><p>&emsp;&emsp;<strong>第一</strong>，RNN 受到短期记忆的影响。如果序列很长，他们将很难将信息从较早的时间步传送到后面的时间步。LSTM 通过改进 memory，可以更好地保留序列信息。<br>&emsp;&emsp;在每个时间点， RNN 都只用每个 cell 的 output 覆盖 memory 里的值，即每个 tiemstep 中的信息都会被覆盖掉。而在 LSTM 中，它会将 memory 乘上一个权重再加上 input，从而获得新的 memory。它不会每次都 forget memory，除非 forget gate 计算结果等于 0。从公式的角度看就是：</p>
<script type="math/tex; mode=display">
RNN: memory_{new} = cell(memory, input) \\
LSTM: memory_{new} = a * input + b * memory \\</script><p>&emsp;&emsp;cell() 代表一个简单的 sigmoid 函数，memory 与 input 可以做拼接处理，也可以 memory + input，具体自己设计。a 是 input gate 的计算结果，b 是 forget gate 的计算结果，说白了都是一个权重，可以忽略。<br>&emsp;&emsp;这样乍一看好像 RNN 和 LSTM 没什么区别，它们都是会进行一些计算，然后获得一个新的 memory。但是 RNN 的计算方式是将 memory 与 input <strong>一起</strong>输入进 neuron，从而产生一个 output，最后将这个 output 作为新的 memory。你会发现，在 RNN 中，虽然 output 是由 memory 和 input 计算得来的，但是在更新 memory 时不是采用 LSTM 的策略，而是直接用 output 将 memory 覆盖掉，<strong>这既没考虑到原 memory 的值，也没考虑当前 input 的值</strong>。而 LSTM 在覆盖 memory 时，会考虑当前 memory 以及 input 的值。<strong>理解该段的重点是：下面的 1</strong>。</p>
<ol>
<li><strong>个人理解</strong>：RNN 看似用到了原 memory 和 input，但是在实际计算时（即 cell() 函数所做的操作），由它俩 train 出来的权重矩阵只是为了使 cell() 计算的结果尽可能地接近 y，而并非在计算一个好的 memory。这里尤其要注意，cell() 函数的功能跟 memory 没关系，RNN 与 memory 有关的操作仅仅只有一步，即 <script type="math/tex">memory_{new} = output</script>，它只是将以前的 memory 覆盖掉。而 LSTM 不光在更新 memory 时用到了原 memory 和 input，它里面的 3 个 gate 也都需要通过 input 计算，所以 input 对 LSTM 的输出影响很大，对 memory 的更新自然也大。</li>
<li><strong>李宏毅机器学习视频中的说法</strong>：如果 weight 可以影响到 memory 里的值，那么这个影响会一直存在。我觉得李宏毅老师的讲解跟我的应该差不多，重点也是 LSTM 多了一个可以训练 memory 的权重。</li>
<li>memory 本质是在记忆之前所有的 input。</li>
</ol>
<p>&emsp;&emsp;其实 forget gate 还是有几率清空 memory 的，那么为什么不直接取消 forget gate 呢？你不是要充分利用 input 吗？实际上在 LSTM 的第一个版本是没有 forget gate 的，它是后来才加上去的。甚至现在的说法是在训练 LSTM 时，不要给 forget gate 太大的权重，要让它大部分时间都是开着的，即大部分时间都不要清空 memory。如果以后训练 LSTM 时，觉得过拟合严重，可以使用 GRU，GRU 只有两个 gate（无 output gate）。（<em>引用<a href="https://www.bilibili.com/video/av10590361?p=37" target="_blank" rel="noopener">教学视频</a> 22:50 的话</em>）。 </p>
<h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><p>&emsp;&emsp;<strong>第二</strong>，反向传播时出现的问题，以下 Q 为问题，A 为解释。<br>&emsp;&emsp;引子：如《clip gradient》一章黄框中所说，RNN 很容易出现峭壁和平原。<strong>LSTM 只解决了 gradient vanishing 的问题</strong>，没有解决 gradient explode。LSTM 使得 error surface 不那么崎岖，<strong>消除了训练时的一些平坦的地方</strong>。虽然梯度在有些地方依然崎岖，但是不会有太平坦的地方。<strong>所以在训练时可以放心的将 lr 调小</strong>，不需要担心会出现平坦的地方，导致训练过慢。<br>&emsp;&emsp;如果公司问为什么把 RNN 换成 LSTM？<del>回答 LSTM 比较潮、因为 LSTM 比较复杂。</del>回答 LSTM 可以处理 gradient vanishing 的问题。具体解释如下：<br>&emsp;&emsp;<strong>Q</strong>：为什么 LSTM 可以解决 gradient vanishing 的问题？（解决梯度消失也可以说成避免 gradient 特别小（消除平原））<br>&emsp;&emsp;<strong>A</strong>：在 RNN 中利用 memory 的方式是一种复合函数的结构，所以在反向传播时，需要链式求导，梯度与梯度相乘容易造成<strong>梯度消失</strong>和<strong>梯度爆炸</strong>。关于 RNN 反向传播的求导结果可以参考 <a href="https://yan624.github.io/·zcy/AI/dl/深度学习算法（二）：simple RNN 推导与理解.html#simple-RNN-的缺陷">simple RNN 的缺陷</a> 蓝色提示框。<br>&emsp;&emsp;虽然这样的求导大致已经可以解释了梯度消失的问题，但是如果仔细想想就会发现盲点。在此之前，我想先说明 RNN 家族的反向传播路径与其他的神经网络不同，它的 loss 值是每一个 timestep 的真实值 y 与输出值 的 loss 之和。<a href="https://mooc.study.163.com/learn/2001280005?tid=2001391038&amp;_trace_c_p_k2_=72573d316c3441869416d70899cdf382#/learn/content?type=detail&amp;id=2001770031" target="_blank" rel="noopener">此视频</a> 大致讲明白了这个总 loss 值到底是由哪些 loss 相加得到的。<br>&emsp;&emsp;知道了上面的前提条件，就可以很简单的理解这个盲点了，接下来我先介绍一下这个盲点是什么：<strong>参考资料 1 大致解释了这一问题</strong>，这一段可能比较绕，<strong>简单来说就是后面的 timestep（比如下图中 <script type="math/tex">loss_4</script>）在反向传播时，求 <script type="math/tex">\Delta W</script> 会出现梯度消失（注意 RNN 每个 timestep 的 W 都是一样），这是因为在求梯度时，函数已经复合了好几层</strong>。而对 <script type="math/tex">loss_1</script> 求 W 的导数时，由于它本身就在序列的前面，函数还没有复合，所以 <script type="math/tex">\Delta W</script> 的导数还没梯度消失。<strong>最后在计算总的 loss 时，是将各个阶段的梯度加起来</strong>，即使后面的 loss 会得到一个很小的的梯度 <script type="math/tex">\Delta W</script>，但由于 <script type="math/tex">loss_1</script> 的原因，并不会发生梯度消失。<br>&emsp;&emsp;但是事实上是会发生的，那么梯度消失从何而来呢？是因为后面的 loss 在求梯度时，导致前面输入值的梯度很小，从而产生了<strong>信息丢失</strong>。信息丢失就是 RNN 的梯度消失。有一点需要考虑，RNN 在反向传播时，是需要传播到输入值 x 的，即词向量。在计算 <script type="math/tex">loss_4</script> 时，RNN 中 W 的梯度肯定是很小的，但是在更新 <script type="math/tex">x_4</script> 时，这里的梯度 <script type="math/tex">\Delta W</script> 还不是很小，所以信息无问题。但是当反向传播到 <script type="math/tex">x_1</script> 时，梯度已经很小了，由于小梯度导致 <script type="math/tex">x_1</script> 无法得到很好的更新，于是产生了信息丢失。<script type="math/tex">loss_3</script> 以及 <script type="math/tex">loss_2</script> 以此类推，不过对于 <script type="math/tex">loss_1</script> 并无问题，因为它没有复合函数。<br>&emsp;&emsp;注：<strong>上两段，还参考了参考资料 3，个人认为参考资料 1 中内容并不是很完整</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/多领域seq2lf/total loss.jpg" alt="total loss"></p>
<p>&emsp;&emsp;以上的参考资料：</p>
<ol>
<li><a href="https://www.zhihu.com/question/34878706/answer/665429718" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li>
<li><a href="https://www.bilibili.com/video/av10590361?p=37" target="_blank" rel="noopener">李宏毅机器学习</a>)</li>
<li><a href="https://www.bilibili.com/video/av41393758?p=8" target="_blank" rel="noopener">RNN 和语言模式</a>，19.50 开始</li>
</ol>
<p>&emsp;&emsp;而在 LSTM 中，是使用加和的计算方式（博主注：<strong>由于我没有计算过，所以我也不是很肯定</strong>），所以大致解决了梯度消失的问题。注意我没有说解决了梯度爆炸的问题。<br><div class="note success">
            <p>&emsp;&emsp;LSTM 的模型以及参数名参考<a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#长短期记忆——Long-Short-term-Memory-LSTM">此处</a>。</p><script type="math/tex; mode=display">\begin{aligned}    LSTM: z_i & = [a_{i - 1}; x_i] \\    memory_{new} & = g(z_i) * input(z_i) + memory * forget(z_i) \\    a_i & = h(g(z_i) * input(z_i) + memory * forget(z_i)) * output(z_i) \\\end{aligned}</script><p>&emsp;&emsp;对 LSTM 的求导结果很复杂，就不写了（实际上算得我自己都乱了）。。。它的复杂结构使得它不会出现一个数被连乘，导致极小。<strong>注意 LSTM 并没有解决梯度爆炸的问题。可以结合 clipping 训练 LSTM</strong>。<br>参考资料：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/28749444" target="_blank" rel="noopener">LSTM如何解决梯度消失问题</a></li><li><a href="https://zhuanlan.zhihu.com/p/36101196" target="_blank" rel="noopener">漫谈LSTM系列的梯度问题</a></li><li><a href="https://www.zhihu.com/question/34878706/answer/192444888" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li><li><a href="https://zhuanlan.zhihu.com/p/83496936" target="_blank" rel="noopener">人人都能看懂的LSTM介绍及反向传播算法推导（非常详细）</a></li></ol>
          </div></p>
<p>&emsp;&emsp;其他一些思考：LSTM 虽然可以永久的记住以前的 input 信息，但是 memory 说白了就是一个权重矩阵，不可能无限制的记住任何信息。所以可以对 memory 进行一些魔改，比如 memory network 将 memory 修改成用数组存储。<br>&emsp;&emsp;那么问题来了，GRU 解决了什么问题呢？为什么过拟合严重，可以使用 GRU？详见下一章。</p>
<h2 id="其他解决梯度消失的办法"><a href="#其他解决梯度消失的办法" class="headerlink" title="其他解决梯度消失的办法"></a>其他解决梯度消失的办法</h2><ol>
<li>Clockwise RNN</li>
<li>Structurally Constrained Recurrent Network(SCRN)</li>
<li></li>
</ol>
<h2 id="LSTM-为什么没有解决梯度爆炸？"><a href="#LSTM-为什么没有解决梯度爆炸？" class="headerlink" title="LSTM 为什么没有解决梯度爆炸？"></a>LSTM 为什么没有解决梯度爆炸？</h2><p>&emsp;&emsp;理论上，梯度爆炸也同样糟糕。但在实践上，其实我们可以直接砍一刀（原话：it turns out we can actually have a hack），这由 Thomas Mikolov 首次提出。在某种程度上是不精确的，比如说“<strong>现在你有一个很大的梯度 100，让我们把它限制在 5 吧</strong>”。这方法就结束了。你只要定义一个临界值，当梯度大于临界值时，就使梯度等于临界值。虽然不是一个数学方法，但结果表明在实践中效果不错。<br>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://www.bilibili.com/video/av41393758?p=8" target="_blank" rel="noopener">RNN 和语言模式</a> 49.06 - 62.38</li>
</ol>
<h1 id="GRU-改进了-LSTM-的啥"><a href="#GRU-改进了-LSTM-的啥" class="headerlink" title="GRU 改进了 LSTM 的啥"></a>GRU 改进了 LSTM 的啥</h1><p>&emsp;&emsp;简化了 Gate，GRU 能够达到 LSTM 相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率。<br>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/32481747" target="_blank" rel="noopener">人人都能看懂的GRU</a></li>
</ol>
<h1 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h1><p>&emsp;&emsp;本人的笔记</p>
<ol>
<li><a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#Attention">吴恩达李宏毅综合学习笔记：RNN入门</a></li>
<li><a href="https://yan624.github.io/·学习笔记/AI/nlp/CS224n学习笔记.html#attention">CS224n学习笔记</a></li>
<li><strong><a href="https://yan624.github.io/·zcy/AI/dl/深度学习算法（三）：RNN 各种机制.html#Attention">RNN 各种机制</a></strong></li>
</ol>
<p>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/67909876" target="_blank" rel="noopener">浅谈Attention注意力机制及其实现</a></li>
</ol>
<h1 id="Batch-Norm"><a href="#Batch-Norm" class="headerlink" title="Batch Norm"></a>Batch Norm</h1><h1 id="可视化训练结果"><a href="#可视化训练结果" class="headerlink" title="可视化训练结果"></a>可视化训练结果</h1><p>&emsp;&emsp;以前一直用 matplotlib 来画图，现在用了 tensorboardX 之后，感觉人瞬间就爽了，以下为教程。无法启动看 2，启动后网页无法显示看 3，代码不会写看 1。<br>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://tensorboardx.readthedocs.io/en/latest/tutorial.html#what-is-tensorboard-x" target="_blank" rel="noopener">官方文档</a></li>
<li><a href="https://blog.csdn.net/qq_40605167/article/details/95761885" target="_blank" rel="noopener">tensorboard OSError: [Errno 22] Invalid argument错误处理</a></li>
<li><a href="https://blog.csdn.net/weixin_44135282/article/details/86156961" target="_blank" rel="noopener">tensorboard生成的网址打不开的解决方法</a></li>
<li><a href="https://blog.csdn.net/bigbennyguo/article/details/87956434" target="_blank" rel="noopener">详解PyTorch项目使用TensorboardX进行训练可视化</a></li>
</ol>
<h1 id="过拟合-欠拟合"><a href="#过拟合-欠拟合" class="headerlink" title="过拟合/欠拟合"></a>过拟合/欠拟合</h1><h2 id="train-acc很大，gap非常大"><a href="#train-acc很大，gap非常大" class="headerlink" title="train acc很大，gap非常大"></a>train acc很大，gap非常大</h2><p>&emsp;&emsp;在训练过程中出现了一种情况，train acc 94% 左右，但是 val acc 只有 24% 左右，test acc 也是 24% 左右。很多人认为这是<strong>过拟合</strong>状态，因为模型在训练集上拟合的很好，但在验证集上拟合的不好，这两者之间的 gap 很大。也就是<strong>高方差</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/project/多领域seq2lf/train acc很大，gap也很大.svg" alt="train acc很大，gap也很大"></p>
<p>&emsp;&emsp;以下是对此现象的分析：</p>
<ol>
<li><p>再增加 <strong>epoch</strong> 已经没用了，因为该模型的参数已经很好的拟合了数据集，使得准确率接近 100%。</p>
<blockquote class="blockquote-center"><p>说明训练集中的几乎所有信息（不论是对泛化有用的信息还是训练集中的噪声）都全部被模型学习到了。（<a href="https://www.zhihu.com/question/65200055" target="_blank" rel="noopener">来源</a>）</p>
</blockquote>
</li>
<li><p>改变（无论增大还是缩小） rnn 的<strong>隐藏状态</strong>也不会有很大改善了。比如说此文章的模型，我尝试选择隐藏状态大小为 (100, 115, 125, 128)，结果如下所示，可以看到增加减少隐藏状态大小，对 train acc 根本无影响，对 val acc 也只有一点影响：</p>
<blockquote class="blockquote-center"><div class="table-container">
<table>
<thead>
<tr>
<th>hidden size</th>
<th>batch</th>
<th>train acc</th>
<th>val acc</th>
<th>test acc</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>32</td>
<td>94.42%</td>
<td>20.74%</td>
<td>21.28%</td>
</tr>
<tr>
<td>115</td>
<td>32</td>
<td>94.42%</td>
<td>22.80%</td>
<td>23.72%</td>
</tr>
<tr>
<td>125</td>
<td>32</td>
<td>94.42%</td>
<td>24.95%</td>
<td>24.78%</td>
</tr>
<tr>
<td>128</td>
<td>32</td>
<td>94.42%</td>
<td>21.20%</td>
<td>22.58%</td>
</tr>
<tr>
<td>125</td>
<td>128</td>
<td>98.47%</td>
<td>19.13%</td>
<td>20.37%</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<div class="note info">
            <p>&emsp;&emsp;但是我认为这种情况<strong>不算过拟合</strong>（暂时不看 <code>batch=128</code> 那组）。因为过拟合指的是模型在训练集上过度地拟合了数据特征，于是在验证集上出现个比较离群的数据样本，就会得到很差的结果。<br>&emsp;&emsp;如果把训练集拥有的特征集合定义为 A，验证集拥有的特征集合定义为 B，那么 <script type="math/tex">C=A \cap B, D = A \cup B - C</script>，其中 C 代表的是未被神经网络拟合的很小的一部分特征，D 代表的是已经被神经网络拟合的特征（注：这里的拟合并不一定要完全的拟合，在拟合线的附近即可）。神经网络要做的是同时拟合 A 与 B 的所有特征。<br>&emsp;&emsp;但是随着 epoch 的增加，A 中的所有特征肯定被拟合的越来越好，而 B 中有些已经被拟合的特征却越来越远离拟合线，所以导致了 C 中的特征越来越多。（可以通过逻辑回归来先思考，验证集数据原本在拟合线附近，但是增加 epoch 或者增加网络参数等操作导致过拟合，原本在拟合线附近的数据点反而偏离了）<br>&emsp;&emsp;<strong>所以训练集和验证集中的数据的分布应该是这样的：训练集和验证集中有很多数据分布类似（对应即使过拟合 val acc 也不会下降到 0%），但是两个数据集中有些数据只有部分特征的分布是类似的（对应于过拟合），最后两个数据集汇总有一部分数据完全无关联（对应于不管怎么拟合，val acc 始终不可能到 100%）。</strong><br>&emsp;&emsp;话说回来文中的情况，明显训练集和验证集的 acc 到达顶部之后一直处于平稳的状态，增加 epoch 或者增加网络参数都没有使得 acc 增加或下降，这就意味这 C 中的特征没有变化。<br>&emsp;&emsp;<strong>所以这种情况应该是这样的：训练集和验证集中的数据有少量是类似的（对应于 val acc 拥有 20+%），其余大部分，可能有超过 60% 的数据都没有关联（对应于不管怎么训练 val acc 始终只有 20+%），最后有两个数据集中一小部分数据可能有部分特征重叠（对应于随着 epoch 的增加，val acc 有略微的波动）。</strong></p>
          </div>
</li>
</ol>
<p>&emsp;&emsp;对这种情况，一般出现在利用随机采样快速训练模型，只需要加大数据量就行了。所以以后再用小数据量训练时只需要将参数调整到满意的地步就不需要再继续调整了，直接换大数据量。<br>&emsp;&emsp;最后对于 <code>batch = 128</code> 的那组，train acc 上升了，其他的下降了，有过拟合嫌疑，但是幅度不是很大。</p>
]]></content>
      <categories>
        <category>project</category>
      </categories>
  </entry>
  <entry>
    <title>Improved Representation Learning for Question Answer Matching</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/49.%20Improved%20Representation%20Learning%20for%20Question%20Answer%20Matching.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;自从短文级别（passage-level）的问答匹配需要有效的表征以捕获问题与答案之间复杂的语义关联开始，它就成为一个巨大的挑战。本文我们提出一系列的深度学习模型去解决如何选择短文答案。<br>&emsp;&emsp;将短文答案与符合语义关系的问题相匹配，不同于之前的大多数工作，即只使用一个深度学习结构。我们开发了一个混合模型去处理文本，其中用到了 CNN 和 RNN，结合了两种结构提取语言信息的优点。<br>&emsp;&emsp;此外，还开发了简单而有效的注意力机制。在两个数据集 InsuranceQA and TREC-QA 上显示此模型超出基线。<br><a id="more"></a></p>
<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><p>&emsp;&emsp;短文级别的答案选择是 QA 系统的重要组成部分之一。它的定义如下所示：给定一个问题和一群候选短文，挑选出包含候选答案的短文。<br>&emsp;&emsp;一个回答优于另一个回答取决于多种因素。尤其是不同于其他 NLP 对匹配任务，问题与答案之间语言上的相似度对我们的任务既可能有用也可能没用，这取决于问题。此外，虽然一个好的回答必须与问题相关联，但是它们之间没有共通的词汇单元。<br>&emsp;&emsp;因此，与基于深度学习的方法相比，这些挑战使得手工制作的特征变得不那么理想。此外，它们还要求我们的系统学习如何区分有用的片段和不相关的片段，其中更关注前者。</p>
<h1 id="approach"><a href="#approach" class="headerlink" title="approach"></a>approach</h1><p>&emsp;&emsp;作者开发一个模型，同时使用到了 CNN 和 RNN，并且还加上 attention 机制。</p>
<ol>
<li>LSTM<ul>
<li>q 和 a 分别输入 bi-LSTM</li>
<li>拼接 bi-LSTM 的正反向向量</li>
<li>对输出向量做平均（因为序列中有多个词向量，所以需要取平均）</li>
<li>做 max pooling</li>
</ul>
</li>
<li>CNN</li>
<li>Attention LSTM</li>
</ol>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/48%E3%80%81SQLNet%EF%BC%9AGenerating%20Structured%20Queries%20From%20Natural%20Language%20Without%20Reinforcement%20Learning.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;主要描述一下该论文 column attention。</p>
<h1 id="Sequence-to-set"><a href="#Sequence-to-set" class="headerlink" title="Sequence-to-set"></a>Sequence-to-set</h1><p>&emsp;&emsp;直观地说，<strong>where 子句</strong>中出现的列名是<strong>所有列名</strong>的子集。因此，我们可以仅仅预测子集中的列名，而不是生成列名序列（<strong>博主注</strong>：他的意思可能是，不要将 sql 语句中 “select Column A, Column B, Column C…” 的 “Column A, Column B, Column C…” 当做<strong>生成序列</strong>的任务，而是将其当做 slot filling）。我们把这个想法称为 <em>sequence-to-set</em> 的预测。<br>&emsp;&emsp;尤其是我们计算 <script type="math/tex">P_{wherecol}(col|Q)</script>，其中 col 是列名，q 是自然语言问题。为此，将计算 <script type="math/tex">P_{wherecol}(col|Q)</script> 表达为</p>
<script type="math/tex; mode=display">
    P_{wherecol}(col|Q) = \sigma(u^T_c E_{col} + u^T_q E_Q)</script><p>&emsp;&emsp;其中 <script type="math/tex">\sigma</script> 是 sigmoid 函数，<script type="math/tex">E_{col}</script> 和 <script type="math/tex">E_Q</script> 分别是 column name 和自然语言问题的嵌入， <script type="math/tex">u_c</script> 和 <script type="math/tex">u_q</script> 是两个可训练的列向量。。。（<em>后面还有一大段话省略了，主要看 column attention</em>）<br><a id="more"></a></p>
<h1 id="Column-attention"><a href="#Column-attention" class="headerlink" title="Column attention"></a>Column attention</h1><p>&emsp;&emsp;上一节的 <script type="math/tex">P_{wherecol}(col|Q)</script> 的计算公式在使用 <script type="math/tex">E_Q</script> 上有一个问题，因为只计算了自然语言语句的隐藏状态，它也许不能记住在预测<strong>特定</strong>列名时有用的<strong>特定</strong>信息。。。（后面举了个例子）<br>&emsp;&emsp;为了融入这一直觉，我们设计了 column attention 机制去计算 <script type="math/tex">E_{Q|col}</script> 来代替 <script type="math/tex">E_Q</script>。假定 <script type="math/tex">H_Q</script> 是 dxL 的矩阵，L 代表自然语言问题的长度。<script type="math/tex">H_Q</script> 的第 i 列代表问题中对应的第 i 个 token 的 LSTM 的隐藏状态输出。<br>&emsp;&emsp;我们对<strong>问题中的每一个 token</strong> 都计算 attention weight w，w 是 L 维的列向量（博主注：此处应该指的是在计算一个 token 的情况下，论文中未详细指明，仅为猜测）。计算公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    w & = softmax(v) \\
    v_i & = (E_{col})^T W H^i_Q \qquad \forall i \in {1, \dots, L} \\ 
\end{aligned}</script><p>&emsp;&emsp;<script type="math/tex">v_i</script> 表示 v 的第 i 维，<script type="math/tex">H^i_Q</script> 表示 <script type="math/tex">H_Q</script> 的第 i 列，W 是一个 dxd 大小的可训练矩阵。<br>&emsp;&emsp;在 attention weights w 被计算出来之后，我们可以基于 w 计算 <script type="math/tex">E_{Q|col}</script>，作为每个 token 的隐藏输出的加权和，此处的隐藏状态指 LSTM 上的（这句话极其的绕，懒得解释了。只需要注意一点，这句话是在计算一个单词的情况下，而并非计算整个 question）。</p>
<script type="math/tex; mode=display">
E_{Q|col} = H_Q w</script><p>&emsp;&emsp;在 <strong>Sequence-to-set</strong> 中的表达式，我们可以将 <script type="math/tex">E_Q</script> 替换为 <script type="math/tex">E_{Q|col}</script>，以获得 column attention model。</p>
<script type="math/tex; mode=display">
    P_{wherecol}(col|Q) = \sigma(u^T_c E_{col} + u^T_q E_{Q|col})</script><p>&emsp;&emsp;（下面的略）</p>
<h1 id="博主注"><a href="#博主注" class="headerlink" title="博主注"></a>博主注</h1><p>&emsp;&emsp;论文中似乎没有很明确地说明 column 的 embedding 是如何训练的。但是这一环很重要，因为这篇论文是关于 wikiSQL 数据集，训练 seq2sql 的比较前的论文，也就是说后面的论文有一些就是使用这篇论文的方法。所以这一点没搞懂，导致也看不懂其他的论文。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>seq2sql</tag>
      </tags>
  </entry>
  <entry>
    <title>【读书笔记】：《自然语言处理综论》（第二版）</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/nlp/%E3%80%90%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%91%EF%BC%9A%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%BB%BC%E8%AE%BA%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89.html</url>
    <content><![CDATA[<h1 id="正则表达式与自动机"><a href="#正则表达式与自动机" class="headerlink" title="正则表达式与自动机"></a>正则表达式与自动机</h1><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p>&emsp;&emsp;正则表达式首先由 Kleene（1956）研制的。</p>
<h1 id="N元语法"><a href="#N元语法" class="headerlink" title="N元语法"></a>N元语法</h1><p>&emsp;&emsp;猜测哪一个单词最可能跟在下面的句子片段后面。“Please turn your homework…”。其中最可能的单词是“in”，或者是“over”，但不可能是“the”。<br>&emsp;&emsp;我们把这种<strong>猜测单词</strong>的问题采用诸如 N 元语法模型（N-gram model）这样的概率模型来形式化地加以描述。N 元语法模型根据前面出现的 N - 1 个单词猜测下面一个单词。一个 N 元语法是包含 N 个单词的序列：2 元语法一般称为 bigram，如 please turn，turn your，your homework；3 元语法一般称为 trigram，如 please turn your，turn your homework。这种单词序列的概率模型又称为<strong>语言模型</strong>（Language Models，ML）。<br>&emsp;&emsp;</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
  </entry>
  <entry>
    <title>论文笔记：TypeSQL: Knowledge-based Type-Aware Neural Text-to-SQL Generation</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/38%E3%80%81TypeSQL%EF%BC%9AKnowledge-based%20Type-Aware%20Neural%20Text-to-SQL%20Generation.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.gg363.site/pdf/1804.09769.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2018 年。</p><ul><li>所用数据集<ul><li>WikiSQL</li></ul></li></ul>
          </div>
<p>&emsp;&emsp;对 text-sql 任务提出了 TypeSQL 模型，将问题视为 slot filiing task，使用 type（详见下面内容） 信息以更好的理解输入中稀有实体和和数字。<br>&emsp;&emsp;对关系型数据库构建一个自然语言接口是一个重要且具有挑战的问题（(Li and Jagadish, 2014; Pasupat and Liang, 2015; Yin et al., 2016; Zhong et al., 2017; Yaghmazadeh et al., 2017; Xu et al., 2017; Wang et al., 2017a）。本论文使用 WikiSQL，它是 <strong>text-to-SQL</strong> 问题的一个巨大的<strong>基准数据集</strong>。对于该任务，具体来说是给定一个关于数据表的自然语言问题及其协议，系统需要生成与该问题对应的 SQL 查询。<br>&emsp;&emsp;本文基于之前的 state-of-the-art SQLNet（<a href="https://openreview.net/pdf?id=SkYibHlRb" target="_blank" rel="noopener">Xu et al., 2017: Sqlnet: Generating structured queries from natural language without reinforcement learning</a>），TYPESQL 使用一个 <strong>sketch-based</strong> 方法，并将此任务视为 slot filing 问题。<br>&emsp;&emsp;进一步，特定于一个数据库的情况下，自然语言问题通常会包含不常见的实体和数字。之前的一些工作 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.4408&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Agrawal and Srikant, 2003: Searching with numbers</a> 已经展示了这些词汇对许多下游任务起着重要作用，但是在预训练词嵌入模型中，大部分词汇缺乏准确的 embeddings。为了解决这一问题，无论单词来自知识图谱、数据库的列还是数字，TYPESQL 为每一个单词分配一个 type。例如，在图 1 中，我们将“mort drucker”作为 PERSON，对应于我们的知识图谱；将“spoofed title”，“artist”和“issue” 作为 COLUMN，因为它们是数据的列名；最后将 “88.5” 作为 FLOAT。结合这一发明，TYPESQL 进一步提高了 WiKiSQL 上的性能。<br>&emsp;&emsp;此外，先前大部分工作假定用户的查询包含准确的列名和实体，但是这是不切实际的。为了解决这一问题，……。<br><a id="more"></a></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>&emsp;&emsp;类似 SQLNet，我们使用了 sketch-based 方法，并且将该任务视为 slot filling。首先预处理问题输入，识别 type。然后使用两层 bi-directional LSTMs 去 encode 问题的单词，encode 时分别利用了 type 和列名（数据库）。最后用 LSTMs 输出的隐藏状态预测 SQL sketch 中 slot 的值。</p>
<h2 id="Type-Recognition-for-Input-Preprocessing"><a href="#Type-Recognition-for-Input-Preprocessing" class="headerlink" title="Type Recognition for Input Preprocessing"></a>Type Recognition for Input Preprocessing</h2><p>&emsp;&emsp;<strong>首先将每个问题分为长度 2-6 的 n-gram 语法，然后在 table scheme 中使用它们进行搜索</strong>（这步很关键，但是我觉得用 n-gram 语法可能会错过一些列名吧），并且将问题中出现的任意列名打上 COLUMN 标签。其他类别做类似操作，转换如下所示（以下 type 均来自 Freebase）：</p>
<ol>
<li>问题中出现的列名 -&gt; COLUMN</li>
<li>问题中的数字和日期 -&gt; INTEGER, FLOAT, DATE, and YEAR</li>
<li>命名体 -&gt; PERSON, PLACE, COUNTRY, ORGANIZATION, and SPORT</li>
</ol>
<p>&emsp;&emsp;五种类别的命名体以及涵盖了数据集中的大部分实体，因此不再使用 Freebase 提供的其他实体类型。</p>
<h2 id="Input-Encoder"><a href="#Input-Encoder" class="headerlink" title="Input Encoder"></a>Input Encoder</h2><p>&emsp;&emsp;如图 1 所示，我们的 input encoder 由 bi-LSTM 组成，分别为：<script type="math/tex">\text{Bi-LSTM}^{QT}</script> 和 <script type="math/tex">\text{Bi-LSTM}^{COL}</script>。为了编码问题中的一对 word 和 type，<strong>我们将 word 和对应的 type 的嵌入拼接起来，然后将它们输入进 <script type="math/tex">\text{Bi-LSTM}^{QT}</script></strong>。最后分别输出隐藏状态 <script type="math/tex">\text{H}_{QT}</script> 和 <script type="math/tex">\text{H}_{COL}</script>。<br>&emsp;&emsp;为了<strong>编码列名</strong>，SQLNet 使用 Bi-LSTM 对每一个列名编码。我们首先平均具有 COLUMN 类型的单词的嵌入，然后只使用<strong>一个</strong> <script type="math/tex">\text{Bi-LSTM}^{COL}</script> 编码。这样的编码方法提高了 1.5% 的性能，并且使得时间减半。<em>我感觉这篇论文写得好乱，有点读不懂这部分</em>。可能需要看一下 SQLNet</p>
<h2 id="Slot-Filling-Model"><a href="#Slot-Filling-Model" class="headerlink" title="Slot-Filling Model"></a>Slot-Filling Model</h2><p>&emsp;&emsp;接下来，我们预测 SQL sketch 中 slots 的值。<br>&emsp;&emsp;文章<strong>沿用</strong>了 SQLNet 的 <strong>Column Attention</strong> 机制，即将 question 输入 Bi-LSTM 后得到的 <script type="math/tex">H_{QT}</script> 和 column 的 <script type="math/tex">H_{COL}</script> 做 Attention。关于列的编码部分，上面说了看不懂。计算过程为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \alpha_{QT/COL} & = softmax(H_{COL} W_{ct} H^T_{QT}) \\
    H_{QT/COL} & = \alpha_{QT/COL} H_{QT} \\
\end{aligned}</script><p>&emsp;&emsp;最后我们就得到了 <script type="math/tex">H_{QT/COL}</script> 隐藏状态。然后使用这个隐藏状态进行预测。具体公式为<br><strong>MODEL COL-$SELECT COL</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    s & = V^{sel} tanh(W^{sel}_c H^T_{COL} + W^{sel}_{qt} H^T_{QT/COL}) \\
    P_{sel_col} & = softmax(s) \\
\end{aligned}</script><p>&emsp;&emsp;<script type="math/tex">P_{sel_col}</script> 就是每个单词的概率，我们可以使用 argmax() 函数得到最大概率的索引。</p>
<p><strong>MODEL COL-$COND#</strong>：<br>&emsp;&emsp;。。。略</p>
<p>&emsp;&emsp;对于不同的模型，论文中都有说明，就不一一记录了。<br><div class="note info">
            <p>&emsp;&emsp;之前理解错了，还以为跟 encoder-decoder 一模一样，现在才知道原来 slot filling 是这样的。<br>&emsp;&emsp;跟语义解析来比较，就是说大致的生成语句已经给你写好了，剩下的几个空，用 attention 的方法来填充，可以理解为不需要 decoder 部分了。</p>
          </div></p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>seq2sql</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Semantic Parsing over Multiple Knowledge-bases</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/39%E3%80%81Neural%20semantic%20parsing%20over%20multiple%20knowledge-bases.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.gg363.site/pdf/1702.01569.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;思想：将不同领域的数据集合并，以提高训练集大小。<br>&emsp;&emsp;语义分析被认为是将语言语句翻译为可执行的逻辑形式的技术。要做到普遍使用语义分析的一个基本阻碍是<strong>在新领域标注逻辑形式的代价太大</strong>。为了解决这一问题，先前工作的策略有从 denotations、paraphrases、m declarative sentences 训练。<br>&emsp;&emsp;本论文提出一个正交解：将来自不同域中的多个数据集的样本合并到一起，每个数据集对应一个单独的知识库（KB），并在所有示例上训练模型。这次方法由于观察到知识库在实体和属性上有所不同，但语言组合的结构在领域之间重复，所以由此启发而来。例如，语言中的“最大”对应于“argmax”，动词后跟一个名词通常表示连接操作。与仅在单个领域上训练的模型相比，跨域共享信息的模型可以提高泛化能力。<br>&emsp;&emsp;最近 <a href="https://arxiv.gg363.site/pdf/1606.03622.pdf" target="_blank" rel="noopener">Jia and Liang, 2016: Data recombination for neural semantic parsing</a> 以及 <a href="https://arxiv.gg363.site/pdf/1601.01280.pdf" target="_blank" rel="noopener">Dong and Lapata, 2016: Language to logical form with neural attention</a> 提出了用于语义分析的 seq2seq 模型。将语言和逻辑形式简单地表示为向量形式，这些神经网络模型大致上能促进信息共享。我们以他们的工作为基础，研究了在语言编码和逻辑形式解码过程中跨领域共享表示的模型（即研究语言和逻辑形式在不同领域如何表示，如医学和旅游业）。我们最终发现，<strong>通过向解码器提供领域的表征，我们可以在多个领域上训练单个模型，并且与在每个领域上分别训练的模型相比，大大提高了准确性</strong>。在 Overnight 数据集上提高了性能，并减少了网络参数。<br><a id="more"></a></p>
<h1 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h1><p>&emsp;&emsp;seq2seq + attention.</p>
<h1 id="多个-KB-上的模型"><a href="#多个-KB-上的模型" class="headerlink" title="多个 KB 上的模型"></a>多个 KB 上的模型</h1><p>&emsp;&emsp;本文，我们强调一项设置：我们访问来自不同领域的训练集 K，每个领域对应不同的 KB。所有领域的输入都是自然语句，标签都是逻辑形式（<strong>我们假定被标注逻辑形式可以被转换为单个形如 lambda-DCS 的形式语言</strong>）。虽然从单词到 KB 常量的映射在每个域都是特定的，但是我们期望语言所表达的意义可以跨域共享。下面开始描述模型架构。</p>
<h2 id="One-to-one-model"><a href="#One-to-one-model" class="headerlink" title="One-to-one model"></a>One-to-one model</h2><p>&emsp;&emsp;此模型类似于 <strong>Section 2</strong> 所描述的模型（Jia and Liang, 2016），如 Figure 2 所示。它由一个 encoder 和一个 decoder 组成，可以用于生成所有领域的输出。因此，模型所有参数由所有领域共享，并且模型从所有样本中训练。</p>
<h2 id="Many-to-many-model"><a href="#Many-to-many-model" class="headerlink" title="Many-to-many model"></a>Many-to-many model</h2><p>&emsp;&emsp;</p>
<h2 id="One-to-many-model"><a href="#One-to-many-model" class="headerlink" title="One-to-many model"></a>One-to-many model</h2><p>&emsp;&emsp;单个 encoder 共享，但是为每个领域设置一个独立的 decoder。共享的 encoder 捕获每个领域输入的英语单词序列的事实，特定领域的 decoder 学习来自正确领域下词表的输出标记（tokens）。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><ul>
<li>数据集：Overnight</li>
</ul>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>&emsp;&emsp;复制了 Jia and Liang, 2016 的实验配置，使用相同的超参数。。。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>语义解析KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural network-based question answering over knowledge graphs on word and character level</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/36%E3%80%81Neural%20network-based%20question%20answering%20over%20knowledge%20graphs%20on%20word%20and%20character%20level.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://www.researchgate.net/publication/315769814_Neural_Network-based_Question_Answering_over_Knowledge_Graphs_on_Word_and_Character_Level" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;基于向量建模的方法。<br><a id="more"></a></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1506.02075.pdf" target="_blank" rel="noopener">A. Bordes, 2015: Large-scale simple question answering with memory networks</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1604.00727.pdf" target="_blank" rel="noopener">D. Golub and X. He, 2016: Character-level question answering with attention</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1606.03391.pdf" target="_blank" rel="noopener">W. Yin, 2016: Simple question answering by attentive convolutional neural network</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1606.01994.pdf" target="_blank" rel="noopener">Z. Dai, 2016: Cfo: Conditional focused neural question answering with large-scale knowledge bases</a><br>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>向量建模KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>【NLP算法】（三）条件随机场CRF</title>
    <url>/%C2%B7zcy/AI/nlp/%E3%80%90NLP%E7%AE%97%E6%B3%95%E3%80%91%EF%BC%88%E4%B8%89%EF%BC%89%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF.html</url>
    <content><![CDATA[<h1 id="马尔科夫链——Markov-Chain"><a href="#马尔科夫链——Markov-Chain" class="headerlink" title="马尔科夫链——Markov Chain"></a>马尔科夫链——Markov Chain</h1><p>&emsp;&emsp;有时也称为显马尔科夫模型（observed Markov model）。<br>&emsp;&emsp;是一个随机过程模型，它表述了一系列可能的事件，在这个系列当中每一个事件的概率仅依赖于前一个事件。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/【NLP算法】（三）条件随机场CRF/马尔科夫链.jpg" alt="马尔科夫链"><br><a id="more"></a></p>
<h1 id="隐马尔科夫模型——HMM"><a href="#隐马尔科夫模型——HMM" class="headerlink" title="隐马尔科夫模型——HMM"></a>隐马尔科夫模型——HMM</h1><h2 id="概率模型"><a href="#概率模型" class="headerlink" title="概率模型"></a>概率模型</h2><p>&emsp;&emsp;概率模型就是将学习任务归结于计算变量的概率分布的模型。<br>&emsp;&emsp;在生活中，我们经常会根据一些已经观察到的现象来推测和估计未知的东西——这种需求，恰恰是概率模型的推断（Inference）行为所作的事情。<br>&emsp;&emsp;推断的本质是：利用可观测变量来预测未知变量的条件分布。<br>&emsp;&emsp;概率模型可以分为两类：生成模型（generative model）和判别模型（discriminative model）。<br>&emsp;&emsp;我们将可观测变量的集合命名为 O，我们感兴趣的未知变量的集合命名为 Y。<br>&emsp;&emsp;生成模型学习出来的是 O 与 Y 的联合概率分布 P(O,Y)，而判别模型学习的是条件概率分布 P(Y|O)。<br>&emsp;&emsp;例如朴素贝叶斯模型就是生成模型，而逻辑回归就是判别模型。</p>
<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><p>&emsp;&emsp;</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/44042528" target="_blank" rel="noopener">一篇不错的 CRF 入门文章</a>，但是后面求所有路径的分数的算法，说得有点模糊，推荐看<a href="https://www.bilibili.com/video/av52626653/?p=4" target="_blank" rel="noopener">HMM与CRF隐形马尔可夫链与条件随机场</a>，虽然这个视频播放量不高，但足以看懂 CRF 最后的那个算法。</li>
<li><a href="https://zhuanlan.zhihu.com/p/27338210" target="_blank" rel="noopener">Bi-LSTM-CRF for Sequence Labeling</a></li>
<li><a href="https://blog.csdn.net/cuihuijun1hao/article/details/79405740" target="_blank" rel="noopener">Pytorch Bi-LSTM + CRF 代码详解</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650411744&amp;idx=3&amp;sn=494fb10386f5ac50038a772a611a5332&amp;chksm=becd94ba89ba1dac7efc734661a2a9db8a5ca05125949fb098db3f25235dea80de6c286ef71c&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1572502435578&amp;sharer_shareid=68f8b84d7a46cc216b0afdc45278d6be&amp;key=5418d699bf014e61a173d7a9169c96f3be502439f456e034c2804919bdfed20c1c70bd250f7c98ddbdc73301145e34b7d3e342d931881416ab32aef3ee84c84e3232eb4b39b4af48117e174d4ed803ad&amp;ascene=1&amp;uin=MTQxMTUzMzk2MA%3D%3D&amp;devicetype=Windows+10&amp;version=62070152&amp;lang=en&amp;pass_ticket=pZijWLQmmCpNBDcjO4cUImTRWv1ZWLG4JENv1zUqjhXnUnShPGofPjjR%2Bkv1cozV" target="_blank" rel="noopener">NLP硬核入门-条件随机场CRF</a></li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>马尔科夫链</tag>
        <tag>HMM</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/35%E3%80%81Seq2sql%EF%BC%9AGenerating%20structured%20queries%20from%20natural%20language%20using.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1709.00103.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。<br>&emsp;&emsp;这篇论文发布了 wikisql 数据集，同时提出了 seq2sql 任务。</p>
          </div>
<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;本文主要做出两项贡献：1）提出 Seq2SQL，将自然语言问题翻译为其对应的 SQL queries。2）发布 WikiSQL 语料库，其包含 80654 个人工标注的自然语言问题实例， SQL queries 以及从 24241 张 HTML 网页中提取的 SQL 表（网页来自 Wikipedia）。<em>WikiSQL 比以前提供给 logical forms 和自然语句的语义分析数据集大一个数量级</em>。发布 WikiSQL 的同时，我们还发布了一个此数据库的查询引擎（query execution engine）<br><div class="note primary">
            <p>&emsp;&emsp;本论文将自然语言转为 SQL，关系型数据库。而知识图谱是非关系型数据库存储的。</p>
          </div><br>&emsp;&emsp;关系型数据库存储了大量的信息并用此构建了许多应用，但是访问关系型数据库需要使用 sql 语句并且很难精通它。于是 Natural language interfaces(NLI) 寻求一条路径使得人类和计算机交互成为可能，即将自然语言翻译为 sql 语句。<br>&emsp;&emsp;balabala…<br>&emsp;&emsp;在 wikisql 数据集上，seq2sql 比先前 Dong &amp; Lapata(2016) 做的语义解析模型效果要好。<br><a id="more"></a></p>
<h1 id="2-Model"><a href="#2-Model" class="headerlink" title="2 Model"></a>2 Model</h1><p>&emsp;&emsp;我们的基准模型是 Dong &amp; Lapata(2016) 做的 seq2seq + attention 的模型，它在未使用人工语法的语义解析数据集上实现了最高的性能。<strong>但是这个 seq2seq 模型的 softmax 的输出空间对于这个任务太大了</strong>。（博主注：生成 sql 语句时，并不需要在整个字典中找。sql 语句在某些地方是固定的。比如 select balabala from balabala，格式都是固定的，比如 select，count 等）因此我们可以将生成序列的输出空间限制为 <strong>table schema, question utterance, and SQL key words的并集</strong>。最终模型类似于加入了 augmented inputs 的 <strong>pointer network</strong>。我们</p>
<ol>
<li>首先描述 augmented pointer network model；</li>
<li>其次说明我们定义 seq2sql 的局限性，特别是在生成<em>无序查询条件</em>方面。</li>
</ol>
<h2 id="augmented-pointer-network-model"><a href="#augmented-pointer-network-model" class="headerlink" title="augmented pointer network model"></a>augmented pointer network model</h2><h2 id="seq2sql"><a href="#seq2sql" class="headerlink" title="seq2sql"></a>seq2sql</h2><h1 id="博主注"><a href="#博主注" class="headerlink" title="博主注"></a>博主注</h1><p>&emsp;&emsp;论文提出了 seq2sql 模型，为后面的工作铺垫了基础。基线模型是 Dong 2016 年提出的 seq2seq + attention 模型，seq2sql 为第二个模型。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>seq2sql</tag>
      </tags>
  </entry>
  <entry>
    <title>论文复现：Language to Logical Form with Neural Attention</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%EF%BC%9ALanguage%20to%20Logical%20Form%20with%20Neural%20Attention.html</url>
    <content><![CDATA[<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><p>&emsp;&emsp;论文的地址<a href="https://arxiv.org/pdf/1601.01280.pdf" target="_blank" rel="noopener">在此</a>，作者使用了 Lua 语言实现，代码地址<a href="https://github.com/donglixp/lang2logic" target="_blank" rel="noopener">在这</a>，然而我不会 Lua 语言，于是找了找是否有 Python 的实现版本。还真有，Python 版本代码地址<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch" target="_blank" rel="noopener">在这</a>。<br>&emsp;&emsp;但是 Python 版本的代码<strong>篇幅太长</strong>，且<strong>几乎没有注释</strong>，于是我将其重写了一遍，一些工具类是直接复制别人的，但是核心代码我改写了一下，并添加了一些注释。<br><div class="note info">
            <p>&emsp;&emsp;我将里面的数据获取模块移除了。</p>
          </div><br><a id="more"></a></p>
<h1 id="论文实现"><a href="#论文实现" class="headerlink" title="论文实现"></a>论文实现</h1><p>&emsp;&emsp;论文共用了两个办法：1）普通 seq2seq 模型；2）作者自创的 seq2tree 模型。其中每个模型又分别有 <strong>lstm 实现</strong>和 <strong>lstm + attention 实现</strong>两种版本。虽然两个版本使用的技术不同，但是说到底也只是同一个模型。以下讲解原理。</p>
<h2 id="seq2seq-模型"><a href="#seq2seq-模型" class="headerlink" title="seq2seq 模型"></a>seq2seq 模型</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>&emsp;&emsp;论文中使用 LSTM 实现 seq2seq 模型，训练之后，accuracy 大约在 70%。</p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>&emsp;&emsp;我自己用了 Transformer 改写了一下，并且调了几天的参数，发现效果出奇的差，accuracy 最好只有 18%。然后我还发现，对于短句子几乎是百分比预测正确，对于长句子百分比预测错误。所以<strong>我怀疑是否是位置编码那产生的问题，考虑到一个逻辑形式它并不是纯粹的线性结构，它的内部是由很多括号的</strong>。<br>&emsp;&emsp;经调参后得到最好的一组参数如下：</p>
<ol>
<li>learning rate: 0.001</li>
<li>dim_feedforward: 随意（我设置为 256）</li>
<li>h_model: 256</li>
<li>nhead: 4</li>
<li>encoder_layer/decoder_layer: 1</li>
<li>dropout: 0.4</li>
<li>batch_size: 32（16 的效果可能更好）</li>
<li>epoch: 95（epoch 可以进一步修改）</li>
<li>src_mask: False</li>
<li>tgt_mask: True</li>
<li>memory_mask: False</li>
</ol>
<h2 id="seq2tree-模型"><a href="#seq2tree-模型" class="headerlink" title="seq2tree 模型"></a>seq2tree 模型</h2><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习的下一步</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5.html</url>
    <content><![CDATA[<ul>
<li>机器学习能不能知道“我不知道”<br>机器学习的 classifier 可以判断一张图片是不是猫，但是能不能判断出“我不知道这是什么”？这项技术叫做 <strong>Anomaly Detection</strong>。</li>
<li>机器说出为什么“我知道”<ul>
<li>神马汉斯的例子</li>
<li>马辨识器的例子。机器只是辨识了英文字母<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/机器学习的下一步/说出为什么“我知道”.jpg" alt="说出为什么“我知道”"></li>
</ul>
</li>
<li>机器的错觉？<ul>
<li>adversarial attack。感觉是 CV 里的技术<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/机器学习的下一步/机器的错觉.jpg" alt="机器的错觉"></li>
</ul>
</li>
<li>终身学习（Life-long Learning）<br>机器能否终身学习。现在模型一般只能对应一个任务，如果让一个模型去学习下围棋，之后再让它去学习玩星海。那么它就不会下围棋了。这被为 <strong>Catastrophic Forgetting</strong>。</li>
<li>学习如何学习<br>如何写一个<strong>能够写出具有学习能力的程序</strong>的程序。这被称为 <strong>Meta-learning/Learn to learn</strong>。</li>
<li>一定需要很多训练数据吗？<ul>
<li>Few-shot learning</li>
<li>Zero-shot learning</li>
</ul>
</li>
<li>Reinforcement learning</li>
<li>神经网络压缩（Network Compression）<ul>
<li>把大神经网络路缩小</li>
<li>参数二元化<ul>
<li>所有的参数都变成 +1 或 -1</li>
</ul>
</li>
</ul>
</li>
<li>如果训练数据和测试数据长得不一样<ul>
<li>对于 CV 来说，训练数据和测试数据长得差不多，比如手写体识别。但是如果在真实场景中，测试数据是彩色的，可能会出现准确率骤降的情况。那么如何解决呢？<a id="more"></a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>【机器学习算法】半监督学习</title>
    <url>/%C2%B7zcy/AI/ml/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E3%80%91%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>&emsp;&emsp;半监督学习就是在已有的带标签的数据之后，还有一组不带标签的数据。一般来说，在做无监督学习时，unlabeled data 远大于 labeled data。<br>&emsp;&emsp;半监督学习一般分为两种：</p>
<ul>
<li>Transductive learning：unlabeled data 就是你的 testing sdata</li>
<li>Inductive learning：unlabeled data 不是你的 testing sdata<a id="more"></a>
</li>
</ul>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/【机器学习算法】半监督学习/导读.jpg" alt="导读"></p>
<h2 id="为什么做半监督学习"><a href="#为什么做半监督学习" class="headerlink" title="为什么做半监督学习"></a>为什么做半监督学习</h2><p>&emsp;&emsp;有人说机器学习训练数据很少，其实不完全对。因为只是 labeled data 少，unlabeled data 随处可见。所以如果能将这些 unlabeled data 运用进去就好了。原因如下：</p>
<ul>
<li>搜集数据很简单，但是搜集 labeled data 代价昂贵</li>
<li>生活中，我们自己也在做半监督学习</li>
</ul>
<h1 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h1><h1 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h1><h1 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h1><h1 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h1>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>系列</tag>
      </tags>
  </entry>
  <entry>
    <title>Coarse-to-Fine Decoding for Neural Semantic Parsing</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/34%E3%80%81Coarse-to-Fine%20Decoding%20for%20Neural%20Semantic%20Parsing.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;提出一个结构感知的神经架构，将语义解析过程分解为如下两个步骤：给定一个输入语句，1）首先生成它含义的粗略草图（a rough sketch of its meaning），其中低级信息<strong>被掩盖</strong>（如<strong>变量名和参数</strong>）。2）然后考虑输入本身和草图来填充丢失的细节。<br>&emsp;&emsp;RNN 在多种 NLP 任务中的成功应用对 seq2seq 的语义解析产生了强大的冲击力，如<a href="https://arxiv.org/pdf/1606.03622.pdf" title="Data recombination for neural semantic parsing" target="_blank" rel="noopener">Jia and Liang, 2016</a>; Dong and Lapata, 2016; <a href="https://arxiv.org/pdf/1603.06744.pdf" title="Latent predictor networks for code generation" target="_blank" rel="noopener">Ling et al., 2016</a>。<br>&emsp;&emsp;我们认为，这种方法至少有三个优点。首先，分解步骤<strong>将高级语义信息与低级语义信息分离开来</strong>，使译码器能够在不同的粒度级别对语义进行建模。其次，模型可以明确地为具有相同草图（即基本含义）的示例共享粗糙结构的知识，即使它们的实际含义表示不同（例如，由于不同的细节）。第三，在生成草图后，解码器知道语句的基本含义是什么，<strong>模型可以将其作为全局上下文来改进对最终细节的预测</strong>。<br>&emsp;&emsp;使用如下数据集：</p>
<ol>
<li>GEO</li>
<li>ATIS</li>
<li>DJANGO</li>
<li>WikiSQL<a id="more"></a>
</li>
</ol>
<h1 id="问题阐释"><a href="#问题阐释" class="headerlink" title="问题阐释"></a>问题阐释</h1><p>&emsp;&emsp;定义 <script type="math/tex">x = x_1 \dots x_{|x|}</script> 为自然语句，<script type="math/tex">y = y_1 \dots y_{|y|}</script>为意义表示，<script type="math/tex">a = a_1 \dots a_{|a|}</script>为 sketch 表示。<del>注意 <strong>sketch 的定义为</strong>：一个中间变量，如将自然语句转化为 Logical Form，Source Code，SQL，noSQL，SPARQL等表示，这些表示都算是一个 sketch。</del>下图论文架构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Coarse-to-Fine Decoding for Neural Semantic Parsing/Coarse2Fine架构.jpg" alt="Coarse2Fine架构"></p>
<h2 id="Sketch-Generation"><a href="#Sketch-Generation" class="headerlink" title="Sketch Generation"></a>Sketch Generation</h2><p>&emsp;&emsp;encoder 将自然语句编码为向量，decoder 去计算 <script type="math/tex">p(a|x)</script> 从而通过encoding 向量 生成 sketch a。具体来讲，<strong>Input Decoder</strong> 将字转为词向量，并使用 Bi-LSTM 训练。<strong>Coarse Meaning Decoder</strong> 生成 sketch a，也使用 LSTM 并且加上 attention 机制。</p>
<h2 id="Meaning-Representation-Generation"><a href="#Meaning-Representation-Generation" class="headerlink" title="Meaning Representation Generation"></a>Meaning Representation Generation</h2><p>&emsp;&emsp;Meaning representation 由输入 x 以及生成的 sketch a 预测产生，具体就是计算 <script type="math/tex">p(y|x,a)</script>。<strong>Sketch Encoder</strong> 与 Input Decoder 类似，使用 Bi-LSTM 并将 sketch a 映射为词向量。<strong>Fine Meaning Decoder</strong> 与 Coarse Meaning Decoder 类似。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;总的来说就是先用自然语句生成 coarse sketch，然后再用 coarse sketch 生成 fine sketch。一共使用了两个 encoder-deocder 模型，但是将这两个模型连起来用。</p>
<h1 id="三个语义分析任务"><a href="#三个语义分析任务" class="headerlink" title="三个语义分析任务"></a>三个语义分析任务</h1><p>&emsp;&emsp;为了证明我们的框架适用于跨域和意义表示，我们为三个任务开发了模型，即将自然语言解析为逻辑形式、Python 源代码和 SQL 查询。对于每一个任务，我们都描述了使用的数据集以及 sketch 提取的提取步骤。</p>
<h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><p>&emsp;&emsp;</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>语义解析KBQA</tag>
        <tag>sketch-based</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习算法（四）：Transformer</title>
    <url>/%C2%B7zcy/AI/dl/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ATransformer.html</url>
    <content><![CDATA[<h1 id="Transformer位置信息"><a href="#Transformer位置信息" class="headerlink" title="Transformer位置信息"></a>Transformer位置信息</h1><p>&emsp;&emsp;引用自<a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></p>
<blockquote>
<p>&emsp;&emsp;NLP 句子中单词之间的相对位置是包含很多信息的，上面提过，RNN 因为结构就是线性序列的，所以天然会将位置信息编码进模型；而CNN的卷积层其实也是保留了位置相对信息的，所以什么也不做问题也不大。但是对于 Transformer 来说，为了能够保留输入句子单词之间的相对位置信息，必须要做点什么。为啥它必须要做点什么呢？因为输入的第一层网络是 Muli-head self attention 层，我们知道，Self attention 会让当前输入单词和句子中任意单词发生关系，然后集成到一个 embedding 向量里，但是当所有信息到了 embedding 后，位置信息并没有被编码进去。所以，Transformer 不像 RNN 或 CNN，必须明确的在输入端将 Positon 信息编码，Transformer 是用<strong>位置函数</strong>来进行位置编码的，而 Bert 等模型则给每个单词一个 <strong>Position embedding</strong>。将单词 embedding 和单词对应的 position embedding 加起来形成单词的输入 embedding，类似上文讲的 ConvS2S 的做法。</p>
</blockquote>
<h1 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h1><p>&emsp;&emsp;从文章<a href="https://zhuanlan.zhihu.com/p/59629215" target="_blank" rel="noopener">《可视化理解Transformer结构》</a>做的总结，此文为另一篇英文文章的翻译版，英文原文地址为 <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>。此外另一篇文章<a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">《BERT大火却不懂Transformer？读这一篇就够了》</a>也做了翻译。<br>&emsp;&emsp;Transformer 不同于 RNN，它的输入是独立计算的，输入序列的某个时间步并不依赖其它的时间步。也就是说它可以<strong>并行运算</strong>。<br><a id="more"></a></p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>&emsp;&emsp;在第一节中讲到，Transformer 天然地不具有句子的<strong>位置</strong>属性，所以我们需要使用一种办法让它拥有句子的位置属性。<br>&emsp;&emsp;为解决这个问题，Transformer 为每个输入的词嵌入增加了一个向量。<br><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png" alt="To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern."></p>
<p>&emsp;&emsp;如果假定词嵌入维度为4，那真实的位置编码如下：<br><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png" alt="A real example of positional encoding with a toy embedding size of 4"></p>
<p>&emsp;&emsp;位置编码的生成方法在原论文的 section 3.5 中有描述。你可以在 <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py" target="_blank" rel="noopener">get_timing_signal_1d()</a> 函数中看到用于生成位置编码的代码。这并不是生成位置编码的唯一方式。然而，它的优点在于可以扩展到看不见的序列长度（eg. 如果要翻译的句子的长度远长于训练集中最长的句子）。</p>
<h2 id="self-Attention"><a href="#self-Attention" class="headerlink" title="self-Attention"></a>self-Attention</h2><p>&emsp;&emsp;在 <a href="https://www.bilibili.com/video/av46561029?p=60" target="_blank" rel="noopener">此视频</a>（08:30） 中，x 首先要乘一个权重变成 a。但是<strong>实际上是一样的</strong>，视频中的 x 指的是 one hot，而 a 指的是 embedding。然后将 x 分别乘上三个不同的 transformation 来得到<strong>不同</strong>的 q, k, v。<br><img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png" alt="Transformer计算机制"></p>
<blockquote>
<p>注意，这些新创建的向量的维度小于词嵌入向量(embedding vector)。它们（新创建的向量）的维度是 <strong>64</strong>，而词嵌入和编码器的输入输出向量的维度是 <strong>512</strong>。它们不必更小，这是一种架构选择，可以使多头注意力(multiheaded attention)计算不变。 </p>
</blockquote>
<ol>
<li>需要从每个编码器的输入向量创建三个向量，即 <strong>Query</strong> 向量，<strong>Key</strong> 向量和 <strong>Value</strong> 向量；<ul>
<li>那么，究竟什么是“query”，“key”，“value”向量呢？（以下为自己的猜测）<ul>
<li>q 代表当前需要计算 score 的向量，此向量查询 key 中的权重，使得查询到自己需要的 score</li>
<li>k 代表当前单词的权重</li>
<li>v 代表单词的词向量</li>
</ul>
</li>
</ul>
</li>
<li>计算得分（score 权重）。这里的分数是通过将 query 向量与我们正在评分的单词的 key 向量做点积来得到。所以如果我们计算位置 #1 处的单词的 self-attention，第一个得分就是就是 <script type="math/tex">q_1</script> 和 <script type="math/tex">k_1</script> 的点积。第二个得分是 <script type="math/tex">q_1</script> 和 <script type="math/tex">k_2</script> 的点积。<br><img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png" alt="计算 score"></li>
<li>将分数除以8（论文中使用 Key 向量维数的平方根—-64。这可以有更稳定的梯度。实际上还可以有其他可能的值，这里使用默认值）<br><img src="https://jalammar.github.io/images/t/self-attention_softmax.png" alt="计算 score"></li>
<li>然后经过一个softmax操作后输出结果。Softmax可以将分数归一化，这样使得结果都是正数并且加起来等于1<br><img src="https://jalammar.github.io/images/t/self-attention-output.png" alt></li>
<li>将每个 value 向量乘以 softmax 得分（准备将他们相加）</li>
<li>对加权值向量求和，这样就产生了在这个位置的self-attention的输出（对于第一个单词）</li>
</ol>
<p>&emsp;&emsp;上述为 self-attention 单个单词的计算步骤，其实它可以并行计算。<br><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" alt="并行计算第一步"></p>
<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" alt="并行计算第二步"></p>
<h2 id="multi-headed-attention"><a href="#multi-headed-attention" class="headerlink" title="multi-headed attention"></a>multi-headed attention</h2><p>&emsp;&emsp;看英文名感觉“高大上”，其实很简单。就是将上述的 self-attention 做多次，具体做几次你可以自行选择，Transformer 选择了 8 次。这样就产生了 8 个向量，但是我们训练时其实只要一个向量就够了，所以我们将这 8 个向量<strong>拼接</strong>起来，这样就形成了一个巨长无比的向量。那我们怎么得到我们所需长度的向量呢？很简单，只需要再乘一个权重矩阵就行了。<br><img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png" alt></p>
<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png" alt></p>
<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png" alt></p>
<p>&emsp;&emsp;全过程：<br><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="全过程"></p>
<h2 id="The-Residuals-与-layer-normalization"><a href="#The-Residuals-与-layer-normalization" class="headerlink" title="The Residuals 与 layer-normalization"></a>The Residuals 与 layer-normalization</h2><p>&emsp;&emsp;残差连接说白了就是跳过一层，输入到下一层（比较直白，不一定对）。<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png" alt></p>
<p>&emsp;&emsp;layer-normalization，此 <a href="https://www.bilibili.com/video/av56239558" target="_blank" rel="noopener">视频</a> 中（41:00），李宏毅老师说一般情况 layer normalization 会搭配 RNN 使用，而 Transformer 很像 RNN，所以可能是这里使用 layer norm 的理由。<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" alt></p>
<p>&emsp;&emsp;此步骤在解码层中是类似的操作。将内部件画全，就是如下所示一般（图中的 encoder、decoder 各有两个）：<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" alt></p>
<p>&emsp;&emsp;在 self_attention 之后要做一次 normalization，此均值归一化步骤的具体算法，详见：</p>
<ul>
<li><a href="https://www.cnblogs.com/hellcat/p/9735041.html#_label3_0" target="_blank" rel="noopener">『计算机视觉』各种Normalization层辨析</a> </li>
<li><a href="https://www.jianshu.com/p/c357c5717a60" target="_blank" rel="noopener">layer normalization 简单总结</a></li>
</ul>
<h1 id="decodedr"><a href="#decodedr" class="headerlink" title="decodedr"></a>decodedr</h1><p>&emsp;&emsp;在讲解 decoder 之前，看一张动图，了解一下 Transformer 是如何运作的。首先下图中生成了第一个字母 I。<br><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" alt></p>
<p>&emsp;&emsp;编码器开始处理输入序列，然后将顶部编码器的输出变换为一组注意力向量 <strong>K_{encdec}</strong> 和 <strong>V_{encdec}</strong>，这些将在每个解码器的“encoder-decoder attention” 层使用，这有助于解码器集中注意力在输入序列的合适位置。<br>&emsp;&emsp;解释一下 <strong><script type="math/tex">K_{encdec}</script></strong> 和 <strong><script type="math/tex">V_{encdec}</script></strong> 的具体用法，<strong>在 encoder 中的 self-attention 会使用到 <script type="math/tex">Q_{self-attention}</script>、<script type="math/tex">K_{self-attention}</script>、<script type="math/tex">V_{self-attention}</script> 三个向量，但是它们实际上是输入值 x 的三份拷贝再乘上各自不同的权重矩阵得来。对于 decoder 的 self-attention 与 encoder 的如出一撤，但是对于 encoder-decoder attention 却有点不一样。<script type="math/tex">Q_{encoder-decoder \, attention}</script> 还是 x 的拷贝乘上一个权重矩阵，但是 <script type="math/tex">K_{encoder-decoder \, attention}</script> 和 <script type="math/tex">V_{encoder-decoder \, attention}</script> 分别是 <script type="math/tex">K_{encdec}</script> 和 <script type="math/tex">V_{encdec}</script> 乘上各自的权重矩阵。</strong>以上所有向量所乘的权重矩阵均可以由你自己随机初始化产生。<strong>博主注</strong>：以上 QKV 的用法均分析自 pytorch 官方实现版本的源码。<br>&emsp;&emsp;接着下面的 gif 完成了余下的步骤。Transformer 将之前输出的 I 当做下一个时间步的输入，又走了一遍 decoder。以此循环往复，直到输出一个结束标记 <code>&lt;EOS&gt;</code> 才结束循环。<strong>博主注</strong>：这样一来似乎对于 decoder 来说并不能实现并行运算。<strong>博主注2</strong>：后来发现其实也可以。<br><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt></p>
<p>&emsp;&emsp;其中 decoder 中的 self-attention 与 encoder 的 self-attenntion 有所区别，一个就是上上段黑体的内容。另外一个是 decoder 的 self-attention 是 <strong>masked multi-head attention</strong>，而 encoder 的仅仅是 <strong>multi-head attention</strong>。详见下面 <strong>padding mask/sequence mask</strong> 一节。</p>
<h1 id="Generator-The-Final-Linear-and-Softmax-Layer"><a href="#Generator-The-Final-Linear-and-Softmax-Layer" class="headerlink" title="Generator:The Final Linear and Softmax Layer"></a>Generator:The Final Linear and Softmax Layer</h1><p>&emsp;&emsp;如何将其转换为一个单词？这就是最后 Softmax 层和线性层的工作了。<br>&emsp;&emsp;线性层是一个简单的全连接神经网络，它将解码器堆叠(decoder stack)产生的向量映射到一个巨大的向量（词汇表的大小，原来的向量大小是词嵌入的大小）中去，这个向量称为 logits 向量。<br>&emsp;&emsp;Softmax 层将这些分数转化为概率(全部为正数，加起来为 1.0)。选择具有最高概率的单元，并将与其相关的单词作为本时间步的输出。</p>
<h1 id="padding-mask-sequence-mask"><a href="#padding-mask-sequence-mask" class="headerlink" title="padding mask/sequence mask"></a>padding mask/sequence mask</h1><p>&emsp;&emsp;无论 encoder 还是 decoder 都要做 mask（很多对 Transformer 的总结文章都只提到了 decoder 部分的 Masked Multihead Self-Attention，实际上 encoder 也要做一次）。mask 一共分为两种，寻常所见的 decoder 中的 mask 指的是 <strong>sequence mask</strong>，encoder 中的 mask 指的是 <strong>padding mask</strong>。详见：</p>
<ul>
<li><a href="https://luozhouyang.github.io/transformer/" target="_blank" rel="noopener">Transformer模型的PyTorch实现</a></li>
<li><a href="https://www.jianshu.com/p/405bc8d041e0" target="_blank" rel="noopener">The Transformer</a></li>
</ul>
<p>&emsp;&emsp;<strong>对于 padding mask</strong>：在 encoder 中的每次 scaled dot-product 都要做一次 sequence mask。由于我们要让序列的长度相等以便做向量化操作，所以必不可少地需要对输入序列进行<strong>截断</strong>或<strong>补零</strong>操作。所以 sequence mask 的<strong>主要目的</strong>是使得我们的 self-attention 不要过多的关注向量中的 0。<strong>具体操作是</strong>：将序列中补零位置的值置为 -INF，使得序列经过 scaled dot-product 后的 softmax 层时，对应位置会得到<strong>概率 0</strong>。<br>&emsp;&emsp;<strong>对于 sequence mask</strong>：使得 decoder 无法看见未来的信息，decoder 的 attention 只能关注解码单词之前的输出单词，而不能依赖后面未解码出来的单词。<strong>博主注</strong>：这个感觉只要在解码的时候按照正常的方式，对一个一个单词进行解码就已经是 masked 的形式了。</p>
<h1 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h1><p>&emsp;&emsp;在最后的 softmax 层直接输出了概率最大的位置的单词，这叫做<strong>贪婪解码——greedy decoding</strong>。<br>&emsp;&emsp;另一种更合理的解码方式叫做 <strong>Beam Search</strong>。</p>
<h1 id="最后总结"><a href="#最后总结" class="headerlink" title="最后总结"></a>最后总结</h1><p>&emsp;&emsp;刚才（2020.1.23）在知乎上看到一篇文章，感觉对 RNN 和 Transformer 的区别总结的不错，所以记录下。<br>&emsp;&emsp;RNN 通过把已经处理过的所有单词的表征与当前正在处理的单词的表征结合起来以此来捕捉整句话的含义，而 Transformer 的 self-attention 机制则是将所有相关单词的含义通过 weighted sum 来充当我们当前正在处理的单词的表征。<br>&emsp;&emsp;参考文章（“从宏观视角看自注意力机制”一节）：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">BERT大火却不懂Transformer？读这一篇就够了</a></li>
</ul>
<p>&emsp;&emsp;注意，相对于普遍只有一层的 LSTM/RNN 模型来说，Transformer 拥有多层结构。所以上述 Transformer 做的 weighted sum 操作，实际上要在一次处理中做上好几遍。</p>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>&emsp;&emsp;全部引用：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/59629215" target="_blank" rel="noopener">可视化理解Transformer结构</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">BERT大火却不懂Transformer？读这一篇就够了</a></li>
<li><a href="https://www.bilibili.com/video/av46561029?p=60" target="_blank" rel="noopener">李宏毅机器学习2019——Transformer</a><ul>
<li><a href="https://www.bilibili.com/video/av56239558" target="_blank" rel="noopener">李宏毅-Transformer</a></li>
</ul>
</li>
<li><a href="https://www.cnblogs.com/hellcat/p/9735041.html#_label3_0" target="_blank" rel="noopener">『计算机视觉』各种Normalization层辨析</a> </li>
<li><a href="https://www.jianshu.com/p/c357c5717a60" target="_blank" rel="noopener">layer normalization 简单总结</a></li>
<li><a href="https://luozhouyang.github.io/transformer/" target="_blank" rel="noopener">Transformer模型的PyTorch实现</a></li>
<li><a href="https://www.jianshu.com/p/405bc8d041e0" target="_blank" rel="noopener">The Transformer</a></li>
</ol>
]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>【NLP算法】（零）NLP基础算法</title>
    <url>/%C2%B7zcy/AI/nlp/%E3%80%90NLP%E7%AE%97%E6%B3%95%E3%80%91%EF%BC%88%E9%9B%B6%EF%BC%89NLP%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html</url>
    <content><![CDATA[<h1 id="one-hot"><a href="#one-hot" class="headerlink" title="one hot"></a>one hot</h1><h1 id="N-gram-算法"><a href="#N-gram-算法" class="headerlink" title="N-gram 算法"></a>N-gram 算法</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/baimafujinji/article/details/51281816" target="_blank" rel="noopener">参考文章</a></p>
<ul>
<li><strong>基本算法</strong><br>有句子：老王吃过了吗？<br>该句子可表示为 <script type="math/tex">S = w_1w_2w_3w_4w_5w_6</script><br>那么该句子的概率为 p(S)=p(w1w2w3w4w5w6) = <script type="math/tex">\prod</script>p(wt|w1w2…wt-1) = p(w1)p(w2|w1)p(w3|w1w2)…<br>由于该算法太复杂，所以就产生了其他的算法。马尔科夫模型，该模型认为，一个词的出现仅仅依赖于它前面出现的几个词。可理解为是n-gram。</li>
<li><strong>n-gram</strong><br>n-gram只根据前n-1个单词来预测第n个单词的概率。<br>比如n=2时，也被称为bi-gram。公式为： p(wt|wt-1) = p(wtwt-1)/p(wt)<br>意思就是计算wt-1后面出现wt的概率<br>比如t等于1，则就是计算“老”后面出现“王”的概率<br><strong>剩下的问题就是如何计算一个字出现的概率</strong>。</li>
<li><strong>如何计算概率</strong><br>可以做一个统计，将所拥有的数据计算出每个字出现的次数。并且再统计两个字连在一起的次数。比如：p(王|老) = p(老王)/p(王) = 600/10000 = 6%<br>在神经网络模型中，计算概率是通过 softmax 函数计算的，即将 one hot 编码的向量经过矩阵运算后传入 softmax 函数中。从而得到一个类似概率的值。</li>
<li><strong>用途</strong><br><a href="http://blog.sciencenet.cn/blog-713101-797384.html" target="_blank" rel="noopener">参考</a><br>20世纪80年代至90年代初,n-gram技术被广泛地用来进行文本压缩,检查拼写错误,加速字符串查找,文献语种识别。90年代,该技术又在自然语言处理自动化领域得到新的应用,如自动分类,自动索引,超链的自动生成,文献检索,无分隔符语言文本的切分等。<br>目前N-gram最为有用的就是自然语言的自动分类功能。基于n-gram的自动分类方法有两大类,一类是人工干预的分类(Classification),又称分类;一类是无人工干预的分类(Clustering),又称聚类。<a id="more"></a>
</li>
</ul>
<h1 id="词袋模型——BOW"><a href="#词袋模型——BOW" class="headerlink" title="词袋模型——BOW"></a>词袋模型——BOW</h1><p>&emsp;&emsp;例如，有文档：你好棒棒</p>
<ol>
<li>建立字典<br>{‘你’:1, ‘好’:2, ‘棒’:3}</li>
<li>bow模型<br>(1, 1)(2, 1)(3, 2)<br>其中第一个元素代表该单词在字典中的id，第二个元素代表该单词在所在文档中出现的次数。</li>
</ol>
]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>n-gram</tag>
        <tag>BOW</tag>
      </tags>
  </entry>
  <entry>
    <title>【知识图谱】（一）从概念到实战</title>
    <url>/%C2%B7zcy/AI/KG/%E3%80%90%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E3%80%91%EF%BC%88%E4%B8%80%EF%BC%89%E4%BB%8E%E6%A6%82%E5%BF%B5%E5%88%B0%E5%AE%9E%E6%88%98.html</url>
    <content><![CDATA[<h1 id="知识图谱描述"><a href="#知识图谱描述" class="headerlink" title="知识图谱描述"></a>知识图谱描述</h1><p>&emsp;&emsp;知识图谱是一种新型的<strong>数据库</strong>，是一种基于图的数据结构。每个节点表示现实世界中存在的“实体”，每条边为实体与实体之间的“关系”。以下为知识图谱的几点作用：</p>
<ul>
<li>从“关系”分析问题</li>
<li>把不同种类的信息连接在一起</li>
<li>一个关系网络</li>
</ul>
<p>&emsp;&emsp;学习知识图谱首先得掌握以下几种技能：</p>
<ol>
<li><strong>基础知识</strong>：自然语言处理、图数据库操作知识、基本编程能力：Python、SQL；</li>
<li><strong>领域知识</strong>：知识图谱构建方法、知识图谱推理方法；</li>
<li><strong>行业知识</strong></li>
</ol>
<h1 id="知识图谱的构建步骤"><a href="#知识图谱的构建步骤" class="headerlink" title="知识图谱的构建步骤"></a>知识图谱的构建步骤</h1><ol>
<li>数据收集(持续收集与更新)（<strong>关键词抽取</strong>、<strong>命名体识别</strong>、<strong>关系抽取</strong>、<strong>事件抽取</strong>）<ol>
<li>原始数据，通常可能是一篇文章<ol>
<li>爬虫技术<ol>
<li>垂直爬虫</li>
<li>搜索引擎相关的爬虫</li>
</ol>
</li>
</ol>
</li>
<li>语料数据，通常词库，词典，同义词</li>
<li>开源的第三方知识图谱，例如搜狗人物关系图</li>
<li>开源的训练好的词向量(word2vec)模型,tfidf</li>
</ol>
</li>
<li>图谱设计<ol>
<li>实体定义(本体)<br>实体：实体类型<ol>
<li>属性<br>例如,手(长度，面积)，类别：身体器官</li>
</ol>
</li>
<li>属性定义</li>
<li>关系定义<ol>
<li>关系也需要定义类别</li>
<li>需要评估关系可以覆盖的数据量，一般服从28 原则，20%的关系，覆盖80%数据<a id="more"></a></li>
</ol>
</li>
</ol>
</li>
<li>知识清洗<ol>
<li><strong>实体消歧</strong></li>
<li><strong>实体统一</strong></li>
</ol>
</li>
<li>知识融合(实体链接)<ol>
<li>实体与关系的融合</li>
<li>实体扩充(融合外部知识图谱或者数据)（<strong>知识合并</strong>）</li>
</ol>
</li>
<li><strong>知识存储</strong>-图数据库</li>
</ol>
<h2 id="知识图谱的架构与设计"><a href="#知识图谱的架构与设计" class="headerlink" title="知识图谱的架构与设计"></a>知识图谱的架构与设计</h2><p>&emsp;&emsp;略</p>
<h2 id="知识源数据的获取"><a href="#知识源数据的获取" class="headerlink" title="知识源数据的获取"></a>知识源数据的获取</h2><p>&emsp;&emsp;略。可以使用爬虫等技术，或者直接网上搜现成的数据。</p>
<h2 id="信息抽取"><a href="#信息抽取" class="headerlink" title="信息抽取"></a>信息抽取</h2><p>&emsp;&emsp;包括关键词抽取、命名体识别、关系抽取，事件抽取等技术。</p>
<h3 id="关键词抽取"><a href="#关键词抽取" class="headerlink" title="关键词抽取"></a>关键词抽取</h3><h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>&emsp;&emsp;分类算法中的流程：<br>&emsp;&emsp;分词—&gt;(自然语言处理,与,知识,图谱,知识图谱)—&gt;去停词—&gt;(自然语言处理,知识,图谱,知识图谱)—&gt;建立索引—&gt;(1,2,3,432,66)—&gt;one hot—&gt;word2vec—&gt;</p>
<h4 id="语料库"><a href="#语料库" class="headerlink" title="语料库"></a>语料库</h4><p>&emsp;&emsp;jieba 分词同时基于一些<strong>语料库</strong>和手写的<strong>规则</strong>（如隐马尔科夫模型）。<br>&emsp;&emsp;如果想要加入自己的语料库可以使用下面的代码，语料库的格式可在 github jieba 上找到。<br><figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">'/home/python/dictionary.txt'</span>)</span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span>(<span class="string">' '</span>.<span class="keyword">join</span>(seg_list))</span><br></pre></td></tr></table></figure></p>
<ol>
<li>词库<ul>
<li>医药知识图谱<ul>
<li>语料库（网上有现成的，不用自己爬，如：医药行业专业词典）<ul>
<li>医院的名称</li>
<li>疾病的名称</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h4><p>&emsp;&emsp;文本数据的表示模型：</p>
<ul>
<li>布尔模型（boolean model）</li>
<li>向量空间模型（vector space model）</li>
<li>概率模型（probabilistic model）</li>
<li>图空间模型（graph space model）等</li>
</ul>
<p>&emsp;&emsp;以下为几种主要的模型，它们的目标都是：建立文档的向量（矩阵）模型。加粗代表是现在常用的模型</p>
<ol>
<li><strong>TF-IDF</strong></li>
<li>LDA</li>
<li>LSA/LSI</li>
<li><strong>Word2Vec</strong></li>
<li>one-hot</li>
<li>BERT</li>
<li>…</li>
</ol>
<h5 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h5><p>&emsp;&emsp;TF：词频<br>&emsp;&emsp;IDF：逆文档频率。<br>&emsp;&emsp;权重 = TF * IDF<br>&emsp;&emsp;TF-IDF 可能会漏掉一些词。比如一篇文章只出现一次“周杰伦”，但是它已经表示了这篇文章的主旨。可是 TF-IDF 无法为该词分配较高的权重。<br>&emsp;&emsp;另外 jieba 中其实可以直接使用 TF-IDF。导入<code>jieba.analyse</code>即可使用。（TF-IDF 其实就是提取句子的标签）<br><figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">import jieba.analyse <span class="keyword">as</span> ja</span><br><span class="line">ja.extract<span class="constructor">_tags(<span class="params">sentence</span>, <span class="params">topK</span>=3,<span class="params">withWeight</span>=False, <span class="params">allowPOS</span>=()</span>)</span><br></pre></td></tr></table></figure></p>
<h5 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h5><p>&emsp;&emsp;TF-IDF 只考虑单个文字，忽略了句子中的上下文信息。而word2vec 考虑了上下文，输入值为某个单词的前几个单词、后几个单词和其本身。<br>&emsp;&emsp;word2vec 现成的工具包有：1)gensim；2)tensorflow；3)keras。<br>&emsp;&emsp;另外 QA 系统等应用可能不适合使用 word2vec 训练出来的单词。因为它训练出来的词向量没有捕获到<strong>太多</strong>的上下文信息。众所众知，QA 系统和对话系统等应用需要经常使用到很多上下文信息。</p>
<h3 id="命名体识别——NER"><a href="#命名体识别——NER" class="headerlink" title="命名体识别——NER"></a>命名体识别——NER</h3><p>&emsp;&emsp;所谓的命名体（named entity）就是人名、机构名、地名以及其他所有以名称为标识的实体。更广泛的实体还包括数字、日期、货币、地址等等。<br>&emsp;&emsp;难点：1)<strong>同义词、歧义词等</strong>；2)<strong>未登录词判定</strong>。<br>&emsp;&emsp;一般流程：1)<strong>基于规则的方法</strong>；2)<strong>基于模型的方法</strong>，常见的序列标注模型包括 <strong>HMM</strong>（Hidden Markov Model）、<strong>CRF</strong>（Conditional random field）、<strong>RNN</strong>。不过虽然基于模型的方法技术比较新颖，但是由于太过复杂以及太难解释，所以公司还是用基于规则的方法比较多。</p>
<h4 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h4><ul>
<li>基于HMM</li>
<li>基于CRF<div class="note danger">
            <p>&emsp;&emsp;上课的时候没听明白。</p>
          </div></li>
<li>基于RNN<br>&emsp;&emsp;要做命名体识别，首先要做序列标注的任务。目前有以下几种公认的标注体系：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/标注体系.png" alt="标注体系"></li>
</ul>
<h3 id="关系抽取（特征工程）"><a href="#关系抽取（特征工程）" class="headerlink" title="关系抽取（特征工程）"></a>关系抽取（特征工程）</h3><ol>
<li><strong>文本特征提取</strong>，采用 tf-idf</li>
<li><strong>关键字抽取</strong>，比如转让，收购，整合等等</li>
<li><strong>句法特征提取</strong>，主要是与核心词之间的关系，包括企业实体本身和前后词与核心词之间的关系，距离等。即抽取（实体，关系，实体）<strong>三要素</strong>特征<ul>
<li>依存句法分析<ul>
<li>依存树，<a href="http://ltp.ai/demo.html" target="_blank" rel="noopener">demo</a></li>
<li>CCG</li>
</ul>
</li>
<li>分类器</li>
</ul>
</li>
<li>如果使用 NN 训练，可以拼接三要素和 tf-idf 特征</li>
</ol>
<h3 id="事件抽取"><a href="#事件抽取" class="headerlink" title="事件抽取"></a>事件抽取</h3><p>&emsp;&emsp;略，培训中未提到，估计跟关系抽取差不多。</p>
<h2 id="知识融合"><a href="#知识融合" class="headerlink" title="知识融合"></a>知识融合</h2><h3 id="实体链接"><a href="#实体链接" class="headerlink" title="实体链接"></a>实体链接</h3><h4 id="实体统一-实体对齐"><a href="#实体统一-实体对齐" class="headerlink" title="实体统一/实体对齐"></a>实体统一/实体对齐</h4><p>&emsp;&emsp;<strong>注：另一种说法是实体统一和实体对齐并不是同一件事。此处姑且当它们是同一件事。</strong><br>&emsp;&emsp;对同一实体具有多个名称的情况进行实体统一，将多个名称统一替换成一个命名实体。比如，“河北银行股份有限公司”和“河北银行”可以统一成“河北银行”。<br>&emsp;&emsp;大致来说这个应用是使用规则来做实体统一。目前（2019 年 7 月）来说，基于规则的做法大概能解决 70% 左右的问题。还可以使用余弦相似度，分类等算法进行融合使用。</p>
<ul>
<li>分离出地名，比如河北，北京</li>
<li>去除后缀，比如有限公司，集团</li>
<li>提取经营范围，比如医疗，化学</li>
<li>剩余部分为中间字段</li>
<li>最后选择以上四个部分的某些部分进行拼接，成为一个唯一的命名实体，如果有中间字段，则仅使用中间字段即可，并对某些特殊的经营范围做补充，比如银行；否则，优先使用地名加经营范围，其次是地名加后缀。</li>
</ul>
<p>&emsp;&emsp;<strong>更新命名体：在做完实体统一之后，将原数据中的实体进行替换即可</strong>。</p>
<h4 id="实体消歧"><a href="#实体消歧" class="headerlink" title="实体消歧"></a>实体消歧</h4><p>&emsp;&emsp;与实体统一不同。实体统一是将两个不一样名称的实体统一起来，而实体消歧是将同一个名称的实体在不同语境下区分开来，比如：苹果在不同的语境下分别有水果和手机的意思。<br>&emsp;&emsp;中文的不怎么好做，主要运用规则。</p>
<h3 id="知识合并"><a href="#知识合并" class="headerlink" title="知识合并"></a>知识合并</h3><p>&emsp;&emsp;<a href="https://102.alibaba.com/downloadFile.do?file=1518508273059/CoLink An Unsupervised Framework for User Identity Linkage.pdf" target="_blank" rel="noopener">阿里巴巴实体合并框架</a></p>
<h2 id="知识加工"><a href="#知识加工" class="headerlink" title="知识加工"></a>知识加工</h2><h2 id="知识存储与检索"><a href="#知识存储与检索" class="headerlink" title="知识存储与检索"></a>知识存储与检索</h2><h2 id="知识应用"><a href="#知识应用" class="headerlink" title="知识应用"></a>知识应用</h2><h1 id="汉语处理的难点"><a href="#汉语处理的难点" class="headerlink" title="汉语处理的难点"></a>汉语处理的难点</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/汉语处理的难点.jpg" alt="汉语处理的难点"></p>
<h1 id="NLP-工具包"><a href="#NLP-工具包" class="headerlink" title="NLP 工具包"></a>NLP 工具包</h1><p>&emsp;&emsp;略。详见此<a href="https://yan624.github.io/2019 普开培训.html#NLP-工具包">博客</a></p>
<h1 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h1><p>&emsp;&emsp;以上为知识图谱的大致概述，以下以几个例子大致地将构建步骤串联起来。首先给出知识图谱的总结<strong>思维导图</strong>，可以按照图中的内容自行对应查找知识点。思维导图的阅读顺序是<strong>从上至下</strong>，<strong>从右至左</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/知识图谱总结.png" alt="知识图谱总结"></p>
<h2 id="医疗命名体识别"><a href="#医疗命名体识别" class="headerlink" title="医疗命名体识别"></a>医疗命名体识别</h2><p>&emsp;&emsp;<a href="https://github.com/liuhuanyong/MedicalNamedEntityRecognition" target="_blank" rel="noopener">项目地址</a>，使用了基于字向量的<strong>四层双向 LSTM</strong> 与 <strong>CRF 模型</strong>的网络。<br>&emsp;&emsp;本项目大致使用了<strong>信息抽取</strong>-&gt;<strong>命名体识别</strong>的技术。项目中有一个名为 data_origin 的文件夹，其结构为：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">data_origin</span><br><span class="line">  ├─一般项目</span><br><span class="line">  │  ├─一般项目-<span class="number">1</span>.txt</span><br><span class="line">  │  ├─一般项目-<span class="number">1</span><span class="selector-class">.txtoriginal</span><span class="selector-class">.txt</span></span><br><span class="line">  │  └─。。。</span><br><span class="line">  ├─出院情况</span><br><span class="line">  ├─病史特点</span><br><span class="line">  └─诊疗经过</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;<em>一般项目-1.txt</em> 文件包含了由<strong>人工标注</strong>过的数据，<em>一般项目-1.txtoriginal.txt</em> 包含了原始数据，即未经过任何处理的数据。类似以下的格式。第 2 列和第 3 列代表该命名体在原始数据中的开始和结束的索引。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/原始数据.jpg" alt="原始数据"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/人工标注后的数据.jpg" alt="人工标注后的数据"></p>
<p>&emsp;&emsp;以上的数据为项目的原数据（那个由人工标注过的数据也算原数据），我们需要使用一套标注体系（本项目使用 BIO 体系）来将原数据处理一下，以下是处理结果。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/使用 BIO 标注后的数据.jpg" alt="使用 BIO 标注后的数据"></p>
<p>&emsp;&emsp;你可能会疑惑 DISEASE-* 之类的东西是什么意思，以及它是怎么出来的。其实十分简单，如下所示，都是预先定义好的。以 B 结尾，代表一个命名体的开始，以 I 结尾，代表一个命名体的结束。而产生数据的过程也只是写死的一套逻辑，使用 if else 进行判断罢了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/标签字典.jpg" alt="标签字典"></p>
<h3 id="训练之前的准备工作"><a href="#训练之前的准备工作" class="headerlink" title="训练之前的准备工作"></a>训练之前的准备工作</h3><ol>
<li>定义标签</li>
<li>人工将数据一条一条地标注命名体、起始位置以及标签</li>
<li>选择一套标注体系</li>
<li>将<strong>每一份</strong>原数据使用标注体系处理后，存入<strong>一份</strong>文件</li>
</ol>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>&emsp;&emsp;代码中以 <code>[&#39;。&#39;,&#39;?&#39;,&#39;!&#39;,&#39;！&#39;,&#39;？&#39;]</code> 符号作为一份病历的结束。然后将训练数据重新拆分成多分训练样本。我倒是认为在原数据处理完毕合并时，就做一些处理不行吗？如果以那些符号作为判断条件，可能有些不太准。</p>
<ol>
<li>加载字向量；</li>
<li>以 <code>[&#39;。&#39;,&#39;?&#39;,&#39;!&#39;,&#39;！&#39;,&#39;？&#39;]</code> 符号作为一份病历的结束，重新切分数据为多份训练样本；</li>
<li>每一个字都有一个标注，比如训练样本：[感, 染, 风, 寒]和标注：[CHECK-B, CHECK-I, DISEASE-B, DISEASE-I]—转换为—&gt;[32, 8454, 676, 934]和[7, 8, 10, 9]；</li>
<li>程序定义有 150 个时间步，第一层 BiLSTM 为 128 维，第二层的 BiLSTM 为 64 维，各层之间的 Dropout 取 0.5；</li>
<li>将训练样本输入 RNN，RNN 的输出输入 CRF，CRF 输出一个 11 维的向量，即每一个字都会输出一个 11 维的向量。所以可以看做是一个 11 元分类模型，即判断一个字属于哪一类的标注，也就是序列标注的含义——为字标注属性；</li>
<li>训练结束，就完成了一个序列标注模型。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>命名体识别</strong>的功能，使用了 <strong>LSTM</strong> 以及 <strong>CRF</strong> 的技术，原数据采用了<strong>人工标注</strong>的处理方式，原数据转为训练样本采用了<strong>规则模版</strong>的方式。总的来说，没有太大难度。对于此项目，我们需要理解 LSTM 和 CRF 的算法，整个过程的难点就在人工标注上，费时费力。</p>
<h2 id="中文人物关系知识图谱"><a href="#中文人物关系知识图谱" class="headerlink" title="中文人物关系知识图谱"></a>中文人物关系知识图谱</h2><p>&emsp;&emsp;<a href="https://github.com/liuhuanyong/PersonRelationKnowledgeGraph" target="_blank" rel="noopener">项目地址</a>。此项目代码结构有点复杂，涉及了很多爬虫，我对爬虫不是很了解。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>关系抽取</strong>的功能，具体使用了什么技术<strong>未知</strong>。</p>
<h2 id="判断两个企业实体是否存在投资关系"><a href="#判断两个企业实体是否存在投资关系" class="headerlink" title="判断两个企业实体是否存在投资关系"></a>判断两个企业实体是否存在投资关系</h2><p>&emsp;&emsp;<a href="https://github.com/rlistengr/Entity-relationship-extraction" target="_blank" rel="noopener">项目地址</a>。</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>关系抽取</strong>和<strong>实体统一</strong>的功能，基本上使用了人工模版去判断两个企业是否<strong>统一</strong>。是否存在投资关系也是用规则判断的。</p>
<h2 id="金融问答项目"><a href="#金融问答项目" class="headerlink" title="金融问答项目"></a>金融问答项目</h2><p>&emsp;&emsp;此项目（<strong>实验21-1-FinancialKGQA</strong>）实现了一个简单的金融问答项目，前提项目为<strong>实验19-neo4j构建简单的金融知识图谱</strong>，旨在使用爬虫技术构建一个金融知识图谱。数据和代码已经由 2019.6.27 普开知识图谱培训机构提供。</p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;从下图可以看出，只是简单的关键词匹配。然后通过 neo4j 的 CQL 语句进行查询。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/金融问答示例代码.jpg" alt="金融问答示例代码"></p>
<h2 id="企业经营退出风险预测"><a href="#企业经营退出风险预测" class="headerlink" title="企业经营退出风险预测"></a>企业经营退出风险预测</h2><p>&emsp;&emsp;<a href="https://github.com/xiaorancs/business-exit-risk-forecast" target="_blank" rel="noopener">项目地址</a>，还没研究过。同一个项目，<a href="https://github.com/ShawnyXiao/2017-CCF-BDCI-Enterprise" target="_blank" rel="noopener">另一个人的项目地址</a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>2019 普开培训</title>
    <url>/assorted/conference/2019%20%E6%99%AE%E5%BC%80%E5%9F%B9%E8%AE%AD.html</url>
    <content><![CDATA[<h1 id="第一天"><a href="#第一天" class="headerlink" title="第一天"></a>第一天</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>&emsp;&emsp;知识图谱是一种新型的<strong>数据库</strong>，是一种基于图的数据结构。每个节点表示现实世界中存在的“实体”，每条边为实体与实体之间的“关系”。以下为知识图谱的几点作用：</p>
<ul>
<li>从“关系”分析问题</li>
<li>把不同种类的信息连接在一起</li>
<li>一个关系网络</li>
</ul>
<p>&emsp;&emsp;学习知识图谱首先得掌握以下几种技能：</p>
<ol>
<li><strong>基础知识</strong>：自然语言处理、图数据库操作知识、基本编程能力：Python、SQL；</li>
<li><strong>领域知识</strong>：知识图谱构建方法、知识图谱推理方法；</li>
<li><strong>行业知识</strong></li>
</ol>
<p>&emsp;&emsp;现在知识图谱领域中比较火热的是：风控。<strong>企查查</strong>可以查询企业的状态。<br>&emsp;&emsp;知识图谱<strong>核心技术</strong>可分为（大致就是一本书的目录）：</p>
<ol>
<li>知识图谱的架构与设计</li>
<li>知识图谱核心技术-<strong>知识源数据的获取</strong></li>
<li>知识图谱核心技术-信息抽取-<strong>关键词抽取</strong>(属性与数值)</li>
<li>知识图谱核心技术-信息抽取-<strong>实体识别</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-信息抽取-<strong>关系抽取</strong>（深度学习+经典方案）<a id="more"></a></li>
<li>知识图谱核心技术-信息抽取-<strong>事件抽取</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-知识融合概述</li>
<li>知识图谱核心技术-知识融合-实体链接-<strong>实体统一</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-知识融合-实体链接-<strong>实体消岐</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-知识融合-<strong>知识合并</strong></li>
<li>知识图谱核心技术-知识加工概述</li>
<li>知识图谱核心技术-知识加工-<strong>本体构建</strong></li>
<li>知识图谱核心技术-<strong>知识存储与检索</strong></li>
<li>知识图谱核心技术-知识加工-<strong>知识推理</strong></li>
<li>知识应用-智能问答，风控，营销….</li>
<li>知识图谱核心技术-知识加工-知识更新</li>
<li>知识图谱核心技术-知识加工-质量评估</li>
</ol>
<p>&emsp;&emsp;<strong>基本任务</strong>和<strong>主要研究方向</strong>：</p>
<ul>
<li>机器翻译</li>
<li>自动摘要</li>
<li>文本分类与信息过滤</li>
<li>信息检索</li>
<li>信息抽取与文本挖掘<ul>
<li>实体抽取：命名体识别</li>
<li>关系抽取：关系抽取算法</li>
<li>事件抽取<ul>
<li>地区、时间、过程</li>
<li>文本分类（为事件分类）</li>
</ul>
</li>
</ul>
</li>
<li>情感分析</li>
<li>自动问答</li>
<li>……</li>
</ul>
<p>&emsp;&emsp;自然语言处理与知识图谱的<strong>处理步骤</strong>：</p>
<ol>
<li>分词、语料库、文本分类、文本聚类、文本词性分析。。。</li>
<li>信息抽取</li>
<li>知识融合阶段<ul>
<li>实体统一</li>
<li>实体消歧</li>
</ul>
</li>
</ol>
<h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>&emsp;&emsp;分类算法中的流程：<br>(自然语言处理与知识图谱)—&gt;分词—&gt;(自然语言处理,与,知识,图谱,知识图谱)—&gt;去停词—&gt;(自然语言处理,知识,图谱,知识图谱)—&gt;建立索引—&gt;(1,2,3,432,66)—&gt;one hot—&gt;word2vec—&gt;</p>
<h3 id="语料库"><a href="#语料库" class="headerlink" title="语料库"></a>语料库</h3><p>&emsp;&emsp;jieba 分词同时基于一些<strong>语料库</strong>和手写的<strong>规则</strong>（如隐马尔科夫模型）。<br>&emsp;&emsp;如果想要加入自己的语料库可以使用下面的代码，语料库的格式可在 github jieba 上找到。<br><figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">'/home/python/dictionary.txt'</span>)</span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span>(<span class="string">' '</span>.<span class="keyword">join</span>(seg_list))</span><br></pre></td></tr></table></figure></p>
<ol>
<li>词库<ul>
<li>医药知识图谱<ul>
<li>语料库（网上有现成的，不用自己爬，如：医药行业专业词典）<ul>
<li>医院的名称</li>
<li>疾病的名称</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h3><p>&emsp;&emsp;文本数据的表示模型：</p>
<ul>
<li>布尔模型（boolean model）</li>
<li>向量空间模型（vector space model）</li>
<li>概率模型（probabilistic model）</li>
<li>图空间模型（graph space model）等</li>
</ul>
<p>&emsp;&emsp;以下为几种主要的模型，它们的目标都是：建立文档的向量（矩阵）模型。加粗代表是现在常用的模型</p>
<ol>
<li><strong>TF-IDF</strong></li>
<li>LDA</li>
<li>LSA/LSI</li>
<li><strong>Word2Vec</strong></li>
<li>one-hot</li>
<li>BERT</li>
<li>…</li>
</ol>
<h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>&emsp;&emsp;TF：词频<br>&emsp;&emsp;IDF：逆文档频率。<br>&emsp;&emsp;权重 = TF * IDF<br>&emsp;&emsp;TF-IDF 可能会漏掉一些词。比如一篇文章只出现一次“周杰伦”，但是它已经表示了这篇文章的主旨。可是 TF-IDF 无法为该词分配较高的权重。<br>&emsp;&emsp;另外 jieba 中其实可以直接使用 TF-IDF。导入<code>jieba.analyse</code>即可使用。（TF-IDF 其实就是提取句子的标签）<br><figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">import jieba.analyse <span class="keyword">as</span> ja</span><br><span class="line">ja.extract<span class="constructor">_tags(<span class="params">sentence</span>, <span class="params">topK</span>=3,<span class="params">withWeight</span>=False, <span class="params">allowPOS</span>=()</span>)</span><br></pre></td></tr></table></figure></p>
<h4 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h4><p>&emsp;&emsp;TF-IDF 只考虑单个文字，忽略了句子中的上下文信息。而word2vec 考虑了上下文，输入值为某个单词的前几个单词、后几个单词和其本身。<br>&emsp;&emsp;word2vec 现成的工具包有：1)gensim；2)tensorflow；3)keras。<br>&emsp;&emsp;另外 QA 系统等应用可能不适合使用 word2vec 训练出来的单词。因为它训练出来的词向量没有捕获到<strong>太多</strong>的上下文信息。众所众知，QA 系统和对话系统等应用需要经常使用到很多上下文信息。</p>
<h3 id="汉语处理的难点"><a href="#汉语处理的难点" class="headerlink" title="汉语处理的难点"></a>汉语处理的难点</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/汉语处理的难点.jpg" alt="汉语处理的难点"></p>
<h2 id="NLP-工具包"><a href="#NLP-工具包" class="headerlink" title="NLP 工具包"></a>NLP 工具包</h2><ul>
<li>中文分词工具（粗体推荐使用，其他随意）<ul>
<li><strong>jieba</strong>：<a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">下载地址</a>。分词、ti-idf、标注。。。<ul>
<li>全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；</li>
<li>精确模式：试图将句子最精确地切开，适合文本分析；</li>
<li>搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</li>
</ul>
</li>
<li>snownlp：<a href="https://github.com/isnowfy/snownlp" target="_blank" rel="noopener">下载地址</a>。这个有点慢</li>
<li><strong>Hanlp</strong>：<a href="https://github.com/hankcs/pyhanlp" target="_blank" rel="noopener">下载地址</a>。功能较多，比如：<ul>
<li>中文分词</li>
<li>词性标注（pos）</li>
<li>命名实体识别（ner）</li>
<li>关键词提取</li>
<li>自动摘要</li>
<li>短语提取</li>
<li>拼音转换</li>
<li>简繁转换</li>
<li>依存句法分析</li>
<li>word2vec</li>
</ul>
</li>
<li>pkuseg：<a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">下载地址</a>。支持细领域分词，比如海洋、新闻、医药等。MIT 许可证，所以不可商用</li>
<li>THULAC：<a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="noopener">下载地址</a></li>
<li>fudannlp：不怎么更新了<ul>
<li>fastNLP：复旦新开发的一个工具，做了很多模型的集成，如 BERT。</li>
</ul>
</li>
</ul>
</li>
<li>英文分词工具<ul>
<li>gensim：分词、主题分析等</li>
<li><strong>spaCy</strong>：<a href="https://spacy.io/usage/models" target="_blank" rel="noopener">文档</a></li>
</ul>
</li>
</ul>
<h2 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h2><p>&emsp;&emsp;所谓的命名体（named entity）就是人名、机构名、地名以及其他所有以名称为标识的实体。更广泛的实体还包括数字、日期、货币、地址等等。<br>&emsp;&emsp;难点：1)<strong>同义词、歧义词等</strong>；2)<strong>未登录词判定</strong>。<br>&emsp;&emsp;一般流程：1)<strong>基于规则的方法</strong>；2)<strong>基于模型的方法</strong>，常见的序列标注模型包括 <strong>HMM</strong>（Hidden Markov Model）、<strong>CRF</strong>（Conditional random field）、<strong>RNN</strong>。不过虽然基于模型的方法技术比较新颖，但是由于太过复杂以及太难解释，所以公司还是用基于规则的方法比较多。</p>
<h3 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h3><p>&emsp;&emsp;要做命名体识别，首先要做序列标注的任务。目前国家有以下几种标注体系：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/标注体系.png" alt="标注体系"></p>
<ul>
<li>基于HMM</li>
<li>基于CRF<div class="note danger">
            <p>&emsp;&emsp;上课的时候没听明白。</p>
          </div></li>
<li>基于RNN</li>
</ul>
<h1 id="第二天"><a href="#第二天" class="headerlink" title="第二天"></a>第二天</h1><h2 id="医疗命名体识别"><a href="#医疗命名体识别" class="headerlink" title="医疗命名体识别"></a>医疗命名体识别</h2><p>&emsp;&emsp;使用 BIO 标注体系。<br>&emsp;&emsp;命名体识别模型训练步骤：</p>
<ol>
<li>准备数据：<strong>原始数据，即自然语言语句</strong><ol>
<li><strong>定义大类</strong>，如 BODY、SIGN、DISEASE。使用数据抽样的方法，2-3 周</li>
</ol>
</li>
<li>对原始数据进行标注：<strong>对原始数据进行人工标注，如<code>右髋部    21    23    身体部位</code>、<code>疼痛    27    28    症状和体征</code></strong>。data_orign 文件夹中有 *.txt 和 *.txtoriginal.txt 文件。其中 *.txtoriginal.txt 文件中是医生诊断的原始数据，*.txt 中是将原始数据中的特征标注出来（此步骤是人工操作。不过如果有很多数据，其实可以偷个懒，因为<strong>有些特征差不多，在一份病历中标注一次就够了</strong>。比如风寒会出现很多次，其实只要在一份病历中标注一次，之后就可以被程序识别到了，当然多标注几份也行），<strong>如果已经有字典，比如网上下载的，可以不进行此步</strong>。</li>
<li>设置标注格式：如 IO、BIO、BMEWO 等体系。</li>
<li>编写转换程序：<strong>将所有标注的病历数据按标注体系转换，并且合并在一份文件中。如：<code>肺    DISEASE-B</code>、<code>炎    DISEASE-I</code>。详见：transfer_data.py</strong>。在 data/train.txt中。<strong>注：此标注方法不需要进行分词，因为它以字为级别</strong>。</li>
<li>算法模型：LSTM 和 CRF 如何结合？请看下图。不管多大项目，词向量一般选 300 维<ul>
<li>LSTM</li>
<li>CRF<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/BiLSTM + CRF.png" alt="BiLSTM + CRF"><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/LSTM+CRF例子.jpg" alt="LSTM+CRF例子"></li>
</ul>
</li>
<li>算法预测：预测结果较差，可能是因为数据较少。</li>
</ol>
<p>&emsp;&emsp;CRF 并不擅长提取属性，比如病人的受伤面积，可以使用正则表达式。<br>&emsp;&emsp;端到端训练就是一个模型到另一个模型的训练，比如LSTM + CRF。</p>
<h2 id="中国人物关系图谱"><a href="#中国人物关系图谱" class="headerlink" title="中国人物关系图谱"></a>中国人物关系图谱</h2><h2 id="讲到的技术"><a href="#讲到的技术" class="headerlink" title="讲到的技术"></a>讲到的技术</h2><ul>
<li>命名体识别</li>
<li>关系抽取</li>
</ul>
<h1 id="第三天"><a href="#第三天" class="headerlink" title="第三天"></a>第三天</h1><h2 id="实体分词"><a href="#实体分词" class="headerlink" title="实体分词"></a>实体分词</h2><p>&emsp;&emsp;重要内容提示:●交易简要内容:中海(海南)海盛船务股份有限公司将散货船“百花山”轮作为废钢船出售给江门市银湖拆船有限公司，出售价格为人民币17，183，633.49元。<br>&emsp;&emsp;——分词为——&gt;<br>&emsp;&emsp;重要内容, 提示, 交易, 简要内容, 中海, 海南, 海盛船务股份有限公司, 。。。</p>
<h2 id="命名体识别"><a href="#命名体识别" class="headerlink" title="命名体识别"></a>命名体识别</h2><p>&emsp;&emsp;识别所有命名体。</p>
<h3 id="企业实体识别"><a href="#企业实体识别" class="headerlink" title="企业实体识别"></a>企业实体识别</h3><p>&emsp;&emsp;利用 foolnltk 工具包，对每个新闻做命名实体识别，并对企业命名实体做实体统一，最后将每个新闻中的企业实体替换为统一的企业实体。</p>
<h2 id="实体统一-实体对齐"><a href="#实体统一-实体对齐" class="headerlink" title="实体统一/实体对齐"></a>实体统一/实体对齐</h2><p>&emsp;&emsp;对同一实体具有多个名称的情况进行实体统一，将多个名称统一替换成一个命名实体。比如，“河北银行股份有限公司”和“河北银行”可以统一成“河北银行”。<br>&emsp;&emsp;大致来说这个应用是使用规则来做实体统一。目前（2019 年 7 月）来说，基于规则的做法大概能解决 70% 左右的问题。还可以使用余弦相似度，分类等算法进行融合使用。</p>
<ul>
<li>分离出地名，比如河北，北京</li>
<li>去除后缀，比如有限公司，集团</li>
<li>提取经营范围，比如医疗，化学</li>
<li>剩余部分为中间字段</li>
<li>最后选择以上四个部分的某些部分进行拼接，成为一个唯一的命名实体，如果有中间字段，则仅使用中间字段即可，并对某些特殊的经营范围做补充，比如银行；否则，优先使用地名加经营范围，其次是地名加后缀。</li>
</ul>
<h3 id="更新命名体"><a href="#更新命名体" class="headerlink" title="更新命名体"></a>更新命名体</h3><p>&emsp;&emsp;<strong>在做完实体统一之后，将原数据中的实体进行替换即可</strong>。</p>
<h2 id="特征工程（关系抽取）"><a href="#特征工程（关系抽取）" class="headerlink" title="特征工程（关系抽取）"></a>特征工程（关系抽取）</h2><ol>
<li><strong>文本特征提取</strong>，采用 tf-idf</li>
<li><strong>关键字抽取</strong>，比如转让，收购，整合等等</li>
<li><strong>句法特征提取</strong>，主要是与核心词之间的关系，包括企业实体本身和前后词与核心词之间的关系，距离等。即抽取（实体，关系，实体）三要素特征<ul>
<li>依存句法分析<ul>
<li>依存树，<a href="http://ltp.ai/demo.html" target="_blank" rel="noopener">demo</a></li>
<li>CCG</li>
</ul>
</li>
<li>分类器</li>
</ul>
</li>
<li>拼接三要素 + tf-idf 特征</li>
</ol>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>&emsp;&emsp;将特征工程提取到的特征做 onehot 编码（不一定要是 onehot），利用随机森林进行模型拟合。使用贝叶斯超参数调优，调优参数为【决策树数量，决策树的最大深度，随机数生成器】。或者可以使用深度学习的算法，如神经网络。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="实体消歧"><a href="#实体消歧" class="headerlink" title="实体消歧"></a>实体消歧</h3><p>&emsp;&emsp;中文的不怎么好做，主要运用规则。</p>
<h3 id="知识融合"><a href="#知识融合" class="headerlink" title="知识融合"></a>知识融合</h3><p>&emsp;&emsp;实体扩充(融合外部知识图谱或者数据)。<a href="https://102.alibaba.com/downloadFile.do?file=1518508273059/CoLink An Unsupervised Framework for User Identity Linkage.pdf" target="_blank" rel="noopener">阿里巴巴实体链接框架</a></p>
<h2 id="知识图谱构建步骤总结"><a href="#知识图谱构建步骤总结" class="headerlink" title="知识图谱构建步骤总结"></a>知识图谱构建步骤总结</h2><ol>
<li>数据收集(持续收集与更新)（<strong>关键词抽取</strong>、<strong>命名体识别</strong>、<strong>关系抽取</strong>、<strong>事件抽取</strong>）<ol>
<li>原始数据，通常可能是一篇文章<ol>
<li>爬虫技术<ol>
<li>垂直爬虫</li>
<li>搜索引擎相关的爬虫</li>
</ol>
</li>
</ol>
</li>
<li>语料数据，通常词库，词典，同义词</li>
<li>开源的第三方知识图谱，例如搜狗人物关系图</li>
<li>开源的训练好的词向量(word2vec)模型,tfidf</li>
</ol>
</li>
<li>图谱设计<ol>
<li>实体定义(本体)<br>实体：实体类型<ol>
<li>属性<br>例如,手(长度，面积)，类别：身体器官</li>
</ol>
</li>
<li>属性定义</li>
<li>关系定义<ol>
<li>关系也需要定义类别</li>
<li>需要评估关系可以覆盖的数据量，一般服从28 原则，20%的关系，覆盖80%数据</li>
</ol>
</li>
</ol>
</li>
<li>知识清洗<ol>
<li><strong>实体消歧</strong></li>
<li><strong>实体统一</strong></li>
</ol>
</li>
<li>知识融合(实体链接)<ol>
<li>实体与关系的融合</li>
<li>实体扩充(融合外部知识图谱或者数据)（<strong>知识合并</strong>）</li>
</ol>
</li>
<li><strong>知识存储</strong>-图数据库</li>
</ol>
<h1 id="前三天的总结"><a href="#前三天的总结" class="headerlink" title="前三天的总结"></a>前三天的总结</h1><ol>
<li>知识图谱的架构与设计</li>
<li>知识图谱核心技术-知识源数据的获取</li>
<li>知识图谱核心技术-信息抽取-关键词抽取(属性与数值)</li>
<li>知识图谱核心技术-信息抽取-实体识别（深度学习+经典方案）<ol>
<li>目的：抽取数据中的实体信息，例如人名</li>
<li>方法：<ol>
<li>规则：（正则等）</li>
<li>模型：传统方法CRF，深度学习BiLSTM+CRF</li>
</ol>
</li>
<li>过程：<ol>
<li>按照CRF要求定义好实体的分类与标注体系</li>
<li>标注训练数据</li>
<li>编写BiLSTM+CRF模型</li>
<li>使用模型预测</li>
<li>组合预测的结果</li>
<li>纠错预测的结果</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-信息抽取-关系抽取（深度学习+经典方案）<ol>
<li>目的：抽取实体与实体间的关系，例如：出生于</li>
<li>方法：<ol>
<li>规则，例如：通过关键词，进行匹配</li>
<li>模型<ol>
<li>传统 <ol>
<li>分类<ol>
<li>基于CRF+LSTM，需要将实体标签变成关系类别的标签，进行预测</li>
</ol>
</li>
<li>基于语法树<ol>
<li>依托于语法规则，识别关系属于哪两个实体，要求是句子结构要短一点，如果很长，规则不好定义</li>
</ol>
</li>
<li>BootStrapping</li>
</ol>
</li>
<li>深度学习</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-信息抽取-事件抽取（深度学习+经典方案）<ol>
<li>目的：抽取内容中的事件，以及他们的关系</li>
<li>事 件关系的类型：<ol>
<li>因果事件  某一事件导致某一事件发生  A导致B  </li>
<li>事件预警  因果溯源 由因求果 &lt;地震,房屋倒塌&gt; 条件事件  某事件条件下另一事件发生  如果A那么B  </li>
<li>事件预警  时机判定  &lt;限制放宽,立即增产&gt; 反转事件  某事件与另一事件形成对立  虽然A但是B  预防不测  反面教材  &lt;起步晚,发展快&gt;</li>
<li>顺承事件  某事件紧接着另一事件发生  A接着B  事件演化  未来意图识别  &lt;去旅游,买火车票&gt; </li>
</ol>
</li>
<li>主要的方法：<ol>
<li>规则</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识融合概述<ol>
<li>目的：将信息抽取中，抽取的实体与关系，进行融合<br>例如，(曹操，父子，曹丕) （曹操，父子，曹植）</li>
<li>融合的层次-实体链接<ol>
<li>实体与实体的融合</li>
<li>实体与外部数据的融合</li>
<li>知识图谱与知识图谱的融合</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识融合-实体链接-实体统一（深度学习+经典方案）<ol>
<li>目的：统一实体的名称，例如杭州阿里巴巴集团，阿里巴巴</li>
<li>统一的方法：<ol>
<li>规则：例如去掉杭州阿里巴巴集团的集团，与地区，比较与简称的差距</li>
<li>基于模型：如入A与B，判断是否为一个实体</li>
<li>基于文本相似度：例如使用余弦定理，</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识融合-实体链接-实体消岐（深度学习+经典方案）<ol>
<li>目的：消除实体间的歧义</li>
<li>方法：<ol>
<li>结合语境， 例如该文章类别如果是3c数码类文章，那么小米指的是小米<br>然后进行实体补全</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识融合-知识合并<ol>
<li>实体与实体的融合</li>
<li>实体与外部数据的融合</li>
<li>知识图谱与知识图谱的融合</li>
</ol>
</li>
<li>知识图谱核心技术-知识加工概述</li>
<li>知识图谱核心技术-知识加工-本体构建</li>
<li>知识图谱核心技术-知识存储与检索 - Neo4j create<ul>
<li>(:Movie {title:”驴得水”,released:2016})  return p; </li>
<li>(p1:Person {name:’Alice’}) -[:KNOWS][-&gt;(p2:Person {name:’Bob’})</li>
</ul>
</li>
<li>知识图谱核心技术-知识加工-知识推理</li>
<li>知识应用-智能问答，风控，营销….<ol>
<li>智能问答应用<ol>
<li>如何实现基于知识图谱的智能问答？</li>
<li>对用户输入的问题进行语义分析<ol>
<li>问题分类<ol>
<li>问题的类型分类：例如，冬天下雨怎么办，是咨询类问题</li>
<li>问题的问形式上的分类，例如，怎么办，如何办，去哪办</li>
</ol>
</li>
<li>问句解析<ol>
<li>实体提取，例如，中国移动真不错，提取了中国移动实体</li>
<li>意图的预测，例如，万达怎么去，预测客户是想买东西</li>
<li>问题补全，例如，周末去哪吃比较好——&gt;周末去哪(万达附近)吃饭比较好</li>
<li>其他重要词汇识别</li>
</ol>
</li>
<li>将解析过的语句，转换成：cql等图数据查询语句</li>
<li>将查到的结果，结合之前的问题分类与模版，进行模板填充，反馈给客户</li>
</ol>
</li>
<li>风控</li>
<li>营销<ol>
<li>亲人圈发现</li>
<li>朋友圈发现</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识加工-知识更新</li>
<li>知识图谱核心技术-知识加工-质量评估</li>
</ol>
<h1 id="第四天"><a href="#第四天" class="headerlink" title="第四天"></a>第四天</h1><p>&emsp;&emsp;等于没学。搞了一天的环境配置。</p>
<h1 id="第五天"><a href="#第五天" class="headerlink" title="第五天"></a>第五天</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/图谱构建流程概览.png" alt="图谱构建流程概览"><br>&emsp;&emsp;图谱构建流程在第十五讲 PPT。</p>
<h1 id="第六天"><a href="#第六天" class="headerlink" title="第六天"></a>第六天</h1><p>&emsp;&emsp;第十六讲实验步骤。</p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>conference</category>
      </categories>
      <tags>
        <tag>青岛</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>2. 两数相加</title>
    <url>/algorithm/2.%20%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0.html</url>
    <content><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><h1 id="有问题的解法1"><a href="#有问题的解法1" class="headerlink" title="有问题的解法1"></a>有问题的解法1</h1><p>代码<br><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * public class ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) &#123; val = x; &#125;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="symbol">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123;</span><br><span class="line">        <span class="built_in">int</span> nextNodeCarryValue = <span class="number">0</span>;</span><br><span class="line">        ListNode sum = new ListNode(nextNodeCarryValue);</span><br><span class="line">        ListNode s = sum;</span><br><span class="line">        </span><br><span class="line">        ListNode t1 = l1, t2 = l2;</span><br><span class="line">        <span class="built_in">int</span> a = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">int</span> b = <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(t1 != <span class="literal">null</span> || t2 != <span class="literal">null</span>)&#123;</span><br><span class="line">            <span class="comment">// 计算和</span></span><br><span class="line">            <span class="built_in">int</span> res = t1.val + t2.val;</span><br><span class="line">            </span><br><span class="line">            <span class="built_in">int</span> nodeValue = res % <span class="number">10</span>;</span><br><span class="line">            nextNodeCarryValue = res / <span class="number">10</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 先创建下一个节点</span></span><br><span class="line">            ListNode nextNode = new ListNode(nextNodeCarryValue);</span><br><span class="line">            </span><br><span class="line">            s.val += nodeValue;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 移动节点</span></span><br><span class="line">            t1 = t1.next;</span><br><span class="line">            t2 = t2.next;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 顺便处理最后一个节点，这里的代码十分难理解</span></span><br><span class="line">            <span class="keyword">if</span>((t1 != <span class="literal">null</span> || t2 != <span class="literal">null</span>) || nextNodeCarryValue != <span class="number">0</span>)&#123;</span><br><span class="line">                s.next = nextNode;</span><br><span class="line">                s = s.next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 进最后一位</span></span><br><span class="line">        <span class="comment">// if(nextNodeCarryValue != 0)&#123;</span></span><br><span class="line">        <span class="comment">//     s.next = new ListNode(nextNodeCarryValue); </span></span><br><span class="line">        <span class="comment">// &#125;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//处理两个 ListNode 不等长的情况</span></span><br><span class="line">        ListNode t = <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">if</span>(t1 != <span class="literal">null</span>)&#123;</span><br><span class="line">            t = t1;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            t =t2;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(t != <span class="literal">null</span>)&#123;</span><br><span class="line">            s.next = new ListNode(t.val);</span><br><span class="line">            t = t.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>1. 两数之和</title>
    <url>/algorithm/1.%20%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C.html</url>
    <content><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。<br><strong>示例</strong>：</p>
<blockquote>
<p>给定 nums = [2, 7, 11, 15], target = 9<br>因为 nums[0] + nums[1] = 2 + 7 = 9<br>所以返回 [0, 1]<br><a id="more"></a></p>
</blockquote>
<h1 id="暴力破解"><a href="#暴力破解" class="headerlink" title="暴力破解"></a>暴力破解</h1><ul>
<li>时间复杂度：O(<script type="math/tex">n^2</script>)</li>
<li>空间复杂度：O(1)</li>
<li>用时 39ms</li>
</ul>
<p>首先想到了暴力破解的方法，但是后来发现其实没必要遍历整个数组，内部的循环从 0 开始遍历会浪费时间，与将 <code>for(int j = 0; j &lt; len; j++)</code> 改为了 <code>for(int j = i + 1; j &lt; len; j++)</code>。</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="symbol">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="built_in">int</span>[] twoSum(<span class="built_in">int</span>[] nums, <span class="built_in">int</span> target) &#123;</span><br><span class="line">        <span class="built_in">int</span> len = nums.length;</span><br><span class="line">        <span class="built_in">int</span>[] res =&#123;<span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="built_in">int</span> j = i + <span class="number">1</span>; j &lt; len; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[i] + nums[j] == target)&#123;</span><br><span class="line">                    res[<span class="number">0</span>] = i; res[<span class="number">1</span>] = j;</span><br><span class="line">                    <span class="keyword">return</span> res;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="两遍哈希表"><a href="#两遍哈希表" class="headerlink" title="两遍哈希表"></a>两遍哈希表</h1><p>暴力破解的办法时间复杂度较高，还有一种方法可以减少时间复杂度，但是会增加空间复杂度。创建一个 Map 来暂存数据。</p>
<ul>
<li>时间复杂度：O(n)</li>
<li>空间复杂度：O(n)</li>
<li>用时 10ms</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        Map&lt;Integer, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">            <span class="built_in">map</span>.put(nums[i], i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> diff = target - nums[i];</span><br><span class="line">            Integer j = <span class="built_in">map</span>.get(diff);</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">map</span>.containsKey(diff) &amp;&amp; j != i)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;i, j&#125;;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ArithmeticException(<span class="string">"无解"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="一遍哈希表"><a href="#一遍哈希表" class="headerlink" title="一遍哈希表"></a>一遍哈希表</h1><ul>
<li>时间复杂度：O(n)</li>
<li>空间复杂度：O(n)</li>
<li>用时 6ms</li>
</ul>
<p>一遍就能做完题目看似不可能，因为看似无法遍历完所有组合，但是实际上可以，只需要仔细思考一下。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        Map&lt;Integer, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> diff = target - nums[i];</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">map</span>.containsKey(diff))&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[] &#123;i, <span class="built_in">map</span>.get(diff)&#125;;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">map</span>.put(nums[i], i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ArithmeticException(<span class="string">"无解"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/30%E3%80%81Complex%20Sequential%20Question%20Answering%EF%BC%9ATowards%20Learning%20to%20Converse%20Over%20Linked%20Question%20Answer%20Pairs%20with%20a%20Knowledge%20Graph.html</url>
    <content><![CDATA[<div class="note warning">
            <p>&emsp;&emsp;<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17181/15750" target="_blank" rel="noopener">论文地址</a>。<br>&emsp;&emsp;<strong>凉了。读完论文，发现论文中的实验是使用 python2 写的，而且由于没有 VPN 无法下载实验附带的数据，训练数据有 17G，我都不想下了。</strong></p>
          </div>
<h1 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h1><p>&emsp;&emsp;人机对话时，人们通常会提出许多问题，其中大部分都可以通过大规模的 KG 回答。为此，我们提出了 Comples Sequential QA（CSQA） 任务，它由以下两种任组成：</p>
<ol>
<li>在拥有百万个实体的 KG 上进行复杂的推理从而回答事实性问题；</li>
<li>通过一系列连贯的链接问答对去学习交谈。</li>
</ol>
<p>&emsp;&emsp;接着还让工作人员创建了一个数据集，包括总共 1.6M 轮的 200k 的对话数据。我们还要求数据集含有<strong>逻辑推理</strong>（logical），<strong>定量推理</strong>（quantitative）以及<strong>比较推理</strong>（comparative ）的能力（此三种能力下面有详解）。因此这就迫使我们的模型要做到：</p>
<ol>
<li>解析复杂的自然语言问题；</li>
<li>使用对话上下文解析表达中的<strong>共指</strong>（coreferences ）、<strong>省略</strong>（ellipsis ）问题；</li>
<li>要求理清<strong>含糊不清</strong>（ambiguous ）的问题；</li>
<li>检索相关的 KG 的子图去回答这些问题。</li>
</ol>
<p>&emsp;&emsp;说明：</p>
<ul>
<li><strong>共指问题</strong>（coreferences）：就是说一个代词指向多个对象，机器人无法理解具体指向哪个</li>
<li><strong>省略问题</strong>（ellipsis）：表达没问题，但是表达中省略了一部分信息，需要人自己去上文中推测</li>
<li><strong>含糊不清的问题</strong>（ambiguous）：（与 1 类似，请看 1）<a id="more"></a>
</li>
</ul>
<h1 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h1><ol>
<li>引入 CSQA 的概念；</li>
<li>展示一流的 QA 和对话系统的处理方法在解决这些任务时的不足之处；</li>
<li>对 CSQA 提出了一个模型，由一流的 hierarchical conversation model（<strong>HRED</strong>）（<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11957/12160" target="_blank" rel="noopener">Serban 2016a</a>，<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14567/14219" target="_blank" rel="noopener">Serban 2017</a>） 和 key value（<strong>KV</strong>） based memory network model（<a href="https://arxiv.org/pdf/1606.03126.pdf" target="_blank" rel="noopener">Miller 2016</a>） 组成。</li>
</ol>
<h1 id="数据集创建"><a href="#数据集创建" class="headerlink" title="数据集创建"></a>数据集创建</h1><p>&emsp;&emsp;论文花了很大的篇幅描写了数据集是如何创建的。使用 14-Nov-2016 的 wiki data 创建，其中包含了 5.2k 的 relation（谓语），12.8M entity（主语），52.3M facts（宾语）。但是省略了像“ISO 3166-1 alpha-2 code”、“NDL Auth ID”等 relation，因为不期望用户会问这些模糊的问题。接着论文分别描述了 Simple Questions、Complex Questions 和 Linked Sequential QA 是如何创建的。</p>
<h2 id="Simple-Questions"><a href="#Simple-Questions" class="headerlink" title="Simple Questions"></a>Simple Questions</h2><p>&emsp;&emsp;为了发现问题，我们要求 annotators 自己提出问题并用 KG 中的<strong>单个</strong>三元组进行回答。后来 annotators 认为对于个三元组，主要有三种类型的问题：</p>
<ol>
<li>基于宾语（object）的问题，问题中包含三元组中的主语和关系，答案包含三元组中的宾语；</li>
<li>基于主语（subject）的问题，问题中包含三元组中的宾语和关系，答案包含三元组中的主语；</li>
<li>基于关系（relation）（理解成谓语也可以）的问题。后来在创建的数据集中发现，此类问题没有多大的意义。比如，数据集中有人问了一个很不自然的问题“Q:How is Himalayas related to India? A:located in”。<strong>所以论文只关注前两个问题</strong>。</li>
</ol>
<h2 id="Complex-Questions"><a href="#Complex-Questions" class="headerlink" title="Complex Questions"></a>Complex Questions</h2><p>&emsp;&emsp;接下来要求 annotators 建立一些逻辑推理（Logical Reasoning）、定量推理（Quantitative Reasoning）、比较推理（Comparative Reasoning）类型的问题。</p>
<ol>
<li>逻辑推理：考虑问题“哪些河流流经中国和印度？”，为了回答这个问题首先需要创建两组集合i){flowthrough, India, river}，ii){flowthrough, China, river}。最后求交集。此类问题可由 Simple Questions 修改得到，如 <strong>AND</strong> 操作：<strong>“哪些河流流经印度”</strong>修改为<strong>“哪些河流流经印度”+“和中国”</strong>；<strong>OR</strong>操作：<strong>“哪些河流流经印度”</strong>修改为<strong>“哪些河流流经印度”+“或中国”</strong>。全部的操作包括以下三种：<ul>
<li>AND</li>
<li>OR</li>
<li>NOT</li>
</ul>
</li>
<li>定量推理：如遇到max、min、count、at least/almost/approxmately/equal to N 等问题需要做定量推理。中文类似。</li>
<li>比较推理：基于某一个关系的问题需要做推理。如：“哪个国家拥有的河流比印度多？”</li>
</ol>
<h2 id="Linked-Sequential-QA"><a href="#Linked-Sequential-QA" class="headerlink" title="Linked Sequential QA"></a>Linked Sequential QA</h2><p>&emsp;&emsp;现在开始通过上述的 QA 对创建连续的对话，简单来说，如果两个问题共享一个 relation 或者 entity，那么就将两个问题放在一起。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>&emsp;&emsp;CSQA 由对话和 QA 组成，我们提出的模型由 <strong>HRED</strong> 模型和 <strong>key value memory network</strong> 模型组合而成。其中 HRED 模型是对话系统中的一流模型，key value memory network 模型是 QA 系统中的一流模型。我们的模型由以下组件构成：</p>
<ol>
<li><strong>Hierarchical Encoder</strong>：</li>
<li><strong>Handling Large Vocabulary</strong></li>
<li><strong>Candidate generation</strong></li>
<li><strong>Key Value Memory Network</strong></li>
<li><strong>Decoder</strong></li>
</ol>
<p>其中 1234 是 encoder，5是 decoder。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/复杂的连续问答：使用知识图谱在关联的问答对上学习交谈能力/提出的模型架构.jpg" alt="提出的模型架构"></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>&emsp;&emsp;使用 <strong>Adam</strong> 算法作为优化算法。<br>&emsp;&emsp;然后调整以下超参数：<strong>learning rate</strong> <script type="math/tex">\in</script> {1e-3, 4e-4}， <strong>RNN hidden unit size</strong>、 <strong>word embeddingsize</strong>、 <strong>KG embedding size</strong> <script type="math/tex">\in</script> {256, 512}，<strong>batch size</strong> <script type="math/tex">\in</script> {32, 64}，<strong>dialog context size</strong> as 2。<br>&emsp;&emsp;使用 Precision 和 Recall 作为评估指标。对于验证和计数的问题我们使用 accuracy 作为评估指标，此类问题会产生 YES/NO 或者 counts 的结果。最后对于需要阐明（clarification）的问题，系统产生自然语言回应，这通常是 KG 实体和非 KG 单词的序列，因此使用 Precision/Recall 作为 KG 实体的预测，使用 BLEU 作为语义相似度的衡量指标。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/复杂的连续问答：使用知识图谱在关联的问答对上学习交谈能力/实验结果.jpg" alt="实验结果"></p>
<h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>根据表 4 中的结果，我们讨论了现有方法的一些缺点，并提出了未来研究的领域。</p>
<ol>
<li><strong>Simple v/s Complex Questions</strong>：很明显在我们的模型上，与简单问题相比，复杂问题的性能非常差。改进点有很多，i)不确定现在逻辑函数是否可以处理定量、比较和逻辑推理问题；ii)不清楚现有的 encoder（HRED + key value memory network）是否能够有效地解析复杂问题并为 encoder 提供良好的表示。</li>
<li><strong>Direct v/s Indirect Questions</strong>：用表 4 中的第 3、4 行跟第 2 行比较，发现在处理不完整的问题时，模型性能有所下降，这些问题都需要依赖上下文才能解决共指、省略等难点。即使现在的对话系统（HRED）确实捕捉到了上下文，也没有什么作用。因为其中的一个<em>关键点</em>是<strong>对于我们创建的数据集有一个巨大的挑战</strong>：数据集里的 <strong>named entities</strong> 和 <strong>relations</strong> 比上下文中<strong>其他单词更重要</strong>，所以我们需要一个更好的模型，可以在训练时标出 relations 和 entities 的重要性（例如：<strong>注意力机制</strong>）。</li>
<li><strong>Candidate Generation</strong>：</li>
<li><strong>Better organization of the memory</strong>：对于某些问题，特别是设计多个实体和逻辑操作的复杂问题，不可避免地需要使用大量的内存存储元组。大约有 15% 的问题需要超过 100k 个候选元组。这会使 GPU 超负荷，并且也会使 softmax 的计算开销巨大，所以需要i)更好的内存组织方式，ii)SoftMax 函数的近似方法。</li>
</ol>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>向量建模KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（八）：Adaboost</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9AAdaboost.html</url>
    <content><![CDATA[<h1 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h1><p>&emsp;&emsp;分类算法有很多，比如逻辑回归、kNN 算法、决策树、朴素贝叶斯算法、支持向量机等，它们各有优缺点。我们自然可以<strong>将不同的分类器组合起来</strong>，而这种组合结果则被称为<strong>集成方法</strong>（ensemble method）或者元算法（meta-algorithm）。使用形式多种多样，可以是不同算法的集成，还可以是相同算法不同配置的集成，也可以自行发挥。</p>
<h1 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h1><p>&emsp;&emsp;自举汇聚法（bootstrap aggregating），也称为 bagging 方法。是从原始数据集选择 S 次后得到 S 个新数据集的一种技术。新数据集与原数据集的大小相等。<br>&emsp;&emsp;在 S 个数据集建好之后，将某个学习算法分别作用域每个数据集就得到了 S 个分类器。使用这 S 个分类器进行分类，然后将结果中最多的类别作为最后的分类结果。<br>&emsp;&emsp;当然还有一些更先进的 bagging 方法，比如随机森林（random forest）。<br><a id="more"></a></p>
<h1 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h1><p>&emsp;&emsp;boosting 方法拥有多个版本，这里只关注其中一个最流行的版本 AdaBoost。</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
        <tag>Adaboost</tag>
      </tags>
  </entry>
  <entry>
    <title>【NLP算法】（一）word2vec</title>
    <url>/%C2%B7zcy/AI/nlp/%E3%80%90NLP%E7%AE%97%E6%B3%95%E3%80%91%EF%BC%88%E4%B8%80%EF%BC%89word2vec.html</url>
    <content><![CDATA[<h1 id="词嵌入模型"><a href="#词嵌入模型" class="headerlink" title="词嵌入模型"></a>词嵌入模型</h1><p>&emsp;&emsp;以前在训练语言模型的时候使用one-hot编码，现在使用的是word embedding。但是这样理解其实稍微有点问题，因为如果这样理解，就难以解释embedding层是用来干嘛的了。<br>&emsp;&emsp;正解：<br>&emsp;&emsp;<strong>其实现在使用的还是one-hot</strong>，在keras框架中输入的就是one-hot编码。我以前一直是上面的那种理解，所以我一直不懂为什么要输入one-hot，而不直接输入word embedding。众所周知，one-hot编码乘上词嵌入矩阵可以得到词向量，这步操作看起来有点像查表。但其实这里用到了<strong>预训练</strong>。<br>&emsp;&emsp;词嵌入矩阵是预先训练好的，其实就是一个权重矩阵，而我们做的就是初始化了输入层的权重矩阵，只不过以前是随机初始化，而现在是直接初始化为词嵌入矩阵。embedding层只用了一步就获得了词的特征，但以前需要训练。所以我们算是使用了预训练的方法，直接获得了embedding层，但是相比于cv领域，目前我们还无法初始化高层的权重。<br>&emsp;&emsp;<strong>总结一下就是词嵌入矩阵是input层到embedding层的权重矩阵。</strong></p>
<ul>
<li><strong>缺陷</strong><br>多义词问题无法解决，所以导致word embedding一直效果不好。<br>以下引用自<a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">专栏</a><br>Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。<a id="more"></a>
</li>
</ul>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p><a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="noopener">word2vec 数学原理目录</a><br><a href="https://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="noopener">word2vec 数学原理</a><br><a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="noopener">word2vec原理推导与代码分析</a></p>
<h1 id="训练词向量得到的-accuracy"><a href="#训练词向量得到的-accuracy" class="headerlink" title="训练词向量得到的 accuracy"></a>训练词向量得到的 accuracy</h1><p>word2vec 的 accuracy 貌似没卵用。我反正用 keras 搭的 CBOW 模型，accuracy 极低，只有 1%不到。<br><a href="https://www.zhihu.com/question/271782463" target="_blank" rel="noopener">参考</a>  </p>
<h1 id="自己训练-word2vec-还是直接用别人的"><a href="#自己训练-word2vec-还是直接用别人的" class="headerlink" title="自己训练 word2vec 还是直接用别人的"></a>自己训练 word2vec 还是直接用别人的</h1><p><a href="https://bbs.csdn.net/topics/392144812?list=4625705" target="_blank" rel="noopener">参考</a><br>可能还是自己训练 word2vec 比较好，用别人的 word2vec 不太行。</p>
<h1 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h1>]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>代价函数</title>
    <url>/%C2%B7zcy/AI/ml/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><div class="table-container">
<table>
<thead>
<tr>
<th>代价函数</th>
<th>选择</th>
</tr>
</thead>
<tbody>
<tr>
<td>binary cross entropy</td>
<td>典型选择：二元分类</td>
</tr>
<tr>
<td>cross entropy</td>
<td>典型选择：多元分类</td>
</tr>
<tr>
<td>mse</td>
<td>典型选择：线性回归</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Softmax-Loss"><a href="#Softmax-Loss" class="headerlink" title="Softmax Loss"></a>Softmax Loss</h1><p>&emsp;&emsp;Softmax Loss 是由 softmax 和 cross entropy loss 组合而成，在 pytorch，caffe，tensorflow 等开源框架的实现中，直接将二者合并在一层。如在 pytorch 中不需要再输出层加上 softmax 层用于分类，直接使用 cross entropy loss 即可。<br><a id="more"></a></p>
<h1 id="各类代价函数"><a href="#各类代价函数" class="headerlink" title="各类代价函数"></a>各类代价函数</h1><p><a href="https://blog.csdn.net/cqfdcw/article/details/78173839" target="_blank" rel="noopener">方差、协方差、标准差、均方差、均方根值、均方误差、均方根误差</a><br><a href="https://www.cnblogs.com/shujuxiong/p/9339916.html" target="_blank" rel="noopener">L1正则和L2正则的比较分析详解</a></p>
<h1 id="各类距离公式"><a href="#各类距离公式" class="headerlink" title="各类距离公式"></a>各类距离公式</h1><p><a href="https://blog.csdn.net/guojingjuan/article/details/50396254" target="_blank" rel="noopener">python 各类距离公式实现</a></p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>英文</th>
<th>公式</th>
<th>别称</th>
</tr>
</thead>
<tbody>
<tr>
<td>残差平方和 SSE</td>
<td>Sum of Squares for Error</td>
<td>SSE = <script type="math/tex">\sum^m_{i=1}(y_i - \hat{y}_i)^2</script></td>
<td>剩余平方和 RSS</td>
</tr>
<tr>
<td>回归平方和 SSR</td>
<td>Sum of Squares for Regression</td>
<td>SSR = <script type="math/tex">\sum^m_{i=1}(\hat{y}_i - \bar{y})^2</script></td>
<td>解释平方和 ESS</td>
</tr>
<tr>
<td>总离差平方和 SST</td>
<td>Sum of Squares for Total</td>
<td>SST = <script type="math/tex">\sum^m_{i=1}(y_i - \bar{y})^2</script></td>
<td>总离差平方和 TSS</td>
</tr>
</tbody>
</table>
</div>
<p>三者之间的关系是 SST = SSR + SSE</p>
<script type="math/tex; mode=display">R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}</script>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>代价函数</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Enquire: Learning to Query Tables in Natural Language</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/29%E3%80%81Neural%20Enquirer%EF%BC%9ALearning%20to%20Query%20Tables%20in%20Natural%20Language.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;我们提出使用一种神经网络结构结合知识库来回答自然语言（NL）问题。与之前端到端的语义解析器不同，NEURAL ENQUIRER 是完全“神经化”的：它提供查询和 KB 表的分布式表示，并通过一系列可微的操作执行查询。该模型可以通过 end-to-end 和 step-by-step 的监督进行梯度下降训练。在训练期间，查询和 KB 表的表示将与查询执逻辑（query execution logic）一起进行优化。实验表明，该模型可以学习对结构丰富的 KB 表执行复杂的 NL 查询。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><a id="more"></a>]]></content>
      <categories>
        <category>paper</category>
      </categories>
  </entry>
  <entry>
    <title>Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/28%E3%80%81Neural%20Symbolic%20Machines%EF%BC%9ALearning%20Semantic%20Parsers%20on%20Freebase%20with%20Weak%20Supervision.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>本文介绍了一种神经符号机（Neural Symbolic Machine, NSM）</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><a id="more"></a>]]></content>
      <categories>
        <category>paper</category>
      </categories>
  </entry>
  <entry>
    <title>论文笔记：Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/27%E3%80%81Sequence-to-Action%EF%BC%9AEnd-to-End%20Semantic%20Graph%20Generation%20for%20Semantic%20Parsing.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://mp.weixin.qq.com/s?__biz=MzI2NjkyNDQ3Mw==&amp;mid=2247486979&amp;idx=2&amp;sn=2d95556630820c853f2ca9b2855dd60a&amp;chksm=ea87f6d5ddf07fc3cc8477d3a0cd5142e9191d91ff3161847524c37539b372b306a8f9b700a8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">某篇解析</a>；<a href="http://tongtianta.site/paper/11795" target="_blank" rel="noopener">某篇解析</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1809.00773.pdf" target="_blank" rel="noopener">论文地址</a>。<br>&emsp;&emsp;本文提出一种神经语义分析方法——Sequence-to-Action，将语义分析当做一个端到端的<strong>语义图生成</strong>的过程。我们同时使用了最近语义分析两个有前途的方向，<strong>首先</strong>我们的模型使用了一个语义图来表示一个句子的含义，该语义图与知识库紧密相关（博主注：<strong>即可以将语义图看作是知识库的一个子图</strong>）。<strong>其次</strong>，利用神经网络强大的表示学习和预测能力，提出一种 RNN 模型，能够有效的将句子映射到动作序列，从而生成语义图（博主注：<strong>此动作序列就是指生成语义图的动作，将这些动作看作是一个序列</strong>）。实验表明该方法在 OVERNIGHT 数据集上展现了一流的性能，在 GEO 以及 ATIS 数据集上得到了有一定竞争力的性能。<br>&emsp;&emsp;语义分析旨在<strong>将自然语言句子映射为逻辑形式</strong>（Zelle andMooney, 1996; Zettlemoyer and Collins, 2005;Wong and Mooney, 2007; Lu et al., 2008;Kwiatkowski et al., 2013）。例如“Which states border Texas?”将会被映射为 <em>answer (A, (state (A),nextto (A, stateid ( texas ))))</em>。<br>&emsp;&emsp;语义分析器需要两个函数，一个处理结构预测，另一个处理语义基础。传统的语义解析器通常基于复合语法，如 CCG（Zettlemoyer and Collins, <a href="https://arxiv.org/pdf/1207.1420" target="_blank" rel="noopener">2005</a>, <a href="https://www.aclweb.org/anthology/D07-1071" target="_blank" rel="noopener">2007</a>），DCS（<a href="https://www.aclweb.org/anthology/P11-1060" target="_blank" rel="noopener">Liang et al., 2011</a>）等。不幸的是，设计语法和学习精确的词汇仍是一个挑战，特别是在开放域。而且设计有效的特性往往很困难，它的学习过程也不是端到端的。为了解决上述问题，本文提出了两种有前途的研究方向：<strong>基于语义图</strong>的方法和<strong>基于 seq2seq</strong> 方法。<br><a id="more"></a><br>&emsp;&emsp;基于语义图的方法(Reddy et al.,2014, 2016; Bast and Haussmann, 2015; Yih et al.,2015)将句子的含义表示为语义图（即知识库的子图，参考图 1 中的例子）并<strong>将语义分析视为语义图匹配/生成过程</strong>。<strong>与逻辑形式相比，语义图与知识库有着紧密的关系</strong>(Yih et al., 2015), ，与句法结构有许多共性（Reddy et al.,2014）。基于语义图的句法分析的主要挑战是如何有效地构造句子的语义图，目前语义图是通过与模式匹配（Bast and Haussmann, 2015），从依赖树转换（Reddy et al., 2014, 2016），或者通过 staged heuristic search algorithm（Yih et al.,2015）构建的。这些方法都是基于人工设计的构造过程，它们很难处理开放/复杂的情况。<br>&emsp;&emsp;近年来，得益于 RNN 模型有较强的表示能力和预测能力，其在 Seq2Seq 模型上取得了成功，比如机器翻译。许多 Seq2Seq 模型也用于语义分析（Xiaoet al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016），不需要高质量的词典、人工构建的语法和特性。这些模型通过端到端的训练，利用注意力机制（Bahdanauet al., 2014; Luong et al., 2015）学习句子和逻辑形式之间的软对齐。<br>&emsp;&emsp;本文提出了一种新的神经语义分析框架——Sequence-to-Action。它可以同时利用语义图表示的优点和 seq2seq 模型强大的预测能力。具体来说，我们将语义分析建模为一个端到端的语义图生成过程。例如，在图 1 中，我们的模型将通过生成一系列变量[add variable:a，addtype:state，…]来解析“which states border Texas”这句话。为了实现上述目标，我们首先设计了一个动作集，对语义图的生成过程进行编码（包括节点动作：add variable,add entity,add type，边动作：add edg 以及操作动作：argmin,argmax,count,sum 等）然后我们设计了一个 RNN 模型，该模型可以生成一个动作序列来构造句子的语义图。最后，我们在解码过程中合并结构和语义约束来进一步增强解析。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1>]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>语义解析KBQA</tag>
        <tag>seq2action</tag>
      </tags>
  </entry>
  <entry>
    <title>Language to Logical Form with Neural Attention</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/26%E3%80%81Language%20to%20Logical%20Form%20with%20Neural%20Attention.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1601.01280.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2016 年。</p>
          </div>
<p>&emsp;&emsp;语义分析的目的是将自然语言映射到机器可解释的有意义表示。传统的方法依赖于高质量的词汇、人工构建的模板以及特定领域或特定表示的语言特征，本文提出了一种注意力增强的 encoder-decoder 通用模型。将输入的话表示为向量形式，并通过调节输出序列或者树生成逻辑形式（总结来说，就是<strong>将话语转为逻辑形式</strong>，详情请看图 1）。<br>&emsp;&emsp;下图将一句话转为了逻辑形式，不同于以前的方法，它是通过神经网络生成的，而以前的方法依赖于手写的规则。图片取自 <a href="https://www.aclweb.org/anthology/W00-1317" title="Automated construction ofdatabase interfaces: Intergrating statistical and rela-tional learning for semantic parsing" target="_blank" rel="noopener">Tang and Mooney200</a>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/语句转为逻辑形式.jpg" alt="语句转为逻辑形式"></p>
<p>&emsp;&emsp;基于 RNN 的 encoder-decoder 已成功应用于各种 NLP 任务，图 1 中使用了 LSTM，我们的做法是提出了两个变体模型。<strong>第一个模型</strong>将语义解析视为普通的序列转换任务，<strong>第二个模型</strong>配备了层次树解码器，该解码器明确地捕获逻辑形式的组合结构。我们还引入了<strong>注意力机制</strong>，并提出一个识别步骤来<strong>识别很少提到的实体</strong>和<strong>数字</strong>。<br>&emsp;&emsp;对<strong>四个数据集</strong>的实验结果表明，我们的方法在不使用人工设计特征的情况下具有竞争力，并且易于迁移。<br>&emsp;&emsp;我们的工作综合了两种标准研究，即<strong>语义分析</strong>和 <strong>encoder-decoder 架构的神经网络</strong>。<br><a id="more"></a></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;学习语义解析器的问题引起了广泛的关注，可以追溯到 Woods（1973年）。。。。</p>
<h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>&emsp;&emsp;我们的目标是学习一个模型，将<u><strong>自然语言输入 <script type="math/tex">q = x_1 \dots x_{|q|}</script></strong></u> 映射为其含义的<u><strong>逻辑形式（logical form）表示 <script type="math/tex">a = y_1 \dots y_{|a|}</script></strong></u>。条件概率被分解为：</p>
<script type="math/tex; mode=display">
\begin{align}
    p(a|q) & = \prod^{|a|}_{t=1} p(y_t|y_{<t},q) \tag 1\\
    y_{<t} & = y_1 \dots y_{t-1}
\end{align}</script><p>&emsp;&emsp;我们的模型包含一个编码器和一个解码器，编码器负责将输入的自然语言 q 编码成向量，解码器负责生成 <script type="math/tex">y_1 \dots y_{|a|}</script>。下面将仔细描述。</p>
<h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>&emsp;&emsp;对于普通的 Seq2Seq 任务，使用 LSTM 来计算，如下图所示。<script type="math/tex">h^l_t</script> 代表第 l 层的第 t 个时间步的隐藏层，公式为：</p>
<script type="math/tex; mode=display">
\begin{align}
    h^l_t = \text{LSTM}(h^l_{t-1},h^{l-1}_t) \tag 2
\end{align}</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/Seq2Seq.jpg" alt="Seq2Seq"><br>&emsp;&emsp;在实验中，遵循 <a href="https://arxiv.org/pdf/1409.2329.pdf" title="RECURRENT NEURAL NETWORK REGULARIZATION" target="_blank" rel="noopener">Zaremba et al. 2015</a> 提出的架构。不过，使用其他类型的门控激活函数也是可以的（例如<a href="https://arxiv.org/pdf/1406.1078.pdf" title="Learning phrase representations using RNN encoder-decoder for statistical machine translation" target="_blank" rel="noopener">Cho et al. 2014</a>）。<strong>对于 encoder</strong>，<script type="math/tex">h^0_t = W_qe(x_t)</script>（注：此公式是第 0 层的运算步骤，即输入层）是 RNN 中输入的词向量，<script type="math/tex">W_q \in \mathbb{R}^{n \times |V_q|}</script> 代表输入层的权重值矩阵，e(·) 代表对应 token 的索引。<strong>对于 decoder</strong>，<script type="math/tex">h^0_t = W_ae(y_{t-1})</script> 代表前一个预测词的词向量，其中 <script type="math/tex">W_a \in \mathbb{R}^{n \times |V_a|}</script>。接下来，最后的 LSTM <script type="math/tex">h^L_t</script> 被用于预测 <script type="math/tex">t</script>-th 输出 token，计算公式为：</p>
<script type="math/tex; mode=display">
\begin{align}
    p(y_t|y_t,q) = softmax(W_oh^L_t)^T e(y_t) \tag 3
\end{align}</script><p>&emsp;&emsp;<strong>该公式用于预测每一个 token</strong>。另外补充一点，增加了 “start-of-sequence” <code>&lt;s&gt;</code> 和 “end-of-sequence” <code>&lt;/s&gt;</code>。<br>&emsp;&emsp;该模型总的来说，就是 LSTM 的计算方法，也没什么好说的。</p>
<h2 id="Seq2Tree"><a href="#Seq2Tree" class="headerlink" title="Seq2Tree"></a>Seq2Tree</h2><p>&emsp;&emsp;Seq2Seq 模型有一个<strong>缺点</strong>就是它<strong>忽略了逻辑形式的层次结构</strong>。所以，要改良的话，它需要记住各种辅助信息（比如括号对），以此生成格式良好的输出。如下图 3 所示，是一个层次树 decoder：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/Seq2Tree模型.jpg" alt="Seq2Tree模型"></p>
<p>&emsp;&emsp;Seq2Tree 与 Seq2Seq 的编码器类似，不同的是解码器。Seq2Tree 以自上而下的方式生成逻辑，为了定义树结构，我们定义了一个表示子树的 “nonterminal” <code>&lt;n&gt;</code> 标记。如图 3 所示，将<strong><em>逻辑形式 “lambda $0 e (and (&gt;(departure_time $0) 1600:ti) (from $0 dallas:ci))”</em></strong>预处理为树，方法是<strong>用 nonterminal 替换括号对之间的标记</strong>（token）。<strong>特殊记号 <code>&lt;s&gt;</code> 和 <code>&lt;(&gt;</code> 分别表示序列和 nonterminal 序列的开头</strong>（由于缺少空间，图3中省略了），<strong>记号 <code>&lt;/s&gt;</code> 代表序列结束</strong>。具体步骤是：</p>
<ol>
<li>编码输入值 q；</li>
<li>层次树解码器使用 RNN 在逻辑形式 a（在<strong>任务定义</strong>中已经说明了 q 和 a 的含义）的对应部分的子树中生成 tokens（注意这里的 token 带了 s）；</li>
<li>如果预测的 token 为 <code>&lt;n&gt;</code>，则通过调节 nonterminal 的隐藏向量来解码序列。（博主注：举个例子理解一下：看图 3 的第一层，先是使用 encoder 进行编码，接着开始对逻辑形式进行解码，逻辑形式就是上面的斜体部分。接下来预测到了 token<code>&lt;n&gt;</code> 于是调用 nonterminal 的隐藏向量来进行解码，即生成一棵子树。以此类推，碰到 toekn <code>&lt;n&gt;</code> 就开始解码）</li>
<li>与 Seq2Seq 解码器不同，当前的隐藏状态不仅仅取决于上一个时间步，为了更好地利用 parent nonterminal 的信息，我们引入了一个 parent-feeding 的连接，其中 parent nonterminal 的隐藏向量与输入连接（concatenated）并喂入 LSTM。</li>
</ol>
<p>&emsp;&emsp;再举个例子帮助理解一下，如图 4 所示。逻辑形式为 <strong><em>A B (C)</em></strong>，其中 <script type="math/tex">y_1 \dots y_6</script> 代表不同的时间步，<strong><em>(C)</em></strong> 对应子树。解码一共有<strong>两个步骤</strong>：一旦输入值 q 被编码，首先在深度为 1 处生成 <script type="math/tex">y_1 \dots y_4</script>，直到 token <code>&lt;/s&gt;</code> 被预测到；接下来通过调节 nonterminal <script type="math/tex">t_3</script> 的隐藏向量来生成 <script type="math/tex">y_5, y_6</script>，<script type="math/tex">p(a|q)</script> 的概率是这<strong>两个序列解码步骤</strong>的乘积：</p>
<script type="math/tex; mode=display">
p(a|q) = p(y_1 y_2 y_3 y_4 | q) p(y_5 y_6 | y_{\leq 3},q)</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/一个简单的Seq2Tree的例子.jpg" alt="一个简单的Seq2Tree的例子"></p>
<h2 id="Attention-机制"><a href="#Attention-机制" class="headerlink" title="Attention 机制"></a>Attention 机制</h2><p>&emsp;&emsp;<strong><em>Attention 的原理</em></strong>。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>&emsp;&emsp;我们的目标是<strong>最大化</strong>由自然语言语句作为输入时产生的逻辑形式的可能性，所以目标函数为：</p>
<script type="math/tex; mode=display">
\text{minimize} - \sum_{(q,a) \in D} logp(a|q)</script><p>&emsp;&emsp;其中 <script type="math/tex">D</script> 是所有自然语言逻辑形式训练对的集合，<script type="math/tex">p(a|q)</script> 按式（1）计算。采用 <strong>RMSProp</strong> 算法解决了这一非凸优化问题。此外，使用 <strong>Dropout</strong> 进行正则化。</p>
<h2 id="推论"><a href="#推论" class="headerlink" title="推论"></a>推论</h2><p>&emsp;&emsp;暂时略。</p>
<h2 id="参数识别"><a href="#参数识别" class="headerlink" title="参数识别"></a>参数识别</h2><p>&emsp;&emsp;大多数的语义分析数据集都是为问答开发的。在经典的系统中，问题被映射乘逻辑形式，并在知识库中获取答案。由于问答任务的性质，许多自然语言的语句都包含实体或数字，它们通常被解析为逻辑形式的参数。其中不可避免地会有一些罕见或者根本不会出现在数据集中的实体或数字（对于小规模数据集尤其如此）。传统的序列编码器只是简单地用一个特殊的位置单词符号替换稀有单词（<a href="https://arxiv.org/pdf/1410.8206.pdf" title="Addressing the Rare Word Problem in Neural Machine Translation" target="_blank" rel="noopener">Luong et al. 2015a</a>; <a href="https://arxiv.org/pdf/1412.2007.pdf" title="On Using Very Large Target Vocabulary for Neural Machine Translation" target="_blank" rel="noopener">Jean et al. 2015</a>），这对语义分析是有害的。<br>&emsp;&emsp;为此开发了一个简单的参数识别程序。具体来说就是在输入的问题中标识实体和数字，并用它们的<strong>类型</strong>和<strong>唯一 id</strong> 替换它们。例如，将训练样本“<em>jobs with a salary of 40000</em>”及其逻辑形式“job(ANS), salary_greater_than(ANS,40000, year)”预处理为“jobs with a salary of <em><script type="math/tex">num_0</script></em>”和“job(ANS), salary_greater_than(<em>ANS</em>,<em><script type="math/tex">num_0</script></em>,<em>year</em>)”。一旦解码完毕，后处理步骤就会将所有标记 <script type="math/tex">type_i</script> 恢复到它们以前的实体或数字。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;我们将我们的方法在四个数据集上分别与以前的多个系统进行比较，下面将描述这些数据集。代码可在此处获得<a href="https://github.com/donglixp/lang2logic" target="_blank" rel="noopener">https://github.com/donglixp/lang2logic</a>（lua 版，官方），<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch" target="_blank" rel="noopener">https://github.com/Alex-Fabbri/lang2logic-PyTorch</a>（python 版，非官方）。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>&emsp;&emsp;<strong>JOBS</strong>    工作<br>&emsp;&emsp;<strong>GEO</strong>        Geoquery data<br>&emsp;&emsp;<strong>ATIS</strong>    Airline Travel Information System（航空旅行信息系统）<br>&emsp;&emsp;<strong>IFTTT</strong>    if this then that（<a href="https://ifttt.com/" target="_blank" rel="noopener">地址</a> <a href="https://baike.baidu.com/item/ifttt/8378533" target="_blank" rel="noopener">百度百科介绍</a>），<a href="https://www.aclweb.org/anthology/P15-1085" target="_blank" rel="noopener">Quirk et al.2015</a> 从 IFTTT 网站提取大量的 if-this-then-that 来创建此数据库<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/数据集介绍.jpg" alt="数据集介绍"></p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>&emsp;&emsp;自然语言语句是小写的，并且使用基于维基百科的常见拼写错误列表来纠正拼写错误。使用 NLTK 来限制词汇（[Bird et al.2009] Natural Language Processing with Python. O’Reilly Media.），对于 IFTTT 过滤了在训练集中出现少于五次的 token，channels 和 functions。对于其他数据集，过滤了在训练集中至少两次没有出现的输入词，但保留了逻辑形式中的所有 token。并且使用了<strong>参数识别</strong>，当然也可以使用更复杂的办法。<br>&emsp;&emsp;超参数在 JOBS 和 GEO 上使用了交叉验证，使用了 ATIS 和 IFTTT 作为标准开发集（就是验证集，不同的叫法而已 development/validation）。</p>
<ul>
<li><strong>RMSProp</strong>：batch size = 20；parameter = 0.95；</li>
<li><strong>梯度修剪</strong>为 5 以缓解梯度爆炸（<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">Pascanu et al.2013</a>）；</li>
<li><strong>参数</strong>从均匀分布 <script type="math/tex">U(-0.08, 0.08)</script> 中随机初始化；</li>
<li>两层 <strong>LSTM</strong> 用于 IFTTT，单层 LSTM 用于其他数据集；</li>
<li><strong>dropout</strong> <script type="math/tex">\in</script> {0.2,0.3,0.4,0.5}；</li>
<li>隐藏向量和词嵌入<strong>维度</strong>从 {150, 200, 250} 选择；</li>
<li><strong>early stopping</strong>；</li>
<li>输入句子在进入编码器之前被反转（<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever et al.2014</a>）；</li>
<li><strong>贪婪搜索</strong>生成逻辑形式；</li>
<li><strong>softmax</strong> 用于分类。</li>
</ul>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>&emsp;&emsp;Attention 机制可以提高性能，对于小数据集<strong>参数识别</strong>至关重要。</p>
<h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><h1 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h1><p>&emsp;&emsp;<a href="http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf" title="Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars" target="_blank" rel="noopener">论文</a>使用 Logical Form 对基准数据集做实验，数据集包括 Geo880 和 Jobs640，论文中使用的是 Logical Form 的其中一种表示——<strong>PCCG</strong>，它是 CCG 的改进版。他们将数据集分割为训练集和测试集，Language to Logical Form with Neural Attention 沿用了此分割方式（比如说将 GEO 分割为 680 个训练样本，200 个测试样本），并且采用此论文的思想，即：将自然语言映射为 Logical Form。<br>&emsp;&emsp;虽然 PCCG 的 Logical Form 效果不错，但是作者没有使用他，而是使用了 <a href="https://www.aclweb.org/anthology/D11-1140" target="_blank" rel="noopener">lambda-caculus</a>。<strong>作者将 Geo880 等数据集改写为了 lambda-calculus 的形式</strong>。<em>在<a href="https://github.com/yuxuan1995liu/Semantic-Parsing-Data-Pre-Processing" target="_blank" rel="noopener">此处</a>可找到全部数据，但是这里面的格式不是 lambda-calculus。我有点搞不懂他提供的数据到底是什么意思</em>。<strong>19.09.16 补充</strong>：经过多方查找，终于找到了 geo880 最初的<a href="https://link_springer.gg363.site/content/pdf/10.1007/3-540-44795-4_40.pdf" title="Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing" target="_blank" rel="noopener">论文</a>，在<a href="https://www.cs.utexas.edu/~ml/publications/year/2001" target="_blank" rel="noopener">此处</a>找到的。GEO880 最初版本并不是 lambda calculus。<br>&emsp;&emsp;作者将数据改写为 lambda-calcullus 形式是我估计的。因为全文找不到数据的来源，格式转换的说明也找不到。只是在 Section 4.1 Datasets 中说到：</p>
<blockquote>
<p>&emsp;&emsp;<strong>GEO 有 880 个示例，将其分割为 680 个训练样本以及 200 个测试样本（Zettlemoyer and Collins, 2005）， 我们使用了基于 lambda-calculus 的具有相同含义的表示</strong>。</p>
</blockquote>
<p>&emsp;&emsp;所以我推测作者应该是将原本的 PCCG 表示的 GEO 改成了 lambda-calculus 表示。<br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1805.04793" title="Coarse-to-Fine Decoding for Neural Semantic Parsing" target="_blank" rel="noopener">论文</a>是作者对 Language to Logical Form with Neural Attention 的改进版。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>语义解析KBQA</tag>
        <tag>seq2tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Large-scale Simple Question Answering with Memory Network</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/5%E3%80%81Large-scale%20Simple%20Question%20Answering%20with%20Memory%20Network.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1506.02075v1.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2015 年。<br>&emsp;&emsp;开放域问答系统的目的是在不受域限制的情况下，为用自然语言表达的问题提供准确的答案。问答系统有很长的历史，它们搜索文本文档或在网络上提取答案（see e.g.(Voorhees and Tice, 2000; Dumais et al., 2002)）。最近在公开的大型知识库（KBS）方面也取得了进展，如 Freebase 知识库。然而，尽管最近大家都在关注设计一个具有推理能力的系统，它可以检索并使用 KB 中的<strong>多重事实</strong>进行问答。但是其实只涉及 <strong>KB 中单个事实的简单问答</strong>都还没被解决，本论文中将其称为 Simple Question Answering。<br>&emsp;&emsp;KBQA 现存的方法：1）将 question 转为结构化的 KB 查询语句（Berant et al. 2013）；或者 2）学习将 question 以及 facts 嵌入到低维向量空间中，然后在这些向量中通过计算相似度检索答案（<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a</a>）。<br>&emsp;&emsp;本文贡献有二：</p>
<ul>
<li>其一，为了<strong>研究现有系统</strong>以及<strong>通过多任务学习在不同数据源上同时训练</strong>成为可能，我们收集了第一个基于知识库的大规模的问答数据集，称为 SimpleQuestions。包含了人类编写和 Freebase facts 相关的超过 10 万个问题，另外现有的基准数据集 WebQuestions 包含的问题少于 6 千个，这些问题是使用 google suggest api 自动创建的。</li>
<li>其二，提出了一种基于词嵌入的问答系统，在 Memory Networks (MemNNs)（<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Weston et al., 2015</a>;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">Sukhbaatar et al., 2015</a>） 框架下开发而成。</li>
</ul>
<p>&emsp;&emsp;虽然我们的模型与之前的 QA 嵌入模型（<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a</a>;<a href="https://arxiv.org/pdf/1404.4326.pdf" target="_blank" rel="noopener">Bordes et al., 2014b</a>）相似，但使用 MemNNs 的框架为未来工作中更复杂的推理方案提供了思路，因为 MemNNs 在复杂的推理问答任务上表现了很好的性能（<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Weston et al., 2015</a>）。<br><a id="more"></a></p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><ol>
<li>Sections 3, 4：介绍了基于词嵌入的问答系统；</li>
<li>Section 5：相关工作；</li>
<li>Section 6：实验结果。</li>
</ol>
<h1 id="Memory-Network-for-Simple-QA"><a href="#Memory-Network-for-Simple-QA" class="headerlink" title="Memory Network for Simple QA"></a>Memory Network for Simple QA</h1><p>&emsp;&emsp;Memory network 由一个 memory（一个索引对象数组）和一个神经网络组成。神经网络由 Input map(I), Generalization(G), Output map(O) and Response(R) 构成。其工作流如下所示：</p>
<ol>
<li>Storing Freebase：第一阶段。解析 Freebase（可以是 FB2M 或 FB5M，取决于配置）并且将它存进 memory。它使用 Input module 去预处理数据；</li>
<li>Training：第二阶段。训练 MemNN 去回答问题。此处使用 Input, Output and Response modules，训练主要关注核心 Output module 嵌入模型的参数；</li>
<li>Connecting Reverb：第三阶段。将来自 Reverb 的 new facts 添加到 memory 中。这是在训练完毕后进行的，为了测试 MemNNs 在不需要重新训练的情况下处理 new facts 的能力。它使用 Input module 去预处理 Reverb facts 并且使用 Generalization module 将它们和已经被存储的 facts 连接。</li>
</ol>
<h2 id="Input-module"><a href="#Input-module" class="headerlink" title="Input module"></a>Input module</h2><p>&emsp;&emsp;此组件预处理 3 种类型的数据，它们会被输入进神经网络：</p>
<ol>
<li>Freebase facts：用于填充 memory；</li>
<li>questions：系统需要回答的问题；</li>
<li>Reverb facts：在 workflow 第二阶段中，我们用它扩展 memory。</li>
</ol>
<h3 id="preprocessing-Freebase"><a href="#preprocessing-Freebase" class="headerlink" title="preprocessing Freebase"></a>preprocessing Freebase</h3><p>&emsp;&emsp;Freebase 数据最初存储原子 facts，包括将单个实体作为主语或者宾语，再在它们之间加上一个联系（即谓语）。<strong>但是这样的存储需要从两个方面与 QA 任务适应</strong>。</p>
<ol>
<li>为了回答不止有一个答案的问题，我们将 fact 重新定义为一个三元组，其包含 subject，relationship 以及通过 relationship 连接至 subject 的一组 objetcs 。这个分组过程将 atomic facts 转为 grouped facts，以下将其简单的称为 facts。Table 2 显示了这样分组可以减少 facts 的数量。</li>
<li></li>
</ol>
<h3 id="Preprocessing-Freebase-facts"><a href="#Preprocessing-Freebase-facts" class="headerlink" title="Preprocessing Freebase facts"></a>Preprocessing Freebase facts</h3><p>&emsp;&emsp;</p>
<h3 id="Preprocessing-questions"><a href="#Preprocessing-questions" class="headerlink" title="Preprocessing questions"></a>Preprocessing questions</h3><h3 id="Preprocessing-Reverb-facts"><a href="#Preprocessing-Reverb-facts" class="headerlink" title="Preprocessing Reverb facts"></a>Preprocessing Reverb facts</h3><h2 id="Generalization-module"><a href="#Generalization-module" class="headerlink" title="Generalization module"></a>Generalization module</h2><p>&emsp;&emsp;此模块负责将新的元素增加到 memory 中。在我们的例子中，memory 具有一个 multigraph 结构，其中每个节点都是 Freebase 的一个实体，multigraph 中被标记的 arcs 是 Freebase 中的 relationships：预处理之后，所有 Freebase 的 facts 都使用此结构存储。<br>&emsp;&emsp;<br>&emsp;&emsp;为了将 Reverb 的 subject 和 object 链接到 Freebase 实体，我们使用 precomputed entity links (<a href="https://www.aclweb.org/anthology/W12-3016.pdf" target="_blank" rel="noopener">Lin et al., 2012</a>)。。。。</p>
<h2 id="Output-module"><a href="#Output-module" class="headerlink" title="Output module"></a>Output module</h2><p>&emsp;&emsp;Output 模块通过给定 input ，在 memory 中执行查表（lookup）操作，返回该问题的 supporting facts。在我们的 simple QA 例子中，此模块只返回一个 supporting fact。为了避免为所有存储的 facts 评分（即为了避免时间代价太大），我们先执行一步<em>近似实体链接</em>（proximate entity linking），以生成一个小的候选 facts 集合。最后， supporting fact 指的是与嵌入模型中的问题最相似的候选 fact。</p>
<h3 id="Candidate-generation"><a href="#Candidate-generation" class="headerlink" title="Candidate generation"></a>Candidate generation</h3><p>&emsp;&emsp;略。</p>
<h3 id="Scoring"><a href="#Scoring" class="headerlink" title="Scoring"></a>Scoring</h3><p>&emsp;&emsp;略。</p>
<h2 id="Response-module"><a href="#Response-module" class="headerlink" title="Response module"></a>Response module</h2><p>&emsp;&emsp;在 memory network 中，Response 模块对 Output 模块的结果进行后处理操作，以计算预期的答案。在我们的例子中，它返回被挑选出来的 supporting fact 的对象集（博主注：这个对象集我猜测是 KG 中的三元组）。<br>&emsp;&emsp;注：不必纠结 Response 模块的具体功能，可以自己定制，必然在<a href="https://zhuanlan.zhihu.com/p/29590286" target="_blank" rel="noopener">记忆网络之Memory Networks</a>中写到最初的 memory network 的 response 模块只是简单地将向量转成单词。</p>
<h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/liuchonge/article/details/78128238" target="_blank" rel="noopener">记忆网络之open-domain QA 应用</a>，csdn 的一篇博客，也对此论文的训练方法做了总结。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>memory network</tag>
      </tags>
  </entry>
  <entry>
    <title>An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/24%E3%80%81An%20End-to-End%20Model%20for%20Question%20Answering%20over%20Knowledge%20Base%20with%20Cross-Attention%20Combining%20Global%20Knowledge.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P17-1021" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;随着知识库数量的增加，人们越来越希望寻找到一些有效的方法来获取这些资源。现在有几种专门为<strong>查询 KBs</strong> 设计的<strong>语言</strong>：SPARQL（<a href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/" target="_blank" rel="noopener">rudhommeaux and Seaborne, 2008</a>）。但要使用这些语言，用户不仅需要熟悉它们，还要了解 KBs 的体系结构。相比之下，<strong>以自然语言为查询语言</strong>的 KB-QA 是一种更友好的方案，近年来已成为研究热点。这项任务<strong>以前</strong>有两个主流的研究方向：</p>
<ol>
<li>基于语义解析（semantic parsing-base, SP-based）</li>
<li>基于信息检索（information  retrieval-based, IR-based）</li>
</ol>
<p>&emsp;&emsp;<strong>现在</strong>随着神经网络方法的发展，基于神经网络的 KB-QA 已经取得了令人瞩目的成果。其中至关重要的步骤就是计算<strong>问题和候选答案</strong>之间的相似性分数，这一步骤的<strong>关键一点</strong>就是学习它们的表示。然而以往的研究更注重<strong>答案的学习表示</strong>。例如，<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al. 2014a</a> 考虑候选答案子图的重要性，<a href="https://www.aclweb.org/anthology/P15-1026" target="_blank" rel="noopener">Dong et al. 2015</a>利用上下文和答案的类型。无论如何，<strong>问题的表示</strong>终究还是表达不全。现有的方法 <a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a,</a> <a href="https://arxiv.org/pdf/1404.4326.pdf" target="_blank" rel="noopener">b</a> 使用 bag-of-word 模型将问题表示为一个向量，但是这样<strong>问题与答案的关联性</strong>还是被忽视了。我们认为一个问题应该根据回答时不同的侧重面来表示（注：<em>其实就是想用注意力机制</em>，回答的侧重面可以是答案实体本身、答案类型、答案上下文等）。<br>&emsp;&emsp;因此本文提出了一个端到端的神经网络模型，通过 <strong>cross-attention</strong> 机制，根据不同的候选答案动态地表示问题及对应的分数。此外还利用了 KB 中的全部知识，旨在将 KB 中丰富的知识集成到答案中，以此缓解 out-of-vocabulary(<strong>OOV</strong>) 的问题，从而帮助 cross-attention 更精确地表示问题。最后实验结果表明了该方法确实有效。<br>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P15-1026" title="Question Answering over Freebase wit hMulti-Column Convolutional Neural Networks" target="_blank" rel="noopener">论文</a>（<a href="https://yan624.github.io/·论文笔记/dialogue/QA/KBQA/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html">论文笔记地址</a>）中的方法很有启发性，但是由于简单地选择三个独立的 CNN ，因此过于机械化。所以我们使用了基于 cross-attention 的神经网络模型。<br>&emsp;&emsp;模型架构如下，步骤与之前的论文的步骤类似。<strong>1)</strong>先找到问题的主题（main entity/topic entity）；<strong>2)</strong>然后在知识库中找到主题相连的节点作为候选答案，<strong>3)</strong>最后送入 score layer 进行评分，排序分数选出分数最高的候选答案作为正确答案。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge/MCCNN总览.jpg" alt="MCCNN总览"></p>
<p>&emsp;&emsp;为了方便描述，我们将任何一种基本元素称为资源（resource），无论是实体还是关系。比如 (/m/0f8l9c,location.country.capital,/m/05qtj) 的描述是法国的首都是巴黎，其中的 <em>/m/0f8l9c</em> 和 <em>/m/05qtj</em> 分别代表法国和巴黎，<em>location.country.capital</em> 是一种关系。<br><a id="more"></a></p>
<h1 id="我们的方法"><a href="#我们的方法" class="headerlink" title="我们的方法"></a>我们的方法</h1><h2 id="候选者生成"><a href="#候选者生成" class="headerlink" title="候选者生成"></a>候选者生成</h2><p>&emsp;&emsp;略，我已经写过无数遍了。使用 Freebase API 构建的。</p>
<h2 id="The-Neural-Cross-Attention-Model"><a href="#The-Neural-Cross-Attention-Model" class="headerlink" title="The Neural Cross-Attention Model"></a>The Neural Cross-Attention Model</h2><p>&emsp;&emsp;下图是模型的架构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge/MCCNN架构.jpg" alt="MCCNN架构"></p>
<ul>
<li>问题表示（图 2 中左侧部分显示了处理步骤）<ol>
<li>使用向量表示问题中的每个单词，这跟其他 NLP 任务差不多，不过它是随机初始化的词嵌入矩阵 <script type="math/tex">E_w \in \mathbb{R}^{d \text{x} v_w}</script>，然后取出对应单词的词向量。d 代表词向量的维度，<script type="math/tex">v_w</script> 代表词表的大小。</li>
<li>将词向量送入 LSTM，值得注意的是我们没有使用单向 LSTM，因为这样一个单词表示只会捕获到之前的单词的信息而不会包含之后的单词。为此我们使用了双向 LSTM 外加 Bahdanau（<a href="https://arxiv.org/pdf/1409.0473.pdf" title="Neural machine translation by jointly learning to align and translate" target="_blank" rel="noopener">Bahdanau, 2014</a>） attention 的处理；</li>
<li>这样就会获得两个表示 <script type="math/tex">(\overrightarrow{h_1}, \overrightarrow{h_2}, \dots, \overrightarrow{h_n})</script> 以及 <script type="math/tex">(\overleftarrow{h_1}, \overleftarrow{h_2}, \dots, \overleftarrow{h_n})</script>，然后将两个表示拼接起来组成 [<script type="math/tex">\overrightarrow{h_i};\overleftarrow{h_i}</script>]，正反向 LSTM 单元的大小都是 <script type="math/tex">\frac{d}{2}</script>。</li>
</ol>
</li>
<li>回答的不同侧面表示（图 2 中右侧下方部分）<ol>
<li>直接使用 KB 的嵌入矩阵 <script type="math/tex">E_k \in \mathbb{R}^{d \text{x} v_k}</script>，其中 <script type="math/tex">v_k</script> 代表知识库中资源的大小，该嵌入矩阵随机初始化并在训练时学习表示，使用全局信息对表示的进一步提高将在 3.3 节 Combining Global Knowledge（原论文）描述。具体来说我们使用回答的四个方面：问答实体 <script type="math/tex">a_e</script>，回答关系 <script type="math/tex">a_r</script>，回答类型 <script type="math/tex">a_t</script>，回答上下文 <script type="math/tex">a_c</script>。它们的嵌入被分别表示为 <script type="math/tex">e_e</script>, <script type="math/tex">e_r</script>, <script type="math/tex">e_t</script>, <script type="math/tex">e_c</script>；</li>
<li>值得注意的是问答上下文由多个 KB 资源组成，我们将它们定义为 (<script type="math/tex">c_1, c_2, \dots, c_m</script>)，首先获得它们的嵌入 (<script type="math/tex">e_{c_1}, e_{c_2}, \dots, e_{c_m}</script>)，然后计算它们的平均值 <script type="math/tex">e_c = \frac{1}{m} \sum^m_{i=1} e_{c_i}</script></li>
</ol>
</li>
<li>Cross-Attention model（图 2 中右侧上方部分以及最上方部分），详见 3.2.3 Cross-Attention model</li>
</ul>
<h2 id="Combining-Global-Knowledge"><a href="#Combining-Global-Knowledge" class="headerlink" title="Combining Global Knowledge"></a>Combining Global Knowledge</h2><p>&emsp;&emsp;Combining Global Knowledg，利用TransE得到knowledge embedding。</p>
<h1 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h1><ol>
<li>使用了 Bahdanau Attention 处理；</li>
<li>使用了双向 LSTM，会得到两个向量，最后将这两个向量拼接在一起，就是 BiLSTM 这层的最终向量。另外正反的 LSTM 的长度都是 <script type="math/tex">\frac{d}{2}</script>；</li>
<li>回答通过问答实体 <script type="math/tex">a_e</script>，回答关系 <script type="math/tex">a_r</script>，回答类型 <script type="math/tex">a_t</script>，回答上下文 <script type="math/tex">a_c</script> 四个方面来表示，其中 ac 是所有词向量的平均值。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li><a href="https://arxiv.org/pdf/1404.4326" title="Open question answering with weakly supervised embedding models" target="_blank" rel="noopener">Antoine Bordes 等 2014b</a>；</li>
<li><a href="https://arxiv.org/pdf/1406.3676" title="Question Answering with Subgraph Embeddings" target="_blank" rel="noopener">Antoine Bordes 等 2014a</a>；</li>
<li><a href="https://www.aclweb.org/anthology/P14-2105" title="Semantic Parsing for Single-Relation Question Answering" target="_blank" rel="noopener">Yih W 等 2014</a>，实际上是基于语义解析的，但是用了词向量；</li>
<li><a href="https://www.aclweb.org/anthology/D14-1071" title="Joint relational embeddings for knowledge-based question answering" target="_blank" rel="noopener">Min-Chul Yang 等 2014</a>，实际上是基于语义解析的但是用了词向量；</li>
<li><a href="https://www.aclweb.org/anthology/P15-1026" title="Question Answering over Freebase with Multi-Column Convolutional Neural Networks" target="_blank" rel="noopener">Dong 等 2015</a>，这篇是跟我们的文章最相近的（使用了 CNN 而非 RNN + Attention）；</li>
<li><a href="https://www.aclweb.org/anthology/C16-1226" title="Hybrid Question Answering over Knowledge Base and Free Text" target="_blank" rel="noopener">Kun Xu 等 2016b</a>；<a href="https://arxiv.org/pdf/1603.00957.pdf" title="Question Answering on Freebase via Relation Extraction and Textual Evidence" target="_blank" rel="noopener">Xu K 等 2016a</a>。</li>
</ol>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>向量建模KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Question Answering over Freebase with Multi-Column Convolutional Neural Networks</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/23%E3%80%81Question%20Answering%20over%20Freebase%20with%20Multi-Column%20Convolutional%20Neural%20Networks.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P15-1026" target="_blank" rel="noopener">论文地址</a>，发表于 2015 年。<br>&emsp;&emsp;大多数现有的系统通常依靠人工制作的特性和规则来进行<em>问题理解</em>以及<em>答案排序</em>。此外，一些方法（<a href="https://arxiv.org/pdf/1406.3676.pdf" title="Question Answering with Subgraph Embeddings" target="_blank" rel="noopener">Bordes et al., 2014a</a>; <a href="https://arxiv.org/pdf/1404.4326.pdf" title="Open Question Answering with Weakly Supervised Embedding Models" target="_blank" rel="noopener">Bordeset al., 2014b</a>）使用问题的词嵌入的总和来表示问题，但是这忽略了<strong>词序信息</strong>，无法处理复杂问题，例如 who killed A 和 who A killed 两个问题的表示是一样的。本文介绍了 multi-column convolutional neural networks (MCCNNs)，从三个方面（<strong>回答路径（Answer Type），回答上下文（Answer Context），回答类型（Answer Path）</strong>）理解问题。使用 Freebase 作为知识库，在 WebQuestions 数据集上进行了广泛的实验。最终表明，此方法拥有更好的性能。<br>&emsp;&emsp;神经网络训练步骤：</p>
<ol>
<li>MCCNNs 从输入的问题中使用不同 column networks 去提取<strong>回答路径，回答上下文，回答类型</strong>。跟 Bordes 的论文一样，该论文知识库（本文就是 FreeBase）中的实体和关系也由向量表示。</li>
<li>然后评分层（score layer）根据问题和候选答案的表示进行排序（点积）。</li>
</ol>
<h1 id="处理步骤"><a href="#处理步骤" class="headerlink" title="处理步骤"></a>处理步骤</h1><p>&emsp;&emsp;给定一个自然语言问题 <script type="math/tex">q = w_1 \dots w_n</script>，从 FreeBase 中检索相应的实体和属性，然后将它们作为候选答案 <script type="math/tex">C_q</script>。比如，问题 <em>when did Avatar release in UK</em> （阿凡达在英国的发行时间）的答案是 <em>2009-12-17</em>。需要注意的是对于该问题也许有一系列的正确答案。以下数据将被使用到：<strong>WebQuestions</strong>，<strong>FreeBase</strong>，<strong>WikiAnswers</strong>。<br>&emsp;&emsp;MCCNN 概览如图 1 所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Question Answering over Freebase with Multi-Column Convolutional Neural Networks/MCCNN概览.jpg" alt="MCCNN概览"><br><a id="more"></a></p>
<p>&emsp;&emsp;比如说，对于问题 whendid Avatar release in UK，从 FreeBase 中查询 <strong>Avatar</strong>（可以称为 <strong>main entity</strong> 或者 <strong>topic entity</strong>） 的<strong>相连节点</strong>（related nodes），这些相连节点被认为是候选答案（<script type="math/tex">C_q</script>）。然后对于每个候选答案 a，模型将会预测一个分数 S(q,a) 以判断 a 是否为正确答案。<br>&emsp;&emsp;对于问题的三个侧面的向量表示分别以 <script type="math/tex">f_1(q)</script> <script type="math/tex">f_2(q)</script> <script type="math/tex">f_3(q)</script> 表示，同理答案的三个侧面分别以 <script type="math/tex">g_1(a)</script> <script type="math/tex">g_2(a)</script> <script type="math/tex">g_3(a)</script> 表示。<script type="math/tex">f_i(q)</script> 和 <script type="math/tex">g_i(a)</script>拥有相同的维度。使用这些问答的表示，我们可以计算问答对 (q,a) 的分数。具体来说，评分函数 S(q,a) 定义为（如图 1 所示，评分层计算分数并将其加起来）：</p>
<script type="math/tex; mode=display">
S(q,a) = \underbrace{f_1(q)^Tg_1(a)}_{\text{answer path}} + \underbrace{f_2(q)^Tg_2(a)}_{\text{answer context}} + \underbrace{f_3(q)^Tg_3(a)}_{\text{answer type}}</script><h2 id="候选者生成"><a href="#候选者生成" class="headerlink" title="候选者生成"></a>候选者生成</h2><p>&emsp;&emsp;训练神经网络的<strong>第一步是</strong>从 FreeBase 中为问题检索候选答案。用户提出的问题应该包含一个<strong>可识别</strong>的实体，该实体与知识库相连。我们使用 <strong>Freebase Search API</strong>（<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf" title="Freebase: a collaboratively created graph database for structuringhuman knowledge" target="_blank" rel="noopener">Bollacker et al., 2008)</a>） 查询问题中的命名体。如果没有任何命名体，则查询名词短语，我们使用调用 API 返回的列表中的第一个实体。这个实体解决办法也被 <a href="https://www.aclweb.org/anthology/P14-1090" title="Information Extraction over Structured Data: Question Answering with Freebase" target="_blank" rel="noopener">Yao and Van Durme, 2014)</a> 使用，还可以研发更好的办法，但不是本论文的关注点。<strong>最后关联实体的所有 2-hops（应该是周围的意思，我没有查到是什么意思，但是在<a href="https://yan624.github.io/·论文笔记/dialogue/QA/KBQA/20、Open Question Answering with Weakly supervised Embedding Models.html#论文总结">博客笔记</a>中有所总结） 节点被认为是候选答案</strong>。并把问题 q 的候选答案集合称为 <script type="math/tex">C_q</script>。</p>
<h2 id="MCCNNs-for-Question-Understanding"><a href="#MCCNNs-for-Question-Understanding" class="headerlink" title="MCCNNs for Question Understanding"></a>MCCNNs for Question Understanding</h2><p>&emsp;&emsp;MCCNNs 使用多列（<strong>列</strong>指的是图 1 中左侧那三片）卷积网络从字嵌入中学习不同方面。使用 <a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf" title="Natural Language Processing (Almost) from Scratch" target="_blank" rel="noopener">Collobert R 等 2011</a> 的方法解决语言长度不一的问题。具体的做法可参考原论文 <strong>4.2 MCCNNs for Question Understanding</strong>。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&emsp;&emsp;未来的探索方向：</p>
<ol>
<li>整合更多的外部知识源，如clueweb；</li>
<li>以多任务学习方式训练MCCNN；</li>
<li>由于我们的模型能够检测到问题中最重要的单词，因此使用结果挖掘有效的问题模式将是非常有趣的。</li>
</ol>
<!-- more -->]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>向量建模KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Joint Relational Embeddings for Knowledge-based Question Answering</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/22%E3%80%81Joint%20Relational%20Embeddings%20for%20Knowledge%20based%20Question%20Answering.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/D14-1071" target="_blank" rel="noopener">论文地址</a>，发表于 2014 年。<br>&emsp;&emsp;将自然语言（natural language，NL）问题转换为对应的逻辑形式（logical form，LF）是基于知识库问答（KB-QA）任务的核心任务，转换问题也被称作语义分析。<del>在 KB-QA 任务领域，与以往（Mooney, 2007; Liang et al., 2011;Cai and Yates, 2013; Fader et al., 2013; Berant etal., 2013; Bao et al., 2014）<u>基于词汇化短语（lexicalized phrases）和逻辑谓语（logical predicates）之间的映射作为词汇触发器（lexical trigger）来执行语义分析中的转换任务</u>不同（其中 Fader 2013 提出的论文在<a href="https://yan624.github.io/·论文笔记/dilogue/QA/KBQA/20、Open Question Answering with Weakly supervised Embedding Models.html">论文笔记1</a>和<a href="https://yan624.github.io/·论文笔记/dialogue/QA/KBQA/21、Question Answering with Subgraph Embeddings.html">论文笔记2</a>中具有提及，ctrl f 之后搜索 <em>Paraphrase-Driven Learning for Open Question Answering</em> 或者 <em>Fader</em> 即可找到对应位置）</del>，本论文进一步提出了一种<strong>将 NL 问题映射到 LFs 中</strong>的新的<strong>嵌入式</strong>方法，其利用<strong>词汇表达</strong>与 <strong>KB 中的属性</strong>在隐含空间中的语义关联来实现。实验表明，在两个公开的 QA 数据集上，该方法优于其他三种 KB-QA 的基线方法。<br>&emsp;&emsp;先前工作必须处理以下两种限制：</p>
<ol>
<li>由于逻辑谓语的含义通常具有不同的自然语言表达（natural language expression，NLE）形式，因此从谓语提取的词汇触发器可能有时会受到大小限制；</li>
<li>由于命名体识别（named entity recognition，NER）组件检测到的实体将用于与逻辑谓语一起组成逻辑形式，因此它们的类型也应该与谓语一致。然而，现有的 KB-QA 系统使用的 NER 组件大都独立于 NLE 到谓语的映射步骤。<a id="more"></a>
</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;一如既往地（我为什么要说一如既往？因为前两篇论文笔记都记录了）说明<strong>语义分析</strong>有多糟糕，需要使用大量的人力，继而只能被限制在特定的领域（以后关于这些劣势都不写了）。<br>&emsp;&emsp;一如既往地描述了 FreeBase。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Question Answering with Subgraph Embeddings</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/21%E3%80%81Question%20Answering%20with%20Subgraph%20Embeddings.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1406.3676" target="_blank" rel="noopener">论文地址</a>，发表于 2014 年。<br>&emsp;&emsp;本文的作者在同年发表了另一篇论文，将上一篇论文称为 A，此论文称为 B，对于 A 论文我也做了<a href="https://yan624.github.io/·论文笔记/dilogue/QA/KBQA/20、Open Question Answering with Weakly supervised Embedding Models.html">论文笔记</a>，本论文是上一篇论文的改进版。A 只是对简单问题进行研究，B 研究如何改进模型并回答更复杂的问题。<br>&emsp;&emsp;开放域问答中的一流技术大致可以分为两大类：1)基于信息检索；2)基于语义解析。<strong>信息检索</strong>系统首先通过 KBs 的搜索 API（转换方式估计是手写模版，论文中未细说） <strong>将问题转换为有效的查询语句</strong>（比如 neo4j 数据库的 CQL）以此检索到大量的候选答案，然后再仔细地识别准确的答案（<a href="https://www.sciencedirect.com/science/article/pii/S0020025511003860" title="A survey on question answering technology from an information retrieval perspective" target="_blank" rel="noopener">Kolomiyets O 等 2011</a>，<a href="https://www2012.universite-lyon.fr/proceedings/proceedings/p639.pdf" title="Template-based Question Answering over RDF Data" target="_blank" rel="noopener">Unger C 等 2012</a>，<a href="https://www.aclweb.org/anthology/P14-1090" title="Information Extraction over Structured Data: Question Answering with Freebase" target="_blank" rel="noopener">Yao X 等 2014</a>）。<strong>语义解析</strong>旨在通过语义分析系统正确<strong>解释</strong>问题的含义，<strong>解释步骤</strong>的做法是把问题转换为数据库查询语句（这里的查询语句应该是逻辑形式，比如<strong>组合范畴法</strong>），以此查询到正确的答案。尽管这两种方法有能力去处理大规模知识库，但是需要专家手动的创建词汇、语法以及 KB 协议才能有所成效。且<strong>没有通用性</strong>，<strong>无法方便地扩展到</strong>具有其他模式、更广泛词汇或英语以外语言的<strong>新数据库</strong>。<br>&emsp;&emsp;相反，<a href="https://www.aclweb.org/anthology/P13-1158" target="_blank" rel="noopener">Paraphrase-Driven Learning for Open Question Answering</a> 提出了一个几乎不需要人工注释的开放域 QA 框架，虽然这是一种有趣的方法，但是它被其他方法超越了。即第二段提到的论文 A。<br>&emsp;&emsp;相比于论文 A，作者作出了以下几点<strong>改进</strong>：1）对于候选答案，考虑更多更长的路径（之前只考虑了 main entity 周围的节点）；2）对候选答案进行更有意义的表示：答案的表示包含问答路径以及周围的子图。</p>
<h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>&emsp;&emsp;假设所有潜在的答案都是 KB 中的实体，当 KB 中不存在该实体时，可以使用一些方法解决（论文中具体没说，只是说了一种极简单的方式：<em>When this entity is not given, plain string matching is used to perform entity resolution</em>）。<br><a id="more"></a><br>&emsp;&emsp;此外 N 代表词典的大小，其中 <script type="math/tex">N = N_W + N_S</script>，<script type="math/tex">N_W</script> 代表词嵌入的大小，<script type="math/tex">N_S</script> 代表实体和关系的数量。</p>
<h1 id="改进：考虑多维度的信息"><a href="#改进：考虑多维度的信息" class="headerlink" title="改进：考虑多维度的信息"></a>改进：考虑多维度的信息</h1><p>&emsp;&emsp;以下描述一个候选答案的特征表示，论文将以三个角度进行表示：</p>
<ol>
<li>Single Entity：此表示方式与上一篇论文一样，没什么讲究。就是 Freebase 中的一个实体，<script type="math/tex">\psi(a)</script> 代表答案的 1-of-<script type="math/tex">N_S</script>（one hot）表示；</li>
<li>Path Representation：答案被认为是一条 path，该 path 从<strong>问题中被提及的实体</strong>到<strong>答案实体</strong>。此实验中，考虑 1-hop 或者 2-hops 级别的 path。比如，(barack obama, people.person.place of birth, honolulu) 是 1-hop path，(barack obama, people.person.place of birth, location. location.containedby, hawaii) 是 2-hop path。这导致了 <script type="math/tex">\psi(a)</script> 代表 3-of-<script type="math/tex">N_S</script> 或者 4-of-<script type="math/tex">N_S</script> 的向量，至于为什么是 *-of-<script type="math/tex">N_S</script>，显而易见。</li>
<li>Subgraph Representation：我们将 2 中的 <strong>Path</strong> 和连接候选答案的整个<strong>子图</strong>进行编码。<em>具体看论文，写的有点看不懂</em>。</li>
</ol>
<p>&emsp;&emsp;我们的假想是将所有的信息都编码进表示以提高结果，但是这不大可能。所以还是采用将子图编码进表示的方法。下图即为实验的模型，右下角显示了编码方式。<br><img src="https://img-blog.csdn.net/20171101002818501?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTEFXXzEzMDYyNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="架构图" title="架构图"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;与论文 A 差不多，多了一个多任务训练，其他的细枝末节没仔细看。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>向量建模KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Open Question Answering with Weakly supervised Embedding Models</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/20%E3%80%81Open%20Question%20Answering%20with%20Weakly%20supervised%20Embedding%20Models.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1404.4326" target="_blank" rel="noopener">论文地址</a>，论文发表于 2014 年。<br>&emsp;&emsp;建立一个能够回答任何问题的计算机是人工智能的一个长期目标。这一领域一个重要的发展时大规模知识库的建立，如 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Freebase</a> 和 <a href="https://content.iospress.com/download/semantic-web/sw134?id=semantic-web%2Fsw134" target="_blank" rel="noopener">DBPedia</a>，它们存储了大量的通用信息。它们由三元组的形式构成一个数据库，通过各种关系和格式连接成实体对。那么回答问题被定义为<strong>给定一个用自然语言表达的查询语句</strong>（一个查询语句的例子：中国的首都在哪？）<strong>从知识库中检索正确的实体或实体集的任务</strong>。<br>&emsp;&emsp;最近，通过将问题映射为<strong>逻辑形式</strong>或者类似<strong>数据库查询</strong>的方法取得了富有希望的进展。虽然这种方法可能有效，但是缺点是要采用大量的人为标记的数据或者需要工作人员定义词汇表和语法。<br>&emsp;&emsp;本文采用一种激进的学习方式，将问题映射为向量（无法人为解释）的特征表示。并且将重点放在回答一些基于比较宽泛的主题的简单事实性问题。这项任务的难点来自词汇的多样性，而不是句法的复杂性。<br>&emsp;&emsp;该方法采用随机梯度下降，然后使用 fine-tuning 进行训练。经验表明该模型能够捕获一些有意义的信号，且这是唯一一种能够在弱标记数据上训练的方法。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><ol>
<li>Section 2：讨论了之前的工作；</li>
<li>Section 3：介绍了开放域问答的问题；</li>
<li>Section 4：给出了模型；</li>
<li>Section 5：实验结果。<a id="more"></a>
</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li>大规模的问答历史悠久，主要由 TREC tracks（<a href="https://arxiv.org/pdf/cs/0110053.pdf" target="_blank" rel="noopener">Voorhees 2000</a>） 发起，这是第一个成功地<strong>将问题转换为查询</strong>的问答系统。将问题转换为查询之后，又<strong>将查询提供给 web 搜索引擎</strong>，然后<strong>从返回的页面或片段中取出答案</strong>（<a href="http://aiweb.cs.washington.edu/research/projects/ai3/mulder/mulder-www10.pdf" target="_blank" rel="noopener">Kwok 2001</a>, <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-06/SS02-06-002.pdf" target="_blank" rel="noopener">Banko 2002</a>）。这种方法需要大量的人工操作来处理查询，然后解析和搜索结果。</li>
<li>大型 KBs 的出现，如 FreeBase 和 DBPedia（论文地址已在第一章给出），改变了上述状况，但是也带来巨大的挑战。语言的多样性以及 KBs 规模的庞大，使得需要通过监督学习来处理大量的<strong>带标签</strong>的数据。最早的方法是基于手写模板的 KBs 开放问答，然而对于日新月异 KBs（增加/删除三元组和实体） 还不够成熟。之后开始尝试使用较少的监督情况下<strong>学习 KBs 和自然语言之间的联系</strong>，但是这项工作实际上在解决<strong>信息提取</strong>的问题（<a href="https://www.aclweb.org/anthology/P09-1113" title="Distant supervision for relation extraction without labeled data" target="_blank" rel="noopener">Mintz M 等 2009</a>，<a href="https://www.aclweb.org/anthology/P11-1055" title="Knowledge-Based Weak Supervision for Information Extractionof Overlapping Relation" target="_blank" rel="noopener">Hoffmann R 等 2011</a>，<a href="https://www.aclweb.org/anthology/D12-1093" title="Reading The Web with Learned Syntactic-Semantic Inference Rules" target="_blank" rel="noopener">Lao N 等 2012</a>，<a href="https://www.aclweb.org/anthology/N13-1008" title="Relation Extraction with Matrix Factorization and Universal Schemas" target="_blank" rel="noopener">Riedel S 等 2013</a>）。以上以及本文未提及到的这些通过直接或者间接的监督机器学习来获得更多表现力的解决办法实际上是为了避开标签数据过多的问题。</li>
<li>近年来，有一种基于语义解析器（<a href="https://www.aclweb.org/anthology/P13-1042" title="Large-scale Semantic Parsing via Schema Matching and Lexicon Extension" target="_blank" rel="noopener">Cai Q 等 2013</a>，<a href="https://www.aclweb.org/anthology/D13-1160" title="Semantic Parsing on Freebase from Question-Answer Pairs" target="_blank" rel="noopener">Berant J 等 2013</a>，<a href="https://www.aclweb.org/anthology/D13-1161" title="Scaling Semantic Parsers with On-the-fly Ontology Matching" target="_blank" rel="noopener">Kwiatkowski T 等 2013</a>）的新的问答系统被提出，它只具有少量标记数据。但仍需要耗费大量精力去仔细设计词汇，语法和知识库。</li>
<li>所以本文（2014 年）提出了基于嵌入式的问答模型。据我们所知，这是以前从未尝试过的。</li>
</ol>
<h1 id="开放域问答"><a href="#开放域问答" class="headerlink" title="开放域问答"></a>开放域问答</h1><p>&emsp;&emsp;本文使用 <a href="https://www.aclweb.org/anthology/P13-1158" target="_blank" rel="noopener">Fader 2013</a> 的问答框架，并使用了相同的数据。</p>
<h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>&emsp;&emsp;我们将回答问题的任务看作为：给定一个问题 q，对应的答案由 KB 中的三元组 t 给出。这意味着我们的问题由<strong>一组三元组 t</strong> 提供对问题及其答案的解释，例如：</p>
<blockquote>
<p>q: What environment does a dodo live in?（渡渡鸟生活在什么样的环境中？）<br>t: (dodo.e, live-in.r, makassar.e)<br>q: What are the symbols for Hannukah?（光明节的象征是什么？）<br>t: (menorah.e, be-for.r, hannukah.e)<br>q: What is a laser used for?（极光可以用来做什么？）<br>t: (hologram.e,be-produce-with.r,laser.e)</p>
</blockquote>
<p>&emsp;&emsp;这里每个问题我们只给出一个 t，但是实际上它可以有很多，所以上文说是一组三元组。本文其余部分，<strong>使用 <script type="math/tex">\kappa</script>（读作 kappa） 代表 KB ，使用 <script type="math/tex">\epsilon</script> 代表 KB 中的实体或者关系。问题的词表用 V 表示，<script type="math/tex">n_V</script> <script type="math/tex">n_{\epsilon}</script>分别表示 V 和 <script type="math/tex">\epsilon</script> 的大小</strong>。<br>&emsp;&emsp;我们的模型在于<strong>函数 S(·)</strong>，它可以为 question-answer triple pairs (q,t) 打分。因此，找到问题 q 的 top-ranked 的答案 <script type="math/tex">\hat{t}</script>(q) 直接由以下公式得出：</p>
<script type="math/tex; mode=display">
\hat{t}(q) = arg \max_{t' \in \kappa}S(q, t')</script><p>&emsp;&emsp;为了处理多个答案，我们将结果呈现为排完序的列表并对其评分，而不是直接采用最前面的预测结果。<br>&emsp;&emsp;使用评分函数可以直接查询 KB，而不需要在<strong>语义分析系统</strong>中一样为问题定义一个中间的结构化逻辑表示。我们的目标是学习 S(·)，余下将讲述用于训练的数据的创建步骤。</p>
<h2 id="用于训练的数据"><a href="#用于训练的数据" class="headerlink" title="用于训练的数据"></a>用于训练的数据</h2><div class="note info">
            <p>待续</p>
          </div>
<h1 id="Embedding-based-model"><a href="#Embedding-based-model" class="headerlink" title="Embedding-based model"></a>Embedding-based model</h1><p>&emsp;&emsp;模型使用了词嵌入（2019 年了，应该谁都知道了，不做解释）。</p>
<h2 id="Question-KB-Triple-Scoring"><a href="#Question-KB-Triple-Scoring" class="headerlink" title="Question-KB Triple Scoring"></a>Question-KB Triple Scoring</h2><p>&emsp;&emsp;我们的框架关注的是函数 S(q,t) 的学习，该函数的目的是对一个<strong>问题 q</strong> 和 一个<strong>来自 <script type="math/tex">\kappa</script> 的三元组 t</strong> 进行打分。该评分方法受到了先前工作 labeling images withwords 的启发（<a href="https://link.springer.com/content/pdf/10.1007/s10994-010-5198-3.pdf" target="_blank" rel="noopener">Weston 2013</a>），我们采用该方法将图片和标签替换成了问题和三元组。直观来讲就是：<br>有点难翻译，故给出原文：</p>
<blockquote>
<p>&emsp;&emsp;Intuitively, it consists of projecting questions, treated as a bag of words(and possibly n-grams as well), on the one hand, and triples on the other hand,into a shared embedding space and then computing a similarity measure (the dot  product  in  this  paper)  between  both  projections.<br>&emsp;&emsp;大致意思，将问题和三元组使用词袋模型（也可以是 n-gram 模型）投射到共享的嵌入空间，然后计算二者的相似度（本文使用点积的方式）。</p>
</blockquote>
<p>&emsp;&emsp;那么评分函数为:</p>
<script type="math/tex; mode=display">
S(q,t) = f(q)^Tg(t)</script><p>&emsp;&emsp;<strong>其中 f(·) 将问题中的单词映射到 <script type="math/tex">\mathbb{R}^{\kappa}</script>，<script type="math/tex">f(q) = V^T \Theta(q)</script>。V 是关于 <script type="math/tex">\mathbb{R}^{n_v \times \kappa}</script> 包含所有词嵌入 v 的矩阵。<script type="math/tex">\Theta(q)</script>是 q（<script type="math/tex">\in \{0,1\}^{n_v}</script>） 的二进制（稀疏）表示。同样，g(·) 将 KB 三元组中的实体和关系映射到 <script type="math/tex">\mathbb{R}^{\kappa}</script>，<script type="math/tex">g(t) = W^T\Psi(t)</script>，W 是关于 <script type="math/tex">\mathbb{R}^{n_e \times \kappa}</script> 包含所有实体和关系的嵌入 w 的矩阵，<script type="math/tex">\Psi(t)</script> 是 t（<script type="math/tex">\in \{0,1\}^{n_e}</script>） 的二进制（稀疏）表示。</strong><br><div class="note info">
            <p>&emsp;&emsp;注：上一段太长了，解释一下。f(q) 就是词向量，g(t) 就是实体和关系的向量（下一段原文写到 g(t) 是将三元组中的嵌入全部相加）。</p>
          </div></p>
<p>&emsp;&emsp;将单词表示为词袋模型似乎有一点局限性，但是由于我们特定的设置，语法都很简单，因此含有的信息十分有限，所以词袋模型应该也能带来不错的性能。当然也有反例，比如 <em>What are cats afraid of ?vs.What are afraid of cats ?</em> 这将会有不同的答案。不过这种情况十分罕见。未来考虑将 parse tree features 或者 semantic role labels 作为输入放入嵌入模型中。<br>&emsp;&emsp;与以前的工作（<a href="https://arxiv.org/pdf/1307.7973" target="_blank" rel="noopener">Weston 2013</a>）不同的是，在我们的模型中，实体出现三元组的不同侧面（左右侧）时，实体并非拥有相同的嵌入。KB 中的关系并不是对称的，所以会出现三元组中左侧和右侧的实体是不同的情况。<strong>由于 g(·) 是将三元组中的所有成分相加，所以每一个实体我们都需要两个嵌入</strong>。<br>&emsp;&emsp;这样就可以很容易地对任何三元组进行评分：</p>
<script type="math/tex; mode=display">
\hat{t}(q) = arg \max_{t' \in \kappa}S(q, t') = arg \max_{t' \in \kappa}(f(q)^Tg(t'))</script><p>&emsp;&emsp;接下来花了好几段讲怎么训练。</p>
<h2 id="Fine-tuning-the-Similarity-between-Embeddings"><a href="#Fine-tuning-the-Similarity-between-Embeddings" class="headerlink" title="Fine-tuning the Similarity between Embeddings"></a>Fine-tuning the Similarity between Embeddings</h2><p>&emsp;&emsp;由于受到数据大小的限制，需要使用微调来改进性能。</p>
<h1 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h1><p>&emsp;&emsp;通读论文之后还是有点搞不清论文是怎么训练的，后来看了一下 CCF ADL100 刘康老师的 PPT ，感觉有点理解了，以下是训练步骤：</p>
<ol>
<li>输入自然语言表达的问题，比如：姚明的老婆的是哪里人？</li>
<li>使用 entity linking（论文中貌似没有这步，我在看 PPT 时也是一知半解，好在前几天我刚好在一篇论文中看到了这个 entity linking！<a href="https://yan624.github.io/·论文笔记/19、Semantic Parsing via Staged Query Graph Generation：Question Answering with Knowledge Base.html#链接主题实体">博客地址</a>，entity linking 源于<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）找到 main entity，main entity 周围的 entity 均是候选 entity。如下图，姚明是 main entity，姚明周围的实体都算作候选 entity，比如叶莉、火箭队、上海等。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Open Question Answering with Weakly supervised Embedding Models/姚明的老婆是谁的知识图谱的子图.jpg" alt="姚明的老婆是谁的知识图谱的子图"></li>
<li>计算问题和候选 entity 的相似度，其中问题由词向量表示，候选 entity 是一个三元组的形式，难以直接用词向量表示，方法是将三元组中的三个对象分别用词向量表示，然后将三个词向量相加。这样就得到了问题的词向量和 entity 的词向量，点乘获得相似度。</li>
<li>由于候选 entity 不一定只有一个，所以可以获得多个相似度。进行排序即可获得最相似的候选 entity。</li>
</ol>
<div class="note danger">
            <p>&emsp;&emsp;以上的训练步骤并不是论文中的训练步骤，只是我为了给自己加深映像写的，具体的训练步骤在原论文第 4 节，具体在 4.1。</p>
          </div>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>向量建模KBQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base</title>
    <url>/%C2%B7%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/dilogue/QA/KBQA/19%E3%80%81Semantic%20Parsing%20via%20Staged%20Query%20Graph%20Generation%EF%BC%9AQuestion%20Answering%20with%20Knowledge%20Base.html</url>
    <content><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>&emsp;&emsp;此论文为 2015 年的论文。<br>&emsp;&emsp;本文会出现一个名为<strong>谓语序列（predicate sequence）</strong>的名词，论文中没有详细说明。但是估计就是：一个实体至另一个实体的有向路径上的所有谓语的连接形式。如下文第一张图 Family Guy-&gt;cvt1-&gt;Mila Kunis 的谓语序列就是 cast-actor。</p>
<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;节选自摘要部分：</p>
<blockquote>
<p>&emsp;&emsp;论文提出了一个基于知识库问答的新的语义解析（semantic parsing）框架。首先定义一个类似于知识库的<strong>子图（subgraph）</strong>的查询图（query graph），可以直接映射到一个语义的逻辑形式（如<script type="math/tex">\lambda</script>-calculus）。所以<strong>语义分析简化为查询图的生成</strong>，并将其表示为一个阶段性搜索问题。然后通过使用先进的实体链接系统（<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）以及深度卷积网络来实现问题与谓语序列之间的匹配。在 WEBQUESTIONS 的数据集上，F1 指标达到了 52.5% 的水平，高于以前的方法。</p>
</blockquote>
<p>&emsp;&emsp;以下大型知识库已经成为支持开放领域问答的重要资源：</p>
<ul>
<li>DBPedia</li>
<li>Freebase</li>
</ul>
<p>&emsp;&emsp;最先进的 KB-QA 方法都是基于<strong>语义解析</strong>的，在语义解析中一个问题或者一种表达被映射到它具有一定意义的表示上（如逻辑形式，具体来说可以是 <script type="math/tex">\lambda</script>-calculus），即将自然语言映射为表达式，然后被翻译为一个 <strong>KB 查询</strong>。最后，只需要执行查询就可以检索问题的答案。<strong>但是大多数<u>传统的</u>语义解析方法在很大程度上都<u>脱离</u>知识库</strong>。由于没有前人的贡献累积，因此 QA 问题面临着一系列的挑战。例如：</p>
<ul>
<li>当在逻辑形式中使用与知识库中的谓语不同的谓语时，可能需要用到本体匹配（ontology matching）的问题（Kwiatkowski et al., 2013）。</li>
<li>即使表示语言与知识库的模式接近，从知识库中的大量词汇表中寻找正确的谓语与语句的描述相关联仍然是一个难题（Berant and Liang, 2014）。<a id="more"></a>
</li>
</ul>
<p>&emsp;&emsp;由（Yao and Van Durme, 2014; Bao etal., 2014）的启发，该论文提出了一个语义解析框架，定义一个查询图可以直接地映射到由 <script type="math/tex">\lambda</script>-calculus 表达的逻辑形式。从语义上来讲，与 <script type="math/tex">\lambda</script>-DCS（Liang, 2013）十分接近。将解析行为分为 3 步：</p>
<ol>
<li>定位问题中的主题实体；</li>
<li>找到回答与主题实体之间的主要关联；</li>
<li>（通过额外的约束扩大查询图，约束即回答需要附加的额外属性，如最早时间等）或者（答案与其他实体之间的关联）。</li>
</ol>
<p>&emsp;&emsp;至此将一个语义解析问题划分成了一系列的子问题。例如 entity linking 和 relation matching。</p>
<h2 id="文章内容介绍"><a href="#文章内容介绍" class="headerlink" title="文章内容介绍"></a>文章内容介绍</h2><ol>
<li>Sec. 2: 介绍了图知识库（估计就是知识图谱）的概念和查询图的设计；</li>
<li>Sec. 3: 介绍了基于搜索方法的查询图生成；</li>
<li>Sec. 4: 实验结果；</li>
<li>Sec. 5: 论文中的方法和其他相关工作的比较；</li>
<li>Sec. 6: 总结。</li>
</ol>
<h1 id="Knowledge-Base"><a href="#Knowledge-Base" class="headerlink" title="Knowledge Base"></a>Knowledge Base</h1><p>&emsp;&emsp;论文中的知识库 K 是一个包含主语、谓语、宾语的三元组（e1, p, e2）的集合，其中 e1 和 e2<script type="math/tex">\in</script>E，是一个实体。p<script type="math/tex">\in</script>P，是一个谓语。这种形式的知识库通常称为知识图谱。每一个实体是一个节点，两个相关联的实体由谓语标记的有向边连接，边的方向是从主语实体到宾语实体。如下图就是一个 Freebase 的子图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Freebase subgraph of Family Guy.jpg" alt="Freebase subgraph of Family Guy"></p>
<div class="note info">
            <p>&emsp;&emsp;Freebase 中有一个叫 <a href="https://developers.google.com/freebase/guide/basic_concepts#cvts" target="_blank" rel="noopener">CVT</a>（此链接需要翻墙访问） 的特殊实体类型，它不是一个真正的实体，而是用于收集事件或特殊的关联的多个字段。</p>
          </div>
<h1 id="Query-graph"><a href="#Query-graph" class="headerlink" title="Query graph"></a>Query graph</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>&emsp;&emsp;给定一个知识图谱。执行逻辑形式的查询等价于寻找一个子图，该子图的表现形式可以映射到查询动作。之后解析绑定的变量。<br><div class="note warning">
            <p>&emsp;&emsp;接下来，以实体这个属性来表示真实世界的实体和 CVT 实体以及日期或高度等属性，这些实体之间的区别对于论文中的方法来说并不重要。</p>
          </div><br>&emsp;&emsp;就像知识图谱一样，查询图中的相关节点也是通过有向边连接，并用 K 中的谓语标记。查询图由四中类型的节点组成：</p>
<ol>
<li>grounded entity：圆角矩形表示。grounded entity 是在知识库 K 中已存的实体。</li>
<li>existential variable：圆形表示。existential variable 是 un-grounded entity。</li>
<li>lambda variable：阴影圆形表示。lambda variable 是 un-grounded entity。尤其，该论文表示希望<strong>检索</strong>能够映射到 lambda variable 的所有实体<strong>作为</strong>最终答案。其也被称为<strong>answer 节点</strong>。</li>
<li>aggregation function：菱形表示。aggregation function 被用于操作特定的实体，该实体通常具有一些数值属性。</li>
</ol>
<p>&emsp;&emsp;下图展示了一个查询图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Query graph that represents the question “Who first voiced Meg on Family Guy？”.jpg" alt="Query graph that represents the question “Who first voiced Meg on Family Guy？”"></p>
<p>&emsp;&emsp;上图是“谁第一次为 Family Guy 中的 Meg 配音？”的问题。MegGriffin 和 FamilyGuy 由圆角矩形表示，圆圈节点 y 表示应该存在一个实体来描述扮演关系，比如角色、演员和开始饰演此角色的时间。阴影圆圈节点也被称为 <strong>answer 节点</strong>。菱形节点 argmin 限制答案必须是扮演此角色的最早的演员。同样不含聚合函数的<script type="math/tex">\lambda-calculus</script>逻辑形式查询为<script type="math/tex">\lambda x.\exists y.cast(FamilyGuy,y) \Lambda actor(y,x) \Lambda character(y,MegGriffin)</script>。在使用聚合函数之前，对 K 运行此查询图会匹配 LaceyChabert 以及 MilaKunis，请看第一张图。但是只有 LaceyChabert 是正确答案，因为是她最早开始扮演这个角色。<br><div class="note info">
            <p>查询图的设计灵感来源于（Reddyet al., 2014），但是他的查询图是从问题的 CCG 解析中映射出来的，在映射到子图前还需要进一步的转换。从语义上来说，该论文的查询图更像简单的 <script type="math/tex">\lambda-DCS</script>。</p>
          </div></p>
<h2 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h2><p>&emsp;&emsp;<strong>首先</strong>树图（tree graph）的根由一个实体节点组成，称为主题实体（topic entity）。<strong>其次</strong>，只有一个 lambda 变量 x 作为答案节点，从根到 x 有一个定向路径，其中含有 0 个或多个 existential variables。论文中将此路径称为图的核心推理链，因为它描述了答案和主题实体之间的主要关系。这个链除了根节点外只有变量节点。<strong>最后</strong>，可以将 0 个或多个实体或者聚合函数节点附加到每个变量节点，包括 answer 节点。例如，上图 Family Guy 是根，而 Family Guy-&gt;y-&gt;x 是核心推理链，分支 y-&gt;MegGriffin 阐述了角色，而 y-&gt;argmin 限制答案必须是该角色最早的参与者。<br>&emsp;&emsp;定义状态（state）集合<script type="math/tex">S = \{\phi, S_e, S_p, S_c\}</script>，其中每个状态可以是一个空的图（<script type="math/tex">\phi</script>），一个主题实体的单节点图（<script type="math/tex">S_e</script>），一个核心推理链（<script type="math/tex">S_p</script>）或者带有额外约束的更复杂的查询图（<script type="math/tex">S_c</script>）。<br>&emsp;&emsp;定义动作（action）集合<script type="math/tex">A = \{A_e, A_p, A_c, A_a\}</script>，其中<script type="math/tex">A_e</script>选取实体节点，<script type="math/tex">A_p</script>确定核心推理链，<script type="math/tex">A_c</script>和<script type="math/tex">A_a</script>分别约束和聚合节点。<br>&emsp;&emsp;给出一个示例<script type="math/tex">q_{ex}</script> = “Who first voiced Meg of Family Guy?”。</p>
<h3 id="链接主题实体"><a href="#链接主题实体" class="headerlink" title="链接主题实体"></a>链接主题实体</h3><p>&emsp;&emsp;从初始状态<script type="math/tex">S_0</script>开始，正确的操作是创建一个与给定问题中的主题实体相对应单节点图。例如，<script type="math/tex">q_{ex}</script>中可能的主题实体是 Family Guy 和 MegGriffin，如下图所示。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Two possible topic entity linking actionsapplied to an empty graph, for question “Who firstvoiced[Meg]on[Family Guy]？”.jpg" alt="Two possible topic entity linking actionsapplied to an empty graph, for question “Who firstvoiced[Meg]on[Family Guy]？”"></p>
<p>&emsp;&emsp;使用的<strong>实体链接系统</strong>是专为短且有噪声的文本设计的，源于（<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）。具体不做赘述，详情可参考相关论文。</p>
<h3 id="确定核心推理链"><a href="#确定核心推理链" class="headerlink" title="确定核心推理链"></a>确定核心推理链</h3><p>&emsp;&emsp;给定与主题实体 e 对应的单节点图的状态 s，扩展该图的正确操作是确定核心推理链，即主题实体和答案之间的关系。下图展示了扩展<script type="math/tex">s_1</script>中的单节点图的三个可能的链。具体做法是，当中间的 existential variable 链接 CVT 时，探索长度为 2 的所有路径，如果没有链接，则探索长度为 1 的路径。<br><div class="note primary">
            <p>&emsp;&emsp;本节主要描述了如何确定核心推理链，不过上文一段先描述了如何确定候选的核心推理链。具体做法上一段也已经给出，但是由于原论文讲的也有点不清楚，此处加以说明，以下只是推测。</p><ol><li>扩展主题节点 Family Guy 的三个可能的核心推理链，应该是从知识库 K 中入手。请看第一张图，它是知识库 K 中的一张子图。从 Family Guy 中开始可以看到有三条边，两条边上是 cast，一条边上是 writer。由于两条边相同，于是就融为了一条推理链。至于最后一条推理链的谓语是 genre，可能是第一张图的子图中没有标出造成的。总而言之，那三条推理链就是从知识库 K 中获取。</li><li>existential variable 即 y，lambda variable 即 x。可以把知识库 K 中的 CVT 节点看作是 y，答案看作是 x。</li></ol>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Candidate core inferential chains start from the entity FamilyGuy.jpg" alt="Candidate core inferential chains start from the entity FamilyGuy"></p>
<p>&emsp;&emsp;这样做的目的是将自然表达映射到正确的谓语序列上。对于问题“Who first voiced Meg on [Family Guy]?”，需要衡量的是在{cast-actor, writer-start, genre}中每个序列（<em>注：这个元组就是上图的三个候选核心推理链上的谓语</em>）正确捕捉 Family Guy 和 Who 之间关系的可能性。因此将这个问题简化为使用神经网络测量语义相似度。</p>
<h4 id="Deep-Convolutional-Neural-Networks"><a href="#Deep-Convolutional-Neural-Networks" class="headerlink" title="Deep Convolutional Neural Networks"></a>Deep Convolutional Neural Networks</h4><p>&emsp;&emsp;虽然是陈述一个相同的问题，但是以语义等价的方式来重新表达该问题仍旧拥有巨大的多样性。并且还存在自然语言表达与知识库中的谓语不匹配的情况。<strong>为了处理上述两个问题</strong>，论文建议使用 Siamese neural networks（<a href="http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf" target="_blank" rel="noopener">Bromley et al., 1993</a>）来识别核心推理链（暹（xiān）罗神经网络，也可以叫连体神经网络。看见这个中文就很好理解了。Siamese neural networks 可以进行语义相似度分析，QA 的匹配等操作。详情可以先看看<a href="https://www.jianshu.com/p/92d7f6eaacf5" target="_blank" rel="noopener">这篇</a>博客）。注：由于上图可以得知一个问题可以获得几个候选得到核心推理链，这就是因为语言的多样性造成的，所以需要一个方法来识别一条最核心的推理链。<br>&emsp;&emsp;例如，将一个问题映射到一种<strong>模式</strong>上，方法是将实体替换为通用符号 &lt;e&gt;，然后将其与<strong>候选链</strong>比较。比如问题“who first voiced meg on &lt;e&gt;”和 cast-actor。该模型由两个神经网络组成，一个处理<strong>模式</strong>，一个处理<strong>核心推理链</strong>（这个模型说白了就是 Siamese neural networks）。两个神经网络都映射到 k 维向量作为网络的输出，最后使用距离函数（如余弦相似度）计算语义相似度。<br><div class="note info">
            <p>&emsp;&emsp;该论文处理<strong>匹配问题</strong>使用了 CNN 模型。你可能会有点疑惑<strong>匹配问题</strong>是什么问题，前面压根就没提到过。是的，论文里也没说过，我只能猜测，这里的 CNN 其实就是上述模型的两个神经网络的具体实现。处理模型和处理核心推理链可能都用了 CNN 模型。另外论文中也没有说如何将核心推理链送入 CNN 中。论文中倒是稍微提了一下如何将问题送入 CNN 中，使用 word hashing 技术（<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf" target="_blank" rel="noopener">Huang et al., 2013</a>）。</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/CNN架构.jpg" alt="CNN架构"></p>
<h3 id="增加约束和聚合函数"><a href="#增加约束和聚合函数" class="headerlink" title="增加约束和聚合函数"></a>增加约束和聚合函数</h3><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>&emsp;&emsp;<strong>Topic Entity</strong>：由实体链接系统返回的分数直接作为特征。<br>&emsp;&emsp;<strong>Core Inferential Chain</strong>：使用不同的 CNN 模型的相似度分数来衡量核心推理链的质量，以下为 3 个模型。</p>
<ul>
<li><strong>PatChain</strong>：比较模式和谓语序列。</li>
<li><strong>QuesEP</strong>：将主题实体的名称与谓语序列拼接完成之后，将其与原问题比较。</li>
<li><strong>ClueWeb</strong>：使用 ClueWeb 语料库的 Freebase 注释训练 ClueWeb 模型</li>
</ul>
<p>&emsp;&emsp;<strong>Constraints &amp; Aggregations</strong>：当查询图中有约束节点，使用一些简单的特征来检查问题中是否存在单词可以与约束实体或者属性相关联。相似地，也可以使用一些预定义的关键字，比如“first”、“current”或者“latest”作为 argmin 节点的特征。<br>&emsp;&emsp;<strong>Overall</strong>：回答节点的个数和总节点个数也都作为特征。<br>&emsp;&emsp;比如下图，（1）属于 Topic Entity，（2）（3）（4）属于 Core Inferential Chain，（5）（6）（7）属于 Constraints &amp; Aggregations，（8）（9）属于 Overall：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/特征举例.jpg" alt="特征举例"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;使用 WEBQUESTIONS 数据集，评价指标有：precision，recall 和 F1。其中 F1 的平均值作为主要的评价指标。</p>
<h1 id="其他参考资料"><a href="#其他参考资料" class="headerlink" title="其他参考资料"></a>其他参考资料</h1><p>&emsp;&emsp;在浏览此篇论文时，发现还有其他人也看过这篇论文并且留下了笔记（中文）。<br>&emsp;&emsp;<a href="https://bigquant.com/community/t/topic/121147" target="_blank" rel="noopener">笔记1</a><br>&emsp;&emsp;<a href="https://blog.csdn.net/qq_32782771/article/details/82773048" target="_blank" rel="noopener">笔记2</a><br><!-- more --></p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>KBQA</tag>
        <tag>Query Graph</tag>
        <tag>semantic parsing</tag>
      </tags>
  </entry>
  <entry>
    <title>在hexo中添加绘制流程图及其他图的功能</title>
    <url>/assorted/hexo/%E5%9C%A8hexo%E4%B8%AD%E6%B7%BB%E5%8A%A0%E7%BB%98%E5%88%B6%E6%B5%81%E7%A8%8B%E5%9B%BE%E5%8F%8A%E5%85%B6%E4%BB%96%E5%9B%BE%E7%9A%84%E5%8A%9F%E8%83%BD.html</url>
    <content><![CDATA[<p>hexo 本身不支持绘制流程图，但是可以使用以下命令安装插件来实现此功能。<br><figure class="highlight processing"><table><tr><td class="code"><pre><span class="line">npm install --<span class="built_in">save</span> hexo-<span class="built_in">filter</span>-flowchart</span><br></pre></td></tr></table></figure></p>
<p><a href="https://github.com/bubkoo/hexo-filter-flowchart" target="_blank" rel="noopener">插件地址</a><br>语法可以<a href="https://cloud.tencent.com/developer/article/1142260" target="_blank" rel="noopener">在这</a>找<br>一个简单的例子<br>(`乘3)flow<br>st=&gt;start: Start|past:&gt;<a href="http://www.google.com[blank" target="_blank" rel="noopener">http://www.google.com[blank</a>]<br>e=&gt;end: End:&gt;<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br>op1=&gt;operation: My Operation|past<br>op2=&gt;operation: Stuff|current<br>sub1=&gt;subroutine: My Subroutine|invalid<br>cond=&gt;condition: Yes<br>or No?|approved:&gt;<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br>c2=&gt;condition: Good idea|rejected<br>io=&gt;inputoutput: catch something…|request</p>
<p>st-&gt;op1(right)-&gt;cond<br>cond(yes, right)-&gt;c2<br>cond(no)-&gt;sub1(left)-&gt;op1<br>c2(yes)-&gt;io-&gt;e<br>c2(no)-&gt;op2-&gt;e<br>(`乘3)<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（七）：K-NN</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9AK-NN.html</url>
    <content><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p>K-NN 算法采用测量不同特征值之间的距离的方法进行分类。<br>工作原理：<br>存在一个<strong>样本数据集</strong>，也称作训练样本集，并且样本集中每个数据都存在标签。输入<strong>没有标签的新数据</strong>后，将<strong>新数据</strong>的每个特征与<strong>样本集</strong>中的数据对应特征进行比较，然后算法提取样本集中特征最相似的数据（最邻近）的分类<strong>标签</strong>。一般来说，只选择样本数据集中前 k 个最相似的数据，这就是 k-NN 算法中 k 的出处，通常 k 是不大于 20 的整数。<br>最后选择在 k 个最相似的数据中出现次数最多的分类，作为新数据的分类。</p>
</blockquote>
<div id="flowchart-0" class="flow-chart"></div>

<p>简单来说，K-NN 算法使用了一种计算特征之间的距离的公式，然后选择距离前 k 近的数据，获取这些数据的标签。通过一个简单的统计，获取这 k 项数据中最多的类别。最后我们将新数据看作是这个类别。<br><a id="more"></a></p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    group = np.array([[<span class="number">1.0</span>, <span class="number">1.1</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.1</span>]])</span><br><span class="line">    labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(input, dataset, labels, k)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    K-NN 分类</span></span><br><span class="line"><span class="string">    :param input: 输入数据，即待分类的数据</span></span><br><span class="line"><span class="string">    :param dataset: 训练数据集</span></span><br><span class="line"><span class="string">    :param labels: dataset 对应的标签</span></span><br><span class="line"><span class="string">    :param k: 显而易见</span></span><br><span class="line"><span class="string">    :return: input 的类别</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算特征之间的距离，只是一个很简单的算法</span></span><br><span class="line">    <span class="comment"># 先算差，再平方，然后将一个项数据的所有特征累加，最后开方</span></span><br><span class="line">    all_distances = np.sqrt(np.sum(np.power((input - dataset), <span class="number">2</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 对距离进行逆序排序</span></span><br><span class="line">    sorted_distance_indices = all_distances.argsort()</span><br><span class="line">    <span class="comment"># 对类别进行计数</span></span><br><span class="line">    class_count = &#123;&#125;</span><br><span class="line">    <span class="comment"># 选取前 k 项数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        <span class="comment"># 第 i 项数据的标签</span></span><br><span class="line">        label = labels[sorted_distance_indices[i]]</span><br><span class="line">        <span class="comment"># 标签存在则加 1，不存在就默认是 0 再加 1</span></span><br><span class="line">        class_count[label] = class_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据标签的数量排序</span></span><br><span class="line">    sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_class_count[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    X, Y = create_dataset()</span><br><span class="line">    res = classify([<span class="number">0</span>, <span class="number">0</span>], X, Y, <span class="number">3</span>)</span><br><span class="line">    print(res)</span><br></pre></td></tr></table></figure>
<h2 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h2><p>由于有些数据范围波动较大，可以进行均值归一化处理。</p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>引用《机器学习实战》中的应用。</p>
<ol>
<li>可以分类电影的类别，已知数据：打斗镜头、接吻镜头、<strong>电影的类别</strong>。如果给定一部新电影，则可以根据该电影的打斗镜头、接吻镜头来计算此部电影属于哪种类别。</li>
<li>改进约会网站配对效果。已知数据：每年获得的飞行常客里程数、玩视频游戏所耗时间百分比、每周消费的冰淇淋公升数、<strong>用户交往对象的类别</strong>。其中<strong>用户交往对象的类别</strong>指：<ul>
<li>不喜欢的人</li>
<li>魅力一般的人</li>
<li>极具魅力的人<br>则可以输入一个新的约会对象的数据，从而判断此人属于哪种类别，如果属于不喜欢的人的类别，那么用户可以提前得知，并且决定不去约会。</li>
</ul>
</li>
<li>甚至可以识别手写数字。将图片转换成 0 1 表示，即数字部分用 1 表示，其他部分用 0 表示。组成一个 32 x 32 数字矩阵，然后将矩阵转换为 1 x 1024 的向量。其中的每一维度的值可以看作为一个特征。算法类似。</li>
</ol>
<!-- more --><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: 新数据
op1=>operation: 计算新数据特征
与样本集特征之间的距离
op2=>operation: 提取前 k 个
最相似的数据的标签
count=>inputoutput: 统计标签
e1=>end: 返回出现次数最多的分类
e2=>end: 程序无法继续执行
c1=>condition: k 小于等于
样本集个数

st(right)->c1
c1(yes, right)->op1(right)->op2->count(right)->e1
c1(no)->e2(left)->st</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
        <tag>knn</tag>
      </tags>
  </entry>
  <entry>
    <title>2019 CCF会议总结</title>
    <url>/assorted/conference/2019%20CCF%E4%BC%9A%E8%AE%AE%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<h1 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h1><h2 id="知识图谱问答系统概述"><a href="#知识图谱问答系统概述" class="headerlink" title="知识图谱问答系统概述"></a>知识图谱问答系统概述</h2><p>现在的<strong>搜索引擎</strong>工作流程是输入要搜索的内容，搜索引擎返回一大堆内容，供你自己选择。<br><strong>问答系统</strong>是下一代的搜索引擎的基本形态。</p>
<blockquote>
<p>以直接而准确的方式回答用户自然语言提问的自动问答系统将构成下一代搜索引擎的基本形态。</p>
</blockquote>
<p>下图展示问答系统在近几十年的发展历史。</p>
<ol>
<li>1960 年的问答系统属于专家系统（模版系统）</li>
<li>1990 - 2000 年的问答系统属于基于信息检索的 QA 系统</li>
<li>2000 - 2010 年的问答系统属于社区 QA 系统</li>
<li>2011 年之后的问答系统属于基于知识图谱的 QA 系统</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/问答系统的历史.jpg" alt="问答系统的历史"><br><a id="more"></a></p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>问答系统的分类（或者说三个阶段）：</p>
<ol>
<li>IR-based QA：基于<strong>关键词匹配 + 信息抽取</strong>，任然是基于<strong>浅层语义分析</strong></li>
<li>Community QA：依赖于网民贡献，问答过程任然依赖于<strong>关键词检索技术</strong></li>
<li>KB-based QA：Knowledge Base，例如：WolfframAlpha</li>
</ol>
<p>根据问答形式分类：</p>
<ol>
<li>一问一答：字面意思，也是演讲的主题</li>
<li>交互式问答：就是进行连续的复杂的问答</li>
<li>阅读理解</li>
</ol>
<div class="note warning">
            <p>KB-QA 现在只能解决事实性的问题，无法解决：</p><ol><li>怎么去天安门</li><li>西红柿炒鸡蛋怎么做等提问</li></ol><p>某公司（在会议上没听清，可能是一个公司）只有 5% 的问题能用 KB-QA 解决。</p>
          </div>
<h2 id="什么是知识图谱"><a href="#什么是知识图谱" class="headerlink" title="什么是知识图谱"></a>什么是知识图谱</h2><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/知识图谱示例.jpg" alt="知识图谱示例"></p>
<h3 id="知识图谱基本架构"><a href="#知识图谱基本架构" class="headerlink" title="知识图谱基本架构"></a>知识图谱基本架构</h3><p>图中三元组中的 Ent1、Ent2 等指的是 entity。entity 可以在架构中选取，比如将 concept 作为 entity 或者将 instance 作为 entity。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/知识图谱基本架构.jpg" alt="知识图谱基本架构"></p>
<h3 id="运用知识图谱问答"><a href="#运用知识图谱问答" class="headerlink" title="运用知识图谱问答"></a>运用知识图谱问答</h3><p>语义如何表示是其中的一个问题：</p>
<ol>
<li>使用符号表示的形式（传统方法）</li>
<li>使用分布式表示方法</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/运用知识图谱问答.jpg" alt="运用知识图谱问答"></p>
<h3 id="知识图谱问答的两类方法（根据技术路线分）"><a href="#知识图谱问答的两类方法（根据技术路线分）" class="headerlink" title="知识图谱问答的两类方法（根据技术路线分）"></a>知识图谱问答的两类方法（根据技术路线分）</h3><ol>
<li>语义解析(Semantic Parsing)：问句转换成形式化的查询语句，进行结构化查询得到答案</li>
<li>语义检索（Answer Retrieval &amp; Ranking）：简单的搜索得到候选答案，利用问句和候选答案的匹配程度(特征)抽取答案</li>
</ol>
<h2 id="公开的评测数据集"><a href="#公开的评测数据集" class="headerlink" title="公开的评测数据集"></a>公开的评测数据集</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/公开的评测数据集.jpg" alt="公开的评测数据集"><br>例如：</p>
<script type="math/tex; mode=display">
    \text{图数据结构}
    \begin{cases}
        QALD \\
        WebQuestions \\
        Simple Question\\
    \end{cases}\\
    \text{表数据结构}
    \begin{cases}
        WikiSQL & \text{一个表} \\
        Spider & \text{多个表} \\
    \end{cases}</script><h2 id="知识图谱问答基于的几种方法"><a href="#知识图谱问答基于的几种方法" class="headerlink" title="知识图谱问答基于的几种方法"></a>知识图谱问答基于的几种方法</h2><ol>
<li>基于符号语义解析的知识图谱问答<ul>
<li>语义表示（lambda 验算，DCS Tree）</li>
<li>语义解析方法（CCG）<ul>
<li>还有许多语义解析方法，略</li>
</ul>
</li>
</ul>
</li>
<li>基于语义检索的知识图谱问答<ul>
<li>基于显示特征的知识检索</li>
<li>基于端到端的知识图谱问答</li>
</ul>
</li>
<li>基于神经符号计算的知识图谱问答<ul>
<li>基于序列学习的解析方法</li>
<li>基于动作序列的解析方法</li>
<li>基于对战神经网络的端到端问答方法</li>
</ul>
</li>
</ol>
<h3 id="基于符号语义解析的知识图谱问答"><a href="#基于符号语义解析的知识图谱问答" class="headerlink" title="基于符号语义解析的知识图谱问答"></a>基于符号语义解析的知识图谱问答</h3><p>两种技术的具体实现过程略过，对比如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/Lambda演算vs.DCSTree.jpg" alt="Lambda演算vs.DCSTree"></p>
<h3 id="基于语义检索的知识图谱问答"><a href="#基于语义检索的知识图谱问答" class="headerlink" title="基于语义检索的知识图谱问答"></a>基于语义检索的知识图谱问答</h3><ul>
<li>基于显示特征的知识检索<ul>
<li>关键词检索</li>
<li>文本蕴含推理</li>
<li>逻辑表达式</li>
<li><div class="note primary">
            <p>给出了许多研究进展。</p>
          </div></li>
</ul>
</li>
<li>基于端到端的知识图谱问答<ul>
<li>LSTM</li>
<li>Attention Model</li>
<li>Memory Network</li>
<li><div class="note primary">
            <p>其中有部分问题：</p><ol><li>如何学习？<ul><li>RNN</li><li>CNN</li><li>Transformer</li></ul></li><li>问句如何表示？<ul><li>取所有词向量的平均值</li><li>关注答案不同的部分，问句的表示应该问句的不同部分</li><li>等</li></ul></li><li><strong>考虑多维度的相似度</strong><ul><li>从多个角度计算问句和知识的语义匹配（语义相似度）</li><li>问句如何表示？</li><li>依据问答特点，考虑答案不同维度的信息</li></ul></li></ol><p>PPT 中给出了许多研究进展，包括最基本的做法。</p>
          </div>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/基于语义检索的知识图谱问答.jpg" alt="基于语义检索的知识图谱问答"></li>
</ul>
</li>
</ul>
<h3 id="基于神经符号计算的知识图谱问答"><a href="#基于神经符号计算的知识图谱问答" class="headerlink" title="基于神经符号计算的知识图谱问答"></a>基于神经符号计算的知识图谱问答</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/符号语义解析vs.深度学习.jpg" alt="符号语义解析vs.深度学习"></p>
<ul>
<li><p>基于序列学习的解析方法</p>
<ul>
<li>seq2seq<ul>
<li>RNN-based</li>
<li>with Attention</li>
</ul>
</li>
<li><p>基于序列学习的神经符号计算</p>
<div class="note primary">
            <p>就是运用<strong>基于符号语义解析的知识图谱问答</strong>的原理，让神经网络生成这些符号，而不是生成文字。</p>
          </div>
<blockquote>
<p>基于序列学习的方法将问句和答案的逻辑表达式看作为两个序列</p>
<ul>
<li>使用序列转换的神经网络模型（如 Seq2Seq）来建模</li>
<li>神经网络生成的逻辑表达式可能不合语法规范</li>
</ul>
</blockquote>
<ul>
<li>Seq2Tree</li>
</ul>
</li>
</ul>
</li>
<li>基于动作序列的解析方法<ul>
<li>Seq2Action</li>
</ul>
</li>
<li>基于对战神经网络的端到端问答方法</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>限定域的深度问答的准确度比较高，开放域的深度问答的准确度还是处于较低的水平。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/深度问答的性能.jpg" alt="深度问答的性能"></p>
<h1 id="对话系统"><a href="#对话系统" class="headerlink" title="对话系统"></a>对话系统</h1><p>对话系统也可以直白的称为聊天机器人。<br>目前 54% 的用户会使用闲聊（开放域对话）功能。26% 的用户会选择使用某些功能性功能，比如查出行路线、查天气等。其余小部分用户使用其他的功能。<br>目前大部分的聊天机器人都基于<strong>微软小冰</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/各种聊天机器人.jpg" alt="各种聊天机器人"></p>
<p>聊天机器人一共分为两种：</p>
<ol>
<li>检索式</li>
<li>生成式</li>
</ol>
<h2 id="Response-Selection-for-Retrieval-based-Chatbots"><a href="#Response-Selection-for-Retrieval-based-Chatbots" class="headerlink" title="Response Selection for Retrieval-based Chatbots"></a>Response Selection for Retrieval-based Chatbots</h2><p>检索式又分为单轮和多轮。<br>单轮不考虑回复历史。下图展示了一个单轮回复的场景，用户提出一个问题，机器人需要在一堆回复中检索出一个最有可能的结果来对用户进行回复。多轮回复与单轮类似，只不过多轮需要考虑上下文的对话。最后也是选择一个最优可能的结果进行回复。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复.jpg" alt="检索式单轮回复"></p>
<div class="note info">
            <p>&emsp;&emsp;对于单轮：<br>&emsp;&emsp;回复不只回复 Top1 的候选回复，而是要训练一个 classifier，从而随机地返回一个回复。因为如果回复总是为同一个，用户可能会感觉很无聊。<br>&emsp;&emsp;对于多轮：<br>&emsp;&emsp;有一些挑战：</p><ul><li>A hierarchical data structure<ul><li>Words -&gt; utterances -&gt; session</li></ul></li><li>Information redundancy<ul><li>Not all words and utterances are useful for response selection</li></ul></li><li>Logics<ul><li>Order of utterances matters in response selection</li><li>Long-term dependencies among words and utterances</li><li>Constraints to proper responses</li></ul></li></ul>
          </div>
<p>下面是检索式单轮回复系统架构图和多轮回复系统架构图的对比。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复架构图.jpg" alt="检索式单轮回复架构图"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式多轮回复架构图.jpg" alt="检索式多轮回复架构图"></p>
<h3 id="单轮回复中使用的模型"><a href="#单轮回复中使用的模型" class="headerlink" title="单轮回复中使用的模型"></a>单轮回复中使用的模型</h3><p>一共有两种框架，分别为：Framework I 和 Framework II。<br><strong>Framework I 和 Framework II 的区别是</strong>：</p>
<ol>
<li>Framework I 是将句子表示为向量，Framework II 将字表示为向量。</li>
</ol>
<p><strong>Framework I 和 Framework II 的比较：</strong></p>
<ul>
<li>Efficacy（功效）：<ol>
<li>一般来讲，在外界公布出的数据集上，Framework II 模型比 Framework I 模型更好。因为在 Framework II 中的 interaction 充分保留了一个 message-response pair 中的匹配信息。</li>
</ol>
</li>
<li>Efficiency（效率）：<ol>
<li>由于过多的 interaction，Framework II 的模型普遍比 Framework I 的模型在计算上代价更大。</li>
<li>由于可以预先计算 messages and responses 的表示并将它们以索引形式存储。所以当对线上响应时间有严格要求时， Framework I 的模型更可取。</li>
</ol>
</li>
</ul>
<p>下图是 Framework I 的架构，其中最下层的 sentence embedding layer 大概就是词向量，然后需要经过一个 Representation function（这个 function 下面会给出架构）。最后将已经经过 Representation function 转换后的 q 和 r 送入 Matching layer，该层有一个 Matching function（这个 function 下面也会给出架构）。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I.jpg" alt="检索式单轮回复的 Framework I"></p>
<p>下图是 Representation funtion 的结构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I 的 Representation funtion.jpg" alt="检索式单轮回复的 Framework I 的 Representation funtion"></p>
<p>下图是 Matching funtion 的结构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I 的 Matching funtion.jpg" alt="检索式单轮回复的 Framework I 的 Matching funtion"></p>
<p><strong><em>有一些特殊的模型：Arc-I，Attentive LSTM 等</em></strong></p>
<p>Framework II 的架构与 Framework I 类似，只是多了一个 Interaction Function。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework II.jpg" alt="检索式单轮回复的 Framework II"></p>
<p>Interaction 由两种形式：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework II 中 Interaction 的两种类型.jpg" alt="检索式单轮回复的 Framework II 中 Interaction 的两种类型"></p>
<p><strong><em>有一些特殊的模型：Match Pyramid，Match LSTM 等</em></strong></p>
<p>PPT 中有数据集。以及很多 reference。</p>
<h3 id="多轮回复中使用的模型"><a href="#多轮回复中使用的模型" class="headerlink" title="多轮回复中使用的模型"></a>多轮回复中使用的模型</h3><p>&emsp;&emsp;对于多轮回复也有两种框架，分别为：Framework I 和 Framework II。<br>&emsp;&emsp;具体的架构略。PPT 里都有。</p>
<h1 id="技术总结"><a href="#技术总结" class="headerlink" title="技术总结"></a>技术总结</h1><h2 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h2><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2>]]></content>
      <categories>
        <category>assorted</category>
        <category>conference</category>
      </categories>
      <tags>
        <tag>北京</tag>
        <tag>QA</tag>
        <tag>对话系统</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习算法（三）：RNN 各种机制</title>
    <url>/%C2%B7zcy/AI/dl/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ARNN%20%E5%90%84%E7%A7%8D%E6%9C%BA%E5%88%B6.html</url>
    <content><![CDATA[<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>&emsp;&emsp;该<a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#长短期记忆——Long-Short-term-Memory-LSTM">博客</a>中描述了一个 LSTM 的例子，已经把大部分的东西概括了。但是今天看了别人的代码，这是第一次见到代码形式的 LSTM，感觉还是有些地方有问题。以下就记录这些问题。<br>&emsp;&emsp;下图是吴恩达深度学习第五周作业中的图片，是一个 LSTM 单元。<strong>与李宏毅老师做的图有略微不同，并且在下图中将 input gate 称为了 update gate，并且在李宏毅老师所提供的图片中，g(z) 是由 sigmoid 函数计算出来的，而这里是由 tanh 计算出来的，即下图 update gate 旁边的函数</strong>。另外在李宏毅老师提供的图片中，为了简便，并没有使用上个时间步的激活值。</p>
<ol>
<li>首先是<strong>输入的问题</strong>。一般来说一个 LSTM 的输入是前一<strong>个</strong> LSTM 的输出值 <script type="math/tex">a</script> 以及输入值 <script type="math/tex">x</script>（对于第 2 层的 LSTM 的 输入值就是前一<strong>层</strong>的输出值）。但是众所周知，<strong>LSTM 每个门的输入肯定只有一个向量，<script type="math/tex">a</script> 和 <script type="math/tex">x</script> 是两个向量，那么如何处理呢？</strong> <ul>
<li>在下图中使用了 <script type="math/tex">[a^{<t-1>},x^{<t>}]</script> 进行向量拼接。</li>
<li>在我看的代码中直接使用了加法进行相加，代码<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch/blob/master/seq2seq/atis/lstm/main.py" target="_blank" rel="noopener">在这</a>，但是代码量太大了，随便看看就行了（<strong>2020.2.25 更新</strong>：该代码使用了加法是基于一种较为特殊的情况，即 lstm 的隐藏状态维度等于词向量维度，所以正好可以使用加法，但是一般情况下，它们的维度不相同，所以<strong>只能使用拼接的方式</strong>）。</li>
</ul>
</li>
<li>之前说过 update gate 就是 input gate，它的输出 <script type="math/tex">\Gamma^{<t>}_u</script> 实际上也是一个向量，而 <script type="math/tex">\tilde{c^{<t>}}</script> 就是输入向量。<script type="math/tex">\Gamma^{<t>}_u</script> 的意思就是限制 <script type="math/tex">\tilde{c^{<t>}}</script> 的信息进入 memory，试想 <script type="math/tex">\Gamma^{<t>}_u</script> 的输出值范围为 (0, 1)，这不就是在说 <script type="math/tex">\Gamma^{<t>}_u</script> 将 <script type="math/tex">\tilde{c^{<t>}}</script> 的每个元素都按其比例进行调整？就类似于将 <script type="math/tex">\tilde{c^{<t>}}</script> 中的信息丢失一部分。如果 <script type="math/tex">\tilde{c^{<t>}}</script> 的输出全是 1，就代表 <script type="math/tex">\tilde{c^{<t>}}</script> 中的信息我全都要。如果 <script type="math/tex">\tilde{c^{<t>}}</script> 的输出全是 0，就代表 <script type="math/tex">\tilde{c^{<t>}}</script> 中的信息我全都不要。</li>
<li>问：由于第一个 LSTM 不存在前一个LSTM，那么它的输入值怎么处理？答：<strong>暂且使用随机初始化，具体还要补充</strong>（感觉 0 也可以，婴儿出生的时候不就是一张白纸吗。。。）。<a id="more"></a></li>
<li>记忆单元（下图中的 c，也可以称作 memory(m)）中的数据也可以随机初始化或者直接为 0。</li>
<li>每一层的 LSTM 都权重共享。意思是每一层都有多个 LSTM，里面的权重值其实是同一份。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/LSTM cell.jpg" alt="LSTM cell"></p>
<p>&emsp;&emsp;下图是多个 LSTM 运行的示意图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/多个 LSTM.jpg" alt="多个 LSTM"></p>
<p>&emsp;&emsp;下图是 LSTM 的反向传播，被称为 BPTT（backpropagation through time）。由于还没遇到过，并且 pytorch 都已经是自动求导，所以目前处于待补充状态。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/LSTM反向传播.jpg" alt="LSTM反向传播"></p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><div class="note info">
            <p>&emsp;&emsp;对于 Sequence2Sequence 模型，其实很好写代码。Seq2Seq 模型是使用了某个神经细胞作为它的核心。你可以使用 simple-RNN、GRU、LSTM 甚至是 Transformer。而这些构件其实框架都已经实现过了，直接调用代码即可。真正的难点在于理解 Seq2Seq 模型的概念，而这个概念实际上是很简单就能理解的。</p><ul><li>Encoder：<br>无非是对<strong>前一个单元激活值</strong>以及当前的输入值进行一系列的处理（如何处理完全取决于自己，可以相加也可以拼接），然后将处理后的向量当做 encoder 的输入。</li><li>Decoder：<br>与 Encoder 类似，无非是将<strong>前一个单元激活值</strong>替换成 <strong>Encoder 的输出值</strong>（当然这里也是取决于自己，<strong>不一定非要是 Encoder 的输出值</strong>，模型千变万化，全凭自己做主），其他步骤类似。</li><li>Generator：<br>输出层就是一个简单的变换层。对于二元分类你可以接一个 Sigmoid 函数；对于多元分类你可以接一个 Softmax 函数。你也可以自定义其他的函数，比如 log_softmax。nn.LSTM 默认实现了 Softmax</li></ul>
          </div>
<p>&emsp;&emsp;该<a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#seq2seq">博文</a>也可供参考。以前想过 encoder-decoder 如何实现，以为很复杂。现在自己实现了一下，实际上就是很简单的代码，<a href="https://github.com/yan624/machine_learning_algorithms/blob/master/dl/seq2seq.py" target="_blank" rel="noopener">代码地址</a>。<br>&emsp;&emsp;由于 seq2seq 有两个输入。对于 encoder 输入包含隐藏状态 a 以及 输入值 x。对于 decoder 输入包含隐藏状态 a 和 encoder 的输出。这实际上跟 LSTM 差不多，如果 seq2seq 的神经元使用 LSTM 的话，实际上就一模一样了。我只使用了简单的加法，将两个输入合并。当然也可以使用其他方法，比如拼接。其他的方法可以自行发挥。<br>&emsp;&emsp;另外对于 seq2seq 学习，已经不需要每个输入的长度都相等了，可以在句子的最后加入一个结束符，如<code>&lt;EOS&gt;</code>，以此判断输入是否结束。但是这样做如何进行向量化呢？暂时未知，待补充。</p>
<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>&emsp;&emsp;<a href="https://www.zhihu.com/question/68482809/answer/264632289" target="_blank" rel="noopener">参考文章</a>；<a href="https://www.cnblogs.com/robert-dlut/p/5952032.html" target="_blank" rel="noopener">Attention历史</a><br>&emsp;&emsp;实际上九几年的时候在CV领域已经有这概念了。RNN 领域第一篇文章《Recurrent Models of Visual Attention》。<br>&emsp;&emsp;<a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#Attention">博客</a>中有写到如何计算 Attention。</p>
<h2 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h2><p>&emsp;&emsp;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;<a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#Memory-Network">博客</a>有记一些基础的东西。</p>
<!-- more -->]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
        <tag>GRU</tag>
        <tag>LSTM</tag>
        <tag>seq2seq</tag>
        <tag>attention</tag>
        <tag>memory network</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习算法（二）：simple RNN 推导与理解</title>
    <url>/%C2%B7zcy/AI/dl/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asimple%20RNN%20%E6%8E%A8%E5%AF%BC%E4%B8%8E%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_recurrent_neural_network" target="_blank" rel="noopener">github地址</a><br>simple RNN只实现了正向传播，反向传播没有实现。是因为simple RNN有梯度消失的问题，索性直接不写了。下一节直接写LSTM。</p>
<h1 id="simple-RNN和simple-NN的对比"><a href="#simple-RNN和simple-NN的对比" class="headerlink" title="simple RNN和simple NN的对比"></a>simple RNN和simple NN的对比</h1><p>本来觉得simple RNN挺简单的，只不过是simple NN的扩展，区别无非是将simple NN的神经元换成一个RNN cell。但是实际上没那么简单，特别是加上将数据向量化后计算。感觉简直和simple NN是两种架构。</p>
<h2 id="input"><a href="#input" class="headerlink" title="input"></a>input</h2><p>下图是一个simple NN的架构，其中的一层也叫做全连接层。所以也叫做前馈神经网络。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（二）：simple RNN 推导与理解/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>网络中输入的x是一个纯数字，而不是向量。如果输入的是向量，就代表一次输入了多条样本。总的来说，一条样本向量化为：<br><a id="more"></a></p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1\\
    x_2\\
    x_3\\
\end{pmatrix}</script><p>多条样本向量化为：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 & x_4\\
    x_2 & x_5\\
    x_3 & x_6\\
\end{pmatrix}</script><p>但是在simple <strong>R</strong>NN中，因为RNN可以用做自然语言处理，在NLP领域一个数据就是一个单词或者一个词组，我们需要先将词组用数字表示，但是一个字只用一个数表示显然是不现实的。我们通常使用词向量（word vector）表示，所以问题就来了。如果我用simple NN来做自然语言处理，我该怎么处理这些数据。<br>我需要输入这句话——我 是 一名 学生。将其用变量一一对应：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 = \text{我}\\
    x_2 = \text{是}\\
    x_3 = \text{一名}\\
    x_4 = \text{学生}\\
\end{pmatrix}</script><p>如果再输入一句话——今天 天气 好像 不错。将其用变量一一对应：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 = \text{我} & x_5 = \text{今天}\\
    x_2 = \text{是} & x_6 = \text{天气}\\
    x_3 = \text{一名} & x_7 = \text{好像}\\
    x_4 = \text{学生} & x_8 = \text{不错}\\
\end{pmatrix}</script><p>这样会出现<strong>极大的问题</strong>，即文字无法进行数学运算。所以需要将文字转为词向量。但是问题是我如何在一个神经元输入一个向量？我如果输入多条样本，那么我整个输入值x就会变成<strong>3维的矩阵</strong>。simple NN显然是处理不了的。所以有个折中的方法，即将每个词的词向量加起来除以词的个数。即：</p>
<script type="math/tex; mode=display">
\frac{(v_{\text{我}} + v_{\text{是}} + v_{\text{一名}} + v_{\text{学生}})}{4} =  v_{\text{我是一名学生}}</script><p>其中v代表某个词对应的词向量。这样就将4个向量合并成一个向量了。然后simple NN就可以处理了。<br>但是这样处理肯定太勉强了，所以就出现了simple RNN，它可以处理上述这种问题。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（二）：simple RNN 推导与理解/吴恩达深度学习中的RNN示意图.jpg" alt="吴恩达深度学习中的RNN示意图"><br>上图就是一个simple <strong>R</strong>NN架构，看起来跟simple NN不一样。但是其实你只要将图片往右旋转90度，就一样了。还有一点不太一样，就是simple <strong>R</strong>NN中的一个神经元跟simple NN中的一层神经元一样。<br><div class="note info">
            <p>simple <strong>R</strong>NN其实就是simple NN的神经元被替换成了simple <strong>R</strong>NN的神经元。而simple <strong>R</strong>NN的一个神经元本身里面还有多个单元，图上就是一个神经元里有4个单元。<br>所以simple <strong>R</strong>NN的神经元可以被叫做记忆细胞。<br>如果重新描述一遍就是simple <strong>R</strong>NN其实就是simple NN的神经元被替换成了simple <strong>R</strong>NN的记忆细胞。<br>而simple <strong>R</strong>NN的记忆细胞里面有多个神经元用来处理进行向量计算。</p>
          </div></p>
<h2 id="simple-RNN的另一个输入值a"><a href="#simple-RNN的另一个输入值a" class="headerlink" title="simple RNN的另一个输入值a"></a>simple RNN的另一个输入值a</h2><p>下图是simple RNN的一个记忆细胞。由于其概念大都需要数值来演示，但是大量的数值难以书写，并且用语言实在难以描述。所以以下均使用一个矩阵的字母表示来演示。<br>此处会有几个问题：</p>
<ol>
<li>乍一看很简单，但是在实现代码的时候，脑子会转不过来。<strong>因为你碰到的是向量化后的运算</strong>。所以你对各个W，b，A以及X的形状难以确定。不信就自己写一下代码，如果你以前没写过simple RNN的代码，肯定要在确定形状这卡至少一个小时。</li>
<li>A的形状尤其难确定，会一时之间绕不过来。</li>
<li>如下图所示，虽然在图里看上去是执行加法，但是在计算时，A 与 X 其实是做向量拼接。<script type="math/tex; mode=display">
W_{ax} x^{<t>} + W_{aa} a^{<t-1>} = 
\begin{pmatrix} 
 W_{ax} & W_{aa}
\end{pmatrix} * 
\begin{pmatrix} 
 x^{<t>} \\
 a^{<t-1>} \\
\end{pmatrix}</script></li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（二）：simple RNN 推导与理解/simple rnn cell.png" alt="simple rnn cell"></p>
<h3 id="以一个记忆细胞为例"><a href="#以一个记忆细胞为例" class="headerlink" title="以一个记忆细胞为例"></a>以一个记忆细胞为例</h3><p>设词向量的维度为300，一个记忆细胞的units为32——keras代码表示：simpleRNN(32)，并且进行的是18元分类问题。</p>
<h4 id="以一个样本为例"><a href="#以一个样本为例" class="headerlink" title="以一个样本为例"></a>以一个样本为例</h4><p>，输入样本数为1。<br>则运算过程为：</p>
<script type="math/tex; mode=display">
\begin{align}
    A\_next & = tanh(Wx_{32, 300} * X_{300, 1} + Wa_{32, 32} * A\_prev_{32, 1} + ba_{32, 1})\\
    y & = softmax(Wy_{18, 32} * A\_next + by_{18, 1})
\end{align}</script><p>(数字, 数字) 的表示形式是在线性代数里表示形式，这样看起来方便点。第一个数字表示行，第二个数字表示列。tanh和softmax就不解释了。<br>我依次解释：</p>
<ol>
<li>权重值Wx比较好理解，由于输入值X是一个 (300, 1) 的矩阵（也可以叫向量）。Wx为 (32, 300) 是因为记忆细胞的一个单元为32，并且词的特征为300。如果将一个记忆细胞看作是一个隐藏层，那么神经元个数就是32，300就代表前一层的输入。</li>
<li>权重值Wa为什么是 (32, 32) ?由于a实际上就是激活值，激活值我们可以通过w * x计算得到。显然结果是 (32, 1) ，那么权重值Wa的第二个参数就可以确定大小了（矩阵相乘，第一个矩阵的第二维和第二个矩阵的第一维必须一样），权重值Wa的第一个参数实际上跟权重值Wx一样，都是units。</li>
<li>Wy为什么是 (18, 32) ?18是因为这是一个18元分类问题，32是因为A的第一维是32。</li>
</ol>
<p>其中最难确定的就是Wa的形状。经过上面推导就可以知道了，第一维代表units，第二维代表需要与A的第一维，即为A的第一维的大小。</p>
<h4 id="以128个样本为例"><a href="#以128个样本为例" class="headerlink" title="以128个样本为例"></a>以128个样本为例</h4><p>如果上面没懂可以再看一遍多个样本</p>
<script type="math/tex; mode=display">
\begin{align}
    A\_next & = tanh(Wx_{32, 300} * X_{300, 128} + Wa_{32, 32} * A\_prev_{32, 128} + ba_{32, 1})\\
    y & = softmax(Wy_{18, 32} * A\_next + by_{18, 1})
\end{align}</script><p>说白了A其实激活值，看起来比较晕是因为多了一个初始的A0，实际上A0就是激活值，只不过需要初始化一下，它的形状就是激活值的形状。</p>
<h3 id="以多个记忆细胞为例"><a href="#以多个记忆细胞为例" class="headerlink" title="以多个记忆细胞为例"></a>以多个记忆细胞为例</h3><p>过程就是上面的过程，唯一有区别的是输入值X。因为有了多个记忆细胞，所以X变成了3维，第3维就是timestep——时间步。<br>但是其实计算，跟上面一模一样，假设现在有6个时间步。伪代码如下：<br><figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">rnn_cell</span><span class="params">(A_prev, Xt, parameters)</span></span>:</span><br><span class="line">	<span class="keyword">do</span>上面的操作</span><br><span class="line"></span><br><span class="line">timesteps = <span class="number">6</span></span><br><span class="line">X = rand((<span class="number">300</span>, <span class="number">128</span>, timesteps))</span><br><span class="line">A0 = rand((<span class="number">32</span>, <span class="number">128</span>))</span><br><span class="line">parameters = 初始化所有参数</span><br><span class="line"><span class="keyword">for</span> ts <span class="keyword">in</span> range(timesteps):</span><br><span class="line">	rnn_cell(A0, X[:, :, ts], parameters)</span><br></pre></td></tr></table></figure></p>
<p>其实就是遍历每一个时间步而已。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（二）：simple RNN 推导与理解/RNN.png" alt="RNN"></p>
<h2 id="反向传播（BPTT）"><a href="#反向传播（BPTT）" class="headerlink" title="反向传播（BPTT）"></a>反向传播（BPTT）</h2><p>&emsp;&emsp;以《input》一章的第二张图为例，序列长度设为 3，则公式为如下所示，其中 S 代表 sigmoid 函数，下标用于区分不同复合函数，并且注意 w 和 b 是共享的。</p>
<script type="math/tex; mode=display">
\begin{aligned}
    a^0 & = 0 \\
    a^1 & = S_1(W_{aa} * a^0 + W_{ax} * x^1 + b) \\
    a^2 & = S_2(W_{aa} * a^1 + W_{ax} * x^2 + b) \\
    a^3 & = S_3(W_{aa} * a^2 + W_{ax} * x^3 + b) \\
\end{aligned}</script><p>&emsp;&emsp;则对 <script type="math/tex">W_{aa}</script> 的求导结果 <script type="math/tex">\Delta W_{aa}</script> 如下所示，其中 <script type="math/tex">\frac{\partial{a^0}}{\partial{W_{aa}}}</script> 等于 0，并且 <script type="math/tex">\frac{\partial{a^1}}{\partial{W_{aa}}}</script> 没必要求。因为 <script type="math/tex">a^0 = 0</script>，所以 <script type="math/tex">S_1</script> 的表达式中 <script type="math/tex">W_{aa}</script> 根本不存在，自然就没有 <script type="math/tex">W_{aa}</script> 的导数，但是由于为了具有一般性，我还是都写上了。</p>
<script type="math/tex; mode=display">
\frac{\partial{a^3}}{\partial{W_{aa}}} = S_3 * (1 - S_3) * 
\{
    a^2 + W_{aa} * \underbrace{
        S_2 * (1 - S_2)[
            a^1 + W_{aa} * \underbrace{
                S_1 * (1 - S_1) * \frac{\partial{a^0}}{\partial{W_{aa}}}
            }_{\frac{\partial{a^1}}{\partial{W_{aa}}}}
        ]
    }_{\frac{\partial{a^2}}{\partial{W_{aa}}}}
\}</script><p>&emsp;&emsp;实际上上式很容易就可以发现是一个递归形式的表达式，所以可以写为以下的形式，其中 k 代表第几个激活值，也可以理解为第几个时间步。</p>
<script type="math/tex; mode=display">
\frac{\partial{a^k}}{\partial{W_{aa}}} = S_k * (1 - S_k) * (a^{k - 1} + W_{aa} * \frac{\partial{a^{k - 1}}}{W_{aa}})</script><h1 id="simple-RNN-的缺陷"><a href="#simple-RNN-的缺陷" class="headerlink" title="simple-RNN 的缺陷"></a>simple-RNN 的缺陷</h1><p>&emsp;&emsp;RNN一个最大的缺陷就是梯度消失与梯度爆炸问题，由于这一缺陷，使得RNN在长文本中难以训练，这才诞生了LSTM及各种变体，来源于<a href="https://zhuanlan.zhihu.com/p/44163528" target="_blank" rel="noopener">专栏</a>。梯度消失的原因：参考<a href="https://zhuanlan.zhihu.com/p/28687529" target="_blank" rel="noopener">专栏</a>。<br>&emsp;&emsp;个人的解释如下：<br>&emsp;&emsp;在 RNN 中利用 memory 的方式是一种复合函数的结构，所以在反向传播时，需要链式求导，即 <script type="math/tex">f(g(x)) = f'(g(x))·g'(x)</script>，梯度与梯度相乘容易造成<strong>梯度消失</strong>和<strong>梯度爆炸</strong>。而在 LSTM 中，是使用加和的计算方式，所以大致解决了梯度消失的问题。<br><div class="note info">
            <p>&emsp;&emsp;在 RNN 中，求导得到的表达式为连乘，而 sigmoid 的输出值在 [0-1] 之间，造成了梯度消失。当然也会造成梯度爆炸，因为激活函数不一定非是 sigmoid，换成 ReLU 输出值就会大于 1 了。<br>&emsp;&emsp;接下来我带入数字来讲解一遍，<strong>具体公式详见上一节，下面只讲求导后的函数</strong>。我们先把公式拿过来，并设 <script type="math/tex">a^3 = 0.2</script>, <script type="math/tex">a^2 = 0.1</script>, <script type="math/tex">a^1 = 0.05</script>, <script type="math/tex">w_{aa} = 0.07</script>。并且<strong>易得</strong> <script type="math/tex">S^1 = a^1</script>, <script type="math/tex">S^2 = a^2</script>, <script type="math/tex">S^3 = a^3</script>（易得部分都不懂的，建议看看上面的《反向传播（BPTT）》就明白了。</p><script type="math/tex; mode=display">\frac{\partial{a^3}}{\partial{W_{aa}}} = S_3 * (1 - S_3) * \{    a^2 + W_{aa} * \underbrace{        S_2 * (1 - S_2)[            a^1 + W_{aa} * \underbrace{                S_1 * (1 - S_1) * \frac{\partial{a^0}}{\partial{W_{aa}}}            }_{\frac{\partial{a^1}}{\partial{W_{aa}}}}        ]    }_{\frac{\partial{a^2}}{\partial{W_{aa}}}}\}</script><p>&emsp;&emsp;则</p><script type="math/tex; mode=display">\frac{\partial{a^3}}{\partial{W_{aa}}} = a^3 * (1 - a^3) * \{a^2 + W_{aa} * a^2 * (1 - a^2) * [a^1 + 0]\}</script><p>&emsp;&emsp;首先需要明白一点，对于 RNN 来说，激活值 a 就是 memory。我们将设置的值带入得到 <script type="math/tex">\frac{\partial{a^3}}{\partial{W_{aa}}} = 0.0160504</script>。可以发现梯度很小，而这仅仅是在序列长度为 3 的情况下。那么假设我们修改值，将 <script type="math/tex">w_{aa}</script> 改为 0.7，则 <script type="math/tex">\frac{\partial{a^3}}{\partial{W_{aa}}} = 0.016504</script>，将 <script type="math/tex">w_{aa}</script> 改为 7，则 <script type="math/tex">\frac{\partial{a^3}}{\partial{W_{aa}}} = 0.02104</script>。我们发现即使 <script type="math/tex">w_{aa}</script> 的变化很大，梯度值的变化效果也不是明显。也就是说对于 <script type="math/tex">\frac{\partial{a^3}}{\partial{W_{aa}}}</script>，<script type="math/tex">W_{aa}</script> 这个值不是很重要，在计算梯度时，可以忽略。<br>&emsp;&emsp;综上所述，我们发现 W 几乎无法阻止梯度越变越小。而剩下的值全都是 [0-1] 范围内的激活值 a，所以就产生了梯度消失——gradient vanishing。<br>&emsp;&emsp;<del>反之，如果 a 是大于 1 的值，就造成了梯度爆炸。</del>（<strong>2020.2.27 更新</strong>：a 貌似不可能大于 1，除了 tanh。梯度爆炸应该是对 <script type="math/tex">W_{ax}</script> 求导产生的连乘造成的）<br>&emsp;&emsp;然而梯度消失的真正原因并不完全是这个，详见 <a href="https://yan624.github.io/project/多领域seq2lf.html#LSTM-解决了-RNN-的什么问题">LSTM 解决了 RNN 的什么问题</a> 的第二点。</p>
          </div></p>
<p>&emsp;&emsp;RNN 的 loss 是每个 timestep 上的 loss 的加和，下图显示了这种情况下是如何求导的。不过有时候 RNN 的 loss 也就最后一个 tiemstep 会有一个 loss，比如分类任务。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（二）：simple RNN 推导与理解/vanishing gradient problem details.jpg" alt="vanishing gradient problem details"></p>
<p>&emsp;&emsp;参考资料：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/28749444" target="_blank" rel="noopener">LSTM如何解决梯度消失问题</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34203833" target="_blank" rel="noopener">深入理解lstm及其变种gru</a></li>
<li><a href="https://www.zhihu.com/question/34878706/answer/192444888" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/83496936" target="_blank" rel="noopener">人人都能看懂的LSTM介绍及反向传播算法推导（非常详细）</a></li>
</ol>
<!-- more -->]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
        <tag>simple RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224n学习笔记</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/nlp/CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<div class="note info">
            <p>&emsp;&emsp;本笔记记录的内容来源于 <a href="https://www.bilibili.com/video/av41393758" target="_blank" rel="noopener">b站——CS224n 斯坦福深度自然语言处理课</a>。<br>&emsp;&emsp;之后我补充了18 章之后的内容，其来源为<a href="https://www.bilibili.com/video/av46216519" target="_blank" rel="noopener">b站——(2019)斯坦福CS224n深度学习自然语言处理课程 by Chris Manning</a></p>
          </div>
<h1 id="开场白"><a href="#开场白" class="headerlink" title="开场白"></a>开场白</h1><p>&emsp;&emsp;略</p>
<h1 id="词向量表示：word2vec"><a href="#词向量表示：word2vec" class="headerlink" title="词向量表示：word2vec"></a>词向量表示：word2vec</h1><p>&emsp;&emsp;课程计划如下：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/课程计划.jpg" alt="课程计划"></p>
<p>&emsp;&emsp;<strong>神经网络词嵌入学习的通用做法</strong>：定义一个模型，根据中心词 <script type="math/tex">w_t</script> 去预测上下文单词。给定 <script type="math/tex">w_t</script> 的条件下 context 的概率。</p>
<script type="math/tex; mode=display">
p(context|w_t) = \dots</script><p>&emsp;&emsp;然后用损失函数判断预测的准确性，例如：</p>
<script type="math/tex; mode=display">
J = 1 - p(w_{-t}|w_t), \quad  \text{-t 代表 t 周围的单词}</script><p>&emsp;&emsp;如果可以精准地根据 t 预测到这些单词，那么概率就为 1，于是损失就没有了。但通常情况下，做不到这点。<strong>所以我们应该调整词汇表示，从而使损失最小化</strong>。<br><a id="more"></a><br>&emsp;&emsp;下图是以前的低维词向量表示方法，2003 年 Bengio 发表的这篇现在属于开创性的论文其实并没有太多人关注，因为那时候深度学习并没有很流行。但是当这篇论文开始流行的时候，就开始大行其道了。于是 2008 年 Collobert 和 Weston 开启了一个新方向，<strong>他们觉得如果我们只想要得到好的单词表示，我们甚至不需要构建一个具有预测功能的概率语言模型（probabilistic language model），我们只需要找到一种学习单词表示的方法即可</strong>。于是 2013 年有了 word2vec 模型。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/以前的低维词向量表示方法.jpg" alt="以前的低维词向量表示方法"></p>
<p>&emsp;&emsp;word2vec 是一个软件，实际上，它里面包含很多东西。有两个用于生成词汇向量的算法（Hierarchical softamx，negative sampling），还有两套效率中等的训练方法（Skip-grams，CBOW）。<em>这里的软件应该指的不是那种可以运行 exe 文件</em>。本节只讲 skip-grams 算法，并且不会讲那两个高效的词向量生成算法，而是将一个效率极低的算法（因为比较简单且包含了基本概念）。<br>&emsp;&emsp;skip-grams 模型的概念是：在每一个估算步中，都取一个词为中心词汇，然后尝试预测它<strong>一定范围内</strong>的上下文的词汇。这个模型将定义一个概率分布：<strong>给定一个中心词汇预测某个单词在它上下文中出现的概率</strong>。如下图所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/skip-grams模型.jpg" alt="skip-grams模型"></p>
<p>&emsp;&emsp;我们将会选取词汇的向量表示，以让概率分布值最大化。</p>
<h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>&emsp;&emsp;我们需要做的是定义一个半径 m，然后从中心词汇开始到距离为 m 的位置来预测周围的词汇。这句话比较抽象，因为到这为止，你还是构建不出一个模型（优化目标）。下面先给出模型的公式，注意一撇不是求导：</p>
<script type="math/tex; mode=display">
J'(\theta) = \prod^T_{t=1} \prod_{-m \leq m, j \neq 0} p(w_{t+j}|w_t;\theta)</script><p>&emsp;&emsp;其中定义一句话有 T 个单词，word t = 1 <script type="math/tex">\dots</script> T。上式中 m 为半径窗口，j 为整个窗口之中的索引。先不看第一个累乘符号，当 t = 1 时，也就是当中心词的索引为 1 时，以 m 为半径，预测该中心词汇的上下文单词出现的概率，并将所有的概率累乘。即公式： <script type="math/tex">\prod_{-m \leq m, j \neq 0} p(w_{t+j}|w_t;\theta)</script>。而第一个累乘符号指的是，将句子中每一个字都当做一次中心词汇，然后将概率再累乘起来。当然当中心词的索引比较靠前时，可能窗口会超出句子的前部，比如 中心词汇所以为 1，而 m = 5，则需要预测 -4，-3… 的位置，这显然不可能，所以需要自己做一下处理。<br>&emsp;&emsp;公式中的 <script type="math/tex">\theta</script> 是模型唯一的参数，让上下文所有词汇出现的概率都尽可能的高，其实 <script type="math/tex">\theta</script> 就是词向量，而模型的输入就是 one-hot 表示。但是，处理概率问题是一件很不爽的事，我们要做最大化操作，实际上就是解决对数分布的问题。这样求积就会变成求和，如下所示：</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{T} \sum^T_{t=1} \sum_{-m \leq m, j \neq 0} log \, p(w_{t+j}|w_t;\theta)</script><p>&emsp;&emsp;这样我们就得到了<strong>负的对数似然</strong>，上述公式就是最终版。但是这里还有一小点就是 m 其实也算是模型的参数，但是确是<strong>超参数</strong>，需要自己手动改的。所以上面说“<em>公式中的 <script type="math/tex">\theta</script> 是模型唯一的参数</em>”也没错。事实上这个模型还有很多其他的超参数，但是现在暂且视为常数。<br>&emsp;&emsp;公式前面有个负号，是因为我们要求最小化问题，而原式只能取最大值，所以取了个负号。</p>
<h3 id="确定相应的概率分布"><a href="#确定相应的概率分布" class="headerlink" title="确定相应的概率分布"></a>确定相应的概率分布</h3><p>&emsp;&emsp;那么我们具体应该怎么通过中心词汇来预测周围单词出现的概率呢？也就是说公式中的函数 p 应该是什么。其实 p 就是 softmax 函数。具体来说就是用由词向量构成的中心词汇去预测周围词汇的概率分布。下图就是 softmax 函数。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/softmax.jpg" alt="softmax"></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>&emsp;&emsp;最前面讲到需要有一个损失函数来判断预测的准确性。我们使用 cross-entropy loss。</p>
<h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><script type="math/tex; mode=display">
\begin{align}
     & \frac{\partial}{\partial v_c} log \frac{exp(u^T_o v_c)}{\sum^v_{w=1} exp(u^T_w v_c))} \\
    = & \frac{\partial}{\partial v_c} (\underbrace{log \, exp(u^T_o v_c)}_{1} - \underbrace{log \, \sum^v_{w=1} exp(u^T_w v_c)}_2) \\
     & \frac{\partial}{\partial v_c} log \, exp(u^T_o v_c) & \text{1} \\
    = & \frac{\partial}{\partial v_c} u^T_o v_c = u_o  \\
     & \frac{\partial}{\partial v_c} log \, \sum^v_{w=1} exp(u^T_w v_c) & \text{2} \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \frac{\partial}{\partial v_c} \sum^v_{x=1} exp(u^T_x v_c) \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \sum^v_{x=1} \frac{\partial}{\partial v_c} exp(u^T_x v_c) \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \sum^v_{x=1} exp(u^T_x v_c) \, u_x \\
    = & \sum^v_{x=1} \frac{exp(u^T_x v_c)}{\sum^v_{w=1} exp(u^T_w v_c)} \, u_x \\
    = & \sum^v_{x=1} p(x|c) u_x \\
     & u_o - \sum^v_{x=1} p(x|c) u_x & \text{合并}\\
\end{align}</script><h1 id="高级词向量表示"><a href="#高级词向量表示" class="headerlink" title="高级词向量表示"></a>高级词向量表示</h1><p>&emsp;&emsp;略。说了一些 word2vec 算法以及 GloVe 等算法。</p>
<h1 id="Word-Window分类与神经网络"><a href="#Word-Window分类与神经网络" class="headerlink" title="Word Window分类与神经网络"></a>Word Window分类与神经网络</h1><p>&emsp;&emsp;略。讲 Word Window 分类和简单的神经网络。</p>
<h1 id="反向传播和项目建议"><a href="#反向传播和项目建议" class="headerlink" title="反向传播和项目建议"></a>反向传播和项目建议</h1><p>&emsp;&emsp;讲反向传播，略。</p>
<h1 id="※-依存分析"><a href="#※-依存分析" class="headerlink" title="※ 依存分析"></a>※ 依存分析</h1><p>&emsp;&emsp;6分38秒开始进入正题，之前都在说学校里的事。</p>
<h2 id="语言结构的两种观点"><a href="#语言结构的两种观点" class="headerlink" title="语言结构的两种观点"></a>语言结构的两种观点</h2><p>&emsp;&emsp;Constituency=phrase structure grammar=context-free grammars(CFGs)。上下文无关文法。<br>&emsp;&emsp;Dependency。依存句法分析。<br>&emsp;&emsp;传统上讲，语言学家和自然语言处理器想做的是描述人类语言结构。过去有两个工具可以做到这点，1. 上下文无关文法（计算机科学中）/短语结构文法（语言学家）；2. 依存句法结构。<br>&emsp;&emsp;依存句法分析做的是<strong>通过找到句子中每一个词所依赖的部分来描述句子的结构</strong>。如果一个词修饰另一个词或者是另一些词的论证，那么它就是那个词的依赖。例：“barking dog”，barking 是 dong 的依赖，因为 barking 修饰 dog。“dog by the door”，by the door 也是 dog 的依赖。我们可以<strong>在词之间添加依存关系，通常用箭头表示它们之间的依存关系</strong>，可以参考 <a href="http://hanlp.com/" target="_blank" rel="noopener">这里</a> 加以理解。<br>&emsp;&emsp;对于语义含糊的例子，都可以考虑使用依存分析，例如下图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/scientistis study whales from space的句法分析.jpg" alt="scientistis study whales from space的句法分析"></p>
<p>&emsp;&emsp;一个重要的概念：<strong>人类的语言确实有歧义，我们希望可以通过这些依存关系来描述人类语言</strong>。可以发现我们分析出了两组关系。<br>&emsp;&emsp;另一个重要的概念是：<strong>完整的语言学以树库（treebanks）的形式标注数据</strong>。1990年开始，将网络上的句子的句法结构描述为依存关系图，如下图所示，这是来自雅虎问答上的句子，我们将这些称为树库。1990年，我们投入了大量资源来建立这种标注型树库。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/example of treebanks.jpg" alt="example of treebanks"></p>
<p>&emsp;&emsp;此处没有讲上下文语法，在过去的 10 年间（视频中的时间是 2017 年），nlp 中，<strong>依存句法分析</strong>已经取代了<strong>上下文无关文法</strong>。人们发现<strong>依存分析文法</strong>是一种（依存句法分析应该是依存分析文法的一种实现）仅仅构建语义表征就能轻松得到语言理解的合适框架。</p>
<h2 id="Dependency-Grammar-and-Dependency-Structure"><a href="#Dependency-Grammar-and-Dependency-Structure" class="headerlink" title="Dependency Grammar and Dependency Structure"></a>Dependency Grammar and Dependency Structure</h2><p>&emsp;&emsp;开始时间 22.19。<br>&emsp;&emsp;上面了解了什么是<strong>依存分析语法</strong>（从此节开始称之为语法，我感觉“文法”翻译得怪怪的），接下来讲解具体应该怎么做。<br>&emsp;&emsp;句法分析的思想是<strong>一个句法模型就是我们有一个词法项之间的关系或者词之间的关系</strong>。也就是说我们在词法项之间画箭头，这些箭头就是依存。通常我们做依存分析时，要做的工作要比这多。通常我们会根据一些语法关系来给这些依存关系分类并命名，比如主语、谓语、辅助修饰词等。</p>
<h2 id="Dependency-parsing"><a href="#Dependency-parsing" class="headerlink" title="Dependency parsing"></a>Dependency parsing</h2><p>&emsp;&emsp;依存分析有多种方式，视频中采用 Greedy transition-based parsing。视频开始于 52.05。<br>&emsp;&emsp;</p>
<h1 id="Tensorflow-入门"><a href="#Tensorflow-入门" class="headerlink" title="Tensorflow 入门"></a>Tensorflow 入门</h1><p>&emsp;&emsp;略，不学 tensorflow。</p>
<h1 id="RNN和语言模式"><a href="#RNN和语言模式" class="headerlink" title="RNN和语言模式"></a>RNN和语言模式</h1><p>&emsp;&emsp;讲了传统语言模型，例如马尔科夫模型，n-gram 模型等。讲了 simple RNN ，bi-RNN and deep bi-RNN。提到了梯度消失，梯度爆炸，grad clipping 等。大部分都会，主要记录一些不会的内容。</p>
<h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><p>&emsp;&emsp;19.50 - 49.05</p>
<h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><p>&emsp;&emsp;49.06 - 62.38</p>
<h2 id="序列模型用于其他任务"><a href="#序列模型用于其他任务" class="headerlink" title="序列模型用于其他任务"></a>序列模型用于其他任务</h2><p>&emsp;&emsp;66:00</p>
<ul>
<li>NER</li>
<li>Entity level sentiment in context</li>
<li>opinionated expressions</li>
</ul>
<h1 id="机器翻译和高级循环神经网络"><a href="#机器翻译和高级循环神经网络" class="headerlink" title="机器翻译和高级循环神经网络"></a>机器翻译和高级循环神经网络</h1><p>&emsp;&emsp;花了二三十分钟讲机器翻译，然后讲解各类 RNN，包括 GRU, LSTM, <strong>Pointer-Sentinel Model</strong>。</p>
<h1 id="神经机器翻译和注意力模型"><a href="#神经机器翻译和注意力模型" class="headerlink" title="神经机器翻译和注意力模型"></a>神经机器翻译和注意力模型</h1><p>&emsp;&emsp;先将机器翻译，后讲 attention。</p>
<h2 id="神经机器翻译"><a href="#神经机器翻译" class="headerlink" title="神经机器翻译"></a>神经机器翻译</h2><h2 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h2><p>&emsp;&emsp;下图是 attention 的工作原理。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/attention机制.jpg" alt="attention机制"></p>
<ol>
<li>图中的 a 代表 score，<script type="math/tex">\bar{h}_s</script> 代表 encoder 中每个 time step 生成的隐藏状态向量，<script type="math/tex">c_t</script> 代表 attention 之后的向量；</li>
<li>首先将开始标志输入到一个 decoder，代表开始进行翻译，输出一个单词后，将该 decoder 的<strong>隐藏状态</strong>（注意是隐藏状态而不是输出值，此节课视频中有明确指出）与 encoder 中的<strong>隐藏状态</strong>进行计算得到一个 score。打分的公式为 <script type="math/tex">score(h_{t - 1}, \bar{h}_s)</script>，score 具体是什么公式可以自己定义，最简单就是向量内积，下面会细说；</li>
<li>关于 score 函数，它有多种选择，<strong>注意一点</strong>下面的 score 函数只是对<strong>一个</strong>时间步上的隐藏状态打分，<script type="math/tex">\bar{h}_s</script> 也可以是个矩阵，即一步计算所有时间步的 attention score（这做法是最好的）。以下罗列几种做法，被广泛采用（2017 年的说法，现不知）的是第二个表达式，第三个表达式的 <script type="math/tex">v_a</script> 也是一个向量参数。另外对于第三个表达式 <script type="math/tex">v_a tanh(W_a [h_t;\bar{h}_s])</script>，它不是 score function，而是 Bahdanau，不知道为什么把它放到 score function 这。<script type="math/tex; mode=display">
score(h_t, \bar{h}_s) = 
\begin{cases}
h^T_t \bar{h}_s \\
h^T_t W_a \bar{h}_s \\
v_a tanh(W_a [h_t;\bar{h}_s])
\end{cases}</script></li>
<li>将 score 送入 softmax 得到概率；</li>
<li>通过公式 <script type="math/tex">c_t = \sum_s a_t(s)\bar{h}_s</script>，将所有的向量乘上注意力分数加起来；</li>
<li>将此新向量当做下一个 decoder 的输入；</li>
</ol>
<p>&emsp;&emsp;下图是加 attention 机制和不加的区别。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/加入attention机制后的性能.jpg" alt="加入attention机制后的性能"></p>
<h2 id="coverage"><a href="#coverage" class="headerlink" title="coverage"></a>coverage</h2><p>&emsp;&emsp;coverage = more attention，想法源于计算机视觉，请看下图。神经网络读入一张图片，要求输出一段话。但是我们知道一段话不仅要描写图中的鸟，还要描写鸟旁边的事物，所以就引出了多次注意，即神经网络需要注意图中更多的地方。将这一想法引入 NLP 中，其实就是多做几次 attention，<em>注：这一想法貌似就是后来 transformer 的 multi-head attention</em>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/more attention（coverage）.jpg" alt="more attention(coverage)"></p>
<h2 id="※-search"><a href="#※-search" class="headerlink" title="※ search"></a>※ search</h2><p>&emsp;&emsp;</p>
<h1 id="※-GRU及NMT的其他议题"><a href="#※-GRU及NMT的其他议题" class="headerlink" title="※ GRU及NMT的其他议题"></a>※ GRU及NMT的其他议题</h1><h2 id="GRUs-LSTMs"><a href="#GRUs-LSTMs" class="headerlink" title="GRUs/LSTMs"></a>GRUs/LSTMs</h2><p>&emsp;&emsp;gated unit 是如何解决 BPTT 的。</p>
<h2 id="NMT-evaluation"><a href="#NMT-evaluation" class="headerlink" title="NMT evaluation"></a>NMT evaluation</h2><h1 id="语音处理的端对端模型"><a href="#语音处理的端对端模型" class="headerlink" title="语音处理的端对端模型"></a>语音处理的端对端模型</h1><p>&emsp;&emsp;略，不做语音。</p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>&emsp;&emsp;略，不学 CNN。</p>
<h1 id="※-树RNN和短语句分析"><a href="#※-树RNN和短语句分析" class="headerlink" title="※ 树RNN和短语句分析"></a>※ 树RNN和短语句分析</h1><p>&emsp;&emsp;人类语言具有嵌套结构（训练结构、树结构），如：[The man from [the company that you spoke with about [the project] yesterday]]。<br>&emsp;&emsp;那么如何使用向量来表示这些句子的语义呢？可以使用 tree RNN，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/Recursive vs. recurrent neural network.jpg" alt="Recursive vs. recurrent neural network"></p>
<blockquote>
<p>&emsp;&emsp;<strong>Tree recursive neural network 的问题在于你需要得到一个树形结构</strong>，这是一个比较大的问题。树形网络并没有火遍全球，在语言方面确实有原因喜欢这类的模型（原因后面会有讲到），但是如果你在 arxiv 里面找，人们在语言神经网络研究中所使用的的方法时，你会发现人们并不多使用树形结构模型。LSTMs 的比例几乎是其十倍之多。<br>&emsp;&emsp;这里面比较大的原因是树形递归神经网络的使用者必须构建一个树形结构。<strong>在你构建完成后，使用反向传播学习模型会是一个问题</strong>。<br>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a> 25分开始。</p>
</blockquote>
<h2 id="simple-tree-RNN"><a href="#simple-tree-RNN" class="headerlink" title="simple tree RNN"></a>simple tree RNN</h2><h3 id="树RNN的计算"><a href="#树RNN的计算" class="headerlink" title="树RNN的计算"></a>树RNN的计算</h3><p>&emsp;&emsp;那么具体如何使用树形递归神经网络计算呢？比如下图中使用向量 [3 3] 和 [8 5] 计算，输入进神经网络之后，就会输出一个向量 [8 3] 和一个分数 1.3，这个分数代表输出的向量 [8 3] 是否合理（即结构是否合理。如果不太理解什么是结构是否合理，请看下两张图以及博客内容即可理解）。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive neural network for training.jpg" alt="recursive neural network for training"></p>
<p>&emsp;&emsp;具体的做法如下图所示。应该很好理解，就不详细说明了，其中对于计算 score 的 U，我猜测可能是一个 trainable 的参数，视频中并没有详细的说明。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive neural network details.jpg" alt="recursive neural network details"></p>
<p>&emsp;&emsp;那么到了真正的实战阶段应该怎么做呢？训练一个贪心的解析器，对于单词两两组合，然后发现最前的两个单词 “The cat” 组成的短语训练之后的分数最高，然后我们将 “The cat” 看作一个成分并且尤其对应的语义 [5 2]。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 1.jpg" alt="parsing a sentence with rnn"></p>
<p>&emsp;&emsp;接下来继续重复做，请注意现在的 “The cat” 是一个成分（可看作单词），而不是两个单词。又做一遍解析之后发现 “the mat” 的分数最高。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 2.jpg" alt="parsing a sentence with rnn——2"></p>
<p>&emsp;&emsp;再将 “the mat” 看作一个成分，并拥有对应的语义。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 3.jpg" alt="parsing a sentence with rnn 3"></p>
<p>&emsp;&emsp;以此类推，我们发现 “on the mat” 的分数最高，然后发现 “sat on the mat” 的分数最高，最后就得到 “The cat sat on the mat” 的分数最高。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 4.jpg" alt="parsing a sentence with rnn 4"></p>
<p>&emsp;&emsp;这是一棵解析树（parse tree），我们会得到这棵解析树的分数，它的分数由每个节点的分数加和得到。<strong>我们要做的就是找到由这堆节点所能组成的分数最高的解析树</strong>。<br>&emsp;&emsp;还需要一个优化目标，similar to max-margin parsing(Taskar et al. 2004), a supervised max-margin objective:</p>
<script type="math/tex; mode=display">J = \sum_i s(x_i, y_i) - \max_{y \in A(x_i)} (s(x_i, y) - \Delta(y, y_i))</script><p>&emsp;&emsp;最后我们还需要反向传播算法进行计算，这一工作早在 20 世纪 90 年代就由几个德国人做过了。Goller 和 Kuchler 提出了这个算法，并命名为 <strong>back propagation through structure</strong>.</p>
<h2 id="Syntactically-Untied-RNN"><a href="#Syntactically-Untied-RNN" class="headerlink" title="Syntactically-Untied RNN"></a>Syntactically-Untied RNN</h2><p>&emsp;&emsp;语义解绑树形递归神经网络，这被证明是构建高质量解析器的一个成功的方法。<br>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a>，50 分开始。</p>
<h2 id="Compositionality-Through-Recursive-Matrix-Vector-Spaces"><a href="#Compositionality-Through-Recursive-Matrix-Vector-Spaces" class="headerlink" title="Compositionality Through Recursive Matrix-Vector Spaces"></a>Compositionality Through Recursive Matrix-Vector Spaces</h2><p>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a>，65 分开始。</p>
<h2 id="related-work-for-parsing"><a href="#related-work-for-parsing" class="headerlink" title="related work for parsing"></a>related work for parsing</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/related work for parsing.jpg" alt="related work for parsing"></p>
<h1 id="※-共指解析"><a href="#※-共指解析" class="headerlink" title="※ 共指解析"></a>※ 共指解析</h1><p>&emsp;&emsp;<script type="math/tex">B^3</script>(B-CUBED) 算法用于评估。其他的一些算法：</p>
<ul>
<li>MUC Score (Vilain et al., 1995)</li>
<li>BEAF (Luo 2005); entity based</li>
<li>BLANC (Recasens and Hovy 2011) Cluster RAND-index</li>
</ul>
<p>&emsp;&emsp;在于语言学中，人们常区分两种关系。其中之一是共指，即两个词指代同一个实体，这和文本结构无关；另一种关系是首语重复，它指的是文本中某一项，一个照应语（或一个指代，anaphor）指代的事物由另一项决定，即先行词。<br>&emsp;&emsp;一些共指消解的做法：</p>
<ul>
<li>Mention Pair models</li>
<li>Mention Ranking models</li>
<li>Entity-Mention models</li>
</ul>
<p>&emsp;&emsp;神经共指模型，人们通过深度学习和共指做的内容。</p>
<ul>
<li>Wisemean, Rush, Shieber, and Weston (ACL 2015)<ul>
<li>Mention-pair model. Only partially neural network system over conventional, categorical coreference features</li>
</ul>
</li>
<li>Wiseman, Rush and Shieber (NAACL 2016)<ul>
<li>Use RNNs to learn global representations of entity clusters from mentions</li>
</ul>
</li>
<li>Clark and Manning (ACL 2016)<ul>
<li>An entity-mention model based around clustering using distributed representations of mentions and entity clusters</li>
</ul>
</li>
<li>Clark and Manning (EMNLP 2016)<ul>
<li>Expolores deep reinforcement learning to improve a metion-pair model</li>
</ul>
</li>
</ul>
<div class="note info">
            <p>&emsp;&emsp;此节视频讲了很多共指的理论，我没有记下来。实际内容比这里记的还要多一点。</p>
          </div>
<h1 id="用于回答问题的动态神经网络"><a href="#用于回答问题的动态神经网络" class="headerlink" title="用于回答问题的动态神经网络"></a>用于回答问题的动态神经网络</h1><p>&emsp;&emsp;略，这节听不懂。</p>
<h1 id="※-NLP的问题和可能性架构"><a href="#※-NLP的问题和可能性架构" class="headerlink" title="※ NLP的问题和可能性架构"></a>※ NLP的问题和可能性架构</h1><p>&emsp;&emsp;tree-RNN、pointer model、sub-word and character-based model 等。</p>
<h1 id="※-应对深度-NLP-的局限性"><a href="#※-应对深度-NLP-的局限性" class="headerlink" title="※ 应对深度 NLP 的局限性"></a>※ 应对深度 NLP 的局限性</h1>]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习算法（一）：simple NN（前馈神经网络的正反向推导）</title>
    <url>/%C2%B7zcy/AI/dl/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9Asimple%20NN%EF%BC%88%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%8F%8D%E5%90%91%E6%8E%A8%E5%AF%BC%EF%BC%89.html</url>
    <content><![CDATA[<div class="note info">
            <p>本文的公式不存在次方的说法，所以看见上标，不要想成是次方。<br>对于权重的表示问题，请看<a href="https://yan624.github.io/·学习笔记/AI/dl/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">博客</a>，但是由于是以前的学习笔记，不保证完全正确。<br>如果想了解为什么梯度下降要对w和b求导，可以看<a href="https://yan624.github.io/·学习笔记/AI/ml/梯度下降算法的推导.html">这篇</a>。<br><strong>建议边看边写，否则思维跟不上。</strong></p>
          </div>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>参考<a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">文章</a><br>以如下神经网络架构为例。参考<a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">文章</a>中使用了一个2 2 2的神经网络架构，但是现实中神经网络架构不会这么整整齐齐。所以还是使用了略复杂的架构，此外原文中未对bias（偏差）更新。另外原文也没有实现向量化后的计算。虽然在后面的代码写了，但是由于代码太长了，有一种代码我给出来了，你们自己去看的感觉。说实话没多少注释，都没看的欲望(╬￣皿￣)。然后她所使用的符号让我不太习惯，因为看吴恩达以及李宏毅老师使用的符号都是<script type="math/tex">w^l_{ji}\ a^l_i</script>等等，所以自己重新推导一遍，并且使用了数学公式，而不是截图，更好看一点。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>解释一下最下面的神经元，这个神经元初始化为1，也就是意味着1 * b = b。输入值为1，一个偏差乘1还是偏差本身。<br><a id="more"></a></p>
<h2 id="关于函数选用"><a href="#关于函数选用" class="headerlink" title="关于函数选用"></a>关于函数选用</h2><p>本文所有激活函数选择sigmoid函数，代价函数选择binary_crossentropy。</p>
<h2 id="一些约定"><a href="#一些约定" class="headerlink" title="一些约定"></a>一些约定</h2><div class="note info">
            <p>本文所有的输入值，激活值，输出值都是<strong>列向量</strong>。</p>
          </div>
<h1 id="初始化数据以及正向传播"><a href="#初始化数据以及正向传播" class="headerlink" title="初始化数据以及正向传播"></a>初始化数据以及正向传播</h1><h2 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h2><p>此处初始化各层的权重值，偏差。由于是演示，所以顺便把输入层也初始化了。<br>设</p>
<script type="math/tex; mode=display">
\begin{cases}
    x_1 = a^0_1 = 0.55, x_2 = a^0_2 = 0.72\\
    y_1 = 0.60, y_2 = 0.54\\
\end{cases}\\
\begin{cases}
    w^1_{11}=0.4236548, w^1_{12}=0.64589411\quad|\quad w^1_{21}=0.43758721, w^1_{22}=0.891773\quad|\quad w^1_{31}=0.96366276, w^1_{32}=0.38344152\\
    b^1_1=0.79172504, b^1_2=0.52889492, b^1_3=0.56804456\\
    w^2_{11}=0.92559664, w^2_{12}=0.07103606, w^2_{13}=0.0871293\quad|\quad w^2_{21}=0.0202184, w^2_{22}=0.83261985, w^2_{23}=0.77815675\\
    b^2_1=0.87001215, b^2_2=0.97861834\\
\end{cases}</script><p>不用多看，反正也用不到几次。。。</p>
<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>对于正向传播，应该是很熟悉了，所以我直接一次写完，不做过多解释。</p>
<h3 id="输入层到隐藏层"><a href="#输入层到隐藏层" class="headerlink" title="输入层到隐藏层"></a>输入层到隐藏层</h3><script type="math/tex; mode=display">
z^1_1 = w^1_{11} * a^0_1 + w^1_{12} * a^0_2 + 1 * b^1_1\\
z^1_2 = w^1_{21} * a^0_1 + w^1_{22} * a^0_2 + 1 * b^1_2\\
z^1_3 = w^1_{31} * a^0_1 + w^1_{32} * a^0_2 + 1 * b^1_3\\</script><p>带入sigmoid函数中，以下开始省略bias乘的1：</p>
<script type="math/tex; mode=display">
a^1_1 = \sigma{(z^1_1)}\\
a^1_2 = \sigma{(z^1_2)}\\
a^1_3 = \sigma{(z^1_3)}\\</script><h3 id="隐藏层到输出层"><a href="#隐藏层到输出层" class="headerlink" title="隐藏层到输出层"></a>隐藏层到输出层</h3><script type="math/tex; mode=display">
z^2_1 = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1\\
z^2_2 = w^2_{21} * a^1_1 + w^2_{22} * a^1_2 + w^2_{13} * a^1_3 + b^2_2\\</script><p>带入sigmoid函数中：</p>
<script type="math/tex; mode=display">
a^2_1 = \sigma{(z^2_1)}\\
a^2_2 = \sigma{(z^2_2)}\\</script><h3 id="计算代价"><a href="#计算代价" class="headerlink" title="计算代价"></a>计算代价</h3><p>以字母J记为代价函数的名称，最后一个表达式为最简版：</p>
<script type="math/tex; mode=display">
\begin{align}
    J & = -[(y_1 * \log(a^2_1) + (1 - y_1) * \log(1 - a^2_1) + (y_2 * \log(a^2_2) + (1 - y_2) * \log(1 - a^2_2)]\\
    J & = -\Sigma^2_{i = 1}{[(y_i * \log(a^2_i) + (1 - y_i) * \log(1 - a^2_i)]}\\
    J & = -\Sigma{[(y * \log(a^2) + (1 - y) * \log(1 - a^2)]}\\
\end{align}</script><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>上述的表达式全部是一个一个列出来的，如果使用向量来表示乘积那就方便很多。可以看到下面只用了五行就写完了上面<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#正向传播">正向传播</a>的所有步骤。<br><div class="note warning">
            <p>如果无法理解这一步那就是不会线性代数的问题，线性代数不在此文的介绍范围之内。</p>
          </div></p>
<script type="math/tex; mode=display">
\begin{align}
    z^1 & = w^1 * a^0 + b^1 & \text{输入层到隐藏层}\\
    a^1 & = \sigma{(z^1)} & \text{带入隐藏层的激活函数}\\
    z^2 & = w^2 * a^1 + b^2 & \text{隐藏层到输出层}\\
    a^2 & = \sigma{(z^2)} & \text{带入输出层的激活函数}\\
    J & = -\Sigma{[(y * \log(a^2) + (1 - y) * \log(1 - a^2)]} & \text{计算代价}\\
\end{align}</script><h2 id="上述表达式代码实现"><a href="#上述表达式代码实现" class="headerlink" title="上述表达式代码实现"></a>上述表达式代码实现</h2><p>最后几节有神经网络numpy实现的全部代码，可以直接跳过本节看下一节，这里的代码只是给出一个直观的理解，可以自己运行看看。<br>受到keras以及万物皆对象的启发，首先建立一个神经元对象<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">'sigmoid'</span>)</span>:</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line">        self.hyperparameters = dict()</span><br><span class="line">        <span class="comment"># W, b, A_prev的导数</span></span><br><span class="line">        self.grads = dict()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hyperparameters</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化超参数，在神经网络中权重值不能初始化为0，偏差可以</span></span><br><span class="line"><span class="string">        :param shape: 神经元的形状，(units, input_shape)</span></span><br><span class="line"><span class="string">        :return: hyperparameters，该神经元的超参数。可以通过hyperparameters['W']、hyperparameters['b']取值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 由于是演示，所以使用了随机初始化</span></span><br><span class="line">        W = np.random.rand(*shape)</span><br><span class="line">        b = np.random.rand(shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> W, b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构造神经元，目前只执行初始化超参数的步骤</span></span><br><span class="line"><span class="string">        :param input_shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        W, b = self._init_hyperparameters(shape=(self.units, input_shape))</span><br><span class="line">        self.hyperparameters[<span class="string">'W'</span>] = W</span><br><span class="line">        self.hyperparameters[<span class="string">'b'</span>] = b</span><br></pre></td></tr></table></figure></p>
<p>为方便起见，将大部分的函数都放入Model中，下面给出所有的代码<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> simple_neural_network.activation_function <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> simple_neural_network.cost_function <span class="keyword">import</span> *</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">'sigmoid'</span>)</span>:</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hyperparameters</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化超参数，在神经网络中权重值不能初始化为0，偏差可以</span></span><br><span class="line"><span class="string">        :param shape: 神经元的形状，(units, input_shape)</span></span><br><span class="line"><span class="string">        :return: hyperparameters，该神经元的超参数。可以通过hyperparameters['W']、hyperparameters['b']取值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 用于是演示，所以使用了随机初始化</span></span><br><span class="line">        hyperparameters = &#123;</span><br><span class="line">                <span class="string">'W'</span>: np.random.rand(*shape),</span><br><span class="line">                <span class="string">'b'</span>: np.random.rand(shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hyperparameters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构造神经元，目前只执行初始化超参数的步骤</span></span><br><span class="line"><span class="string">        :param input_shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.hyperparameters = self._init_hyperparameters(shape=(self.units, input_shape))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 全部的神经元，并且根据神经元数量计算神经网络架构的层数，在计算时需要减1因为输入层不算入神经网络层数</span></span><br><span class="line">        self.neurons = list()</span><br><span class="line">        <span class="comment"># 按顺序缓存A, (Z, W, b)，由于输入层不需要任何缓存，所以放入None填充此位置。方便根据索引取值</span></span><br><span class="line">        self.value_caches = [<span class="keyword">None</span>]</span><br><span class="line">        <span class="comment"># 代价函数</span></span><br><span class="line">        self.cost = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, neuron)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        在模型中添加神经元</span></span><br><span class="line"><span class="string">        :param neuron:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.neurons.append(neuron)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(self, A_prev, W, b)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        正向传播，线性运算：Z = W * A + b</span></span><br><span class="line"><span class="string">        :param A_prev: 前一层的激活值</span></span><br><span class="line"><span class="string">        :param W: 权重值</span></span><br><span class="line"><span class="string">        :param b: 偏差</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        Z: 运算结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        cache = A_prev, (Z, W, b)</span><br><span class="line">        self.value_caches.append(cache)</span><br><span class="line">        <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nonlinear_forward</span><span class="params">(self, A_prev, W, b, activation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        进入激活函数进行非线性计算</span></span><br><span class="line"><span class="string">        :param A_prev:</span></span><br><span class="line"><span class="string">        :param W:</span></span><br><span class="line"><span class="string">        :param b:</span></span><br><span class="line"><span class="string">        :param activation:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Z = self.linear_forward(A_prev, W, b)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'sigmoid'</span>:</span><br><span class="line">            <span class="keyword">return</span> sigmoid(Z)</span><br><span class="line">        <span class="keyword">elif</span> activation == <span class="string">'relu'</span>:</span><br><span class="line">            <span class="keyword">return</span> relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deep_forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param X: 输入值</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        A: 最后一层的运算结果，也就是输出层的激活值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 计算神经网络层数，减1是为了去掉输入层，众所周知输入层不需要进行计算</span></span><br><span class="line">        L = len(self.neurons) - <span class="number">1</span></span><br><span class="line">        A = X</span><br><span class="line">        <span class="comment"># 循环整个神经网络，进行正向传播，从1开始，因为索引0是输入层</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 根据索引获取神经元实例</span></span><br><span class="line">            neuron = self.neurons[l]</span><br><span class="line">            A_prev = A</span><br><span class="line">            W = neuron.hyperparameters[<span class="string">'W'</span>]</span><br><span class="line">            b = neuron.hyperparameters[<span class="string">'b'</span>]</span><br><span class="line">            A = self.nonlinear_forward(A_prev, W, b, neuron.activation)</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compile</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, epochs=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入层的神经元添加进去</span></span><br><span class="line">        self.neurons.insert(<span class="number">0</span>, SimpleNN(len(X)))</span><br><span class="line">        <span class="comment"># 初始化神经元的超参数</span></span><br><span class="line">        <span class="keyword">for</span> i, n <span class="keyword">in</span> enumerate(self.neurons[<span class="number">1</span>:]):</span><br><span class="line">            input_shape = self.neurons[i].units</span><br><span class="line">            n.build(input_shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始反向传播</span></span><br><span class="line">        AL = self.deep_forward(X)</span><br></pre></td></tr></table></figure></p>
<p>激活函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> z * (z &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    dZ = np.array(dA, copy=<span class="keyword">True</span>)  <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line"></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    s = sigmoid(Z)</span><br><span class="line">    <span class="keyword">return</span> dA * s * (<span class="number">1</span> - s)</span><br></pre></td></tr></table></figure></p>
<p>测试一下<br><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入值的大小</span></span><br><span class="line">input_size = 2</span><br><span class="line"><span class="comment"># 输出值的大小</span></span><br><span class="line">output_size = 2</span><br><span class="line"><span class="comment"># 方便书写，截断小数</span></span><br><span class="line">X0 = np.round(np.random.rand(input_size, 1), 2)</span><br><span class="line">Y0 = np.round(np.random.rand(output_size, 1),2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该模型为2 3 2架构</span></span><br><span class="line">model = Model()</span><br><span class="line">model.add(SimpleNN(3))</span><br><span class="line">model.add(SimpleNN(output_size))</span><br><span class="line">model.compile()</span><br><span class="line">model.fit(X0, Y0)</span><br><span class="line">print(model.value_caches[0])</span><br></pre></td></tr></table></figure></p>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><div class="note primary">
            <p>说是说反向传播，实际上整个流程就是在<strong><a href="https://baike.baidu.com/item/链式法则/3314017?fr=aladdin" target="_blank" rel="noopener">链式求导</a></strong>。如果把这点想通了，整个神经网络的难点就只在向量化上了。一定要理解为什么整个流程只是在做链式求导的问题，在这里我并不是随便一提。</p>
          </div>
<p>为了便于查找，把之前的图再放这。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"></p>
<h2 id="首先更新输出层的权重值（W）以及偏差值（b）"><a href="#首先更新输出层的权重值（W）以及偏差值（b）" class="headerlink" title="首先更新输出层的权重值（W）以及偏差值（b）"></a>首先更新输出层的权重值（W）以及偏差值（b）</h2><p>梯度下降公式大家应该都知道：<script type="math/tex">W = W - \alpha * grad</script>。其中的grad实际上就是W的导数，<a href="https://yan624.github.io/·学习笔记/AI/ml/梯度下降算法的推导.html">参考</a>。<br>可以对照上图观察，<strong>输出层</strong>的权重值分别为:</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    w^2_{11}&w^2_{12}&w^2_{13}\\
    w^2_{21}&w^2_{22}&w^2_{23}\\
\end{pmatrix}</script><p>所以我们需要分别求：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} \tag{3.1.1}\label{3.1.1}</script><h3 id="求矩阵中第一个w的导数并更新w"><a href="#求矩阵中第一个w的导数并更新w" class="headerlink" title="求矩阵中第一个w的导数并更新w"></a>求矩阵中第一个w的导数并更新w</h3><p>先求<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>，我们知道这个神经网络的代价的表达式是</p>
<script type="math/tex; mode=display">
J = -[(y_1 * \log(a^2_1) + (1 - y_1) * \log(1 - a^2_1) + (y_2 * \log(a^2_2) + (1 - y_2) * \log(1 - a^2_2)]</script><p><strong>为了方便对照我将隐藏层到输出层的正向传播的步骤</strong>也写在下面：</p>
<script type="math/tex; mode=display">
\begin{align}
    z^2_1 & = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1\\
    z^2_2 & = w^2_{21} * a^1_1 + w^2_{22} * a^1_2 + w^2_{13} * a^1_3 + b^2_2\\
    a^2_1 & = \sigma{(z^2_1)} = \frac{1}{1 - e^{-z^2_1}}\\
    a^2_2 & = \sigma{(z^2_2)} = \frac{1}{1 - e^{-z^2_2}}\\
\end{align}</script><p>根据链式求导法则得：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}}</script><p>我们将其拆解，一步一步地求：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] & \text{首先对a求导，此步如果你不会微积分会有疑惑} \tag{3.1.2}\label{3.1.2}\\
    \frac{\partial a^2_1}{\partial z^2_1} & = (a^2_1) * (1 - a^2_1) & \text{这是对sigmoid函数的求导，百度一下求导过程}\\
    \frac{\partial J}{\partial z^2_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1} & \text{其次对z求导}\\
                                      & = \frac{\partial J}{\partial a^2_1} * (a^2_1) * (1 - a^2_1) \tag{3.1.3}\label{3.1.3}\\
    \frac{\partial z^2_1}{\partial w^2_{11}} & = a^1_1\\
    \frac{\partial J}{\partial w^2_{11}} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}} & \text{最后对w求导} \tag{3.1.4}\label{3.1.4}\\
                                         & = \frac{\partial J}{\partial z^2_1} * a^1_1\\
    \frac{\partial J}{\partial w^2_{11}} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1) * a^1_1 & \text{整合在一起}\\
\end{align}</script><p>其中<script type="math/tex">[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1)</script>实际上是可以化简的，化简为<script type="math/tex">a^2_1 - y_1</script>，同时去掉了负号，所以</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = (a^2_1 - y_1) * a^1_1</script><p>我们将数值带入其中，之前的正向传播已经得到了所有激活值。</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial w^2_{11}} = (0.85220348 - 0.60) * 0.81604509 = 0.20580941153491322</script><p>对<script type="math/tex">w^2_{11}</script>更新，</p>
<script type="math/tex; mode=display">
w^2_{11} = w^2_{11} - \alpha * \frac{\partial J}{\partial w^2_{11}}</script><p>学习速率<script type="math/tex">\alpha</script>选1，经过简单的运算，<script type="math/tex">w^2_{11} = 0.92559664 - 1 * 0.20580941153491322 = 0.7197872284650868</script><br><div class="note info">
            <p>如果细心点就会发现，<script type="math/tex">\frac{\partial J}{\partial w^2_{11}}</script>其实就等于这层的z的导数乘上前一层的激活值a。如果没发现也没关系，下面<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#向量化-1">向量化</a>这节会做一个总结。</p>
          </div></p>
<h3 id="求所有w的导数"><a href="#求所有w的导数" class="headerlink" title="求所有w的导数"></a>求所有w的导数</h3><p>同理可以求出所有的导数</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} \tag{\ref{3.1.1}}</script><h3 id="向量化-1"><a href="#向量化-1" class="headerlink" title="向量化"></a>向量化</h3><div class="note danger">
            <p>上面只求了一个w的导数，虽然其他的w的求导都是类似操作，但是真要算起来，对于自己没去算过的人，可能花一天都没有办法将其用<strong>向量化表示</strong>。<br>求导是十分简单的，但是向量化可能会有点问题。问题的主要来源是<strong>想偷懒</strong>。对于这种问题，最好得到解决办法是暴力破解，即求出所有的w的导数，然后再将其向量化。</p>
          </div>
<p>首先观察上述公式<script type="math/tex">\ref{3.1.4}</script>：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}}</script><p>它由两部分组成，一个是<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>，第二部分是<script type="math/tex">\frac{\partial z^2_1}{\partial w^2_{11}}</script>，如果你自己求过导就会发现其实<script type="math/tex">\frac{\partial z^2_1}{\partial w^2_{11}} = a^1_1</script>。为了方便你们观察，我列出所有式子：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial z^2_1} * a^1_1 \quad \frac{\partial J}{\partial w^2_{12}} = \frac{\partial J}{\partial z^2_1} * a^1_2 \quad \frac{\partial J}{\partial w^2_{13}} = \frac{\partial J}{\partial z^2_1} * a^1_3\\
    \frac{\partial J}{\partial w^2_{21}} = \frac{\partial J}{\partial z^2_2} * a^1_1 \quad \frac{\partial J}{\partial w^2_{22}} = \frac{\partial J}{\partial z^2_2} * a^1_2 \quad \frac{\partial J}{\partial w^2_{23}} = \frac{\partial J}{\partial z^2_2} * a^1_3\\
\end{align}</script><p><strong>可能到这你有点烦躁了，因为表达式实在太多了。没关系，下方蓝色的note会给出总结，直接一步求解完毕。</strong><br>有没有发现，里面有一半是重复的元素？我们可以将它们组成向量得到：</p>
<script type="math/tex; mode=display">
\eqref{3.1.1}
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} = 
\begin{pmatrix}
\frac{\partial J}{\partial z^2_1}\\
\frac{\partial J}{\partial z^2_2}\\
\end{pmatrix} * 
\begin{pmatrix}
a^1_1 & a^1_2 & a^1_3
\end{pmatrix} \tag{3.1.5}</script><p>公式3.1.5和上面那六个表达式实际上计算的东西是一样的。进一步缩写为</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2} = \frac{\partial J}{\partial z^2} * (a^1)^T \tag{3.1.6}</script><p>这里加了一个T代表转置，实际上我们所有的输入值，激活值，输出值都是列向量。<br><div class="note info">
            <p>总结一下，在这里<strong>求<script type="math/tex">\frac{\partial J}{\partial w}</script>的步骤为：求权重值所在层的z的导数<script type="math/tex">\frac{\partial J}{\partial z}</script>再乘上前一层的激活值</strong>。这是对一个w求导所做的运算，而对一整个W矩阵求导那就是公式3.1.6的那个向量化操作。但是观察公式3.1.6发现，其实求一个w和求一个W矩阵并无区别，无非是将数字相乘改为向量（矩阵）相乘。<br>另外，其实这对神经网络中每一层的操作都是一样。如果不信可以自己算一下。所以以后理解的时候，可以用这种方式理解，加快理解速度。</p>
          </div></p>
<h3 id="更新偏差"><a href="#更新偏差" class="headerlink" title="更新偏差"></a>更新偏差</h3><p>偏差比权重简单很多。</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] \tag{\ref{3.1.2}}\\
    \frac{\partial J}{\partial z^2_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1}\\
                                      & = \frac{\partial J}{\partial a^2_1} * (a^2_1) * (1 - a^2_1) \tag{\ref{3.1.3}}\\
    \frac{\partial J}{\partial b^2_1} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial b^2_1} \\
                                      & = \frac{\partial J}{\partial z^2_1}\\
    \frac{\partial J}{\partial b^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1)\\
\end{align}</script><p>可以看到</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial b^2_1} = \frac{\partial J}{\partial z^2_1}</script><p>所以偏差的向量化比较简单：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial b^2_1}\\
    \frac{\partial J}{\partial b^2_2}\\
\end{pmatrix} = 
\begin{pmatrix}
    \frac{\partial J}{\partial z^2_1}\\
    \frac{\partial J}{\partial z^2_2}\\
\end{pmatrix}</script><h3 id="整理"><a href="#整理" class="headerlink" title="整理"></a>整理</h3><p>整理一下上一波的求导过程。目标是求得<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>，但是上面我并没有一步求导到底，相反我将每一步都写出来了，这是有原因的。因为<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>会在前一层对w求导时使用，所以在代码上当然需要保存副本。而<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>已经在这次的反向传播中使用过了，它的价值也算是用完了。<br>在<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#首先更新输出层的权重值（W）以及偏差值（b）">首先更新输出层的权重值（W）以及偏差值（b）</a>中<strong>略有瑕疵</strong>的步骤（也就是上述所有步骤）是：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script></li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script></li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script></li>
</ol>
<div class="note info">
            <p>根据上面三步，我们可以观察出，如果需要求出一层的<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script>（<strong>步骤3</strong>），需要<strong>先</strong>求出<strong>同一层</strong>的<script type="math/tex">\frac{\partial J}{\partial a^2}\ \frac{\partial J}{\partial z^2}</script>（<strong>步骤1和2</strong>）。<strong><em>所以</em></strong>如果我们需要求出<strong>前一层</strong>的<script type="math/tex">\frac{\partial J}{\partial w^1}\ \frac{\partial J}{\partial b^1}</script>，必须先求出<strong>前一层</strong>的a的导数<script type="math/tex">\frac{\partial J}{\partial a^1}\ \frac{\partial J}{\partial z^1}</script>，而由于公式<script type="math/tex">z^2_1 = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1</script>，可以观察到上述<strong>步骤2</strong>对z求导之后其实拥有三个选项：</p><ol><li>求w的导数（<strong>步骤3</strong>）</li><li>求b的导数（<strong>步骤3</strong>）</li><li>求上一层a的导数</li></ol>
          </div>
<p>所以正确的步骤是：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script>（<strong>不变</strong>）</li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script>（<strong>不变</strong>）</li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script>（<strong>不变</strong>）</li>
<li>最后求出前一层的<script type="math/tex">\frac{\partial J}{\partial a^1}</script>，准备下一步的计算。</li>
</ol>
<p><strong>也就是说，我们在一层中进行求导，需要分别求4个参数的导数，即当前层的a，w，b以及前一层的a的导数。</strong></p>
<h2 id="更新隐藏层的权重值以及偏差值"><a href="#更新隐藏层的权重值以及偏差值" class="headerlink" title="更新隐藏层的权重值以及偏差值"></a>更新隐藏层的权重值以及偏差值</h2><p>由于上述步骤太多，来回滑动网页略繁琐，我再次把图放出来，以供参考。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>隐藏层的更新与输出层略微不同，由于看公式不太形象，可以看上面的图。观察发现，隐藏层的某一个神经元链接着输出层的<strong>所有</strong>神经元。所以隐藏层的神经元的误差其实来源于与它相连接的输出层的神经元。<br>根据链式求导法则，我们知道：一个函数对一个变量求导，如果有多条路径可以到达该变量，那么就需要对每条路径都求导，最后将结果相加。转换成数学公式就跟下面公式3.2.1的求导过程一样。</p>
<h3 id="对第一个w求导"><a href="#对第一个w求导" class="headerlink" title="对第一个w求导"></a>对第一个w求导</h3><p>我们按照上一节<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#整理">《整理》</a>的四个步骤来做，先求出a的导数：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^1_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial a^1_1} + \frac{\partial J}{\partial a^2_2} * \frac{\partial a^2_2}{\partial a^1_1} & \text{输出层两个神经元均要求导再相加}\\
                                      & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial a^1_1} + \frac{\partial J}{\partial z^2_2} * \frac{\partial z^2_1}{\partial a^1_1} & \text{之前求过z的导数，为了方便书写用它替换}\\
                                      & = \frac{\partial J}{\partial z^2_1} * w^2_{11} + \frac{\partial J}{\partial z^2_2} * w^2_{21} \tag{3.2.1}\label{3.2.1}\\
\end{align}</script><div class="note info">
            <p>我们可以观察到隐藏层的a的导数<script type="math/tex">\frac{\partial J}{\partial a^1_1}</script>实际上就是<strong>输出层</strong>的z的导数<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>乘上与之相连的<strong>输出层</strong>的神经元的w。<br>一般化之后就是：<strong>除了输出层</strong>，其他所有层的<strong>a的导数</strong>都是<strong>后一层</strong>的<strong>z的导数</strong>乘上<strong>后一层</strong>的w。因为输出层的<strong>a的导数</strong>是通过代价函数求的。</p>
          </div>
<p>所以下一步就是求z的导数：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial z^1_1} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} + \frac{\partial J}{\partial z^2_2} * \frac{\partial z^2_1}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1}\\
                                      & = \frac{\partial J}{\partial z^2_1} * w^2_{11} * \frac{\partial a^1_1}{\partial z^1_1}  + \frac{\partial J}{\partial z^2_2} * w^2_{21} * \frac{\partial a^1_1}{\partial z^1_1}\\
                                      & = \frac{\partial J}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} & \text{这里a的导数参考}\ref{3.2.1}\\
    \frac{\partial a^1_1}{\partial z^1_1} & = a^1_1 * (1 - a^1_1) & \text{对sigmoid函数求导，前面已经说过了}\\
\end{align}</script><p>最后求出w的导数</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial w^1_{11}} & = \frac{\partial J}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} * \frac{\partial z^1_1}{\partial w^1_{11}}\\
                                         & = \frac{\partial J}{\partial z^1_1} * \frac{\partial z^1_1}{\partial w^1_{11}}\\
                                         & = \frac{\partial J}{\partial z^1_1} * a^0_1
\end{align}</script><h3 id="向量化-2"><a href="#向量化-2" class="headerlink" title="向量化"></a>向量化</h3><p>你肯定已经想把它向量化了。先列出所有的表达式。</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^1_{11}} = \frac{\partial J}{\partial z^1_1} * a^0_1\\
\frac{\partial J}{\partial w^1_{12}} = \frac{\partial J}{\partial z^1_1} * a^0_2\\
\frac{\partial J}{\partial w^1_{21}} = \frac{\partial J}{\partial z^1_2} * a^0_1\\
\frac{\partial J}{\partial w^1_{22}} = \frac{\partial J}{\partial z^1_2} * a^0_2\\
\frac{\partial J}{\partial w^1_{31}} = \frac{\partial J}{\partial z^1_3} * a^0_1\\
\frac{\partial J}{\partial w^1_{32}} = \frac{\partial J}{\partial z^1_3} * a^0_2\\</script><p>可以发现这其实跟上面的向量化步骤一模一样：</p>
<script type="math/tex; mode=display">
\begin{align}
    \begin{pmatrix}
        \frac{\partial J}{\partial w^1_{11}} & \frac{\partial J}{\partial w^1_{12}}\\
        \frac{\partial J}{\partial w^1_{21}} & \frac{\partial J}{\partial w^1_{22}}\\
        \frac{\partial J}{\partial w^1_{31}} & \frac{\partial J}{\partial w^1_{32}}\\
    \end{pmatrix} & = 
    \begin{pmatrix}
        \frac{\partial J}{\partial z^1_1}\\
        \frac{\partial J}{\partial z^1_2}\\
        \frac{\partial J}{\partial z^1_3}\\
    \end{pmatrix} * 
    \begin{pmatrix}
        a^0_1 & a^0_2
    \end{pmatrix} \\
    \frac{\partial J}{\partial w^1} & = \frac{\partial J}{\partial z^1} * (a^0)^T
\end{align}</script><h3 id="对偏差求导"><a href="#对偏差求导" class="headerlink" title="对偏差求导"></a>对偏差求导</h3><p>这一步更是简单，直接给结果了。</p>
<script type="math/tex; mode=display">
\begin{align}
    \begin{pmatrix}
        \frac{\partial J}{\partial b^1_1}\\
        \frac{\partial J}{\partial b^1_2}\\
        \frac{\partial J}{\partial b^1_3}\\
    \end{pmatrix} & = 
    \begin{pmatrix}
        \frac{\partial J}{\partial z^1_1}\\
        \frac{\partial J}{\partial z^1_2}\\
        \frac{\partial J}{\partial z^1_3}\\
    \end{pmatrix}\\
    \frac{\partial J}{\partial b^1} & = \frac{\partial J}{\partial z^1}
\end{align}</script><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><p>上述步骤看起来没什么问题，但是在实际编程中会有很大问题。在向量化的时候，我直接使用了<script type="math/tex">\frac{\partial J}{\partial z^1}</script>，但是问题就是<script type="math/tex">\frac{\partial J}{\partial z^1}</script>的向量化我直接跳过了。要向量化<script type="math/tex">\frac{\partial J}{\partial z^1}</script>，实际上得先向量化<script type="math/tex">\frac{\partial J}{\partial a^1}</script>。观察表达式<script type="math/tex">\ref{3.2.1}</script>，先给出所有的式子：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial a^1_1} = \frac{\partial J}{\partial z^2_1} * w^2_{11} + \frac{\partial J}{\partial z^2_2} * w^2_{21}\\
\frac{\partial J}{\partial a^1_2} = \frac{\partial J}{\partial z^2_1} * w^2_{12} + \frac{\partial J}{\partial z^2_2} * w^2_{22}\\
\frac{\partial J}{\partial a^1_3} = \frac{\partial J}{\partial z^2_1} * w^2_{13} + \frac{\partial J}{\partial z^2_2} * w^2_{23}\\</script><h4 id="向量化-3"><a href="#向量化-3" class="headerlink" title="向量化"></a>向量化</h4><script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial a^1_1}\\
    \frac{\partial J}{\partial a^1_2}\\
    \frac{\partial J}{\partial a^1_3}\\
\end{pmatrix} = 
\begin{pmatrix}
w^2_{11} & w^2_{12} & w^2_{13}\\
w^2_{21} & w^2_{22} & w^2_{23}\\
\end{pmatrix}^T * 
\begin{pmatrix}
\frac{\partial J}{\partial z^2_1} & \frac{\partial J}{\partial z^2_2}
\end{pmatrix}</script><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在反向传播中，每一层都只需要重复如下几步：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script></li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script></li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script></li>
<li>最后求出前一层的<script type="math/tex">\frac{\partial J}{\partial a^1}</script>，准备下一步的计算。此步骤的向量化操作在<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#注意点">注意点</a>。</li>
</ol>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">使用numpy实现一个简单的神经网络</a></p>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>在做反向传播代码时，验算了很多遍，发现公式推导没有问题，但是梯度却一直在上升，心态都炸了。<br>最后发现，用了大半年的crossentropy在最前面居然要加上一个“-”号。以前由于是偷懒，在求导的时候一般不加负号，在求完导之后再补上。然后由于写习惯了，导致我忘记crossentropy居然是有负号的。</p>
<h1 id="撒花"><a href="#撒花" class="headerlink" title="撒花"></a>撒花</h1><p>在第二节有一步是初始化数据，但是全篇都没用几个地方用到。是因为数据测试起来太麻烦了，我需要在代码里一步一步分析神经网络的计算过程，从而获得数据。以后再补充吧。</p>
<!-- more -->]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>深度学习算法</tag>
        <tag>simple NN</tag>
      </tags>
  </entry>
  <entry>
    <title>linux非root用户配置环境变量</title>
    <url>/IT-stuff/linux/linux%E9%9D%9Eroot%E7%94%A8%E6%88%B7%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F.html</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/gstblog/p/10160976.html" target="_blank" rel="noopener">参考文章</a><br>本文以配置anaconda的环境变量为例。</p>
<p>切换到用户目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br></pre></td></tr></table></figure></p>
<p>输入，发现有一个名为<code>.bashrc</code>的文件<br><figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">ll</span></span><br></pre></td></tr></table></figure></p>
<p>编辑它<br><figure class="highlight jboss-cli"><table><tr><td class="code"><pre><span class="line">vim ~<span class="string">/.bashrc</span></span><br></pre></td></tr></table></figure></p>
<p>在最后一行加上如下代码，保存并退出。<br><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=~/anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>
<div class="note warning">
            <p>PATH和=之间不能有空格。由于写java代码习惯了，加上了空格，导致报错。</p>
          </div>
<p>更新配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>梯度下降算法的推导</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/ml/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E5%AF%BC.html</url>
    <content><![CDATA[<p>梯度下降算法大家都知道，公式是<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，其中J是代价函数。但是这个算法具体是怎么来的，可能不太清楚。<br>本文参考<br><a href="https://mp.weixin.qq.com/s/k26Fm0GL3fdVA9VbQIVAuQ" target="_blank" rel="noopener">微信公众号</a><br><a href="https://baike.baidu.com/item/梯度/13014729" target="_blank" rel="noopener">梯度-百度百科</a><br>由于没有专业的制图工具，所以只能手画了。。。</p>
<h1 id="梯度下降问题"><a href="#梯度下降问题" class="headerlink" title="梯度下降问题"></a>梯度下降问题</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/梯度下降草图.jpg" alt="梯度下降草图"></p>
<p>由图中可以观察到，我们将参数初始化到A点，我们的目标是将点移动到最小值点（或者极小值点）。那么问题就是如何移动了。<br>先给出梯度下降公式：<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，J是代价函数，这个公式应该不陌生。</p>
<h1 id="一阶泰勒展开式"><a href="#一阶泰勒展开式" class="headerlink" title="一阶泰勒展开式"></a>一阶泰勒展开式</h1><p>如果学过高数，应该知道<strong>一阶泰勒展开式</strong>的公式是：<script type="math/tex">f(x) = f(x_0) + (x - x_0) * f'(x_0) + R_n(x)</script>，其中<script type="math/tex">R_n(x)</script>是泰勒公式的余项，可以理解为一个无穷小量。既然是无穷小量那么便可以省略不写，但是即使是无穷小，其实等式的左右边还是有点差距的，所以将等式修改为<strong>约等于号</strong>。<br><a id="more"></a></p>
<script type="math/tex; mode=display">
f(x) \approx f(x_0) + (x - x_0) * f'(x_0)</script><p>但是由于我们最小化的代价函数的参数是<script type="math/tex">\theta</script>，所以我们可以将x替换为<script type="math/tex">\theta</script>，即</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script><p>如果不知道泰勒公式，可以看下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/泰勒公式线性近似.webp" alt="泰勒公式线性近似"></p>
<p>在点<script type="math/tex">\theta_0</script>处，找一条极短的直线来表示曲线，则直线的斜率为<script type="math/tex">f'(\theta_0)</script>，并且已知<script type="math/tex">\theta_0</script>，那么根据初中数学，可以获得直线公式<script type="math/tex">f(\theta) = f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>（还不懂看这个：<script type="math/tex">y-y_0=k(x-x_0)</script>===&gt;<script type="math/tex">y = y_0 + k(x-x_0)</script>）。<br><div class="note warning">
            <p>如果仔细看到了上一行的推导，你也许要问：为什么直线斜率是<script type="math/tex">f'(\theta_0)</script>。百度。</p>
          </div></p>
<div class="note warning">
            <p>如果对上式没有问题，可能要问为什么这个红线的箭头要向下，不能向上？我有强迫症，我就要让它向上，并且我还要让<script type="math/tex">\theta</script>在<script type="math/tex">\theta_0</script>右边。这个下面会讲，但是现在假定以下的步骤均围绕上图展开。</p>
          </div>
<p>至此准备工作完成。</p>
<h1 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h1><p>我们将 <script type="math/tex">f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script> 的 <script type="math/tex">\theta - \theta_0</script> 用字母 <script type="math/tex">\alpha v</script> 代替。其中 <script type="math/tex">\alpha</script> 代表步长，v 代表 <script type="math/tex">\theta - \theta_0</script> 的<strong>单位向量</strong>。其实 <script type="math/tex">\alpha v</script> 可以合并成 <script type="math/tex">\alpha</script> 的，但是为了下面的推导更容易说明梯度下降到底在做什么，还是拆开来表示。</p>
<script type="math/tex; mode=display">
\theta - \theta_0 = \alpha v</script><p>所以公式被简化为如下形式，并且将导数的表示做一下改变，用<strong>倒三角</strong>表示</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + \alpha v * \nabla f(\theta_0)</script><p>由于我们的目标是使得<script type="math/tex">f(\theta)</script>比<script type="math/tex">f(\theta_0)</script>小，也就是使得<script type="math/tex">f(\theta) - f(\theta_0) < 0</script>。那么将公式转变为</p>
<script type="math/tex; mode=display">
f(\theta) -  f(\theta_0) \approx \alpha v * \nabla f(\theta_0) < 0</script><p>省略一部分</p>
<script type="math/tex; mode=display">
\alpha v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">\alpha</script>一般为正值，所以</p>
<script type="math/tex; mode=display">
v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>实际上都是向量。所以上式就转换为<strong>两个向量相乘在什么时候是小于0的</strong>，并且我们希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好（注意这里的<strong>小</strong>指的是小于 0 的尺度上，并非在 0~1 的尺度上），也就是<script type="math/tex">v * \nabla f(\theta_0)</script>越小越好。那么问题又转化为<strong>两个向量相乘在什么时候是最小的</strong>。<br><div class="note warning">
            <p>问题1：为什么<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>是向量。<br>由于以上都是使用二维的图来描述，所以无法体现是向量。但是实际上<script type="math/tex">\theta</script>不只有一个。<br>问题2：为什么希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好。<br>因为希望<script type="math/tex">f(\theta)</script>这一步迈远一点。</p>
          </div></p>
<p>以下为向量乘积的三种形式，由初中的知识可以得知，当向量相反时<script type="math/tex">cos(\alpha)</script>为-1，即cos函数的最小值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/向量的乘积.webp" alt="向量的乘积"></p>
<p>由于公式可以转为如下，其中<script type="math/tex">\beta</script>是向量夹角</p>
<script type="math/tex; mode=display">
|v| * |\nabla f(\theta_0)| * cos(\beta) < 0</script><p>所以当 <script type="math/tex">v</script> 和 <script type="math/tex">\nabla f(\theta_0)</script> 正好相反时，<script type="math/tex">cos(\beta) = -1</script>。也就是说当 <script type="math/tex">v</script> 和 <script type="math/tex">\nabla f(\theta_0)</script> 反向，<script type="math/tex">v * \nabla f(\theta_0)</script>最小。<br>所以现在的问题就是 v 怎么样才能是梯度方向的反方向？众所周知，<script type="math/tex">\nabla f(\theta_0)</script> 就是梯度，也就是梯度方向。那么在 <script type="math/tex">\nabla f(\theta_0)</script> 加个负号不就是相反方向了？所以</p>
<script type="math/tex; mode=display">
v  = -\frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}\\</script><p>之所以要除以 <script type="math/tex">\nabla f(\theta_0)</script> 的模，是因为 <script type="math/tex">v</script> 是单位向量。<br>将 <script type="math/tex">v</script> 带入到 <script type="math/tex">\theta - \theta_0 = \alpha * v</script> 中</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha * \frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}</script><p>一般地，因为<script type="math/tex">|\nabla f(\theta_0)|</script>是标量，可以并入到中，即简化为：</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha *\nabla f(\theta_0)</script><div class="note primary">
            <p>有点需要说明，我认为 <script type="math/tex">\frac{1}{|\nabla f(\theta_0)|}</script> 不能并入 <script type="math/tex">\alpha</script> 因为 <script type="math/tex">\frac{1}{|\nabla f(\theta_0)|}</script> 是一个变量。它是梯度的模长，但是在执行梯度下降时，每一个 epoch 的梯度都是不一样的。故 <script type="math/tex">\frac{1}{|\nabla f(\theta_0)|}</script> 也在每个 epoch 不中不同，但是学习率 <script type="math/tex">\alpha</script> 却是一个定值，比如 0.01, 0.03, 0.1 等。</p>
          </div>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>练习Keras RNN的代码</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/dl/%E7%BB%83%E4%B9%A0Keras%20RNN%E7%9A%84%E4%BB%A3%E7%A0%81.html</url>
    <content><![CDATA[<h1 id="《Python深度学习》第6章预测imdb的影评"><a href="#《Python深度学习》第6章预测imdb的影评" class="headerlink" title="《Python深度学习》第6章预测imdb的影评"></a>《Python深度学习》第6章预测imdb的影评</h1><p><a href="https://github.com/yan624/deep-learning-notes/tree/master/RNN_learning/imdb_predication" target="_blank" rel="noopener">imdb影评代码</a></p>
<h1 id="第五课第二周作业：Emojify"><a href="#第五课第二周作业：Emojify" class="headerlink" title="第五课第二周作业：Emojify"></a>第五课第二周作业：Emojify</h1><p>本文实现吴恩达深度学习第五课第二周的Emojify作业。目的是了解一个小型nlp系统的构建流程。<br>首先需要导入以下模块。由于在Jupyter中打开的ipynb文件全是英文，所以本文的大部分标题也用英文，方便ctrl F。<br>Emojifier-V1略。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>加了很多注释，但是代码的顺序我做了很大的改动。下面博客里面的代码，是作业里面的代码，基本没改几个字。<br><a href="https://github.com/yan624/deep-learning-notes/tree/master/RNN_learning/emojify" target="_blank" rel="noopener">emojify_V2代码</a><br><a id="more"></a></p>
<h2 id="Emojifier-V2-Using-LSTMs-in-Keras"><a href="#Emojifier-V2-Using-LSTMs-in-Keras" class="headerlink" title="Emojifier-V2: Using LSTMs in Keras"></a>Emojifier-V2: Using LSTMs in Keras</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.seed</span>(<span class="number">0</span>)</span><br><span class="line">from keras<span class="selector-class">.models</span> import Model</span><br><span class="line">from keras<span class="selector-class">.layers</span> import Dense, Input, Dropout, LSTM, Activation</span><br><span class="line">from keras<span class="selector-class">.layers</span><span class="selector-class">.embeddings</span> import Embedding</span><br><span class="line">from keras<span class="selector-class">.preprocessing</span> import sequence</span><br><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.seed</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br></pre></td></tr></table></figure>
<h3 id="Overview-of-the-model"><a href="#Overview-of-the-model" class="headerlink" title="Overview of the model"></a>Overview of the model</h3><h3 id="Keras-and-mini-batching"><a href="#Keras-and-mini-batching" class="headerlink" title="Keras and mini-batching"></a>Keras and mini-batching</h3><p>本练习中，我们使用mini-batch算法训练Kears。大部分深度学习框架要求在相同的mini-batch中所有序列都要等长。这使得可以执行向量化，如果你有一个3个单词的句子和一个4个单词的句子，它们之间的计算会不同（一个需要3个timestep，一个需要4个timestep，也就是说需要的LSTM个数不同），所有同时计算它们是不可能的，即无法向量化。<br>通用的解决办法是使用padding。具体来说，设置一个序列的最大长度，然后使其他的序列都与该长度等长。比如序列的最大长度是20，那么将其他的序列在后面补充0，知道长度等于20。所以句子“I love you”会在“you”后面被补充17个0。即<script type="math/tex">\begin{pmatrix}e_i & e_{love} & e_{you} & \overrightarrow{0} & \overrightarrow{0} & \cdots & \overrightarrow{0}\end{pmatrix}</script>，e代表词向量。如果长度大于20的话会被裁剪。<br>以下代码实现将句子转换为句子中的每个单词转为索引，这个索引是GloVe词嵌入的，每一个单词对应一个id。id就是索引。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sentences_to_indices</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将句子转为索引形式，短于max_len的句子后面补充0，长于max_len的句子直接截断</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape</span></span><br><span class="line">    X_indices = np.zeros((m, max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] =word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure></p>
<p>以下设计Embedding层，使用keras的Embedding类。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pretrained_embedding_layer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此方法创建了一个Embedding层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># GloVe的总单词数量</span></span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                  <span class="comment"># adding 1 to fit Keras embedding (requirement)</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    <span class="comment"># 词嵌入矩阵，之前V1压根没用词嵌入矩阵，将这个矩阵放入Embedding层中供keras使用</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable.</span></span><br><span class="line">    <span class="comment"># Use Embedding(...). Make sure to set trainable=False.</span></span><br><span class="line">    embedding_layer = Embedding(vocab_len, emb_dim, trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure></p>
<h3 id="Building-the-Emojifier-V2"><a href="#Building-the-Emojifier-V2" class="headerlink" title="Building the Emojifier-V2"></a>Building the Emojifier-V2</h3><p>以下代码完成Emojify。主要是keras代码。<br><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: Emojify_V2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">def</span> <span class="string">Emojify_V2(input_shape, word_to_vec_map, word_to_index):</span></span><br><span class="line">    <span class="attr">"""</span></span><br><span class="line">    <span class="attr">Function</span> <span class="string">creating the Emojify-v2 model's graph.</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">Arguments</span>:<span class="string"></span></span><br><span class="line">    <span class="attr">input_shape</span> <span class="string">-- shape of the input, usually (max_len,)</span></span><br><span class="line">    <span class="attr">word_to_vec_map</span> <span class="string">-- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line">    <span class="attr">word_to_index</span> <span class="string">-- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">Returns</span>:<span class="string"></span></span><br><span class="line">    <span class="attr">model</span> <span class="string">-- a model instance in Keras</span></span><br><span class="line">    <span class="attr">"""</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">    ### START CODE HERE ###</span></span><br><span class="line"><span class="comment">    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    <span class="attr">sentence_indices</span> = <span class="string">Input(input_shape, dtype='int32')</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    <span class="attr">embedding_layer</span> = <span class="string">pretrained_embedding_layer(word_to_vec_map, word_to_index)</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">    # Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    <span class="attr">embeddings</span> = <span class="string">embedding_layer(sentence_indices)   </span></span><br><span class="line">    </span><br><span class="line"><span class="comment">    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line"><span class="comment">    # Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line"><span class="comment">    # 这个128是神经元的个数</span></span><br><span class="line">    <span class="attr">X</span> = <span class="string">LSTM(128, return_sequences=True)(embeddings)</span></span><br><span class="line"><span class="comment">    # Add dropout with a probability of 0.5</span></span><br><span class="line">    <span class="attr">X</span> = <span class="string">Dropout(0.5)(X)</span></span><br><span class="line"><span class="comment">    # Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line"><span class="comment">    # Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    <span class="attr">X</span> = <span class="string">LSTM(128, return_sequences=False)(X)</span></span><br><span class="line"><span class="comment">    # Add dropout with a probability of 0.5</span></span><br><span class="line">    <span class="attr">X</span> = <span class="string">Dropout(0.5)(X)</span></span><br><span class="line"><span class="comment">    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    <span class="attr">X</span> = <span class="string">Dense(5)(X)</span></span><br><span class="line"><span class="comment">    # Add a softmax activation</span></span><br><span class="line">    <span class="attr">X</span> = <span class="string">Activation('softmax')(X)</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">    # Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    <span class="attr">model</span> = <span class="string">Model(inputs=sentence_indices, outputs=X)</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">    ### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">return</span> <span class="string">model</span></span><br></pre></td></tr></table></figure></p>
<h2 id="系统整体流程"><a href="#系统整体流程" class="headerlink" title="系统整体流程"></a>系统整体流程</h2><p>RNN</p>
<ol>
<li>读取GloVe文件和训练数据</li>
<li>将训练数据的label转为one hot表示</li>
<li>求出所有训练数据中最长句子的长度，该长度就是LSTM的个数。由于向量化的要求，LSTM的个数需要相同，以最长长度作为LSTM的个数，当然并不需要每个项目都这么设置，完全可以自己选，随便举几个例子比如20,50，100等。</li>
<li>设计Embedding层</li>
<li>建立神经网络模型</li>
<li>将每句话转换为索引表示，如果长度不够就填0，够了就截断。《Python深度学习》中使用了pad_sequences类</li>
<li>使用模型预测，第6条就是输入的训练数据，第2条就是输入的标签</li>
</ol>
<p>Embedding层需要输入一个词嵌入矩阵，就是一个二维数组。每行代表一个单词的特征向量，行数就是单词的索引。而输入的训练数据被处理成单词的索引形式。如一组训练样本，每个单词被替换成唯一的索引。<br>Embedding层太过复杂，故不作详解。第一章的代码中全部有注释。</p>
<!-- more -->]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Jupyter出现gbk codec cant decode byte 0x93 in position 3136：illegal multibyte sequence</title>
    <url>/Jupyter%E5%87%BA%E7%8E%B0gbk-codec-cant-decode-byte-0x93-in-position-3136%EF%BC%9Aillegal-multibyte-sequence.html</url>
    <content><![CDATA[<p>一般来说是open()方法没有加encoding=’utf-8’，但是没用，试了其他办法没一个能用。<br>解决办法：重启Jupyter。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达深度学习学习笔记：自然语言处理与词嵌入</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/nlp/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5.html</url>
    <content><![CDATA[<h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>我们一直使用<a href="https://yan624.github.io/·学习笔记/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型.html#one hot编码">one hot编码</a>，这在之前已经记过笔记。这种表示方法的最大缺点是将每个词孤立起来，并且泛化能力不强。由于每个向量的内积都是0，所以它们之间的距离都是一样的。比如</p>
<ol>
<li>I want a glass of orange juice.</li>
<li>I want a glass of apple <em>_</em>.<br>这两个句子是很常见的句子，所以自然而然的想到划线处应该是juice。但是由于one hot编码，程序并不知道orange和apple之间的关系，也就猜不出来。</li>
</ol>
<h2 id="Featurized-representation：-word-embedding"><a href="#Featurized-representation：-word-embedding" class="headerlink" title="Featurized representation： word embedding"></a>Featurized representation： word embedding</h2><p>既然one hot有问题，那么自然就有人发明了新的算法。<br>使用特征来表示每个词。如果适应特征化来表示，那么最后发现orange和apple的特征差不多，就可以推测出划线处应该填写什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Featurized representation： word embedding.jpg" alt="Featurized representation： word embedding"><br><a id="more"></a></p>
<h2 id="Visualizing-word-embedding"><a href="#Visualizing-word-embedding" class="headerlink" title="Visualizing word embedding"></a>Visualizing word embedding</h2><p>可以使用t-SNE算法将数据可视化为二维的图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Visualizing word embedding.jpg" alt="Visualizing word embedding"></p>
<h1 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h1><h2 id="类比"><a href="#类比" class="headerlink" title="类比"></a>类比</h2><p>看下图中的表格，现在已知对应关系man-&gt;woman，能否推出king对应于queen？也就是说king-&gt;<em>_</em>，填空题。<br>解法是：<br>求出man和woman之间的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-1\\
0.01\\
0.03\\
0.09\\
\end{pmatrix} - 
\begin{pmatrix}
1\\
0.02\\
0.02\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>假设计算king和queen的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-0.95\\
0.93\\
0.70\\
0.02\\
\end{pmatrix} - 
\begin{pmatrix}
0.97\\
0.95\\
0.69\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>算法的原理就是找到一个词使得man和woman的差与king和新词的差接近。翻译为代码就是<script type="math/tex">find\ word\ w: argmax\ sim(e_w, e_{king} - e_{man} + e _{woman})</script>。但是算法的准确度只有30%-75%。</p>
<h3 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h3><p>余弦相似度也可以计算相似度。公式为<script type="math/tex">sim(u,v) = \frac{u^Tv}{\parallel u\parallel_2\parallel v\parallel_2}</script></p>
<h1 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h1><p>略。大致意思是一个嵌入矩阵E乘上one hot编码可以得到一个单词的特征向量。E就是全部单词的特征矩阵。</p>
<h1 id="如何train一个词嵌入矩阵"><a href="#如何train一个词嵌入矩阵" class="headerlink" title="如何train一个词嵌入矩阵"></a>如何train一个词嵌入矩阵</h1><p>在早期深度学习的研究人员都是使用比较复杂的算法，但是随着时间的推移，这些复杂的算法被慢慢的简化。以至于现在的新手看到这些简化版的算法时，会疑惑这样简单的算法时怎么工作的。所以现在先介绍一个比较复杂的算法，再慢慢介绍简化版的。<br><div class="note info">
            <p>这节好像是用来讲如何建立神经语言模型的，以后再看。之前讲了嵌入矩阵E，但是E中全部的特征向量是已经假定存在的，那么这些特征从何而来呢？就是这节讲的，去训练得来的。但是其实有已经训练好的，我们可以直接拿来用，网上有很多。</p>
          </div></p>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><h1 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h1><h1 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h1><p>就是词嵌入中可能带有一些偏见，比如男女偏见、种族偏见等。现在的目的就是除去这种偏见。<br>暂且不看，其他的算法都还没学。</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达李宏毅综合学习笔记：RNN入门</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8.html</url>
    <content><![CDATA[<h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>课程</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>2~8</td>
<td>吴恩达深度学习</td>
<td>one hot编码、RNN包括双向和深层、GRU、LSTM</td>
</tr>
<tr>
<td>9~14</td>
<td>李宏毅机器学习</td>
<td>RNN包括双向和深层、LSTM、RNN反向传播、seq2seq</td>
</tr>
<tr>
<td>15~20</td>
<td>李宏毅深度学习</td>
<td>计算图、语言模型中的深度学习、几个有用的网络架构。到原视频的 p12 结束，由于后续部分涉及到了 GAN 等其他模型，所以不在此处做笔记，详见<a href="https://yan624.github.io/·zcy/AI/dl/对神经网络整体的理解.html">对神经网络整体的理解</a>博文中靠后的几节</td>
</tr>
</tbody>
</table>
</div>
<a id="more"></a>
<h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>假设：<br>x: Harry Potter and Hermione Granger invented a new spell.<br>y: 1 1 0 1 1 0 0 0 0<br>其中1代表人名地名之类的单词。这句话一共有九个单词，则x可以表示为：<script type="math/tex">x^{<1>} x^{<2>} \cdots x^{<t>} \cdots x^{<9>}</script>。<br>则y可以表示为：<script type="math/tex">y^{<1>} y^{<2>} \cdots y^{<t>} \cdots y^{<9>}</script><br>输入的长度表示为<script type="math/tex">T_x</script>，则<script type="math/tex">T_x = 9</script>。<br>输出的长度表示为<script type="math/tex">T_y</script>，则<script type="math/tex">T_y = 9</script>。<br>之前在神经网络中<script type="math/tex">X^i</script>或<script type="math/tex">X^(i)</script>代表第i个训练样本。现在在序列模型中，<script type="math/tex">X^{(i)<t>}</script>代表代表第i个训练样本的第t个元素。对应地，<script type="math/tex">T^i_x</script>就代表第i个样本的输入长度。</p>
<h1 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one hot编码"></a>one hot编码</h1><p>在我们做自然语言处理时，一件需要事先决定的是，怎么表示一个序列里的单词。<br>第一件事就是做一张词表（Vocabulary）有时也叫字典（Dictionary），然后将表示方法中要使用的单词列出一列。最后将一个单词用一个稀疏向量表示，如Harry表示为<script type="math/tex">\begin{pmatrix}0&0&0&\cdots&1&0&\cdots&0\end{pmatrix}</script>。1所在位置就是Harry这个单词在词表中的所在位置。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/ont%20hot%E4%BE%8B%E5%AD%90.jpg" alt="ont hot例子"></p>
<h1 id="循环神经网络——RNN"><a href="#循环神经网络——RNN" class="headerlink" title="循环神经网络——RNN"></a>循环神经网络——RNN</h1><p>与Simple Neural Network不同的是，循环神经网络的每一层都要有输入x和输出y。<br>第一步与Simple Neural Network类似，<script type="math/tex">a_1 = w_{ax} * x^{<1>} + b_a</script>，这样就获得了激活值a，但是这时需要使用sigmoid函数或者其他函数直接算出y，另外与Simple Neural Network不同的是，它在计算激活值时需要附带加上前一层的激活值乘上一个权重，此权重与其他的权重类似，也是NN自己训练的。所以第二个序列的计算公式是<script type="math/tex">a_2 = w_{aa} * a_1 + w_{ax} * x^{<2>} + b_a</script>。后面的序列就跟第二个序列一样。<strong>注意一点，RNN中平行方向是时间序列，并不是隐藏层，并且此例中为了方便起见，垂直方向只有一个隐藏层。那几个圆圈是神经元</strong>。看下图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84RNN%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="吴恩达深度学习中的RNN示意图"></p>
<ol>
<li><p>由于为了一般化，第一层需要修改成跟后面的计算类似，所以引入一个零向量<script type="math/tex">a_0</script>来计算<script type="math/tex">a_1</script>。<br>所以RNN的计算公式为：</p>
<script type="math/tex; mode=display">
\left\{ 
 \begin{array}{c}
     a^{<1>} = g_1(w_{aa} * a^{<0>} + w_{ax} * x^{<1>} + b_a)\\
     \hat{y}^{<1>} = g_2(w_{ya} * a^{<1>} + b_y)\\
     a^{<2>} = g_1(w_{aa} * a^{<1>} + w_{ax} * x^{<2>} + b_a)\\
     \hat{y}^{<2>} = g_2(w_{ya} * a^{<2>} + b_y)\\
     \vdots\\
     a^{<t>} = g_1(w_{aa} * a^{<t-1>} + w_{ax} * x^{<t>} + b_a)\\
     \hat{y}^{<t>} = g_2(w_{ya} * a^{<t>} + b_y)\\
 \end{array}
\right.</script><p>注意上式中的<script type="math/tex">w_{aa}</script>、<script type="math/tex">w_{ax}</script>、<script type="math/tex">w_{ya}</script>、<script type="math/tex">b_{a}</script>和<script type="math/tex">b_{y}</script>并没有上标或者下标，所以意味着每一层同一个符号的权重值和偏差值都是一样的。另外对于激活函数也是用户自行选择，在<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html">对神经网络整体的理解</a>一文中已经解释的很清楚了，为了区分输入与输出的激活函数不同，我特意使用了不同的下标，这个下标仅代表这个意思。</p>
</li>
<li><p>为了进一步地一般化，我们将<script type="math/tex">w_{aa}</script>、<script type="math/tex">w_{ax}</script>合并成为<script type="math/tex">w_{a}</script>，如果表示为矩阵形式就是<script type="math/tex">w_{a} = \begin{pmatrix}w_{aa} | w_{ax}\end{pmatrix}</script>，然后将1中的最后两行表达式一般化为：</p>
<script type="math/tex; mode=display">
a^{<t>} = g_1(w_{a} * [a^{<t-1>}, x^{<t>}] + b_a)\\
\hat{y}^{<t>} = g_2(w_{y} * a^{<t>} + b_y)\\</script><p>表达式<script type="math/tex">[a^{<t-1>}, x^{<t>}]</script>的意思是将两个向量堆起来，如果表示为矩阵形式就是<script type="math/tex">\begin{pmatrix} a^{<t-1>}\\ x^{<t>}\\ \end{pmatrix}</script>，上式为了排版问题就不写成矩阵形式了。</p>
</li>
</ol>
<h2 id="RNN的反向传播"><a href="#RNN的反向传播" class="headerlink" title="RNN的反向传播"></a>RNN的反向传播</h2><p>跟Simple Neural Network类似，也要先定义一个cost function，可以选择crossentropy。由于RNN每一层都有输出值y，所以需要对每一层都求出代价，最后将这些代价值加起来</p>
<div class="note primary">
    <p>吴恩达老师在讲反向传播的实现时并没有讲计算过程，所以有点糊里糊涂的。从代价函数到激活值反向传播还可以理解，但是从后一层到前一层的反向传播理解不了。另外由于权重值一样，那么权重值到底该怎么更新？</p>
</div>

<h2 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h2><p>上面讲到的都是<script type="math/tex">T_x = T_y</script>，但是有时候输入和输出的长度并不相同。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84RNN%E5%AE%9E%E4%BE%8B.jpg" alt="不同类型的RNN实例"><br>多对多（many to many）、多对一（many to one）、一对一（one to one）、一对多（one to many）架构<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg" alt="不同类型的RNN结构"></p>
<h2 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h2><h2 id="对新序列采样"><a href="#对新序列采样" class="headerlink" title="对新序列采样"></a>对新序列采样</h2><h2 id="长期依赖，梯度消失"><a href="#长期依赖，梯度消失" class="headerlink" title="长期依赖，梯度消失"></a>长期依赖，梯度消失</h2><p>观察两个句子：</p>
<ul>
<li>The cat, which already ate…, was full.</li>
<li>The cats, which already ate…, were full.</li>
</ul>
<p>这两个句子只有复数形式上的不同，但是开头的名词影响到了最后面的be动词。但是我们目前见到的最基本的RNN不擅长捕获这种长期依赖效应。<br>用梯度消失解释一下为什么，其实原理相同的，这里引用之前的文章<br><a href="https://yan624.github.io//%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8">梯度消失和梯度爆炸</a></p>
<h1 id="GRU单元——Gate-Recurrent-Unit"><a href="#GRU单元——Gate-Recurrent-Unit" class="headerlink" title="GRU单元——Gate Recurrent Unit"></a>GRU单元——Gate Recurrent Unit</h1><p>中文名为门控循环单元。它也解决了梯度消失的问题。</p>
<h2 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h2><p>c = memonry cell，使用<script type="math/tex">c^{<t>}</script>符号表示输出，其中<script type="math/tex">c^{<t>} = a^{<t>}</script>，由于后面的LSTM的c和a代表意思不同，所以这里直接使用c来表示输出值。所以本小章下的c你都看作是a即可。</p>
<h2 id="GRU工作流程"><a href="#GRU工作流程" class="headerlink" title="GRU工作流程"></a>GRU工作流程</h2><p>由于通过<script type="math/tex">c^{<t-1>}</script>来更新<script type="math/tex">c^{<t>}</script>的值，但是现在我们使用GRU，GRU就是来控制是否更新<script type="math/tex">c^{<t>}</script>的值的，这里使用“更新”的名词可能有点怪，因为<script type="math/tex">c^{<t>}</script>实际上是通过<script type="math/tex">c^{<t-1>}</script><strong>计算</strong>出来的。那么公式<script type="math/tex">a^{<t>} = g_1(w_{a} * [a^{<t-1>}, x^{<t>}] + b_a)</script>变为<script type="math/tex">\tilde{c}^{<t>} = tanh(w_{c} * [c^{<t-1>}, x^{<t>}] + b_c)</script>，这里的<script type="math/tex">\tilde{c}^{<t>}</script>是一个候选值——candidate value，类似于中间变量，而激活函数我们选择tanh。<br>GRU的核心是有一个Gate，就是上面说的是否更新值的功能，它的公式为<script type="math/tex">\Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)</script>，<script type="math/tex">\Gamma_u</script>的u的意思是update，sigmoid函数的输出范围在0-1之间，所以就完成了类似更新的功能。如果是0就代表不让你更新，如果是1就代表让你更新，这里听起来还有点绕，没关系看下面的表达式。<br>这时开始执行更新步骤：<script type="math/tex">c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}</script>，这一步可以看出如果<script type="math/tex">\Gamma_u</script>等于1就将<script type="math/tex">c^{<t>}</script>更新为<script type="math/tex">\tilde{c}^{<t>}</script>，如果等于0就相当于不让你更新，结果还是上一个的c，即<script type="math/tex">c^{<t-1>}</script>。<br>将公式写在一起，GRU的工作流程就是：</p>
<script type="math/tex; mode=display">
\begin{cases}
    \tilde{c}^{<t>} = tanh(w_{c} * [c^{<t-1>}, x^{<t>}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)\\
    c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}\\
\end{cases}</script><h2 id="GRU完整版"><a href="#GRU完整版" class="headerlink" title="GRU完整版"></a>GRU完整版</h2><p>可以看到下式中就多了一个<script type="math/tex">\Gamma_r</script>，但是为什么不用上面的简化版呢？那是因为经研究者多年的尝试，发现下面的版本是很实用的，也算是一个标准版，你可以自己开发不同的版本。</p>
<script type="math/tex; mode=display">
\begin{cases}
    \tilde{c}^{<t>} = tanh(w_{c} * [\Gamma_r * c^{<t-1>}, x^{<t>}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)\\
    \Gamma_r = \sigma(w_{r} * [c^{<t-1>}, x^{<t>}] + b_r)\\
    c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}\\
\end{cases}</script><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>吴恩达老师讲得感觉理解起来有点费劲，因为他觉得图片比文字更难理解，所以写了一大堆公式，只是再后面补充了图片。所以我建议看李宏毅老师的深度学习视频来理解LSTM。李宏毅老师的视频用了一张图片很好的解释了LSTM，并且他还举了一个例子，更加生动形象。<br>可能是东西方的差异，我感觉是图片好理解点，所以我选择看李宏毅老师的视频。这里就不写了，因为我在<strong>下面写了</strong>李宏毅老师课程的<strong>笔记</strong>。</p>
<h1 id="双向神经网络"><a href="#双向神经网络" class="headerlink" title="双向神经网络"></a>双向神经网络</h1><h1 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h1><hr>
<p>李宏毅机器学习</p>
<hr>
<h1 id="字母表示-1"><a href="#字母表示-1" class="headerlink" title="字母表示"></a>字母表示</h1><p>跟吴恩达老师讲的类似，李宏毅老师也讲了文字如何表示，与吴恩达老师不同的是，李宏毅老师多讲了几个。<br>最简单的方法利用向量来表示文字，就是上面说过的one-hot：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/1%20of%20N%20encoding.jpg" alt="1 of N encoding"><br>因为会出现某些单词没见到过，所以需要使用other这一维来表示。并且在右边的图中还可以使用字母来表示。然后理想上只要将词向量放入神经网络就会出现结果。但是Feedforward Network其实没办法解决这问题。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/beyond%201-of-N%20encoding.jpg" alt="beyond 1-of-N encoding"><br>可以看到下图，由于Feedforward Network没有记忆，所以两个句子对它来说是一个意思，但是对人来说可以很明显判断出第一句话台北是目的地，第二句话台北是出发地。Feedforward Network它只能训练当前的词，前一个词是什么它并不知道。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward%20Network%E6%97%A0%E6%B3%95%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="Feedforward Network无法解决的问题"></p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>&emsp;&emsp;上面讲到Feedforward Network由于没有记忆，无法记住前一个或者前几个词，所以就诞生了RNN。RNN其实也没那么神秘，就是每次输入并交给激活函数计算完毕后，将计算结果存入缓存中，并且在下一次计算时，将缓存取出来一起计算（这里一起计算的意思是将 memory 也当做 input，也就是说<strong>下图的 RNN 有 4 个输入</strong>）。就是下图的蓝色方框，由于是第一次计算，其中初始化为0。下图第一遍已经在计算了，实际上已经准备更新蓝色方框中的值了。RNN在上面的章节中其实已经写过了，都是类似的。<br>&emsp;&emsp;<strong>注意一点，下图代表一个 RNN，那几个圆圈是一个神经细胞，而不代表一个 RNN</strong>。一个神经细胞中有一个权重向量，对比 simple NN 就能理解了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%80%E4%B8%AARNN%E7%9A%84%E5%B0%8F%E5%9E%8B%E4%BE%8B%E5%AD%90.jpg" alt="一个RNN的小型例子"></p>
<p>&emsp;&emsp;经过上面的例子发现，当前的输入已经在依赖前一个的缓存了，所以当顺序有所变化，或者前一个数据有所变化时，RNN可以察觉到，输出的结果也自然不同。</p>
<h2 id="deep-RNN"><a href="#deep-RNN" class="headerlink" title="deep RNN"></a>deep RNN</h2><p>我一共写了两个RNN的笔记，无论是吴恩达老师的还是李宏毅老师的到目前为止，RNN其实都不是deep的，之前也在疑惑，RNN横轴有很多层，但是实际上那些层只是不同时间的输入，根本不算deep。今天继续看下去，发现这个问题终于有解了，RNN也可以是deep的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/deep%20RNN.jpg" alt="deep RNN"></p>
<h2 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h2><p>上面讲的RNN都被称为Elman Network。还有另一种辩题叫做Jordan Network，它将输出值缓存起来。传说之中Jordan Network可以有更好的性能。<br><div class="note primary">
            <p>为什么有更好的性能</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Elman%20Network%E5%92%8Cordan%20Network.jpg" alt="Elman Network和ordan Network"></p>
<h2 id="双向RNN——Bidirectional-RNN"><a href="#双向RNN——Bidirectional-RNN" class="headerlink" title="双向RNN——Bidirectional RNN"></a>双向RNN——Bidirectional RNN</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Bidirectional%20RNN.jpg" alt="RNN——Bidirectional RNN"></p>
<h1 id="长短期记忆——Long-Short-term-Memory-LSTM"><a href="#长短期记忆——Long-Short-term-Memory-LSTM" class="headerlink" title="长短期记忆——Long Short-term Memory(LSTM)"></a>长短期记忆——Long Short-term Memory(LSTM)</h1><div class="note primary">
            <p>LSTM的神经元个数不同有什么区别？其他的NN架构也有同样的疑问</p>
          </div>
<p>上面讲的memory实际上是最简单的，LSTM才是现在最常用的Memory。Menory在RNN中实际只是一个神经元而已，它负责输入和输出。它们之间的关联是：RNN依旧是RNN，只不过把RNN中的神经元换成了LSTM。我们知道神经元的逻辑其实很简单，只有输入——计算——输入到激活函数——输出激活值，而LSTM只不过麻烦一点罢了。<br>下图就是一个LSTM。Input Gate中如果f(z)是1就代表Gate打开，也就是f(z)*g(z) = 1 * g(z) = g(z)，就相当于可以让外界输入。如果f(z)=0，Gate被关闭，那么 f(z)*g(z)=0，是不是就像不允许外界输入一样？因为你输入多少都被置为0。而Forget Gate也类似，当f(z)=1时，即Forget Gate被打开，这里与直觉有点相反，因为Gate打开，有点感觉像遗忘。但是其实c*f(z) = 1，所以Forget Gate为1其实是记住原本的c的意思。<br>另外图中也写到了，Gate的激活函数一般选sigmoid，里面的值就代表Gate的打开程度。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E7%A4%BA%E4%BE%8B.jpg" alt="LSTM示例"></p>
<h2 id="LSTM的例子"><a href="#LSTM的例子" class="headerlink" title="LSTM的例子"></a>LSTM的例子</h2><p>例子介绍：只有一个LSTM，输入有3维，输出有1维。<script type="math/tex">x_2 = 1</script>则<script type="math/tex">x_1</script>的值就会被存到Memory中，<script type="math/tex">x_2 = -1</script>则重置Memory，<script type="math/tex">x_3 = 1</script>则输出。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E4%BE%8B%E5%AD%90%E4%BB%8B%E7%BB%8D.jpg" alt="LSTM例子介绍"><br>注：下图中的蓝色数字和灰色数字是权重值。<br><div class="note primary">
            <p>&emsp;&emsp;Q：权重值是初始化的？还是固定的？还是初始化后自己可以训练的？其实就是LSTM的反向传播算法要弄懂。<br>&emsp;&emsp;A：是初始化后自己可以训练的。2019 年 11 月 11 日回答。</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E4%BE%8B%E5%AD%90%E8%AE%A1%E7%AE%97.jpg" alt="LSTM例子计算"></p>
<ol>
<li>Input Gate：<br>将偏差设为-10是因为我们通过x2来对Input Gate控制。平常x2=0，计算x*w+b=-10，那么通过sigmoid function就会得到一个接近于0的值，所以就实现了将Input Gate关闭的功能。而如果x2=1，那么x2*100=100，通过sigmoid function就会得到一个接近于1的值，Input Gate就实现了打开的功能。</li>
<li>Forget Gate: 这里的功能跟Input Gate类似。</li>
<li>Output Gate: 如果Output Gate被关闭，那么输出0.</li>
</ol>
<h2 id="多个LSTM工作场景"><a href="#多个LSTM工作场景" class="headerlink" title="多个LSTM工作场景"></a>多个LSTM工作场景</h2><p>里面的<script type="math/tex">x^t</script>就是对应于NN中的一个向量，它分别乘上4个参数矩阵得到4个不同的向量，以此操控LSTM，而LSTM实际上就等于神经元，说白了就是一个类似激活函数的功能。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF.jpg" alt="LSTM实际工作场景"><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF2.jpg" alt="LSTM实际工作场景2"><br>多个LSTM连起来工作就是像下面一样，红线和红线旁边的那个黑色曲线链接的值之前没有讲过，但是下图的这样才是LSTM实际的长相，所以之前讲的那么复杂实际上还是LSTM的简化版。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="LSTM实际工作流程"></p>
<h1 id="RNN反向传播"><a href="#RNN反向传播" class="headerlink" title="RNN反向传播"></a>RNN反向传播</h1><div class="note info">
            <p>&emsp;&emsp;此处的笔记来源于 <a href="https://www.bilibili.com/video/av10590361?p=37" target="_blank" rel="noopener">26: Recurrent Neural Network (Part II)</a> 从 0 分开始。</p>
          </div>
<p>&emsp;&emsp;BPTT——backpropagation through time，与 NN 的 backpropagation 类似，李宏毅老师也没讲原理直接跳过了。<br>&emsp;&emsp;然而不幸的是，RNN 的 training 是很困难的。下面蓝色的线是希望的结果，但是实际上是绿色的线，会出现剧烈地抖动，最后在某个点出现NAN。这就是类似梯度消失问题。可以使用一些办法解决，但是现在用得最多的方法是LSTM。<br>&emsp;&emsp;<strong>视频中花了很长的时间去讲解梯度爆炸和梯度消失的问题，但是我没有将它记录在这，详情可访问下面两个链接。</strong><br><div class="note primary">
            <p>&emsp;&emsp;Q：如何防止出现如下剧烈抖动的 loss 曲线？<br>&emsp;&emsp;A：<a href="https://yan624.github.io/project/多领域seq2lf#clip-gradient">clip gradient 解决了 loss 剧烈抖动的问题</a><br>&emsp;&emsp;Q：LSTM 解决了 RNN 的什么难题？<br>&emsp;&emsp;A：<a href="https://yan624.github.io/project/多领域seq2lf#LSTM-解决了-RNN-的什么问题">LSTM 解决了 RNN 的什么问题</a></p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/RNN%20training%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="RNN trWaining碰到的问题"></p>
<h1 id="其他解决梯度消失的办法"><a href="#其他解决梯度消失的办法" class="headerlink" title="其他解决梯度消失的办法"></a>其他解决梯度消失的办法</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%85%B6%E4%BB%96%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8A%9E%E6%B3%95.jpg" alt="其他解决梯度消失的办法"></p>
<h1 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h1><h2 id="Many-to-one"><a href="#Many-to-one" class="headerlink" title="Many to one"></a>Many to one</h2><p>输入一个向量sequence，只输出一个向量。</p>
<ol>
<li>语义分析。比如分析电影评论是好是坏。</li>
<li>key term extraction。对文档提取关键词。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many to One.jpg" alt="Many to One"></p>
<h2 id="Many-to-many-Output-is-shorter"><a href="#Many-to-many-Output-is-shorter" class="headerlink" title="Many to many(Output is shorter)"></a>Many to many(Output is shorter)</h2><p>输入和输出都是向量sequence，但是输出要短。</p>
<ol>
<li>Speech Recognition 。语音辨识。</li>
</ol>
<h2 id="Many-to-many-No-limitation"><a href="#Many-to-many-No-limitation" class="headerlink" title="Many to many(No limitation)"></a>Many to many(No limitation)</h2><p>输入和输出都是序列且长短不一。被称为 <strong>Sequence to sequence learning</strong> 。</p>
<ol>
<li>Machine Translation. 机器翻译。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many to Many（No Limitation）.jpg" alt="Many to Many(No Limitation)"></p>
<h2 id="Beyond-Sequence"><a href="#Beyond-Sequence" class="headerlink" title="Beyond Sequence"></a>Beyond Sequence</h2><ol>
<li>Syntactic parsing</li>
</ol>
<hr>
<p>李宏毅深度学习</p>
<hr>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>此系列视频还有两个Review视频，分别为第一个视频：Basic Structures for Deep Learning Models(Part 1)， 第二个视频：Basic Structures for Deep Learning Models(Part 2)。<br>个人认为Review视频不需要看，而且这两个视频时间贼长，加起来得有两个多小时。没必要浪费时间，即使你根本没学过Review中的知识点也不用去看。他的Review里不会讲很深，基本上就过过场，就算有很深的东西也完全不影响继续往下学。1P时长80分钟，说实话如果自己属于小白阶段，去看那么长的视频是挺打击人的兴趣的，如果是大佬或者已经入门的人当然看得津津有味了。<br><a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#字母表示-1">此文</a>记录了李宏毅机器学习视频中讲解的RNN的笔记。</p>
<h1 id="Computational-Graph-amp-Backpropagation"><a href="#Computational-Graph-amp-Backpropagation" class="headerlink" title="Computational Graph &amp; Backpropagation"></a>Computational Graph &amp; Backpropagation</h1><div class="note danger">
            <p>2019年6月7号更新：关于计算图这章，现在才发现原来很重要，因为这是完成<strong>自动求导</strong>的关键。学了 pytorch 之后才发现的。</p>
          </div>
<h2 id="什么是Computational-Graph"><a href="#什么是Computational-Graph" class="headerlink" title="什么是Computational Graph"></a>什么是Computational Graph</h2><p>这实际上跟要学的深度学习没什么关系，只是名字好听点，无视就好，如下图就是一个Computational Graph。主要用来在计算神经网络一些输出时，便于理解。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E4%BE%8B%E5%AD%90.jpg" alt="Computational Graph例子"><br>在看一个比较贴近实际的例子，顺便复习一下链式求导法则。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99%E7%A4%BA%E4%BE%8B.jpg" alt="Computational Graph链式求导法则示例"></p>
<h2 id="通过链式求导的例子理解反向传播（Backpropagation）算法"><a href="#通过链式求导的例子理解反向传播（Backpropagation）算法" class="headerlink" title="通过链式求导的例子理解反向传播（Backpropagation）算法"></a>通过链式求导的例子理解反向传播（Backpropagation）算法</h2><p>首先进行正向链式求导，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E6%AD%A3%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="正向链式求导"><br>图中要求计算e对a求偏导，首先给出a=3, b=2。其中c=a+b, d=b+1。<br>按照李宏毅老师使用链式求导法则，先要计算c对a求导得到1。e再对c求导得到b+1，带入b=2，得到3。所以3对a求偏导等于1*3=3。<br>上面这种链式求导法则有点乱，如果没仔细学过<em>微积分</em>可能难以理解。其实对于方程e = (a+b) * (b+1)，e对a求偏导，直接看出来都可以。利用考研时的口诀“左导右不导，左不导右导”（也就是<a href="https://baike.baidu.com/item/%E8%8E%B1%E5%B8%83%E5%B0%BC%E8%8C%A8%E5%85%AC%E5%BC%8F/8779293?fr=aladdin" target="_blank" rel="noopener">莱布尼茨公式</a>），直接得到结果<script type="math/tex">\frac{\partial e}{\partial a} = b+1</script>。<br>然后将b=2带入b+1得到结果还是3。</p>
<p>接着进行反向模式，如下图:<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%8F%8D%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="反向链式求导"><br>现在图中要求计算<script type="math/tex">\frac{\partial e}{\partial a}</script>以及<script type="math/tex">\frac{\partial e}{\partial b}</script>，当然你可以分别进行两次链式求导，得到结果。但是如果从e出发，也就是反向，那么就可以同时得到<script type="math/tex">\frac{\partial e}{\partial a}</script>以及<script type="math/tex">\frac{\partial e}{\partial b}</script>的结果。<br>不要在意e为什么等于1，只不过一个输入而已。<br>此外，如果阅读过《deep learning and neural network》一书，看过吴恩达机器学习视频或者其它资料的应该已经能反应出来。连接线上的求偏导实际上就跟神经网络上的权重一个意思，然后也是一层一层地反向传播。<br>这个输入e实际上就是神经网络中的反向传播算法中的输入。就是最后一层神经元的误差<script type="math/tex">\delta^l = h-y</script>。这里吴恩达老师和《deep learning and neural network》作者的最后一层误差公式不一样，<strong>目前不明</strong>，暂时不做解释，这里的公式是吴恩达老师的。<br>然后就是误差*权重+偏差得到前一层的误差，具体不展开。</p>
<h2 id="反向传播的好处"><a href="#反向传播的好处" class="headerlink" title="反向传播的好处"></a>反向传播的好处</h2><p>如果你的root只有一个，那么这个Computational Graph中的所有偏微分就都可以一次性算出。对应于神经网络，我们就是要这样的效果。</p>
<h2 id="参数共享（Parameter-sharing）"><a href="#参数共享（Parameter-sharing）" class="headerlink" title="参数共享（Parameter sharing）"></a>参数共享（Parameter sharing）</h2><p>略，看了一眼貌似挺简单。16:20</p>
<h2 id="Computational-Graph-for-Feedforword-Net"><a href="#Computational-Graph-for-Feedforword-Net" class="headerlink" title="Computational Graph for Feedforword Net"></a>Computational Graph for Feedforword Net</h2><p>李宏毅深度学习p3从21:16到52:48讲解梯度下降算法、前馈神经网络以及反向传播算法的具体数学原理<br>一直没看懂原理，以后再看。</p>
<h2 id="Computational-Graph-for-Recurrent-Network"><a href="#Computational-Graph-for-Recurrent-Network" class="headerlink" title="Computational Graph for Recurrent Network"></a>Computational Graph for Recurrent Network</h2><h1 id="※-Deep-Learning-for-Language-Modeling"><a href="#※-Deep-Learning-for-Language-Modeling" class="headerlink" title="※ Deep Learning for Language Modeling"></a>※ Deep Learning for Language Modeling</h1><p>语言模型就是预测一个word sequence出现的几率有多大。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Language%20Modeling.jpg" alt="Language Modeling"></p>
<h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>N-gram是自然语言处理中的算法。2-gram读作bi-gram。</p>
<h3 id="传统做法"><a href="#传统做法" class="headerlink" title="传统做法"></a>传统做法</h3><ul>
<li>怎么预测一句话出现的几率</li>
<li>收集大量文本作为训练数据<ul>
<li>然后计算<script type="math/tex">w_1\cdots w_n</script>这句话在训练数据中出现的概率</li>
</ul>
</li>
<li>N-gram语言模型：<ul>
<li>如何计算一小部分的概率？例如下图的p(beach|nice)出现的概率。就是将nice beach出现的次数除以nice出现的次数。</li>
</ul>
</li>
</ul>
<p>前两条是理想的处理办法，但是麻烦的是要预测的句子在语料库——corpus中八成一次都没出现过。于是就需要使用N-gram模型。它的处理办法就是将句子拆成比较小的部分——component，再把每个小部分的概率乘起来就是句子出现的几率。像下图这种只考虑前一个单词的模型叫做2-gram model。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/N-gram.jpg" alt="N-gram"></p>
<h3 id="NN-based-LM"><a href="#NN-based-LM" class="headerlink" title="NN-based LM"></a>NN-based LM</h3><p>怎么做基于NN的N-gram？<br>做法：</p>
<ol>
<li>搜集training数据</li>
<li>learn一个Neural Network，通过两个词predict下一个词，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/NN-based%20LM.jpg" alt="NN-based LM"></li>
<li>使用cross entropy minimize</li>
<li>有了Neural Network后算一个句子的几率，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9A%84%E5%87%A0%E7%8E%87.jpg" alt="计算句子的几率"><br>其中STRAT是一个token，代表句子的起始。</li>
</ol>
<h3 id="RNN-based-LM"><a href="#RNN-based-LM" class="headerlink" title="RNN-based LM"></a>RNN-based LM</h3><p>往上翻<strong>循环神经网络——RNN</strong>，原理就是这个。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/RNN-based LM.jpg" alt="RNN-based LM"></p>
<h3 id="Challenge-of-N-gram"><a href="#Challenge-of-N-gram" class="headerlink" title="Challenge of N-gram"></a>Challenge of N-gram</h3><h4 id="NN-based-model"><a href="#NN-based-model" class="headerlink" title="NN-based model"></a>NN-based model</h4><p>为什么要使用NN-based model。相较于传统方法有什么好处。<br>就是概率估不准，因为永远没有足够的数据。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Challenge%20of%20N-gram.jpg" alt="Challenge of N-gram"><br><div class="note info">
            <p>视频13:20~27:01仔细讲解了为什么要使用NN，而且把我困惑了快一个月的问题解决了，就是将文字转为数字之后进行训练的意义。</p>
          </div></p>
<h4 id="RNN-based-model"><a href="#RNN-based-model" class="headerlink" title="RNN-based model"></a>RNN-based model</h4><p>为什么要使用RNN-based model。相较于传统方法有什么好处。</p>
<h1 id="几个有用的network架构"><a href="#几个有用的network架构" class="headerlink" title="几个有用的network架构"></a>几个有用的network架构</h1><h2 id="Spatial-Transformer-Layer"><a href="#Spatial-Transformer-Layer" class="headerlink" title="Spatial Transformer Layer"></a>Spatial Transformer Layer</h2><p>&emsp;&emsp;<a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" target="_blank" rel="noopener">论文地址</a>，中文可以叫<strong>空间变换层</strong>。<br>&emsp;&emsp;此神经网络架构的出现的原因：CNN 对图片的缩放以及旋转无所谓（CNN is invariant to scaling androtation）。比如说在图片的局部地区中，一个人移动一点点距离，对 CNN 来说其实没什么多大区别。不过距离有点远的话，还是有点影响的。</p>
<h2 id="Highway-Network"><a href="#Highway-Network" class="headerlink" title="Highway Network"></a>Highway Network</h2><p>&emsp;&emsp;先对前馈神经网络和 RNN 进行一下对比。</p>
<ol>
<li>Feedforward NN 不是每一步都有输入。</li>
<li>Feedforward NN 每一层都有不同的参数。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward NN和RNN的对比.jpg" alt="Feedforward NN和RNN的对比"></li>
</ol>
<p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Network 论文地址</a>；<a href="https://arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="noopener">Highway Network 实战论文地址</a><br>&emsp;&emsp;Highway Network 的想法就是把 RNN <strong>立</strong>起来，把它当做前馈神经网络来用。<br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">Highway Network 的改进版论文地址</a>，这个就是<strong>残差神经网络</strong>。</p>
<h2 id="Grid-LSTM"><a href="#Grid-LSTM" class="headerlink" title="Grid LSTM"></a>Grid LSTM</h2><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1507.01526.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;太复杂了，估计以后也很难用到。。。</p>
<h2 id="Recusive-Network"><a href="#Recusive-Network" class="headerlink" title="Recusive Network"></a>Recusive Network</h2><p>&emsp;&emsp;Recursive Network 是 Recurrent Network 更 Generalize 的版本。Recurrent Network 是 Recursive Network 的一个特殊的例子，如果翻译成中文的话，实际上名字都一样。所以可以称之为递归式网络。以下是 RNN 和 Recursive Network 的对比图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Recursive Network示意图.jpg" alt="Recursive Network示意图"></p>
<p>&emsp;&emsp;在做  Recursive Network 之前，需要考虑输入的序列的结构。图中将 <script type="math/tex">x_1</script> 和 <script type="math/tex">x_2</script> 一同输入进一个 function，但是其实可以不这么做，具体要怎么输入，取决于输入数据的结构。<strong>而由于 f 与 f 前后相接，所以在写代码时需要预先做好设计</strong>。<br>&emsp;&emsp;举个具体的例子，要判断“not very good”包含什么情绪，可以先使用语法解析，将句子结构化，然后根据句子的语法结构来使用 Recursive Network 进行训练，如下图：<br>&emsp;&emsp;“very”的词向量和“good”的词向量一同放入 f 中训练，我们可以将得到的向量看做是“very good”的意思。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/根据句子语法结构训练1.jpg" alt="根据句子语法结构训练1" title="根据句子语法结构训练1"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/根据句子语法结构训练2.jpg" alt="根据句子语法结构训练2" title="根据句子语法结构训练2"></p>
<p>&emsp;&emsp;当然两个词向量不能是简单的相加，具体做法可以自行选择。最简单的做法可以参考下图的上半部分，而下图的下半部分被称为 <strong>Recursive Neural Tensor Network</strong>，总而言之就是一个很复杂的做法来解决两个词向量不仅仅是进行简单的拼接或者相加。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Recursive Neural Tensor Network.jpg" alt="Recursive Neural Tensor Network"></p>
<p>&emsp;&emsp;对于 f 还有其他的做法，如 Matrix-Vector Recursive Network，<a href="https://arxiv.org/pdf/1503.00075.pdf" target="_blank" rel="noopener">Tree LSTM 2015</a> 等。具体就不记了，以后可以查 Recursive Network 相关论文。</p>
<h1 id="Conditional-Generation-by-RNN-amp-Attention"><a href="#Conditional-Generation-by-RNN-amp-Attention" class="headerlink" title="Conditional Generation by RNN &amp; Attention"></a>Conditional Generation by RNN &amp; Attention</h1><p>&emsp;&emsp;注意本文讲的是 RNN <strong>入门</strong>，而下面的部分也只是讲普通的 RNN Generation，甚至连 decoder 部分都没用。下图是生成文字，其实也可以生成图片、音频等，我就不一一截图了，第二张图将这些<strong>想法</strong>及其<strong>论文</strong>都汇总了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/一个简单的Generation.jpg" alt="一个简单的Generation"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Generation汇总.jpg" alt="Generation汇总"></p>
<h2 id="Conditional-Generation"><a href="#Conditional-Generation" class="headerlink" title="Conditional Generation"></a>Conditional Generation</h2><p>&emsp;&emsp;但是在真实的场景中，我们不仅仅是希望只生成随机的句子，我们更偏向于生成一些基于某些条件的句子，比如：当看见一张一个人正在跳舞的图片，我们希望电脑生成“A young girl is dancing”；当给予一个条件“Hello”时，我们希望电脑生成“Hello, nice to see you.”。<br>&emsp;&emsp;一个实际的例子，我们可以将一张图片输入进 CNN，从而产生一个向量，再把该向量输入进 decoder 部分，最后生成句子。如下图所示，其他类型的<strong>条件生成</strong>也类似。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Image Caption Generation.jpg" alt="Image Caption Generation" title="Image Caption Generation"></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>&emsp;&emsp;将 <script type="math/tex">z_0</script> 与 <script type="math/tex">h_1 h_2 h_3 h_4</script> 分别做一次 match，至于 match 怎么计算可以看下图右边。<br>&emsp;&emsp;计算步骤可以参考下列公式：</p>
<script type="math/tex; mode=display">
\begin{align}
    h & = [h^1, h^2, h^3, h^4] \\
    s & = h^T z \\
    c^0 & = h^t s \\
\end{align}</script><p>&emsp;&emsp;attention score 的计算公式可以由自己设计，下图使用的是 <script type="math/tex">h^T W z</script>，有兴趣的话，<a href="https://yan624.github.io/·学习笔记/AI/nlp/CS224n学习笔记.html#attention">这里</a>介绍了三个。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制.jpg" alt="Attention机制"></p>
<p>&emsp;&emsp;然后获得 <script type="math/tex">a^1_0 a^2_0 a^3_0 a^4_0</script>，之后将它们输入 softmax 层（有实验发现其实不经过 softmax 层也可以，甚至效果更好），最后将所有 a 分别乘上它们对应的向量并且相加，得到一个向量 <script type="math/tex">c^0</script>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制计算score.jpg" alt="Attention机制计算score"></p>
<p>&emsp;&emsp;使用 Attention 机制计算完毕后，将向量 <script type="math/tex">c^0</script> 输入进 decoder 即可，接下来的计算都是以此类推。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制计算完毕后输入到decoder.jpg" alt="Attention机制计算完毕后输入到decoder"></p>
<h3 id="Attention应用到Speach-Recognition"><a href="#Attention应用到Speach-Recognition" class="headerlink" title="Attention应用到Speach Recognition"></a>Attention应用到Speach Recognition</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention for Speach Recognition.jpg" alt="Attention for Speach Recognition"></p>
<h2 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h2><p>&emsp;&emsp;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">论文地址</a>，<a href="https://www.bilibili.com/video/av9770302/?p=8" target="_blank" rel="noopener">视频地址</a>43:00开始。<br>&emsp;&emsp;Memory Network 最先被用在 Reading Comprehension，说白了就是一个 Attention 机制。下图就是一个简易的 <strong>Memory Network</strong>。</p>
<ol>
<li>首先将 document 由多个句子组成，句子由 vector x 表示。具体如何表示的问题，可以由自定义解决，如 bag of word 或者由词向量表示；</li>
<li>query 就是问题，也由 vector q 表示；</li>
<li>使用 q 对每个句子做 attention 得到 match score <script type="math/tex">\alpha</script>，然后使用 <script type="math/tex">\alpha</script> 和 x 做 weighted sum；</li>
<li>最后将 weighted sum 后的 vector 和 vector q 都丢到 DNN 中，得到答案。</li>
</ol>
<p>&emsp;&emsp;注：这是在做阅读理解。document -&gt; vector 等于 input(I) 和 generalization(G)，attention 等于 output(O)，生成答案等于 response(R)。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/一个简易的Memory Network.jpg" alt="一个简易的Memory Network" title="一个简易的Memory Network"></p>
<p>&emsp;&emsp;Memory Network 还有更复杂的版本，即 attention 的 vector 和抽取信息的 vector 并不需要是同一个，如下图所示。</p>
<ol>
<li>将 document  表示为句子时，使用两组向量。一组用于计算 match score，一组用于 weighted sum。</li>
<li>其他的步骤都差不多，但是有一个地方不一样。在 weighted sum 得到一个 vector 之后，可以和 q 加在一起，得到一个新的 q，再重复 步骤 1。而且这个步骤可以做很多次，做完之后再输入进 DNN 获取答案。这个步骤被称之为 Hopping，注意从 document 抽取的两组 vector 在 hopping 的时候，可以是不一样的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/更复杂的memory network.jpg" alt="更复杂的memory network"></li>
</ol>
<h2 id="Neural-Turing-Machine"><a href="#Neural-Turing-Machine" class="headerlink" title="Neural Turing Machine"></a>Neural Turing Machine</h2><h2 id="Tips-for-Generation"><a href="#Tips-for-Generation" class="headerlink" title="Tips for Generation"></a>Tips for Generation</h2><p>&emsp;&emsp;这里听不太懂，跳过了。有 Beam Search 之类的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/对Generation的建议1.jpg" alt="对Generation的建议1"></p>
<h1 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h1><p>&emsp;&emsp;<a href="https://pdfs.semanticscholar.org/eb5c/1ce6818333560d0d3247c0c74985ef295d9d.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;举一个简单的例子助于理解 Pointer Network。在二维坐标系中任意给出 4 个点，我们的目标是找到几个点，将它们连起来形成一个封闭圈，剩下的那几个点要正好在这个封闭圈之中，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/助于理解Pointer Network的一个例子.jpg" alt="助于理解Pointer Network的一个例子"></p>
<p>&emsp;&emsp;当然，这肯定已经有一些算法可以解了，比如在坐标系中计算距离。但是今天我们使用硬 train 一发的方法，即不管三七二十一将它输入到神经网络里面训练。首先制造一些训练数据，然后给 encoder-deocder 训练。<strong>具体的训练步骤为</strong>：输入点的坐标，输出 one-hot 表示 <code>{1,2,3,4,END}</code> 的五维向量，碰到 END 则代表解码完毕。<br>&emsp;&emsp;但结果是网络训练不起来，因为在上述的例子中我们只输入了 4 个点，我们的目的是得到 1-4 个点。但是如果我们的测试数据是输入 400 个点呢？那么我们也只会得到 1-4 个点，因为 <code>{1,2,3,4,END}</code> 是预先定义好的。你可能会想那就多定义一点啊，但是下次我要是输入 4000 个点呢？要是 40000 个点呢？总有你无法预先定义的时候。<br>&emsp;&emsp;所以我们需要 <strong>Pointer Network</strong> 来<strong>动态的改变类别</strong>（具体做法详见下一小节），注意我这里直接说成类别了，我们可以把 decoder 部分看作是多元分类的工作，如输出 4000 个点，就是 4000 元分类。<br>&emsp;&emsp;<strong>上面的例子其实是 Pointer Netwoek 论文中的一个例子，但是对于这个例子来说，使用 Pointer Network 其实没多大意义，因为问题本身有更简单的解法，下面说一下有意义的用途。</strong><br>&emsp;&emsp;Pointer Network 应用于 <strong>Summarization</strong>，<a href="https://www.aclweb.org/anthology/P17-1099" target="_blank" rel="noopener">论文地址</a>。给定一篇文档，让机器做出总结。对于此类问题，我们会碰到很多<strong>生僻的地名、人名</strong>等等字词。我们可以使用 Pointer Network 来解决这个问题。<br>&emsp;&emsp;下图就是做法，整张图的意思就是在做文本摘要的工作，输入一个句子，输出摘要。先不看中间的黄色圆圈 <script type="math/tex">p_{gen}</script>，看看其他部分（红黄两部分）就是很普通的 encoder-decoder。但是对于这个 encoder-decoder 来说，词表中并没有 <em>Aregentina</em> 这个单词。那么我们就可以使用 Pointer Network，这个 <script type="math/tex">p_{gen}</script> 就是概率（具体描述见下一节）。最后结果就是我们将注意力关注到 <em>Aregentina</em> 这个单词。当然对于 encoder-decoder 这部分的工作也是要做的，我们可以将两个结果加起来，从而判断出最终要产生哪个单词，做法详见原论文。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Pointer Network for Summarization.jpg" alt="Pointer Network for Summarization"></p>
<p>&emsp;&emsp;还可用于 <strong>machine learning</strong>、<strong>chatbot</strong> 等。</p>
<h2 id="具体做法"><a href="#具体做法" class="headerlink" title="具体做法"></a>具体做法</h2><p>&emsp;&emsp;具体的实现就是像下图一样，首先在输入的序列之前加入一个 END 序列，然后将 decoder 删掉。我们还是使用 Attention 机制计算每个序列的 attention score，但是这次的 score 不再乘上它对应的向量，而是直接当做向量输出，意思就是把所以的 score 做一次 max，最大的就输出 1。而<strong>停止条件就是 END 这个序列的 score 是最大的，即为 1 就停止训练。</strong><br>&emsp;&emsp;这样的做法乍一看好像无法理解，我解释一下。由于 encoder 是对序列的长度不敏感的，也就是说如果预先定义的类别是 40 维，而我输入 400 个点，那么对于 encoder 来说，它可以增加神经元的数量从而使得 400 个点<strong>正好</strong>全部输入进 encoder。但是对于 decoder 来说，它输出只能是 40 维。<strong>那么 Pointer Network 的做法是将 decoder 删除，把输出的工作也交给 encoder 去做。所以我输入 400 个点，自然也就可以输出 400 维的类别</strong>（这里应该是 401 维，因为还有一个 END 序列）。看下图的 encoder，<script type="math/tex">h^4</script> 的分数是 0.7，所以我们的输出就是 4，当然对于向量来说就是 <script type="math/tex">(0, 0, 0, 0, 1)</script>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Pointer Network的做法.jpg" alt="Pointer Network的做法"></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>git学习记录</title>
    <url>/%C2%B7zcy/git%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html</url>
    <content><![CDATA[<p>本文是在学习<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">该教程</a>时/后做的笔记。<br>我现在用git基本都是用<a href="https://desktop.github.com/" target="_blank" rel="noopener">Github Desktop</a>，前面的是下载地址。用起来方便又快捷。事实上我也不会用git的命令o(<em>￣︶￣</em>)o所以今天稍微学一下。</p>
<h1 id="git-init"><a href="#git-init" class="headerlink" title="git init"></a>git init</h1><p>切换到想要创建仓库的文件夹，执行命令<code>git init</code>就会在该文件夹下创建一个.git的文件夹，这个文件夹是隐藏的。</p>
<h1 id="git-add-git-commit"><a href="#git-add-git-commit" class="headerlink" title="git add/git commit"></a>git add/git commit</h1><p>使用命令<code>git add whatever.txt</code>将文件添加到仓库。使用命令<code>git commit -m &quot;wrote a file&quot;</code>将文件提交到仓库，-m后面的是描述这份文件你改了什么。其实就是相当于desktop的一个按钮，按一下就把全部有改动文件都提交了。<br>这样就完成了提交一份文件。这里就会有疑问了，为什么设计成先add再commit？直接commit不就行了？因为commit可以提交多份文件，你可以使用add命令一份一份地添加文件，再使用commit一次性提交到仓库。<br>该命令指示推送到本地仓库，并非远程仓库。<br><a id="more"></a></p>
<h1 id="git-status"><a href="#git-status" class="headerlink" title="git status"></a>git status</h1><p>查看仓库当前的修改状态</p>
<h1 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h1><p>发现某份文件被修改了，但是忘记改了什么怎么办？使用命令<code>git diff modified_file.txt</code>查看，它会显示文件哪里被修改了。diff就是difference的意思。</p>
<h1 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h1><p>git可以记住你的历史提交版本，如果有一天电脑损坏，自己干了什么完全忘记，可以使用<code>git log</code>命令。它会显示以前所有的历史记录。这条命令会显示很详细的信息，但是就是因为信息太详细了，人可能看不过来，可以加上<code>--pretty=oneline</code>来限制。类似<code>c3fca95239a4bbe21ee2991e0a914fb522060e74</code>这种是版本号（commit id）。</p>
<h1 id="git-rest"><a href="#git-rest" class="headerlink" title="git rest"></a>git rest</h1><p>该命令可以回退版本。<code>git reset --hard HEAD^</code>，命令里的HEAD代表当前版本，^代表上一个版本，如果想要回退至前100个版本，可以使用HEAD~100。<br>也可以直接指定commit id，如<code>git reset --hard c3fca95239a4bbe21ee2991e0a914fb522060e74</code>，commit id可以不写全，写个开头就行了<code>git reset --hard c3fca</code><br>注意回退版本后，如果关闭git bash那么就无法查询到该版本之后的所有版本。</p>
<h1 id="git-reflog"><a href="#git-reflog" class="headerlink" title="git reflog"></a>git reflog</h1><p>该命令记住了你每一步操作，如果回退版本后后悔了，可以使用该命令查询以前的commit id。</p>
<h1 id="git-checkout-—filename"><a href="#git-checkout-—filename" class="headerlink" title="git checkout —filename"></a>git checkout —filename</h1><p>把文件夹在工作区的修改全部撤销。总之，就是让这个文件回到最近一次git commit或git add时的状态。<br>参考<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374831943254ee90db11b13d4ba9a73b9047f4fb968d000" target="_blank" rel="noopener">文章</a></p>
<h1 id="git-rm"><a href="#git-rm" class="headerlink" title="git rm"></a>git rm</h1><p>删除文件，与linux命令类似。</p>
<h1 id="git-remote"><a href="#git-remote" class="headerlink" title="git remote"></a>git remote</h1><p>将本地的仓库和远程的仓库关联，使用命令<code>git remote add origin git@github.com:github_account_name/repository_name.git</code><br>注意将github_account_name和repository_name分别替换成github账号名和仓库名。添加关联后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的。下一步，就可以把本地库的所有内容推送到远程库上。</p>
<h1 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h1><p><code>git push -u origin master</code></p>
<blockquote>
<p>把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。<br>由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。<br>从现在起，只要本地作了提交，就可以通过命令：<br><code>git push origin master</code><br>把本地master分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！</p>
</blockquote>
<h1 id="git-clone"><a href="#git-clone" class="headerlink" title="git clone"></a>git clone</h1><p>上面说了将本地仓库和远程仓库关联，并将本地仓库的文件推送到远程仓库，那么自然也可以从远程仓库clone文件到本地仓库。<br>使用命令：<code>git clone git@github.com:github_account_name/repository_name.git</code><br>还可以从<a href="https://github.com/yan624/yan624.github.io.git这样的地址克隆" target="_blank" rel="noopener">https://github.com/yan624/yan624.github.io.git这样的地址克隆</a></p>
<h1 id="对分支的管理"><a href="#对分支的管理" class="headerlink" title="对分支的管理"></a>对分支的管理</h1><p>字太多，不想打了。看下面教程。<br><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840038939c291467cc7c747b1810aab2fb8863508000" target="_blank" rel="noopener">教程</a></p>
<h2 id="git-checkout-b-branch-name"><a href="#git-checkout-b-branch-name" class="headerlink" title="git checkout -b branch_name"></a>git checkout -b branch_name</h2><p>创建名为branch_name的分支并切换到该分支，-b参数代表切换。</p>
<h2 id="git-branch"><a href="#git-branch" class="headerlink" title="git branch"></a>git branch</h2><p>查看当前分支，如果分支之前有*就代表这个分支是主分支。</p>
<h2 id="git-merge-branch-name"><a href="#git-merge-branch-name" class="headerlink" title="git merge branch_name"></a>git merge branch_name</h2><p>合并分支</p>
<h2 id="git-branch-d-branch-name"><a href="#git-branch-d-branch-name" class="headerlink" title="git branch -d branch_name"></a>git branch -d branch_name</h2><p>删除名为branch_name分支</p>
<h2 id="合并分支发生冲突"><a href="#合并分支发生冲突" class="headerlink" title="合并分支发生冲突"></a>合并分支发生冲突</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840202368c74be33fbd884e71b570f2cc3c0d1dcf000" target="_blank" rel="noopener">解决办法</a></p>
<h2 id="强大的分支功能"><a href="#强大的分支功能" class="headerlink" title="强大的分支功能"></a>强大的分支功能</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013758410364457b9e3d821f4244beb0fd69c61a185ae0000" target="_blank" rel="noopener">创建分支的策略</a></p>
<h2 id="bug分支。将当前工作暂存，先修改出现的bug"><a href="#bug分支。将当前工作暂存，先修改出现的bug" class="headerlink" title="bug分支。将当前工作暂存，先修改出现的bug"></a>bug分支。将当前工作暂存，先修改出现的bug</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137602359178794d966923e5c4134bc8bf98dfb03aea3000" target="_blank" rel="noopener">暂存命令</a></p>
<h2 id="feature分支"><a href="#feature分支" class="headerlink" title="feature分支"></a>feature分支</h2><p>与上面类似，无非概念不同</p>
<h2 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013760174128707b935b0be6fc4fc6ace66c4f15618f8d000" target="_blank" rel="noopener">教程</a></p>
<h2 id="git-rebase"><a href="#git-rebase" class="headerlink" title="git rebase"></a>git rebase</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="使用github"><a href="#使用github" class="headerlink" title="使用github"></a>使用github</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="使用码云"><a href="#使用码云" class="headerlink" title="使用码云"></a>使用码云</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00150154460073692d151e784de4d718c67ce836f72c7c4000" target="_blank" rel="noopener">教程</a></p>
<h1 id="配置文件的更多配置"><a href="#配置文件的更多配置" class="headerlink" title="配置文件的更多配置"></a>配置文件的更多配置</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00150154460073692d151e784de4d718c67ce836f72c7c4000" target="_blank" rel="noopener">教程</a></p>
]]></content>
      <categories>
        <category>assorted</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>ViewPager无法刷新数据</title>
    <url>/IT-stuff/android/ViewPager%E6%97%A0%E6%B3%95%E5%88%B7%E6%96%B0%E6%95%B0%E6%8D%AE.html</url>
    <content><![CDATA[<p>实现ViewPager刷新数据功能，在网上找了很多资料都已经过时了。<br>由于本人并不是android开发出身，完全是做app玩的。所以很多术语都不知道，如果看不懂就算了。。。<br>实现PagerAdapter类，我命名为HomePagerAdapter<br><a id="more"></a><br><figure class="highlight aspectj"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HomePagerAdapter</span> <span class="keyword">extends</span> <span class="title">PagerAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;View&gt; pageView;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HomePagerAdapter</span><span class="params">(ArrayList&lt;View&gt; pageView)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pageView = pageView;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mChildCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">notifyDataSetChanged</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mChildCount = getCount();</span><br><span class="line">        <span class="keyword">super</span>.notifyDataSetChanged();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getItemPosition</span><span class="params">(Object object)</span>   </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> ( mChildCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            mChildCount --;</span><br><span class="line">            <span class="keyword">return</span> POSITION_NONE;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">return</span> <span class="keyword">super</span>.<span class="title">getItemPosition</span><span class="params">(object)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//获取当前窗体界面数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="function"><span class="keyword">return</span> pageView.<span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//判断是否由对象生成界面</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isViewFromObject</span><span class="params">(View arg0, Object arg1)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="keyword">return</span> arg0==arg1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">destroyItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position, Object object)</span> </span>&#123;</span><br><span class="line">        container.removeView(pageView.get(position));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Object <span class="title">instantiateItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position)</span> </span>&#123;</span><br><span class="line">        View view = pageView.get(position);</span><br><span class="line">        container.addView(view);</span><br><span class="line">        <span class="keyword">return</span> view;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">finishUpdate</span><span class="params">(ViewGroup container)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(container.getChildCount() == <span class="number">0</span>)&#123;</span><br><span class="line">            pageView.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意destroyItem、instantiateItem方法，PagerAdapter中还有两个同名的方法，但是已被废弃，注意参数不同。<br>实现刷新数据主要在destroyItem、instantiateItem、finishUpdate三个方法。其他的方法其实别人的教程也写了，但是我这三个方法是我自己研究的，我没见别人写过。</p>
<p>解释流程。<br>第一步，改变数据，我是将数据保存在了<code>private ArrayList&lt;View&gt; pageView;</code>中。<br>第二步，调用<code>adapter.notifyDataSetChanged();</code>方法，它首先会销毁item，即调用<code>destroyItem(ViewGroup container, int position, Object object)</code>方法。随即调用<code>instantiateItem(ViewGroup container, int position)</code>方法。<br>一般来说大家都是这么干的，因为将数据改变后，调用<code>adapter.notifyDataSetChanged();</code>方法。直觉认为这么做合乎常理。<br>但是这里注意一点，假设<code>private ArrayList&lt;View&gt; pageView;</code>中原先保存两个View，改变数据将这个View删除，从新添加三个新的View，那么在<code>destroyItem(ViewGroup container, int position, Object object)</code>方法中，它无法删除，仔细看里面的代码<code>container.removeView(pageView.get(position));</code>，发现它是通过position这个索引获取对象，再在container容器中通过对象查找删除。那么问题来了，你之前已经将两份View删除了，它还怎么通过position获取到呢？所以在这一步出了问题。<br>当然这一步出了问题后，后面的创建页面步骤更是稀巴烂。</p>
<p>正确步骤如下：<br><figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将页面刷新，渲染新的数据集</span></span><br><span class="line">homePagerAdapter.notify<span class="constructor">DataSetChanged()</span>;</span><br><span class="line">change<span class="constructor">Data(<span class="params">inflater</span>, <span class="params">data</span>)</span>;</span><br><span class="line">homePagerAdapter.notify<span class="constructor">DataSetChanged()</span>;</span><br></pre></td></tr></table></figure></p>
<p>第一步，不要更改数据，直接调用<code>homePagerAdapter.notifyDataSetChanged();</code>，目的是让其删除原先的view。<br>第二步，更改数据。<br>第三步，再次调用<code>homePagerAdapter.notifyDataSetChanged();</code>，完成页面的创建。由于container中已经没有view了，所以删除那个步骤做了也等于没做，但是由于数据已经更新页面还是会被创建出来。</p>
<p>最后强调用一点。在HomePagerAdapter类中一个<code>finishUpdate(ViewGroup container)</code>方法，注意看里面的代码。<strong>以上的所有步骤，全部依赖于这几句代码。</strong><br>上面第一步说到直接调用notifyDataSetChanged()方法目的是删除原先的view，但是view删除后，你必须将<code>private ArrayList&lt;View&gt; pageView;</code>中的数据也删除。<strong>这里补充一点，pageView内是我创建的View，而container中是android自己维护的界面</strong>，我也不知道怎么称呼，就将其称为界面吧。<br>在finishUpdate()方法中判断，如果container中已经没有界面了，那就直接移除pageView中所有的数据，也就是算更新数据了。值得注意的是，这里面逻辑及其复杂，这行清空数据的代码，只有放在<code>finishUpdate(ViewGroup container)</code>中执行，并且必须加上那个if条件判断，app才能正常运行。<br><!-- more --></p>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>android</category>
      </categories>
  </entry>
  <entry>
    <title>在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式</title>
    <url>/assorted/hexo/%E5%9C%A8%E5%86%99%E4%BA%86%E5%A4%A7%E9%87%8F%E5%86%85%E5%AE%B9%E4%BB%A5%E5%8F%8A%E5%A4%A7%E9%87%8F%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%90%8E%EF%BC%8CmathJax%E6%97%A0%E6%B3%95%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BE%BE%E5%BC%8F.html</url>
    <content><![CDATA[<p>之前写的数学表达式明明可以渲染，但是接下去隔了n行的数学表达式无法渲染。推测是因为单行数学表达式在文字前面换行。<br>比如说：</p>
<blockquote>
<p>文字文字文字文字：·￥￥·<br>该表达式渲染正常。</p>
</blockquote>
<p>如果，</p>
<blockquote>
<p>文字文字文字文字：<br>·￥￥·<br>那么下面的数学表达式将全部无法渲染。<br><a id="more"></a></p>
</blockquote>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>运用其他的插件，在hexo中添加提示弹窗</title>
    <url>/assorted/hexo/%E8%BF%90%E7%94%A8%E5%85%B6%E4%BB%96%E7%9A%84%E6%8F%92%E4%BB%B6%EF%BC%8C%E5%9C%A8hexo%E4%B8%AD%E6%B7%BB%E5%8A%A0%E6%8F%90%E7%A4%BA%E5%BC%B9%E7%AA%97.html</url>
    <content><![CDATA[<p>由于我写了很多学习记录，但是这些都是自己看书或者看视频学来的。万一有错误的地方正好被人看见，他又是新手，误以为我的是对的，这样就不好了。所以准备做一个提示弹窗，在所有的带有“学习笔记”的标签的文章中自动弹出提示。</p>
<ol>
<li>下载一个自己喜欢的弹窗插件，可以去<a href="http://www.jq22.com/" target="_blank" rel="noopener">jQuery插件库</a>找。</li>
<li>将css和js放在next主题下的source文件夹中，如下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/next%E4%B8%BB%E9%A2%98%E7%9B%AE%E5%BD%95.jpg" alt="next主题目录"><br>css文件放入css文件，js文件放入js文件夹。我自己创了一个spop的文件夹，用于单独放置我的弹窗插件。</li>
<li>打开layout文件夹，进入_macro文件夹，找到post.swig文件。搜索class=”post-block”，这个标签的位置在下图箭头所指的地方：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/%E5%BC%B9%E7%AA%97%E6%8F%92%E4%BB%B6%E4%BB%A3%E7%A0%81%E5%86%99%E5%85%A5%E7%9A%84%E4%BD%8D%E7%BD%AE.png" alt="弹窗插件代码写入的位置"><br>如果打开了这个文件，找了post-block标签，可以看到标签内部第一行代码为<code>&lt;link itemprop=&quot;mainEntityOfPage&quot; href=&quot;{ config.url }{ url_for(post.path) }&quot;/&gt;</code>。由于hexo渲染问题我将href属性里的值去掉了一对{}。<a id="more"></a>
将下面的代码放在上述代码上面或者下面即可。<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="name">for</span></span> tag <span class="keyword">in</span> post.tags %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml">	<span class="comment">&lt;!--判断该文章是否为学习笔记--&gt;</span></span></span><br><span class="line"><span class="xml">	</span><span class="template-tag">&#123;% <span class="name"><span class="name">if</span></span> tag.name == '学习笔记' and !is_home() %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml">  		<span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"/css/spop/spop.min.css"</span>&gt;</span></span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"/js/spop/spop.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span></span><br><span class="line"><span class="xml">			spop(&#123;</span></span><br><span class="line"><span class="xml">				template: '<span class="tag">&lt;<span class="name">h4</span> <span class="attr">class</span>=<span class="string">"spop-title"</span>&gt;</span>注意<span class="tag">&lt;/<span class="name">h4</span>&gt;</span>此文章仅为博主的学习笔记，其中可能含有极大的理论错误。',</span></span><br><span class="line"><span class="xml">				group: 'tips',</span></span><br><span class="line"><span class="xml">				position  : 'bottom-center',</span></span><br><span class="line"><span class="xml">				style: 'success',</span></span><br><span class="line"><span class="xml">				autoclose: 5500,</span></span><br><span class="line"><span class="xml">				onOpen: function () &#123;</span></span><br><span class="line"><span class="xml">					//这里设置灰色背景色</span></span><br><span class="line"><span class="xml">				&#125;,</span></span><br><span class="line"><span class="xml">				onClose: function() &#123;</span></span><br><span class="line"><span class="xml">					//这里可以取消背景色</span></span><br><span class="line"><span class="xml">					spop(&#123;</span></span><br><span class="line"><span class="xml">						template: 'ε = = (づ′▽`)づ',</span></span><br><span class="line"><span class="xml">						group: 'tips',</span></span><br><span class="line"><span class="xml">						position  : 'bottom-center',</span></span><br><span class="line"><span class="xml">						style: 'success',</span></span><br><span class="line"><span class="xml">						autoclose: 1500</span></span><br><span class="line"><span class="xml">					&#125;);</span></span><br><span class="line"><span class="xml">				&#125;</span></span><br><span class="line"><span class="xml">			&#125;);</span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="xml">	</span><span class="template-tag">&#123;% <span class="name"><span class="name">endif</span></span> %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="name">endfor</span></span> %&#125;</span><span class="xml"></span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>is_home()是next主题的方法，用于判断当前页面是否在主页。因为主页一次性加载了所有的文章，如果不加这个方法，会在主页弹出无数个弹窗。<br>最终效果如下图所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/%E6%8F%90%E7%A4%BA%E5%BC%B9%E7%AA%97%E6%95%88%E6%9E%9C%E5%9B%BE.jpg" alt="提示弹窗效果图"><br><!-- more --></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>Android开发，使用腾讯云的API请求对象存储中的资源始终失败</title>
    <url>/IT-stuff/android/Android%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%BF%E7%94%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E7%9A%84API%E8%AF%B7%E6%B1%82%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E5%A7%8B%E7%BB%88%E5%A4%B1%E8%B4%A5.html</url>
    <content><![CDATA[<ol>
<li>腾讯云api内部在调用时，把url转义了。我的链接是<a href="http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt?abcdefg，它内部给我转义成http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt%3Fabcdefg，就是把&quot;?&quot;转义成了&quot;%3F&quot;。总而言之，我使用api一直获取不到资源，然后我在浏览器上试验了一下。发现把%3F改回?就可以访问了，实际上应该不是这样，反正就给我产生了误导。我想尽办法都不能将其转义回来，最后只好放弃。腾讯云api内部肯定自己转义了一下，真的坑爹。" target="_blank" rel="noopener">http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt?abcdefg，它内部给我转义成http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt%3Fabcdefg，就是把&quot;?&quot;转义成了&quot;%3F&quot;。总而言之，我使用api一直获取不到资源，然后我在浏览器上试验了一下。发现把%3F改回?就可以访问了，实际上应该不是这样，反正就给我产生了误导。我想尽办法都不能将其转义回来，最后只好放弃。腾讯云api内部肯定自己转义了一下，真的坑爹。</a></li>
<li>尝试自己写代码请求资源，结果发现如果url的协议是https就可以访问到资源了。这可能是android的问题，于是我又使用腾讯的api，配置更改如下：<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">CosXmlServiceConfig serviceConfig = <span class="keyword">new</span> <span class="constructor">Builder()</span></span><br><span class="line">				.is<span class="constructor">Https(<span class="params">true</span>)</span></span><br><span class="line">                .set<span class="constructor">Region(<span class="params">region</span>)</span></span><br><span class="line">                .set<span class="constructor">Debuggable(<span class="params">true</span>)</span></span><br><span class="line">                .builder<span class="literal">()</span>;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>将isHttps设为ture，协议就改为了https。可是又报了另一个错：The specified key does not exist.它说我密钥不存在。</p>
<ol>
<li>如果使用http协议访问就会说无法解析域名，总之用腾讯云的api无法访问到资源就对了。</li>
<li>放弃腾讯云的api，自己手写代码去请求资源！</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>android</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>对神经网络整体的理解</title>
    <url>/%C2%B7zcy/AI/dl/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<div class="note info">
            <p>本文虽然理了一遍神经网络的知识点，但还是有些地方不明白，文中对此进行了提问。</p>
          </div>
<h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th>描述的内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2~4</td>
<td>神经网络和深度学习的发展史。</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>从二元分类开始。</td>
</tr>
<tr>
<td style="text-align:center">6~10</td>
<td>浅层神经网络的介绍。如神经网络中一些参数代表的意思、激活函数、梯度下降、随机初始化。</td>
</tr>
<tr>
<td style="text-align:center">11~15</td>
<td>深层神经网络的介绍。正向传播和反向传播中向量化后的计算、参数和超参数、神经网络和大脑的关系。</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td>一个Simple NN的例子。</td>
</tr>
<tr>
<td style="text-align:center">17~22</td>
<td>深度学习的实用性层面。数据切分、偏差与方差、正则化、dropout、其他正则化方法、均值归一化、梯度消失和梯度爆炸、梯度检验。</td>
</tr>
<tr>
<td style="text-align:center">23~25</td>
<td>一些优化算法。Mini-batch、指数加权平均、Momentum、RMSprop、Adam、Adagrad。</td>
</tr>
<tr>
<td style="text-align:center">26~29</td>
<td>超参数调试、Batch正则化、激活函数以及一些深度学习框架。</td>
</tr>
<tr>
<td style="text-align:center">30~end</td>
<td>本文略长，后序的文章请看对应章节的链接。</td>
</tr>
</tbody>
</table>
</div>
<a id="more"></a>
<h1 id="神经网络和深度学习的发展"><a href="#神经网络和深度学习的发展" class="headerlink" title="神经网络和深度学习的发展"></a>神经网络和深度学习的发展</h1><p>TODO</p>
<h1 id="神经网络和深度学习的关系"><a href="#神经网络和深度学习的关系" class="headerlink" title="神经网络和深度学习的关系"></a>神经网络和深度学习的关系</h1><p>TODO</p>
<h1 id="为什么要深度学习"><a href="#为什么要深度学习" class="headerlink" title="为什么要深度学习"></a>为什么要深度学习</h1><p>TODO</p>
<h1 id="从二元分类开始"><a href="#从二元分类开始" class="headerlink" title="从二元分类开始"></a>从二元分类开始</h1><p>暂时省略，因为这里已经会了。</p>
<h1 id="神经网络的表示"><a href="#神经网络的表示" class="headerlink" title="神经网络的表示"></a>神经网络的表示</h1><p>规定如下，l：第几层；w：权重值；b：偏差；z：输出值；a：激活值；i，j：都代表第几个神经元，如<script type="math/tex">w^l_i</script>代表第l层的第i个权重值；W：向量化后的权重值；Z：向量化后的输出值；A：向量化后的激活值；<script type="math/tex">\alpha</script>：学习速率；<script type="math/tex">\lambda</script>：正则化项；</p>
<p>如果输出值z和激活值a无法理解或者区分，没关系，继续往下看就知道了。<br>如下图所示，一般规定input layer为第0层，不算入神经网络的层数中，所以下图是一个三层神经网络架构。</p>
<ol>
<li>input layer的输入值被称为x，下图一共有三个输入所以分别被称为<script type="math/tex">x_1\ x_2\ x_3</script>。为了方便起见，可以将input layer的值x以<script type="math/tex">a^0</script>来代替，下面解释a代表什么。</li>
<li>hidden layer中的值被称为a——<strong>激活值</strong>（activations），图中有四个神经元，所以分别被称为<script type="math/tex">a^1_1\ a^1_2\ a^1_3</script>，上标代表着所在神经网络中的第几层，下标代表着所在层中的第几个神经元。如果表示成向量形式就是<script type="math/tex; mode=display">
\begin{pmatrix}
 x_1\\
 x_2\\
 x_3\\
\end{pmatrix} = 
\begin{pmatrix}
 a^0_1\\
 a^0_2\\
 a^0_3\\
\end{pmatrix} 和
\begin{pmatrix}
 a^1_1\\
 a^1_2\\
 a^1_3\\
 a^1_4\\
\end{pmatrix} 和
\begin{pmatrix}
 a^2_1\\
 a^2_2\\
 a^2_3\\
 a^2_4\\
\end{pmatrix} 和
\begin{pmatrix}
 a^3_1\\
\end{pmatrix}</script></li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%8E%9F%E5%9B%BE.jpg" alt="神经网络架构原图"></p>
<h2 id="神经网络中神经元的一些参数的含义，特别解释w的含义"><a href="#神经网络中神经元的一些参数的含义，特别解释w的含义" class="headerlink" title="神经网络中神经元的一些参数的含义，特别解释w的含义"></a>神经网络中神经元的一些参数的含义，特别解释w的含义</h2><p>hidden layer和output layer的每个神经元都有几个参数。分别为<script type="math/tex">w^l\ b^l</script>，对照上图，这里的<script type="math/tex">w^l</script>是一个(4,3)的矩阵，<script type="math/tex">b^l</script>是一个(4,1)的向量。解释如下：</p>
<script type="math/tex; mode=display">
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4\\
\end{cases}</script><p>可以看到一个公式中有三个w和一个b，一共有四个公式。<script type="math/tex">w^l_{ij}</script>代表第l-1层的第j个神经元到第l层的第i个神经元上的w。如<script type="math/tex">w^1_{12}</script>代表第0层的第2个神经元到第1层的第1个神经元上的w。注意这里的i和j实际上是与直觉相反的，也就是说按直觉来看应该是<script type="math/tex">w^l_{ji}</script>才正常。如果对w的表示有疑惑的，可以看<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%ADweight%E7%9A%84%E8%A1%A8%E7%A4%BA%E9%97%AE%E9%A2%98.html">这篇</a>。<br>注意下这里的z是<strong>输出值</strong>，之前一直在说hidden layer中的值是a——激活值，其实a就是将z放到一个<strong>激活函数</strong>（activation function）中得到的一个值，这个激活函数是随用户挑选的，如果不能理解激活函数是什么，就暂时理解为激活函数自己想设成什么就设成什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg" alt="神经网络架构图"></p>
<h1 id="神经网络中的输出是怎么计算的"><a href="#神经网络中的输出是怎么计算的" class="headerlink" title="神经网络中的输出是怎么计算的"></a>神经网络中的输出是怎么计算的</h1><h2 id="以一个样本为例"><a href="#以一个样本为例" class="headerlink" title="以一个样本为例"></a>以一个样本为例</h2><p>第0层是输入层，所以是不需要计算的，x我本来就有，我还计算什么？对吧。从hidden layer1开始到output layer每一层都需要计算一连串的值，下面给出第一层的计算公式：</p>
<script type="math/tex; mode=display">
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1，a^1_1 = \sigma(z^1_1)\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2，a^1_2 = \sigma(z^1_2)\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3，a^1_3 = \sigma(z^1_3)\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4，a^1_3 = \sigma(z^1_4)\\
\end{cases}</script><p>这里的<script type="math/tex">\sigma(z)</script>函数其实就是上面说的<strong>激活函数</strong>，一般来讲<script type="math/tex">\sigma</script>这个符号特指sigmoid function: <script type="math/tex">\frac{1}{1+e^{-z}}</script>。<br>这4行公式其实在上面已经给出部分，每一行包含两个公式，也就是说一个神经元中实际上先得到了z，然后再通过激活函数将z转为a。这里可能会有疑惑，已经得到z了为什么还要用一个函数将z转为a呢？这样不是毫无意义？下面有一部分会具体解释，也可以看下面几篇的解释：<br><a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">神经网络激励函数的作用是什么？有没有形象的解释？</a><br><a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a><br>现在回到本文，正如我上面所说，我一共写了四个公式（激活函数现在暂时不看），所以我要分别计算四个公式，也就是说要计算四次。那么有没有办法只计算一次就得到所有结果呢？答案是<strong>向量化</strong>（vectorization），现在开始用向量化来解决这个问题。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    z^1_1\\
    z^1_2\\
    z^1_3\\
    z^1_4\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&w^1_{12}&w^1_{13}\\
    w^1_{21}&w^1_{22}&w^1_{23}\\
    w^1_{31}&w^1_{32}&w^1_{33}\\
    w^1_{41}&w^1_{42}&w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^0_1\\
    a^0_2\\
    a^0_3\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^1 = w^1 * a^0 + b^1</script><p>以下是整个神经网络的计算过程，也就是说只需要下面6行就可以代替上文占据几个屏幕的内容。</p>
<script type="math/tex; mode=display">
\begin{array}{c|}
    z^1 = w^1 * a^0 + b^1\\
    a^1 = \sigma(z^1)\\
    z^2 = w^2 * a^1 + b^2\\ 
    a^2 = \sigma(z^2)\\
    z^3 = w^3 * a^2 + b^3\\
    a^3 = \sigma(z^3)\\ 
\end{array} =>记为P</script><p>最后一个a就是整个神经网络的输出值，也就是预测值（prediction），也可以用<script type="math/tex">\hat{y}</script>表示，自然<script type="math/tex">\hat{y} = a^3</script>。</p>
<h2 id="向量化计算多个样本"><a href="#向量化计算多个样本" class="headerlink" title="向量化计算多个样本"></a>向量化计算多个样本</h2><p>上面我没有特意地说明其实我们只使用了一个样本，我们一直在使用<script type="math/tex">a^0_1\ a^0_2\ a^0_3</script>，但是<script type="math/tex">a^0_1\ a^0_2\ a^0_3</script>实际上只是<strong>一个</strong>样本。<script type="math/tex">a^0</script>代表的是一个样本，<script type="math/tex">a^0_1</script>代表的是样本中的第一个特征，如果不明白我可以举个例子：<script type="math/tex">a^0_1</script>代表天气样本中的第一个特征——温度，<script type="math/tex">a^0_2</script>代表湿度，<script type="math/tex">a^0_3</script>代表PM2.5，<script type="math/tex">a^0</script>代表整一个天气样本。<br>那么如果有成千上万个样本，总不能使用P计算成千上万次吧。这里再次使用向量化进行计算。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    z^{11}_1&z^{12}_1&\cdots\\
    z^{11}_2&z^{12}_2&\cdots\\
    z^{11}_3&z^{12}_3&\cdots\\
    z^{11}_4&z^{12}_4&\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&w^1_{12}&w^1_{13}\\
    w^1_{21}&w^1_{22}&w^1_{23}\\
    w^1_{31}&w^1_{32}&w^1_{33}\\
    w^1_{41}&w^1_{42}&w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{01}_1&a^{01}_1&\cdots\\
    a^{01}_2&a^{02}_1&\cdots\\
    a^{01}_3&a^{03}_1&\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^1 = w^1 * a^0 + b^1</script><script type="math/tex; mode=display">
\begin{pmatrix}
    z^{21}_1&z^{22}_1&\cdots\\
    z^{21}_2&z^{22}_2&\cdots\\
    z^{21}_3&z^{22}_3&\cdots\\
    z^{21}_4&z^{22}_4&\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^2_{11}&w^2_{12}&w^2_{13}&w^2_{14}\\
    w^2_{21}&w^2_{22}&w^2_{23}&w^2_{24}\\
    w^2_{31}&w^2_{32}&w^2_{33}&w^2_{34}\\
    w^2_{41}&w^2_{42}&w^2_{43}&w^2_{44}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{11}_1&a^{11}_1&\cdots\\
    a^{11}_2&a^{12}_1&\cdots\\
    a^{11}_3&a^{13}_1&\cdots\\
    a^{11}_3&a^{14}_1&\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^2_1\\
    b^2_2\\
    b^2_3\\
    b^2_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^2 = w^2 * a^1 + b^2</script><p>省略号代表后面有无数个样本，同理矩阵相乘也可以只用一个字母表示。上标的第二个数字代表是第几个样本，第一个数字依旧是代表所属第几层。</p>
<h1 id="※-激活函数"><a href="#※-激活函数" class="headerlink" title="※ 激活函数"></a>※ 激活函数</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>激活函数名称</th>
<th>如何选择</th>
</tr>
</thead>
<tbody>
<tr>
<td>sigmoid</td>
<td><strong>输出层</strong>为<strong>二元分类</strong>时选用。对于隐藏层来说，基本不会用 sigmoid 函数，因为现在已经有更好的激活函数<br> <strong>缺点</strong>：1）会产生梯度消失/弥散（<strong>注：sigmoid 不会导致梯度爆炸</strong>），详见下面的 Sigmoid 章节；2）不是原点对称；3）计算 exp 较耗时。</td>
</tr>
<tr>
<td>tanh</td>
<td><strong>优点</strong>：1）原点对称；2）比 sigmoid 快。<br> <strong>缺点</strong>：1）还是有梯度消失</td>
</tr>
<tr>
<td><strong>ReLU</strong></td>
<td>首选 ReLU，如果 ReLU 不行，再换其他形式的 ReLU。<a href="https://github.com/llSourcell/Which-Activation-Function-Should-I-Use" target="_blank" rel="noopener">观点来源</a><br> <strong>优点</strong>：1）解决了部分梯度消失问题；2）收敛速度更快。<br> <strong>缺点</strong>：1）梯度消失的问题没有完全解决，在激活函数（-）部分相当于让神经元死亡，且无法复活。</td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td></td>
</tr>
<tr>
<td>Parametric ReLU</td>
<td></td>
</tr>
<tr>
<td>Randomized ReLU</td>
<td></td>
</tr>
<tr>
<td>ELU</td>
<td></td>
</tr>
<tr>
<td>SELU</td>
<td></td>
</tr>
<tr>
<td>GELU</td>
<td></td>
</tr>
<tr>
<td>Swish</td>
<td></td>
</tr>
<tr>
<td>softmax</td>
<td><strong>输出层</strong>为<strong>多元分类</strong>时选用。只适合于输出层</td>
</tr>
<tr>
<td>log_softmax</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p>&emsp;&emsp;上文中我们一直假设使用 sigmoid function 作为激活函数。但是事实上还有很多其他选择，甚至其他的激活函数比sigmoid funtion效果要更好。<br>&emsp;&emsp;上面讲过<script type="math/tex">\sigma(z)</script>特指 sigmoid function，现在我们将表达式改为：<script type="math/tex">a = g(z)</script>，用g来表示激活函数，它可以是线性的，也可以是非线性的。<br>&emsp;&emsp;引用吴恩达在深度学习视频中的话：</p>
<blockquote>
<p>有一个函数总是比 sigmoid function 表现得更好，就是tanh函数或者叫双曲正切函数，公式为：<script type="math/tex">\frac{e^z-e^{-z}}{e^z+e^{-z}}, \, x\in(-1,1)</script>，在数学上实际是<script type="math/tex">\sigma</script>函数平移后的版本。<br><strong>事实证明，如果将<script type="math/tex">g(z)</script>选为 tanh 函数，效果几乎总比 <script type="math/tex">\sigma(z)</script> 函数要好。</strong></p>
</blockquote>
<p>&emsp;&emsp;有一个例外是 output layer，它还是使用 sigmoid funtion，因为 output layer 跟普通的分类问题没什么区别，它要得到0~1之间的一个概率。<br>&emsp;&emsp;sigmoid function 的值总是位于 0~1 之间，tanh function 的值总是位于 -1~1 之间。<br><div class="note warning">
            <p>&emsp;&emsp;在 CNN 中 ReLu 激活函数可能是首选，但是对于 RNN 来说，首选是 tanh，而不是 relu。</p>
          </div></p>
<h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><p>&emsp;&emsp;Sigmoid 函数将导致梯度消失。梯度的问题，在下面的章节（21 章左右）中会有讲到，但是此部分只讲 sigmoid 函数的梯度消失问题。<br>&emsp;&emsp;首先我们脑中大概有个 sigmoid 函数的图像，这应该很简单。注意函数的两边，我们发现函数曲线在 <script type="math/tex">-\infty</script> 方向越来越接近 0，在 <script type="math/tex">\infty</script> 方向越来越接近 1。以正方向为例，我们可以得知输入 sigmoid 的值越大，sigmoid 的输出值越接近 1。<br>&emsp;&emsp;做一个小小的测试，当输入值为 3 时，输出值为 0.9526，当输入值为 9 时，输出值为 0.9999。可以观察发现，输入值相差巨大的情况下，输出值居然相差无几。当然如果举一个更极端的例子，比如输入 20 和 2000，就会发现输出值都非常接近 1，详见下图。也就是说，输入值相差巨大，但是经过 sigmoid 之后，输出值居然相差无几。<strong>换句话说就是一个值在经过 sigmoid 之后被衰减了</strong>。通俗来讲，我管你是 2000 还是 20000000，只要经过我 sigmoid，你输出就只能是一个接近 1 的数。这样就是被衰减了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/sigmoid梯度消失.jpg" alt="sigmoid梯度消失"></p>
<p>&emsp;&emsp;其次我们又知道对于一个神经网络而言，它一般会叠的很深，四五层都很常见。有了以上两个基础，下面举个具体的例子。<br>&emsp;&emsp;我们知道梯度下降算法的公式是 <script type="math/tex">W = W - \alpha \Delta W</script>，在进行一次梯度下降后，对于 W 来说，<strong>变化</strong>就是 <script type="math/tex">\alpha \Delta W</script>，不严格的说其实只有 <script type="math/tex">\Delta W</script>。然后将新的 W 传入 sigmoid 函数，我们就会发现 <script type="math/tex">\Delta W</script> 被衰减了（为什么会衰减上面已经说过了）。而 <script type="math/tex">\Delta W</script> 其实是<strong>梯度</strong>，也就是说梯度被衰减了，然后再经过多层神经网络之后，梯度被一减再减。<br>&emsp;&emsp;综上所述，梯度在第一层可能很大，在经过几层 sigmoid 函数之后，可能就<strong>减</strong>没了。<br>&emsp;&emsp;<strong>不过，由于 sigmoid 导数的取值范围是 (0, 0.25)，所以梯度也不会很大，但是这仍然架不住多层的神经网络</strong>。</p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>但是不管是<script type="math/tex">\sigma</script>或者tanh函数都一个缺点，那就是当z非常大或者非常小时，函数的斜率（导数的梯度）很小。这样会拖慢梯度下降。在机器学习中还有一个函数，即ReLU函数——Rectified Linear Unit，表达式为<script type="math/tex">max(0, z)</script>。<br>所以在选择激活函数时有一些经验法则：</p>
<ol>
<li>如果你的输出值是0或1，那么<script type="math/tex">\sigma</script>函数很适合做output layer的激活函数，非二元分类的情况下使用tanh函数几乎都比<script type="math/tex">\sigma</script>优越。藏层单元全用ReLU函数，现在ReLU函数已经是隐藏层的默认激活函数了，大多数人都这么做。</li>
<li>还有个叫Leaky ReLU的函数比ReLU稍微好点，但是目前暂时不是很多人用。</li>
</ol>
<h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>Sogmoid函数<script type="math/tex">\sigma = \frac{1}{1 + e^{-z}}</script>适用于二元分类，那么碰到多元分类怎么么办呢？Softmax函数就可以解决这个问题。<br>Softmax函数计算步骤如下，假设是n元分类：</p>
<script type="math/tex; mode=display">
Z^L = W^L * A^{L-1} + b^L\\
t = e^{Z^L}\\
A^L = \frac{e^{Z^L}}{\sum^n_{i=1}t_i},\quad A^L_i = \frac{t_i}{\sum^n_{i=1}t_i}\\</script><p>多元分类中每一个神经元代表对应标签的概率是多少，并且将概率相加等于1。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/softmax%E4%BE%8B%E5%AD%90.jpg" alt="softmax例子"></p>
<h2 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h2><div class="note info">
            <p>为什么<script type="math/tex">w*x+b=z</script>之后，需要使用一个激活函数<script type="math/tex">a = \sigma(z)</script>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
          </div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。<br>那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。</p>
<script type="math/tex; mode=display">
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w' * a^0 + b' \qquad由于w和b只是一堆常数，所以将w'代替w^2 * w^1，b'同理
    \end{array}
\right.</script><p>通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<script type="math/tex">a^2 = w' * a^0 + b'</script>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。<br>只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降，反向传播算法——backpropagation解析"><a href="#梯度下降，反向传播算法——backpropagation解析" class="headerlink" title="梯度下降，反向传播算法——backpropagation解析"></a>梯度下降，反向传播算法——backpropagation解析</h1><div class="note primary">
            <p>Q：首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？<br>A：（2020.2.21）</p><ol><li><a href="https://yan624.github.io/·学习笔记/AI/ml/梯度下降算法的推导.html">梯度下降算法的推导</a></li><li><a href="https://yan624.github.io/·zcy/AI/深度学习500问笔记.html#词向量乘上权重以及做梯度下降有什么意义">深度学习500问笔记#词向量乘上权重以及做梯度下降有什么意义</a></li></ol>
          </div>
<p>本节的示例均建立在一个样本的情况下，如果是多个样本经过神经网络，可能略微不同。我看了吴恩达老师的深度学习课程，发现多个样本与一个样本的区别，可能只在偏差b那里会有点不同。<br>下图以一个三层神经网络为例，说明正向与反向传播过程。由于神经元之间的链接太多会导致混乱，所以下图只链接了第一个神经元。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="使用导数解释反向传播算法"><br>下图略微简化一下反向传播算法中的导数项，并且完成了最后的权重值优化。值得注意的是：如果cost function不同，下面求导结果会略微不同，本文统一使用<script type="math/tex">cost = \frac{1}{m} * \sum{(\hat{y} - y)^2}</script>，但是神经网络一般是使用<strong>交叉熵</strong>——crossentropy，其公式为：<script type="math/tex">cost = -\frac{1}{m} * (y * log(\hat{y}) + (1 - y) * log(1 - \hat{y}))</script>。使用前者，<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}} = (a^{(3)} - y) * g'(z^{(3)})</script>；如果使用后者，<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}} = a^{(3)} - y</script>。可以看到使用两个不同的代价函数，会有不同的结果，这是因为两个函数求导的结果不一样。而两者对表达式<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}}</script>的结果只差了一个<script type="math/tex">g'(z^{(3)})</script>，这完全是巧合罢了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%AE%80%E5%8C%96%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="简化反向传播算法"><br><strong>另外再提醒一下自己，这里全是以一个样本为例。但是仅仅这样权重已经是一个二维矩阵了，要是如果传入多个样本，权重岂不是是一个三维矩阵？然而不管传入几个样本权重实际上对于不同的样本是没有变化的，所以还是二维矩阵。</strong></p>
<h1 id="※-随机初始化"><a href="#※-随机初始化" class="headerlink" title="※ 随机初始化"></a>※ 随机初始化</h1><p>对于逻辑回归可以将<strong>权重</strong>（weight）全部初始化为0，但是对于神经网络来说，将个权重初始化为0，再使用梯度下降会完全无效。实际上将偏差b初始化为0是可以的，但是权重不行。<br>解释起来太麻烦，详情看吴恩达深度学习——01神经网络和深度学习第三周浅层神经网络，3.11随机初始化。吴恩达老师解释地还是很清楚的。<br>可以像以下这样设置weight：<script type="math/tex">w^l = np.random.randn((2, 2)) * 0.01</script>//这可以产生参数为(2, 2)的高斯分布随机变量，后面再成一个很小的数，比如0.01。而对于b，之前说了初始化为0也可以。<br>对于上式的0.01可能会感到很疑惑，为什么要乘这么一个值。因为我们一般将weight初始化为很小的值，如果weight值很大，最终导致z也很大，那么会落在sigmoid function或者tanh function的平缓部分，会使梯度的写了很小，意味着梯度下降算法会非常慢，所以学习得很慢。</p>
<h2 id="初始化补充"><a href="#初始化补充" class="headerlink" title="初始化补充"></a>初始化补充</h2><p>经在作业中做的测试得出如下结论：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Train accuracy</strong></th>
<th><strong>Problem/Comment</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>3-layer NN with <strong>zeros initialization</strong></td>
<td>50%</td>
<td>fails to break symmetry</td>
</tr>
<tr>
<td>3-layer NN with large <strong>random initialization</strong></td>
<td>83%</td>
<td>too large weights</td>
</tr>
<tr>
<td>3-layer NN with <strong>He initialization</strong></td>
<td>99%</td>
<td><strong>recommended method</strong></td>
</tr>
</tbody>
</table>
</div>
<p>其中”He initialization”最近（论文是2015年的）新搞出来得初始化算法，现在推荐使用此算法进行初始化。</p>
<h1 id="核对矩阵维数"><a href="#核对矩阵维数" class="headerlink" title="核对矩阵维数"></a>核对矩阵维数</h1><p>w的维数应该与dw的维数相同。b和db的维数相同</p>
<h1 id="为什么使用深度表示——Why-deep-representations"><a href="#为什么使用深度表示——Why-deep-representations" class="headerlink" title="为什么使用深度表示——Why deep representations"></a>为什么使用深度表示——Why deep representations</h1><p>引用在2017course深度学习课程上吴恩达老师的话</p>
<blockquote>
<p>深度神经网络能解决很多问题，其实并不需要很大的神经网络，但是得有深度。得有比较多的隐藏层。</p>
</blockquote>
<p>为什么深度神经网络会很好用？</p>
<ol>
<li>深度神经网络到底在计算什么？假设现在在做一个人脸识别系统。那么神经网络的第一层会去找照片里的边缘部分；第二层会去识别人类的特征，比如耳朵，鼻子，嘴巴；第三层会去识别不同的人脸。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%9B%B4%E8%A7%82%E6%98%A0%E5%83%8F.jpg" alt="深度表示的直观映像"><br>这种识别模式可能难以理解，但是会在卷积神经网络——Convolutional Neural Network中详细解释。<br>这视频的这一章节有点难以总结，可以看看<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>，总共也就10分钟。</li>
</ol>
<h1 id="深层神经网络块"><a href="#深层神经网络块" class="headerlink" title="深层神经网络块"></a>深层神经网络块</h1><p>此视频中画出了深度神经网络的<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701023" target="_blank" rel="noopener">代码流程</a>。</p>
<h1 id="参数VS超参数"><a href="#参数VS超参数" class="headerlink" title="参数VS超参数"></a>参数VS超参数</h1><p>有如下超参数（hyperparameters）：W, b, lerning rate <script type="math/tex">\alpha</script>, iterations, hidden layer L, hidden units, choice of activatation function.这些超参数都需要自己设置。<br>上面这些都是基础的，实际上还有其他的超参数，稍后会涉及到。 </p>
<h1 id="神经网络和大脑有什么关系？"><a href="#神经网络和大脑有什么关系？" class="headerlink" title="神经网络和大脑有什么关系？"></a>神经网络和大脑有什么关系？</h1><p>计算机视觉、其他深度学习领域或者其他学科在早期可能都受过人类大脑的启发，但是近年来人类将神经网络类比为大脑的次数越来越少，也就是说近年来大家都不怎么认为这二者有关联。</p>
<h1 id="一个Simple-NN的例子"><a href="#一个Simple-NN的例子" class="headerlink" title="一个Simple NN的例子"></a>一个Simple NN的例子</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">Simple Neural Network</a>例子</p>
<hr>
<p>本节以下开始利用算法改善深层神经网络</p>
<hr>
<h1 id="训练-开发-测试集"><a href="#训练-开发-测试集" class="headerlink" title="训练/开发/测试集"></a>训练/开发/测试集</h1><p>训练集——training set<br>开发集/交叉验证集/验证集——dev set/cross validation set/validation set<br>测试集——test set</p>
<p>以前数据量小的时候，比如100个样本、10000个样本。一般将数据按三七分，七份训练集，三份测试集。验证集（以下均称验证集）在训练集中再细分，比如二八分，八份训练集。<br>但是现在进入大数据时代，验证集和测试集已经没有必要占大量比例了。比如现在有100万的样本，那么验证集和测试集只需要各抽取大约10000的样本即可。也就是98/1/1的比例，甚至验证集和测试集可以再降低占比。</p>
<h2 id="训练集和验证集-测试集分布不匹配"><a href="#训练集和验证集-测试集分布不匹配" class="headerlink" title="训练集和验证集/测试集分布不匹配"></a>训练集和验证集/测试集分布不匹配</h2><p>如下图，吴恩达老师建议最好让<strong>验证集</strong>和<strong>测试集</strong>匹配，即来自同一源，要都来自网络高清图，要么都来自手机低像素拍摄。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E5%BC%80%E5%8F%91%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%86%E5%B8%83%E4%B8%8D%E5%8C%B9%E9%85%8D.jpg" alt="训练集和验证集测试集分布不匹配"><br>如果直接不设置测试集也是可以的。</p>
<h1 id="※-偏差-方差"><a href="#※-偏差-方差" class="headerlink" title="※ 偏差/方差"></a>※ 偏差/方差</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%86%B3%E7%AD%96%E7%95%8C%E9%99%90.jpg" alt="欠拟合和过拟合的决策界限"></p>
<p>下图讲述了什么是<strong>过拟合</strong>，什么是<strong>欠拟合</strong>，如图所示，该神经网络用于判断一张图片是猫还是狗。<br>左边。训练样本中的误差为1%，这个值已经很小了，但是在验证集上的误差有11%。这就代表了过拟合，试想一下，在训练集上误差很小是因为你的决策界限划分的很好，在上图中的最后一个例子，整条决策界限画的十分完美，但是我们要知道在验证集中，这样一条完美的线肯定不能再拟合的很好。因为训练集和验证集即使来源于同一份数据，他们之间的分布也是不一样的，你训练出一条完美的曲线，在另一份数据集上肯定是过于完美了。所以导致了下图中验证集上的误差有11%。我们称这种情况为<strong>高方差</strong>——high variance。<br>中间。训练样本中的误差为15%，这已经不需要再看验证集上的误差了。因为训练集上的误差那么大，肯定是没有拟合好，所以这就是欠拟合，我们称为<strong>高偏差</strong>——high bais。<br>右边。如果训练集中的误差很高，验证集上的误差更高，那么可以判断为同时具有高方差和高偏差。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/欠拟合和过拟合.jpg" alt="欠拟合和过拟合"></p>
<p>如果训练集上的误差为0.5%，验证集上的误差为1%。那这就是低方差和低偏差，这是很好结果。<br>最后一点，以上均建立在人眼判断的误差为0%上以及训练集和验证集来自相同分布。如果人眼判断的误差也高达15%，那么中间的例子也算是可以的结果一般来说<strong>最优误差</strong>也被称为<strong>贝叶斯误差</strong>。（不知道语义解析领域如何定义最优误差？）<br>关于上图同时高方差和高方差，就如同下图紫色线条的决策界限一般。过渡拟合了数据，但是拟合的数据其实狗屁不通。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E5%90%8C%E6%97%B6%E9%AB%98%E6%96%B9%E5%B7%AE%E5%92%8C%E9%AB%98%E5%81%8F%E5%B7%AE%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84.jpg" alt="同时高方差和高偏差是怎么样的"></p>
<h2 id="非理想状态下的偏差-方差"><a href="#非理想状态下的偏差-方差" class="headerlink" title="非理想状态下的偏差/方差"></a>非理想状态下的偏差/方差</h2><p>那么当上图的理想状态被打破时（比如一张图片很模糊，连人眼也无法分辨），该如何分辨偏差/方差呢？<br>可以通过对比训练集的 error 和 验证集的 error 来确定是否高方差。比如训练集的 error 为 1%，验证集的为 15%，那就是高方差。实际上跟上面的判断方法一样。</p>
<h2 id="机器学习遇到偏差或方差的解决办法"><a href="#机器学习遇到偏差或方差的解决办法" class="headerlink" title="机器学习遇到偏差或方差的解决办法"></a>机器学习遇到偏差或方差的解决办法</h2><div class="note info">
    <p>笔记中都记了，懒得再写一遍了。补充一点，遇到偏差或方差都可以更换神经网络架构，比如换成CNN或者RNN，如果是高偏差还可以使用更大的神经网络。</p>
</div>

<p>可以看这个6分半中的小视频，<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702115" target="_blank" rel="noopener">机器学习基础</a>。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>如果出现了过拟合，即高方差的情况，第一件想到的事是<strong>正则化</strong>——regularization。当然也可以增加数据，不过有时候数据不是那么容易获取的。可以对W使用<a href="https://www.baidu.com/s?wd=L2%E8%8C%83%E6%95%B0" target="_blank" rel="noopener">L2范数</a>进行正则化，当然对b也可以进行L2范数正则化，不过一般不加。L2范数的公式为<script type="math/tex">||w||^2_2 = \sum_{j=1}^n w^2_j = W^T * W</script><br>因此代价函数修改为<script type="math/tex">cost = \frac{1}{m} \sum^m_{i=1} g(\hat{y}^i, y^i) + \frac{\lambda}{2m}||w||^2_2</script>，<script type="math/tex">\lambda</script>是正则化的超参数。这里的w实际上是一个二维矩阵，所以L2范数需要把里面的每一个值的平方都加起来。<br>如果加入了正则化项，那么在计算dW时有点变化。将会变为：<script type="math/tex">dW = dZ * A\_prev + \frac{\lambda}{m} w ^ l</script></p>
<h2 id="为什么正则化可以防止过拟合"><a href="#为什么正则化可以防止过拟合" class="headerlink" title="为什么正则化可以防止过拟合"></a>为什么正则化可以防止过拟合</h2><p>略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702116" target="_blank" rel="noopener">1.5 为什么正则化可以减少过拟合？</a></p>
<h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><div class="note primary">
            <p>Q：那么问题又来了，dropout背后的原理是什么？<br>A：<a href="https://yan624.github.io/·zcy/AI/深度学习500问笔记.html#Dropout-系列问题">深度学习500问笔记#Dropout-系列问题</a>（2020.2.21）</p>
          </div>
<p>dropout，中文翻译为<strong>随机失活</strong>。<br>先将神经网络复制一遍，然后dropout会遍历神经网络的每一层，并设置消除神经网络中结点的概率，比如设置0.5。下图的带X的结点就是准备消除的。另外每一层的概率都可以是不同的，如果在某一层不担心会过拟合可以将概率设为1.0，比如输出层。如果觉得某些层比其他层更容易过拟合，可以把那些层的keep-prob设置的更低。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/dropout%E5%BE%85%E5%88%A0%E9%99%A4%E7%BB%93%E7%82%B9.jpg" alt="dropout待删除结点"><br>下图则是消除后的神经网络。将结点的进出的链接全部删除。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8dropout%E5%90%8E%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg" alt="使用dropout后的神经网络"><br>dropout使用之后，就让一个样本进入神经网络进行训练。而对于其他样本也如法炮制，需要再进行复制一遍神经网络，并进行dropout。<br>以上均是逻辑上的做法，接下来讲实际编码该怎么做。</p>
<ol>
<li>设置一个结点保留的概率——keep-prob，假设为0.8。<script type="math/tex">d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) < keep-prob</script>，这样会得到一个True和False的数组，但是python中Ture等于1，False等于0。</li>
<li>让<script type="math/tex">a^3</script>乘上这个向量。<script type="math/tex">a^3 = np.multiply(a^3, d^3)</script>。由于False等于0，所以变相地将<script type="math/tex">a^3</script>中的值失活了。</li>
<li>最后一步看起来有点奇怪，<script type="math/tex">a^3 /= keep-prob</script>。<br>完整代码如下：<script type="math/tex; mode=display">
\begin{cases}
 d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) < keep-prob\\
 a^3 = np.multiply(a^3, d^3)\\
 a^3 /= keep-prob\\
\end{cases}</script></li>
</ol>
<p>对于最后一步，由于<script type="math/tex">Z^4 = W^4 * A^3 + b^4</script>，由于<script type="math/tex">A^3</script>被dropout减少0.2，为了使得<script type="math/tex">Z^4</script>不受影响，所以对<script type="math/tex">A^3</script>除0.8，来保证<script type="math/tex">A^3</script>的值不变。由于早期的版本没有除于keep-prob，使得测试阶段，平均值越来越复杂。<br>最后，从技术上来讲，输入值也可以使用dropout，但是基本不这么做，直接把keep-prob设为1.0即可，当然0.9也可以。不过太低的值一般不会去设置。<br>以上的步骤被称为<strong>Inverted dropout</strong>——<strong>反向随机失活</strong>。<br>dropout在计算机视觉中用的非常多，甚至成了标配。但要记住一点，dropout是一种正则化方法，为了预防过拟合。所以除非算法过拟合，不然不会使用dropout。由于计算机视觉的特殊性，他们才经常用dropout。<br>dropout的缺点是使我们失去了代价函数这一调试功能。我们经常使用代价函数得到误差，从而画出曲线图。但是使用dropout之后，这样的曲线图就不再准确了。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在测试阶段不再使用dropout，因为我们不希望输出结果是随机的，如果使用dropout预测会受到干扰。</p>
<h3 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h3><div class="note info">
            <p>略。有点晦涩。</p>
          </div>
<p>看<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a>。。</p>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ol>
<li>Data augment——数据增强。如果拟合猫咪图片分类器，可以对原图片做一些处理，来增加数据，比如翻转、旋转、随机裁剪等。</li>
<li>Early stopping。在训练时画出代价的曲线图，x轴为迭代次数，再绘制验证时的误差。然后选择验证误差曲线图中最低点的迭代次数，下次训练时就改用这个迭代次数，或者也可以在程序中写一个条件判断。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/early%20stopping.jpg" alt="early stopping"></li>
</ol>
<h1 id="均值归一化输入"><a href="#均值归一化输入" class="headerlink" title="均值归一化输入"></a>均值归一化输入</h1><div class="note info">
    <p>略。其实很简单。</p>
</div>

<h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702118&amp;cid=2001699114" target="_blank" rel="noopener">视频</a><br><a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另外一个参考视频</a>，08:37开始。<br><a href="https://www.bilibili.com/video/av10590361/?p=37" target="_blank" rel="noopener">另一个</a>13:50~18左右</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701047" target="_blank" rel="noopener">视频</a></p>
<h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>Gradient checking(Grad check).<br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701048" target="_blank" rel="noopener">原理视频</a><br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702119" target="_blank" rel="noopener">实战视频</a><br>梯度检验可以帮助我们发现神经网络中的一些bug。具体原理是，通过数学上导数的定义来确认反向传播算法是否正确。如果学过高数就会知道，使用导数的定义求解和直接使用公式求解，两者结果十分接近或者一模一样。如果二者不一样说明肯定是求错了。<br>对应于神经网络，那就肯定是代码写错了。具体操作可在视频中看见，每个视频都不超过10分钟。</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果梯度检验确实发现问题，要检查每一项，看看是哪个i的w和b有问题。</li>
<li>记得正则化项，它也被包含在w的梯度中。</li>
<li>梯度检验不能和dropout一起用。</li>
<li><del>在随机初始化时就运行一遍梯度检验；或许在训练一会后可以再运行一遍梯度检验。当W和b接近于0时，梯度下降正确执行在现实中几乎不太可能。</del>吴恩达老师说这条他在现实中几乎不会这么做，并且第五条的翻译，个人感觉翻得有问题，然后看了英文原文后，感觉原文表达得也不是很好，我看不太懂，所以这条就不算进注意事项了。</li>
</ol>
<h1 id="Mini-batch梯度下降"><a href="#Mini-batch梯度下降" class="headerlink" title="Mini-batch梯度下降"></a>Mini-batch梯度下降</h1><div class="note primary">
            <p>Q：为什么Mini-batch比普通的梯度下降快？</p>
          </div>
<p>普通的梯度下降——vanilla gradient descent，是将整个数据集同时做运算，而Mini-batch梯度下降算法是以一组为单位，分别进行梯度下降，所有组执行完毕后再进行下一次迭代。<br>假设现在有m个样本。</p>
<script type="math/tex; mode=display">
X = \begin{pmatrix}x^1&x^2&x^3&\cdots&x^m\end{pmatrix}\\
Y = \begin{pmatrix}y^1&y^2&x^3&\cdots&y^m\end{pmatrix}\\</script><p>使用Mini-batch，假设每1000个样本为一组：</p>
<script type="math/tex; mode=display">
X = \begin{pmatrix}\underbrace{x^1\cdots x^{1000}}_{X^{\{1\}}} & \underbrace{x^{1001}\cdots x^{2000}}_{X^{\{2\}}} & \cdots&\underbrace{\cdots x^m}_{X^{\{t\}}}\end{pmatrix}\\
Y = \begin{pmatrix}\underbrace{y^1\cdots y^{1000}}_{Y^{\{1\}}} & \underbrace{y^{1001}\cdots y^{2000}}_{Y^{\{2\}}} & \cdots&\underbrace{\cdots y^m}_{Y^{\{t\}}}\end{pmatrix}\\</script><p>如果使用代码实现就是类似下面这样的伪代码：<br><figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>, ..., t</span><br><span class="line">	forwardprop <span class="keyword">on</span> X^&#123;t&#125;</span><br><span class="line">	compute cost</span><br><span class="line">	backprop <span class="keyword">to</span> compute grads</span><br><span class="line">	update weights <span class="keyword">and</span> bais</span><br></pre></td></tr></table></figure></p>
<p>for循环完成之后就完成了神经网络的第一次迭代。</p>
<h2 id="理解mini-batch"><a href="#理解mini-batch" class="headerlink" title="理解mini-batch"></a>理解mini-batch</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/Mini-batch%E5%92%8C%E6%99%AE%E9%80%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="Mini-batch和普通梯度下降的区别"></p>
<ol>
<li>如果将batch设为m，那它就是普通的梯度下降算法。</li>
<li>如果将batch设为1，就叫做随机梯度下降——SGD</li>
<li>batch在1到m之间就是mini-batch</li>
</ol>
<p>SGD和普通梯度下降的区别，“+”代表代价最小点。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/SGD%E5%92%8Cvanilla%20gradient%20descent%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="SGD和vanilla gradient descent的区别"><br>SGD和mini-batch的区别。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/SGD%E5%92%8Cmini-batch%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="SGD和mini-batch的区别"></p>
<p><strong>应该记住的有：</strong></p>
<ol>
<li>普通梯度下降、mini-batch和SGD之间的区别就是执行一次参数更新所需的样本数量不同。</li>
<li>你需要自己调整学习速率<script type="math/tex">\alpha</script>。</li>
<li>当mini-batch的量调整良好时，它通常优于普通梯度下降和SGD（尤其是在训练集特别大时）。</li>
</ol>
<h2 id="mini-bacth实现步骤"><a href="#mini-bacth实现步骤" class="headerlink" title="mini-bacth实现步骤"></a>mini-bacth实现步骤</h2><ol>
<li>打乱数据。创建一个打乱数据之后的副本，其中X和Y的每一列都代表一个训练样本。注意X和Y是同步地随机打乱样本，即X中第<script type="math/tex">i^{th}</script>个样本和Y中第<script type="math/tex">i^{th}</script>标签在打乱之后还是是对应的。此步骤确保样本被随机地分割到不同的mini-batches中。下图是步骤示意图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E7%AC%AC%E4%B8%80%E6%AD%A5%E6%89%93%E4%B9%B1%E6%95%B0%E6%8D%AE.jpg" alt="mini-batch第一步打乱数据"></li>
<li>切分。将打乱数据后的XY切分进<code>mini_batch_size</code>大小（下图是64）的mini-batches中。不过注意训练样本的数量并不总能被<code>mini_batch_size</code>整除。最后的mini-batch可能要小点，但是不需要担心这点。使用<code>math.floor()</code>向上取整即可。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E7%AC%AC%E4%BA%8C%E6%AD%A5%E5%88%87%E5%88%86.png" alt="mini-batch第二步切分"></li>
</ol>
<h2 id="一些经验"><a href="#一些经验" class="headerlink" title="一些经验"></a>一些经验</h2><p>如果小数据量（大约小于2000）的话，<strong>只</strong>执行步骤1即可；如果样本数目较大，执行步骤1和步骤2，一般将batch设置在64~512之间，考虑到电脑的内存设置和使用方式，batch的大小设置为2的次方，代码的运行速度会比较快；</p>
<h1 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h1><p>为了更好地理解其他优化算法，需要使用到指数加权平均。这章介绍一下它。<br>Exponentially weighted averages，在统计学中被称为指数加权移动平均——Exponentially weighted moving averages。<br>指数加权平均有一个公式：<script type="math/tex">V_t = \beta * V_{t-1} + (1- \beta) * \theta_t</script>，<script type="math/tex">V_0 = 0</script>，其目的是使用<script type="math/tex">V_t</script>代替<script type="math/tex">\theta_t</script>。<script type="math/tex">V_t</script>可视为<strong>约等于</strong><script type="math/tex">\frac{1}{1 - \beta}</script>个样本的平均值。这里可能会有疑问，为什么<script type="math/tex">V_t</script>可视为约等于<script type="math/tex">\frac{1}{1 - \beta}</script>个样本的平均值？其实我也不知道，也不想知道，我又不是学统计学或者数学的。<br>下图中的数据为伦敦一年之间的温度，来源于吴恩达的深度学习视频，可以看到其中的数据十分杂乱，也就是常在网络上看到别人所说的“噪点”多。我们可以使用<strong>指数加权平均</strong>来画出一条线，就是下图的红线，来代表温度变化的趋势，这样会使得更容易让人类理解和观察。<br>下图中的<script type="math/tex">\beta</script>为0.9。而<script type="math/tex">\frac{1}{1 - 0.9} = 10</script>，所以<script type="math/tex">V_t</script>代表过去<em>十天</em>内的平均温度。如果<script type="math/tex">\beta</script>为0.98，那么<script type="math/tex">V_t</script>代表过去<em>五十</em>天内的平均温度<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E4%BE%8B%E5%AD%90.jpg" alt="指数加权平均例子"><br>下图是不同<script type="math/tex">\beta</script>值的对比。注意到一点，绿色（<script type="math/tex">\beta</script>=0.98）的线比红色的线要平坦一点，这是因为你多平均了几天的温度，所以这根线波动更新、更平坦。但是缺点是曲线进一步向右移，拟合的不是很好。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%B8%8D%E5%90%8Cbeta%E5%80%BC%E7%9A%84%E5%AF%B9%E6%AF%94.jpg" alt="不同beta值的对比"><br>现在看到了平均了10天和50天温度的曲线，现在试试<script type="math/tex">\beta=0.5</script>，也就是只平均两天的温度。由于只平均了两天的温度，数据太少，所以曲线有更多的噪声，更有可能出现异常值。但是这个曲线能更快适应温度变化。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/beta%E5%80%BC%E7%AD%89%E4%BA%8E0.5%E7%9A%84%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87.jpg" alt="beta值等于0.5的指数加权平均"></p>
<h2 id="理解其作用"><a href="#理解其作用" class="headerlink" title="理解其作用"></a>理解其作用</h2><div class="note primary">
            <p>略。至今看不懂。</p>
          </div>
<h2 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h2><p>之前的曲线其实都是理想状态下的，回想绿色的曲线是50天内的温度平均值。但是其实绿色曲线会是紫色曲线那样的轨迹。初始化<script type="math/tex">V_0=0</script>，原数据中<script type="math/tex">\theta_0 = 40</script>，所以其实<script type="math/tex">V_1 = 0.02 * 40 = 8</script>，从而绿色曲线的起点实际上很低。因为起点并没有计算50天内的温度平均，我们默认将<script type="math/tex">V_0</script>初始化为0。<br>我们可以用下图右边的公式将其修正。算出<script type="math/tex">V_t</script>后再做如下计算：<script type="math/tex">\frac{V_t}{1 - \beta^t}</script>，其中<script type="math/tex">\beta</script>的上标t是指<strong>t次方</strong>。<br>另外由于t越大，<script type="math/tex">\beta^t</script>的值越接近0，所以对后面的值几乎没影响。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/指数加权平均偏差修正.jpg" alt="指数加权平均偏差修正"></p>
<h1 id="※-Learning-rate-decay"><a href="#※-Learning-rate-decay" class="headerlink" title="※ Learning rate decay"></a>※ Learning rate decay</h1><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>介绍 </th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td>问：为什么 SGD 比 batch gradient descent 快？答：因为 SGD 可以使用极小的 epoch 更新多次权重，而 BGD 必须要大 epoch 才可以。详见<a href="https://www.zhihu.com/question/40892922" target="_blank" rel="noopener">此处</a>  </td>
</tr>
<tr>
<td></td>
<td><strong>优点</strong>：<br> 1）在损失函数是凸函数的情况下能够保证收敛到一个较好的全局最优解；2）数据量过大时，batch 方法可以减少机器压力，从而更快地收敛；3）当训练集有很多冗余时（类似的样本出现多次），batch方法收敛更快。<br> <strong>缺点</strong>：<br>1）<script type="math/tex">\alpha</script> 是一个定值，它的选取直接决定了解的好坏，过小会导致收敛太慢，过大会导致震荡而无法收敛到最优解；2）对于非凸问题，只能收敛到局部最优，并且没有任何摆脱局部最优的能力（一旦梯度为0就不会再有任何变化）；3）更新方向完全依赖当前的 batch</td>
</tr>
<tr>
<td>Momentum</td>
<td>累积梯度，充当动量，注：虽然 Momentum 和 RMSprop 类似，但是实际上在计算上并不一样， RMSprop 要多一个除以 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 的步骤，且没有偏差修正这一步骤  </td>
</tr>
<tr>
<td></td>
<td><strong>优点</strong>：1）一定程度上缓解了 SGD 收敛不稳定的问题，并且有一定的摆脱局部最优的能力（即如同一个滚轮下坡一样，它拥有惯性，在到达鞍点时不会立即停止，会因为惯性再向前一点距离，从而可能离开此鞍点）。<br> <strong>缺点</strong>：1）多了一个超参数需要调整，它的选取同样会影响到结果。</td>
</tr>
<tr>
<td>RMSprop</td>
<td>通过除以 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 减小波动幅度从而收敛更快 </td>
</tr>
<tr>
<td></td>
<td><strong>优点</strong>：1）不需要手动调整学习率，可以自动调整。</td>
</tr>
<tr>
<td>Adam</td>
<td><strong>优点</strong>：1）结合 Momentum 和 RMSProp，稳定性好，同时相比于 Adagrad 不用存储全局所有的梯度，适合处理大规模数据。<br> <strong>缺点</strong>：1）有三个超参数需要调整</td>
</tr>
<tr>
<td>Adagrad</td>
<td>RMSProp 的简化版<br> <strong>适用场景</strong>：Adagrad非常适合处理稀疏数据（如 one-hot）  </td>
</tr>
<tr>
<td></td>
<td><strong>优点</strong>：1）不需要手动调节 <script type="math/tex">\alpha</script>，它会发生自适应的变化。<br> <strong>缺点</strong>：1）学习率单调递减，在迭代后期可能导致学习率变得特别小而导致收敛及其缓慢。</td>
</tr>
<tr>
<td>Adadelta</td>
<td>Adadelta 是 Adagrad 的一种扩展算法，以处理Adagrad学习速率单调递减的问题。RMSprop 可以算作 Adadelta 的一个特例  </td>
</tr>
<tr>
<td></td>
<td><strong>优点</strong>：1）不需要手动调整学习率，可以自动调整；2）不需要手动设置<strong>初始</strong> <script type="math/tex">\alpha</script>。<br> <strong>缺点</strong>：1）后期容易在小范围内产生震荡</td>
</tr>
<tr>
<td>—————-</td>
<td><a href="https://blog.csdn.net/gangyin5071/article/details/81810358#11-sgd" target="_blank" rel="noopener">机器学习各优化算法的简单总结</a><br> <a href="http://ruder.io/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a> <br> <a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms ARXIV</a> <br> <a href="https://blog.csdn.net/google19890102/article/details/69942970" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms 中文翻译</a></td>
</tr>
</tbody>
</table>
</div>
<p>凸优化与非凸优化：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/凸优化与非凸优化.jpg" alt="凸优化与非凸优化"></p>
<h2 id="动量梯度下降——Momentum"><a href="#动量梯度下降——Momentum" class="headerlink" title="动量梯度下降——Momentum"></a>动量梯度下降——Momentum</h2><p>&emsp;&emsp;Momentum改进自SGD，让每一次的参数更新方向不仅取决于当前位置的梯度，还受到上一次参数更新方向的影响。<br>&emsp;&emsp;不管是普通的梯度下降、mini-batch、SGD 还是其他的什么，都是通过 <script type="math/tex">W -= \alpha * dW</script> 来更新权重。但是在动量梯度下降中，使用到了<strong>指数加权平均</strong>。尤其是针对mini-batch算法，因为mini-batch算法抖动过大，上面的章节介绍了mini-batch的梯度下降误差曲线，指数加权平均正好可以解决。<br>&emsp;&emsp;可以观察下图发现，梯度下降的波动比较大，也就是噪点较多，我们可以使用指数加权平均来减少噪点。下面的公式就是用其减少了梯度dW和db。下式中还对指数加权平均进行了优化，使用了<strong>偏差修正</strong>。</p>
<script type="math/tex; mode=display">
    \begin{cases}
        compute\ dW, db\\
        V_{dW} = \beta * (V_{dW})_{prev} + (1 - \beta) * dW,\quad V_{db} = \beta * (V_{db})_{prev} + (1 - \beta) * db\\
        V^{corrected}_{dW} = \frac{V_{dW}}{1 - \beta^t},\quad V^{corrected}_{db} = \frac{V_{db}}{1 - \beta^t} \\
        W -= \alpha * V^{corrected}_{dW}\\
        b -= \alpha * V^{corrected}_{db}\\
    \end{cases}</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/梯度下降示意图.jpg" alt="梯度下降示意图"></p>
<p>在梯度下降时的这种波动减慢了下降的速度，无法使用更大的学习速率。因为梯度已经很大了，如果使用更大的学习速率，可能梯度直接爆炸了，直接无法收敛。为了避免摆动过大需要使用较小的学习速率。<br>还可以从另一种角度看待。我们希望在纵轴上学习的慢点，我们希望摆动小点，不就是希望纵轴小点吗。而在横轴上我们又希望学习的快点，因为我们希望越快接近中心越好。<br>这个<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">视频</a>讲的直观一点，可以参考一下，从36:00开始看，虽然讲的是RMSprop但是讲的原理跟Momentum的原理一样。</p>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>在课后练习中有更详细的说明，在此补充一下。</p>
<blockquote>
<p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.</p>
</blockquote>
<p>大致意思就是使用Momentum可以使得mini-batch的振荡更小，观察下图。。。说实话我并没有观察出什么，不知道Coursera是怎么想的。我把此图的提示贴出来：</p>
<blockquote>
<p>Figure 3: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence  v  and then take a step in the direction of  v .</p>
</blockquote>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E4%BD%BF%E7%94%A8Momentum%E5%90%8E.png" alt="mini-batch使用Momentum后"></p>
<blockquote>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable  v . Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of  v  as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.</p>
</blockquote>
<p>Momentum考虑到了之前的梯度，从而用其来缓和参数更新。我们将前一次梯度的“方向”存进变量v。后续不翻译了。</p>
<h2 id="均方根传播——RMSprop"><a href="#均方根传播——RMSprop" class="headerlink" title="均方根传播——RMSprop"></a>均方根传播——RMSprop</h2><div class="note primary">
            <p>&emsp;&emsp;问题：对梯度做平方，且 <script type="math/tex">S_{dW}</script> 的计算公式是加法（即修改过的指数加权平均），那么 <script type="math/tex">S_{dW}</script> 岂不是会越来越大？不就意味着动量越来越大，到最后停不下来了？<br>&emsp;&emsp;<strong>当然不是，可以减小</strong>。设 <script type="math/tex">dW_1 = 15 \quad S_{dW1} = 20 \quad \beta = 0.9 \quad dW_2 = 2</script>，则 <script type="math/tex">S_{dW2} = 0.9 * 20 + (1 - 0.9) * 15 = 19.5</script>，而 <script type="math/tex">S_{dW3} = 0.9 * 19.5 + (1 - 0.9) * 2 = 17.75</script>。比较 <script type="math/tex">S_{dW1} \, S_{dW2} \, S_{dW3}</script> = 20 19.5 17.75，明显在下降，比较 <script type="math/tex">dW_1 \, dW_2</script> 发现梯度减少引起得 <script type="math/tex">dS</script> 减少。<br>&emsp;&emsp;<strong>RMSProp 的本质</strong>是：在梯度大的地方，减小 <script type="math/tex">\alpha</script>（即陡坡则减小步伐）；在梯度小的地方，增大 <script type="math/tex">\alpha</script>（即缓坡则增大步伐）。解释如下：<br>&emsp;&emsp;RMSProp的更新公式为：</p><script type="math/tex; mode=display">    \begin{cases}        S_{dW} = \beta_2 * (S_{dW})_{prev} + (1 - \beta_2) * (dW)^2 \\        W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon} \\    \end{cases}</script><p>&emsp;&emsp;对于 <script type="math/tex">W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}</script> 来说，其实就是普通的权重更新公式多除以一个 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script>。而由于 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 是梯度的累加，故为简便起见，将 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 看作梯度（Q：为什么可以将这个视为梯度？A：<script type="math/tex">\sqrt{S_{dW}}</script> 是对梯度 dW 的类似累加操作，开个根号后差不多就是梯度）。<br>&emsp;&emsp;我们知道在<strong>陡坡</strong>处梯度大，我们需要<strong>减小步伐</strong>，要不然容易一步迈长了，而减小步伐的意思就是减小 <script type="math/tex">\alpha</script>。<br>&emsp;&emsp;假设现在陡坡处梯度为 100，那么就是将原本的梯度下降公式多除以了 100（因为我们已将 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 视为梯度）。是不是梯度越大，则分母越大？分母越大则意味着 <script type="math/tex">\alpha</script> 越小。<br>&emsp;&emsp;反之，<strong>缓坡</strong>梯度小，我们就需要<strong>增大步伐</strong>，假设梯度为 0.1，那么就代表将原本的权重更新公式除以 0.1。分母越小，则 <script type="math/tex">\alpha</script> 越大。</p>
          </div>
<p>Root mean square prop.<br>一个类似Momentum的算法，没必要死记公式，略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d#/learn/content?type=detail&amp;id=2001702124" target="_blank" rel="noopener">视频地址</a>。<br>或者<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另一个参考视频</a>，36:00开始。<br><strong>RMSprop 没有使用偏差修正。</strong>但是在 Adam 中的 RMSprop 使用了偏差修正。</p>
<script type="math/tex; mode=display">
    \begin{cases}
        compute\ dW, db\\
        S_{dW} = \beta_2 * (S_{dW})_{prev} + (1 - \beta_2) * (dW)^2,\quad S_{db} = \beta_2 * (S_{db})_{prev} + (1 - \beta_2) * (db)^2\\
        W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}\\
        b -= \alpha * \frac{db}{\sqrt{S_{db}} + \epsilon}\\
    \end{cases}</script><p>在更新 W 和 b 时的算法与之前的 Momentum 算法略微不同。另外为了防止 dW 和 db 等于 0，导致分母为 0，所以在分母加了一个极小值<script type="math/tex">\epsilon</script>，在 Keras 中取了 1e-7， 吴恩达老师说 1e-8 是个不错的选择。<br><strong>RMSprop 算法也是使用了指数加权平均算法。</strong>并且还结合了 Adagrad。<br><div class="note info">
            <p>&emsp;&emsp;对于理解 RMSprop。<strong>可以观察出 RMSprop 和 Momentum 长得有点像，但是这两个算法的具体关系暂时不清楚</strong>。并且 RMSprop 其实还有简化版的算法，叫做 Adagrad。之前对这些优化算法（Momentum, RMSprop, Adam 等）的理解都是<em>改变 W 和 b 的大小从而使得梯度下降更快</em>。但是又今天看了一遍<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">李宏毅老师的视频</a>，发现还有其他的理解。其实这些算法都在<strong>改变学习速率的大小</strong>。<br>&emsp;&emsp;比如 RMSprop 算法，观察<script type="math/tex">W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}</script>，我们可以改写成<script type="math/tex">W = W - \frac{\alpha}{\sqrt{S_{dW}} + \epsilon} * dW</script>。看dW之前的那项<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>实际上就是对学习速率<script type="math/tex">\alpha</script>乘上了<script type="math/tex">\frac{1}{\sqrt{S_{dW}} + \epsilon}</script>。<br>&emsp;&emsp;所以对RMSprop的理解是：<strong>如果梯度过大<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>就会相对减小，如果梯度过小<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>就会相对增大</strong>。因为其实<script type="math/tex">S_{dW}</script>就是 dW 算出来的，而梯度过大就是 dW 过大，dW过大就是<script type="math/tex">S_{dW}</script>过大。一个很大的数取倒数，这个数就变很小了。梯度过小同理。<br>&emsp;&emsp;而为什么梯度过大就要是学习速率<script type="math/tex">\alpha</script>变小呢？因为梯度过大就是说梯度较为陡峭，可以想象一座陡峭的山，如果跨一大步是不是直接掉下去了？而掉在哪是未知的，很有可能掉到最低点的前面，这样大概率是回不到最低点的（或者是极小值点）。而如果<script type="math/tex">\alpha</script>小点就很好了，因为可以一小步一小步的走，最终可能会走到极小值点（或者最小值点）。梯度过小同理。平原地方肯定要大跨步走，你小步伐走要走到什么时候才能走到极小值点？<br>&emsp;&emsp;<strong>另外 RMSprop 可以算是 Adagrad 算法的改进版，但是这二者的具体关系未知。</strong></p>
          </div></p>
<h2 id="优化算法历史介绍"><a href="#优化算法历史介绍" class="headerlink" title="优化算法历史介绍"></a>优化算法历史介绍</h2><blockquote>
<p>在深度学习的历史中，有不少学者，包括许多知名学者，提出了优化算法并解决了一些问题。但之后这些算法被指出并不能一般化，并不能适用于多种神经网络。<br>时间久了，深度学习圈子里的人开始多少有点质疑全新的优化算法。<br>但是RMSprop和Adam是少有的经受住人们考验的两种算法。已被证明适用于不同的深度学习结构。</p>
</blockquote>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>全称：Adaptive Moment Estimation<br>这里的 RMSprop 使用了偏差修正。<br><strong>Adam 算法是 Momentum 和 RMSprop 结合起来的算法。</strong>Momentum算法解决算法在纵轴上波动过大的问题，它可以使用类似于物理中的动量来累积梯度。而RMSprop可以在横轴上收敛速度更快同时使得波动的幅度更小。所以将两种算法结合起来表现可能会更好。<br><div class="note primary">
            <p>我的理解是 RMSprop 算法也算是在累计梯度。所以我感觉只使用 RMSprop 和使用 Adam 差不多。</p>
          </div></p>
<script type="math/tex; mode=display">
\begin{array}{l}
    compute\ dW, db\\
    V_{dW} = \beta_1 * V_{dW} + (1 - \beta_1) * dW,\quad V_{db} = \beta_1 * V_{db} + (1 - \beta_1) * db\\
    S_{dW} = \beta_2 * S_{dW} + (1 - \beta_2) * (dW)^2,\quad S_{db} = \beta_2 * S_{db} + (1 - \beta_2) * (db)^2\\
    V^{corrected}_{dW} = \frac{V_{dW}}{1 - \beta_1},\quad V^{corrected}_{db} = \frac{V_{db}}{1 - \beta_1}\\
    S^{corrected}_{dW} = \frac{S_{dW}}{1 - \beta_2},\quad S^{corrected}_{db} = \frac{S_{db}}{1 - \beta_2}\\
    W -= \alpha * \frac{V^{corrected}_{dW}}{\sqrt{S^{corrected}_{dW}} + \epsilon}\\
    b -= \alpha * \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}} + \epsilon}\\
\end{array}</script><p><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">adam paper</a>在这。</p>
<h3 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h3><ol>
<li><script type="math/tex">\alpha</script>需要自行调整。</li>
<li><script type="math/tex">\beta_1</script>一般设置为0.9，计算<script type="math/tex">dW</script>。</li>
<li><script type="math/tex">\beta_2</script>Adam的作者推荐0.999，计算<script type="math/tex">(dW)^2</script>。</li>
<li><script type="math/tex">\epsilon</script>其实不是很重要，但是Adam作者推荐设置为<script type="math/tex">10^{-8}</script>。其实不设置也可以，并不会影响算法的性能。</li>
</ol>
<p>所以在该算法中其实只要调整<script type="math/tex">\alpha</script>就够了，其他的参数也可以调整，但是一般不调整。</p>
<h2 id="其他的学习速率衰减算法"><a href="#其他的学习速率衰减算法" class="headerlink" title="其他的学习速率衰减算法"></a>其他的学习速率衰减算法</h2><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>&emsp;&emsp;让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。<br><a href="https://www.bilibili.com/video/av10590361/?p=6" target="_blank" rel="noopener">李宏毅 Adagrad 参考视频</a>，从06:30开始。<br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d#/learn/content?type=detail&amp;id=2001702125" target="_blank" rel="noopener">吴恩达深度学习——学习速率衰减</a></p>
<h2 id="优化算法总结"><a href="#优化算法总结" class="headerlink" title="优化算法总结"></a>优化算法总结</h2><div class="table-container">
<table>
<thead>
<tr>
<th><strong>optimization method</strong></th>
<th><strong>accuracy</strong></th>
<th><strong>cost shape</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient descent</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Momentum</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Adam</td>
<td>94%</td>
<td>smoother</td>
</tr>
</tbody>
</table>
</div>
<h1 id="如何为超参数选择范围"><a href="#如何为超参数选择范围" class="headerlink" title="如何为超参数选择范围"></a>如何为超参数选择范围</h1><p>上面说了那么多算法，其中包括了许多超参数，那么应该怎么为超参数选择值呢？</p>
<h2 id="超参数的重要程度"><a href="#超参数的重要程度" class="headerlink" title="超参数的重要程度"></a>超参数的重要程度</h2><p>按照吴恩达老师的排序，超参数的重要程度如下：</p>
<ol>
<li>learning rate<script type="math/tex">\alpha</script></li>
<li>Momentum的<script type="math/tex">\beta</script>, hidden layer units, mini-batch size</li>
<li>layer的数量，learning rate decay</li>
<li>Adam中的<script type="math/tex">\beta_1\quad \beta_2\quad \epsilon</script>不是很重要，一般按<script type="math/tex">0.9\quad 0.99\quad 10^{-8}</script>设置</li>
</ol>
<h2 id="超参数的取值"><a href="#超参数的取值" class="headerlink" title="超参数的取值"></a>超参数的取值</h2><ol>
<li>随机取值</li>
<li>从粗糙到精细的策略。首先进行随机取值，发现某个点的效果很好，并且附近的点也很好，然后放大这块区域，进行更密集地取值。下图被圈出来的蓝点就是效果不错的，然后被方框画出一大块区域进行密集地取值或者也可以在这块区域随机取值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BB%8E%E7%B2%97%E7%B3%99%E5%88%B0%E7%B2%BE%E7%BB%86%E7%9A%84%E5%8F%96%E5%80%BC%E7%AD%96%E7%95%A5.jpg" alt="从粗糙到精细的取值策略"></li>
</ol>
<h2 id="选择合适的范围"><a href="#选择合适的范围" class="headerlink" title="选择合适的范围"></a>选择合适的范围</h2><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701053" target="_blank" rel="noopener">视频1</a><br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701054" target="_blank" rel="noopener">视频2</a></p>
<h2 id="补充——用神经网络训练另一个神经网络的超参数"><a href="#补充——用神经网络训练另一个神经网络的超参数" class="headerlink" title="补充——用神经网络训练另一个神经网络的超参数"></a>补充——用神经网络训练另一个神经网络的超参数</h2><p>&emsp;&emsp;看完李宏毅深度学习后的补充，他在教学视频中也讲述了如何调整超参数，有个说的挺有创意的，就是<strong>用神经网络来训练超参数如何取值</strong>。典型的例子就是 <strong>Swish</strong>，它可以用神经网络训练出最好的几个激活函数。</p>
<h1 id="batch-normalization——对激活值均值归一化"><a href="#batch-normalization——对激活值均值归一化" class="headerlink" title="batch normalization——对激活值均值归一化"></a>batch normalization——对激活值均值归一化</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;这个原来不是一个算法，它就是让我们对神经网络的每一层都做一次normalization，从而提供性能，而算法是在batch中做的，所以叫这名。<br>&emsp;&emsp;<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036#/learn/content?type=detail&amp;id=2001701055&amp;cid=2001693088" target="_blank" rel="noopener">视频地址</a>，第25章写了均值归一化，它对输入值进行了均值归一，更易于算法优化。而batch normalization对激活值进行了均值归一化，说白了是一个东西。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>&emsp;&emsp;代价函数为<script type="math/tex">cost(\hat{y}, y) = - \sum^n_{j=1} y_j * log(\hat{y_j})</script>。<br><div class="note primary">
            <p>但是这里可能会有点奇怪。因为二元分类的代价函数是<script type="math/tex">cost(\hat{y}, y) = - \sum^n_{j=1} (y_j * log(\hat{y_j}) + (1 - y_j) * log(\hat{1-y_j}))</script>。怎么多元分类的表达式那么短？</p>
          </div></p>
<h1 id="选择深度学习框架"><a href="#选择深度学习框架" class="headerlink" title="选择深度学习框架"></a>选择深度学习框架</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E9%80%89%E6%8B%A9.jpg" alt="深度学习框架选择"></p>
<h1 id="序列模型——RNN"><a href="#序列模型——RNN" class="headerlink" title="序列模型——RNN"></a>序列模型——RNN</h1><p>&emsp;&emsp;<a href="https://yan624.github.io/·学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">传送门</a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>simple NN</tag>
        <tag>back propagation</tag>
        <tag>gradient descent</tag>
        <tag>bias</tag>
        <tag>variance</tag>
        <tag>regularization</tag>
        <tag>dropout</tag>
        <tag>early stopping</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>next主题中使用Mathjax</title>
    <url>/assorted/hexo/next%E4%B8%BB%E9%A2%98%E4%B8%AD%E4%BD%BF%E7%94%A8Mathjax.html</url>
    <content><![CDATA[<p>百度了一圈最后还是没有解决问题，因为其他的博客都说要去<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code>文件修改源代码，但是我压根没有这个文件，hexo-renderer-kramed这个文件夹不存在。</p>
<p>最后还是自己试出来了，步骤如下：</p>
<ol>
<li>安装kramed。hexo 默认的渲染引擎是 marked，但是 marked 不支持 mathjax。所以要卸载它。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">npm <span class="keyword">uninstall</span> hexo-renderer-marked <span class="comment">--save</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-renderer-kramed <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<a id="more"></a>
<ol>
<li><p>停用hexo-math，然后安装 hexo-renderer-mathjax 包。其实我也不知道怎么判断自己有没有在使用hexo-math。总而言之卸载就行了，管它存不存在。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">npm <span class="keyword">uninstall</span> hexo-math <span class="comment">--save</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-renderer-mathjax <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>开启mathjax，在next主题文件夹里，具体路径：/themes/next/_config.yml<br>你只需要把enable属性改为true即可</p>
<figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"># Math Equations Render Support</span><br><span class="line">math:</span><br><span class="line">  enable: <span class="keyword">true</span></span><br><span class="line"></span><br><span class="line">  # <span class="keyword">Default</span>(<span class="keyword">true</span>) will load mathjax/katex script <span class="keyword">on</span> demand</span><br><span class="line">  # That <span class="keyword">is</span> it only render those page who <span class="keyword">has</span> `mathjax: <span class="keyword">true</span>` <span class="keyword">in</span> Front Matter.</span><br><span class="line">  # <span class="keyword">If</span> you <span class="keyword">set</span> it <span class="keyword">to</span> <span class="keyword">false</span>, it will load mathjax/katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: <span class="keyword">true</span></span><br><span class="line"></span><br><span class="line">  engine: mathjax</span><br><span class="line">  #engine: katex</span><br><span class="line"></span><br><span class="line">  # hexo-rendering-pandoc (<span class="keyword">or</span> hexo-renderer-kramed) needed <span class="keyword">to</span> full MathJax support.</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: <span class="keyword">true</span></span><br><span class="line">    # Use <span class="number">2.7</span>.<span class="number">1</span> <span class="keyword">as</span> <span class="keyword">default</span>, jsdelivr <span class="keyword">as</span> <span class="keyword">default</span> CDN, works everywhere even <span class="keyword">in</span> China</span><br><span class="line">    cdn: <span class="comment">//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span></span><br><span class="line">    # <span class="keyword">For</span> direct link <span class="keyword">to</span> MathJax.js <span class="keyword">with</span> CloudFlare CDN (cdnjs.cloudflare.com)</span><br><span class="line">    #cdn: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br><span class="line"></span><br><span class="line">    # See: https:<span class="comment">//mhchem.github.io/MathJax-mhchem/</span></span><br><span class="line">    #mhchem: <span class="comment">//cdn.jsdelivr.net/npm/mathjax-mhchem@3</span></span><br><span class="line">    #mhchem: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0</span></span><br><span class="line"></span><br><span class="line">  # hexo-renderer-markdown-it-plus (<span class="keyword">or</span> hexo-renderer-markdown-it <span class="keyword">with</span> markdown-it-katex plugin) needed <span class="keyword">to</span> full Katex support.</span><br><span class="line">  katex:</span><br><span class="line">    # Use <span class="number">0.7</span>.<span class="number">1</span> <span class="keyword">as</span> <span class="keyword">default</span>, jsdelivr <span class="keyword">as</span> <span class="keyword">default</span> CDN, works everywhere even <span class="keyword">in</span> China</span><br><span class="line">    cdn: <span class="comment">//cdn.jsdelivr.net/npm/katex@0.7.1/dist/katex.min.css</span></span><br><span class="line">    # CDNJS, provided <span class="keyword">by</span> cloudflare, maybe the best CDN, but <span class="keyword">not</span> works <span class="keyword">in</span> China</span><br><span class="line">    #cdn: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css</span></span><br><span class="line"></span><br><span class="line">    copy_tex:</span><br><span class="line">      # See: https:<span class="comment">//github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex</span></span><br><span class="line">      enable: <span class="keyword">false</span></span><br><span class="line">      copy_tex_js: <span class="comment">//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js</span></span><br><span class="line">      copy_tex_css: <span class="comment">//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>最后一步，修改/themes/next/layout/_layou.swig文件<br>将如下代码添加到该文件&lt;/body&gt;标签的上面一行</p>
<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="params">&lt;script type="text/x-mathjax-config"&gt;</span></span><br><span class="line">MathJax.Hub.Config(&#123;</span><br><span class="line">	<span class="comment">//下面的HTML-CSS和SVG用于在小屏幕上，数学公式可以自动换行，我测试过后发现无效，但是貌似有人可以。</span></span><br><span class="line">	<span class="string">"HTML-CSS"</span>: &#123; </span><br><span class="line"><span class="symbol">		linebreaks:</span> &#123; </span><br><span class="line"><span class="symbol">			automatic:</span> true </span><br><span class="line">		&#125; </span><br><span class="line">	&#125;,SVG: &#123;</span><br><span class="line"><span class="symbol">		linebreaks:</span> &#123; </span><br><span class="line"><span class="symbol">			automatic:</span> true </span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,menuSettings: &#123;</span><br><span class="line"><span class="symbol">		zoom:</span> <span class="string">"None"</span></span><br><span class="line">	&#125;,</span><br><span class="line"><span class="symbol">	showMathMenu:</span> false,</span><br><span class="line"><span class="symbol">	jax:</span> [<span class="string">"input/TeX"</span>,<span class="string">"output/CommonHTML"</span>],</span><br><span class="line"><span class="symbol">	extensions:</span> [<span class="string">"tex2jax.js"</span>],</span><br><span class="line"><span class="symbol">	TeX:</span> &#123;</span><br><span class="line"><span class="symbol">		extensions:</span> [<span class="string">"AMSmath.js"</span>,<span class="string">"AMSsymbols.js"</span>],</span><br><span class="line"><span class="symbol">		equationNumbers:</span> &#123;</span><br><span class="line"><span class="symbol">			autoNumber:</span> <span class="string">"AMS"</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,tex2jax: &#123;</span><br><span class="line"><span class="symbol">		inlineMath:</span> [[<span class="string">"\\("</span>, <span class="string">"\\)"</span>]],</span><br><span class="line"><span class="symbol">		displayMath:</span> [[<span class="string">"\\["</span>, <span class="string">"\\]"</span>]]</span><br><span class="line">	&#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="params">&lt;/script&gt;</span></span><br><span class="line"><span class="params">&lt;script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/<span class="number">2.7</span><span class="number">.1</span>/MathJax.js?config=TeX-MML-AM_CHTML"&gt;</span><span class="params">&lt;/script&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>1-3步参考<a href="https://www.jianshu.com/p/523e806d6681" target="_blank" rel="noopener">如何在hexo中支持Mathjax</a><br>4步参考<a href="http://npm.taobao.org/package/hexo-renderer-kramed" target="_blank" rel="noopener">hexo-renderer-kramed</a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>mathjax</tag>
      </tags>
  </entry>
  <entry>
    <title>《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/dl/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%ADweight%E7%9A%84%E8%A1%A8%E7%A4%BA%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p><a href="https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html" target="_blank" rel="noopener">本文中文章节地址</a><br><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">本文英文章节地址</a></p>
<h3 id="神经网络中一些符号的定义"><a href="#神经网络中一些符号的定义" class="headerlink" title="神经网络中一些符号的定义"></a>神经网络中一些符号的定义</h3><p>引用自<a href="https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html#%E7%83%AD%E8%BA%AB%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E8%AE%A1%E7%AE%97%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%82%E7%82%B9" target="_blank" rel="noopener">原文中文翻译</a></p>
<blockquote>
<p>我们首先给出网络中权重的清晰定义。我们使用<script type="math/tex">w^l_{jk}</script>表示从 <script type="math/tex">(l−1)^{th}</script> 层的 <script type="math/tex">k^{th}</script> 个神经元到 <script type="math/tex">(l)^{th}</script> 层的 <script type="math/tex">j^{th}</script> （<font style="color:red">注意：这个地方中文文章中写错了</font>，他写成了<script type="math/tex">l^{th}</script>）个神经元的链接上的权重。例如，下图给出了第二隐藏层的第四个神经元到第三隐藏层的第二个神经元的链接上的权重：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E9%93%BE%E6%8E%A5%E4%B8%8A%E7%9A%84%E6%9D%83%E9%87%8D%E8%A1%A8%E7%A4%BA%E5%9B%BE%E8%A7%A3.png" alt="神经网络中的神经元链接上的权重表示图解"><br>我们对网络偏差和激活值也会使用类似的表示。显式地，我们使用 <script type="math/tex">b^l_J</script> 表示在 <script type="math/tex">l^{th}</script> 层 <script type="math/tex">j^{th}</script> 个神经元的偏差，使用 <script type="math/tex">a^l_j</script> 表示 <script type="math/tex">l^{th}</script> 层 <script type="math/tex">j^{th}</script> 个神经元的激活值。下面的图清楚地解释了这样表示的含义：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE%E5%92%8C%E6%BF%80%E6%B4%BB%E5%80%BC%E5%9B%BE%E8%A7%A3.png" alt="神经网络中的偏差和激活值图解"></p>
</blockquote>
<a id="more"></a>
<p>说白了就是<strong>前一层的</strong>的神经元到后一层的神经元上的链接的权重。<br>这里解释一下，我在<a href="https://www.bilibili.com/video/av9770302" target="_blank" rel="noopener">李宏毅深度学习的视频</a>中，也听到他讲过。<script type="math/tex">w^l_{jk}</script>中的j和k在这个公式中是写反的，可以想想：上文说到<script type="math/tex">w^l_{jk}</script>是代表第k个神经元到第j个神经元链接上的权重，按逻辑来说，似乎写成<script type="math/tex">w^l_{kj}</script>更合理。因为我们说话写字都是按顺序的，没有说是反着来的。<br>之所以这么反着写，是因为后面在对权重（weight）、激活值（activations）进行向量化时，不需要加一个转置。接下来解释一下为什么不用加：<br>以上图为例，有如下权重：<script type="math/tex">w^3_{21}</script> <script type="math/tex">w^3_{22}</script> <script type="math/tex">w^3_{23}</script> <script type="math/tex">w^3_{24}</script> 现在将上标3移除组成一个向量——<script type="math/tex">\begin{pmatrix}w_{21}&w_{22}&w_{23}&w_{24}\end{pmatrix}</script>，大家已经发现了吧，我将这个向量横着写的。众所周知，矩阵的下标第一个数字代表矩阵的行，第二个数字代表矩阵的列。所以如果将w这个矩阵补全就是下面这样。</p>
<script type="math/tex; mode=display">\begin{pmatrix}
w_{11}&w_{12}&w_{13}&w_{14}\\
w_{21}&w_{22}&w_{23}&w_{24}\\
\end{pmatrix}</script><p>这是一个2行4列的矩阵，因为第三层layer只有两个神经元，自然只有两个权重向量。先给出公式：<script type="math/tex">z^3_2</script> = <script type="math/tex">w^3_2</script> * <script type="math/tex">a^2</script>，这里原本应该还要加上偏差b，但是由于改起来太麻烦以下都不加上b了，该公式就是<script type="math/tex">\begin{pmatrix}w_{21}&w_{22}&w_{23}&w_{24}\end{pmatrix}</script>乘<script type="math/tex">a^2</script>，<script type="math/tex">a^2</script>就是第二层的激活值的<strong>列</strong>向量表现形式，这很好理解就不解释了。运用考研线性代数的知识可以知道，行向量乘以列向量结果是一个<strong>数值</strong>，这样就得到了<script type="math/tex">z^3_2</script>的值。<br>我们再将第二层到第三层的其他向量补上就是如下的矩阵运算公式，为了方便查看，我将上标又加上了。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
z^3_1\\
z^3_2\\
\end{pmatrix} = 
\begin{pmatrix}
w^3_{11}&w^3_{12}&w^3_{13}&w^3_{14}\\
w^3_{21}&w^3_{22}&w^3_{23}&w^3_{24}\\
\end{pmatrix} * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix}</script><p>上式可以简化为<script type="math/tex">a^l = \sigma(z^l)</script>，其中<script type="math/tex">z^l</script> = <script type="math/tex">w * a^{l-1}</script>。合并之后写为<script type="math/tex">a^l = \sigma(w * a^{l-1})</script>，<script type="math/tex">\sigma</script>只是代表某个函数而已。</p>
<h4 id="如果不交换jk的位置"><a href="#如果不交换jk的位置" class="headerlink" title="如果不交换jk的位置"></a>如果不交换jk的位置</h4><p>绕了这么一大圈，终于说完了为什么jk交换位置要好，因为w被向量化后被表示成行向量了。行乘列直接就可以乘，不需要再对w转置。如果我们不交换jk的位置，那么w表示成如下形式：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix}</script><p>下面这样是乘不了的。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix} * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix} = 
个屁</script><p>如果下面这样就可以。T代表将矩阵转置。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix}^T * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix} = 
\begin{pmatrix}
z^3_1\\
z^3_2\\
\end{pmatrix}</script><p>简化之后就是<script type="math/tex">\sigma(w^T</script> * <script type="math/tex">a^{l-1})</script> = <script type="math/tex">a^l</script>。所以绕了那么大一圈，其实只是为了少加一个T。。。</p>
<h4 id="w的下标总结"><a href="#w的下标总结" class="headerlink" title="w的下标总结"></a>w的下标总结</h4><p>说实话我还是希望加上T的，因为上面说了那么一大堆，说实话是很绕的，还不如直接不交换jk的位置。从各个方面来说都是极为通顺的，不妥的仅仅是多加了T。<br>最后还是无法理解别人为什么将jk交换位置的，可以试着不去想神经网络，完全去思考数学中的矩阵。我给点提示，对于元素<script type="math/tex">w^3_{24}</script>首先3和2是定死的，同理得到<script type="math/tex">w^3_{21}</script> <script type="math/tex">w^3_{22}</script> <script type="math/tex">w^3_{23}</script>，现在开始不要想关于神经网络的事，想想向量<script type="math/tex">\begin{pmatrix}w^3_{21}&w^3_{22}&w^3_{23}&w^3_{24}\end{pmatrix}</script>是不是行向量？<br>再想想下面的向量是不是列向量</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{12}\\
w^3_{22}\\
w^3_{32}\\
w^3_{42}\\
\end{pmatrix}</script><p>我仅仅是交换了jk的位置对吧，虽然说数值没有任何变化，但是从数学角度讲，从行向量转为了列向量。</p>
<p>然而我觉得这样徒增了学习成本。我佛他们。</p>
<h3 id="计算输出的公式"><a href="#计算输出的公式" class="headerlink" title="计算输出的公式"></a>计算输出的公式</h3><p>公式比较简单，就是<script type="math/tex">a^l = \sigma(z^l)</script>，<script type="math/tex">z^l</script> = <script type="math/tex">w^T</script> * <script type="math/tex">a^{l-1}</script> + <script type="math/tex">b^l</script></p>
<!-- more -->]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.</title>
    <url>/IT-stuff/python/FutureWarning-Conversion-of-the-second-argument-of-issubdtype-from-float-to-np-floating-is-deprecated-In-future-it-will-be-treated-as-np-float64-np-dtype-float-type.html</url>
    <content><![CDATA[<p>网上很多人说要修改h5py的版本，但是我压根没装这个库。<br><a href="https://blog.csdn.net/u013092293/article/details/80447201" target="_blank" rel="noopener">参考文章</a><br>将numpy版本降低即可，我原先好像是1.16，记不清了，我没留意到。<br><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">conda install numpy==<span class="number">1.13</span><span class="number">.0</span></span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（六）：SVM</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9ASVM.html</url>
    <content><![CDATA[<p>看《机器学习实战》这本书的 SVM 部分，感觉始终无法理解，因为书里把数学公式的推导直接省略了。所以在b站找了视频学习 SVM。<br>本文<a href="https://zhuanlan.zhihu.com/p/24638007" target="_blank" rel="noopener">参考文章</a><br>本文<a href="https://www.bilibili.com/video/av37947862/?p=75" target="_blank" rel="noopener">参考视频</a></p>
<p>首先分割超平面（separating hyperplane）的函数表达式是<script type="math/tex">w^T * x + b = 0</script>，而它上下两条间隔最远的超平面的表达式分别为</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 1 \\
    w^T * x + b = & -1 \\
\end{align}</script><p>至于为什么正好等于 1 和 -1，其实是为了方便计算。实际上可以等于任何值。看以下推导，先让其等于连个随机的值，比如 2 和 -3：<br><a id="more"></a></p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 2 \\
    w^T * x + b = & -3 \\
\end{align}</script><p>解一个不等式</p>
<script type="math/tex; mode=display">
\begin{align}
    2u + v = & 1 \\
    -3u + v = & -1 \\
\end{align}</script><p>解得 u = <script type="math/tex">\frac{2}{5}</script>， v = <script type="math/tex">\frac{1}{5}</script><br>于是使</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 2 \\
    w^T * x + b = & -3 \\
\end{align}</script><p>左右分别乘上<script type="math/tex">\frac{2}{5}</script>再加上<script type="math/tex">\frac{1}{5}</script>，即可得到</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 1 \\
    w^T * x + b = & -1 \\
\end{align}</script><p>而 W 和 b 只不过是一个表示的符号而已，虽然经过运算，两个 W 和两个 b 已经不是同一个了，但是这么表示没太大问题。<br>将连个表达式更进一步表示为</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x_1 + b = & 1 \\
    w^T * x_2 + b = & -1 \\
\end{align}</script><p>两者相减得</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * (x_1 - x_2) = 2 \\
\end{align}</script><p>也即</p>
<script type="math/tex; mode=display">
\begin{align}
    ||w^T|| * ||(x_1 - x_2)|| cos\theta = 2 \\
\end{align}</script><p>两条竖线代表，向量的模长，<script type="math/tex">\theta</script>代表 <script type="math/tex">W^T</script>和<script type="math/tex">x_1 - x_2</script>之间的夹角。<br>由于初中知识，我们知道：<script type="math/tex">cos\alpha</script>=邻边比斜边，即邻边=斜边*<script type="math/tex">cos\alpha</script>。所以</p>
<script type="math/tex; mode=display">
\begin{align}
    ||w^T|| * d = & 2 \\
    d = \frac{2}{||w^T||}
\end{align}</script><!-- more -->]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（五）：PCA</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9APCA.html</url>
    <content><![CDATA[<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（四）：K均值（K-means）</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9AK%E5%9D%87%E5%80%BC%EF%BC%88K-means%EF%BC%89.html</url>
    <content><![CDATA[<h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><p>K-均值(K-means)算法是无监督算法，也是聚类(clustering)算法。</p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>随机初始化几个点作为簇的质心，初始化方式有多种，可以自行选择。已知两种：<ul>
<li>随机选择K个样本作为簇的质心。来源于吴恩达机器学习视频</li>
<li>min + (max - min) * (0到1之间的小数)，其中最大值最小值均代表。来源于《机器学习实战》</li>
</ul>
</li>
<li>计算某个样本到每个簇的质心之间的代价（距离），代价函数有多种可自行选择。比如：<ul>
<li>均方误差(mse)<br>选出其中最小的代价，并求出簇的索引，然后将该样本划分给该质心所属的簇。每个样本都执行这步直到样本遍历完毕。</li>
</ul>
</li>
<li>经过步骤2，每个样本都归属于一个簇。然后遍历每一个簇，将簇中的数据求均值，将该均值作为簇的新质心。<a id="more"></a></li>
<li>重复以上步骤，直到发现每个样本都归属于某个簇，并且样本归属不会再发生变化。<br>简化如下：<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line">随机初始化质心</span><br><span class="line">	<span class="keyword">while</span> <span class="literal">true</span></span><br><span class="line">		<span class="keyword">for</span> 每个样本</span><br><span class="line">			<span class="keyword">for</span> 每个质心</span><br><span class="line">				计算每个样本到质心的代价</span><br><span class="line">			选出最小的代价</span><br><span class="line">			将样本分配给这个簇</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span> 每个质心</span><br><span class="line">			求出该簇所属的样本的均值</span><br><span class="line">			将该均值设置为簇的新质点</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">if</span> 样本归属不再发生变化</span><br><span class="line">			<span class="keyword">break</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="举一个简单的例子"><a href="#举一个简单的例子" class="headerlink" title="举一个简单的例子"></a>举一个简单的例子</h3><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/kmeans/simple_demo" target="_blank" rel="noopener">这里有一个简单例子</a><br><!-- more --></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
        <tag>kmeans</tag>
      </tags>
  </entry>
  <entry>
    <title>selenium下打开Chrome闪退</title>
    <url>/IT-stuff/python/selenium%E4%B8%8B%E6%89%93%E5%BC%80Chrome%E9%97%AA%E9%80%80.html</url>
    <content><![CDATA[<p>网上的其他解决办法都不对，后来发现了是因为一个很逗的错。<br>以下代码闪退<br><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">webdriver.Chrome()</span><br></pre></td></tr></table></figure></p>
<p>以下代码正常运行<br><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">chrome = webdriver.Chrome()</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>error</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title>使用WikiExtractor处理维基百科上的数据步骤-2019年3月</title>
    <url>/%E4%BD%BF%E7%94%A8WikiExtractor%E5%A4%84%E7%90%86%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE%E6%AD%A5%E9%AA%A4-2019%E5%B9%B43%E6%9C%88.html</url>
    <content><![CDATA[<p>网上有很多博客介绍如何使用开源工具wiki extractor解压提取维基百科上的数据，但是我试了一下他们的命令发现没一个能用的，而且他们对于该命令基本都是一笔带过，没有做深入的解释，对于我这种小白在第一步就卡死了。另外他们大部分人都是用linux系统，还有些使用Mac系统，用Windows的只有少数，而且他们提供的命令还不好用。所以我在此提供<strong>Windows系统</strong>的使用办法。</p>
<ol>
<li>数据来源：<a href="https://dumps.wikimedia.org/zhwiki/20190320/" target="_blank" rel="noopener">维基百科数据</a></li>
<li>首先进入<a href="https://github.com/attardi/wikiextractor" target="_blank" rel="noopener">wiki extractor的官网</a>。发现里面有很多py文件，与其他人写的博客上的教程完全不一样。别人的教程只有一个<em>WikiExtractor.py</em>文件。</li>
<li>将该项目clone下来，放在你的项目中。如下图：<a id="more"></a>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/wiki%20extractor%E6%96%87%E4%BB%B6%E5%AD%98%E6%94%BE%E8%B7%AF%E5%BE%84.png" alt="wiki extractor文件存放路径"><br>我下的是压缩包，解压开后就存放在wikiextractor-master文件夹。至于test.py只是用来测试gensim的word2vec算法好不好用的，无视就好。</li>
<li>进入wikiextractor-master文件夹执行<code>python setup.py install</code>，该步骤用于安装wiki extractor。其实<a href="https://github.com/attardi/wikiextractor" target="_blank" rel="noopener">wiki extractor的官网</a>也写了这一步。但是不知道为什么其他人的博客没人介绍。</li>
<li>退出该文件夹到nlp_learning文件夹，执行<code>python wikiextractor-master/WikiExtractor.py -b 1200M -o extracted zhwiki-20190320-pages-articles-multistream1.xml-p1p162886.bz2</code>，这个命令应该很好理解，说一下里面的extracted，它是目标文件夹，就是提取出来的文本存放的那个文件夹。可以看到上面的图片里有这个文件夹。-b代表每多大字节输出一份文件，参数的具体使用方法可以到<a href="https://github.com/attardi/wikiextractor#usage" target="_blank" rel="noopener">这里</a>查询。</li>
<li>接下慢慢等就行了，我的文件157MB，大概洗脸刷牙后就提取完了。</li>
</ol>
<p>参考了<a href="https://blog.csdn.net/grafx/article/details/78575850" target="_blank" rel="noopener">博客</a>，但是他这篇博客提供命令我也执行不了。如果你也成功不了，可以试试我的步骤。</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>WikiExtractor</tag>
      </tags>
  </entry>
  <entry>
    <title>运用文本相似度实现（证券）智能客服的记录</title>
    <url>/%E8%BF%90%E7%94%A8%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%AE%9E%E7%8E%B0%EF%BC%88%E8%AF%81%E5%88%B8%EF%BC%89%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E7%9A%84%E8%AE%B0%E5%BD%95.html</url>
    <content><![CDATA[<p>数据来源于维基百科。<br>数据来源：<a href="https://dumps.wikimedia.org/zhwiki/20190320/" target="_blank" rel="noopener">维基百科数据</a></p>
<h3 id="从维基百科提取数据"><a href="#从维基百科提取数据" class="headerlink" title="从维基百科提取数据"></a>从维基百科提取数据</h3><p>不会操作的，具体参考我这篇<a href="https://yan624.github.io/%E4%BD%BF%E7%94%A8WikiExtractor%E5%A4%84%E7%90%86%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE%E6%AD%A5%E9%AA%A4-2019%E5%B9%B43%E6%9C%88.html">博客:使用WikiExtractor处理维基百科上的数据步骤-2019年3月</a><br><a id="more"></a></p>
<h3 id="将数据从繁体字转为简体字"><a href="#将数据从繁体字转为简体字" class="headerlink" title="将数据从繁体字转为简体字"></a>将数据从繁体字转为简体字</h3><p>具体参考该博客：<a href="https://blog.csdn.net/sinat_29957455/article/details/81290356" target="_blank" rel="noopener">windows使用opencc中文简体和繁体互转</a>，我试过里面的教程，没有任何问题。直接将博客往下拉，看<em>3、OpenCC的使用</em>即可。执行命令后，命令行会没有任何反应，这时只是在跑代码而已，我的文件157兆，大概只用了15秒。我把我的命令贴下来：<br><figure class="highlight taggerscript"><table><tr><td class="code"><pre><span class="line">E:<span class="symbol">\p</span>ython_workspace<span class="symbol">\n</span>lp_learning<span class="symbol">\o</span>pencc-1.0.4<span class="symbol">\b</span>in<span class="symbol">\o</span>pencc -i E:<span class="symbol">\p</span>ython_workspace<span class="symbol">\n</span>lp_learning<span class="symbol">\z</span>hwiki-20190320-pages-articles<span class="symbol">\A</span>A<span class="symbol">\w</span>iki_00 -o E:<span class="symbol">\p</span>ython_workspace<span class="symbol">\n</span>lp_learning<span class="symbol">\z</span>hwiki-20190320-pages-articles<span class="symbol">\o</span>pencc<span class="symbol">\w</span>iki_00 -c E:<span class="symbol">\p</span>ython_workspace<span class="symbol">\n</span>lp_learning<span class="symbol">\o</span>pencc-1.0.4<span class="symbol">\s</span>hare<span class="symbol">\o</span>pencc<span class="symbol">\t</span>2s.json</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title>linux Ubuntu安装NVIDIA驱动</title>
    <url>/IT-stuff/linux/linux-Ubuntu%E5%AE%89%E8%A3%85NVIDIA%E9%A9%B1%E5%8A%A8.html</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/ghw15221836342/article/details/79571559" target="_blank" rel="noopener">参考文章</a><br>以下步骤基于ubuntu16，下载的文件在Downloads文件夹。</p>
<ol>
<li>使用命令<code>lspci |grep -i nvidia</code>，查看显卡型号</li>
<li>然后去<a href="http://www.nvidia.com/Download/index.aspx?lang=en-us" target="_blank" rel="noopener">官网</a>查找对应显卡版本。</li>
<li>下载NVIDIA驱动文件，名称类似为NVIDIA-Linux-x86_64-375.20.run。</li>
<li>卸载已有的驱动。由于我的是新机器，所以卸载步骤未执行。<a id="more"></a></li>
<li>然后使用命令禁用nouveau，使用以下命令编辑配置文件。<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">sudo <span class="keyword">vim</span> /etc/modprobe.d/blacklist.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>在最后一行添加： <code>blacklist nouveau</code></p>
<ol>
<li>之后执行<code>sudo update-initramfs -u</code>，完成之后需要重启电脑，重启电脑命令：<code>reboot</code>。电脑重启之后执行<code>lsmod |grep nouveau  #没有输出，即说明安装成功</code>.</li>
<li>接下来开始安装驱动。。。</li>
<li>使用<code>ctrl+alt+f3</code>命令进入命令行界面。</li>
<li><p>给驱动run文件赋予执行权限（若出现[sudo] 计算机名 ◆ ◆ ◆ ◆，这是因为安装了中文的ubuntu，输入登录密码即可）。使用如下命令：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">cd</span> <span class="selector-tag">Downloads</span></span><br><span class="line"><span class="selector-tag">sudo</span> <span class="selector-tag">chmod</span> <span class="selector-tag">a</span>+<span class="selector-tag">x</span> <span class="selector-tag">NVIDIA-Linux-x86_64-375</span><span class="selector-class">.20</span><span class="selector-class">.run</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>安装：<code>sudo ./NVIDIA-Linux-x86_64-375.20.run –no-opengl-files</code></p>
</li>
<li>一直选择OK、yes、continue按回车键</li>
<li>可能会出现没有gcc的错误，如果出现此问题，安装gcc即可。使用<code>sudo apt-get install gcc</code>安装。</li>
<li>可能出现没有make工具的错误。使用<code>sudo apt-get install make</code>安装</li>
<li><code>nvidia-smi</code>查看是否安装成功。</li>
</ol>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习算法（三）：决策树</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91.html</url>
    <content><![CDATA[<h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>由于新手不太懂术语，规定行为一条数据，列为一组特征。<br><strong>特征</strong>代表一列数据。如性别、年龄、身高等。<br><strong>分类</strong>代表一个特征中不同的分类。如性别中分类为男女，天气中分类为晴、阴、雨、雪等，收入中分类为贫困、低收入、小康、中高收入者、富人等。</p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>引用自《机器学习实战》</p>
<blockquote>
<p>划分数据集的大原则：将无需的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种办法就是使用信息论度量信息，信息论是量化处理信息的分支学科。我们可以在划分数据之前或之后使用信息论量化度量信息的内容。<br>在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。</p>
</blockquote>
<p>节选</p>
<blockquote>
<p>集合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父克劳德·香农。</p>
</blockquote>
<a id="more"></a>
<p><strong>下面这一段待修改，在进一步学习后发现有问题</strong></p>
<p>如果不懂什么是信息增益（information）和熵（entropy）也无所谓，只需要知道熵越大则不确定性越大即可。因为我们需要的是不确定性小的<strong>特征</strong>，所以熵越小越好。试想决策树就是将信息一分为二，是不是最好某一个特征只有两种<strong>分类</strong>，如男、女。这样不确定性是极小的，因为它只能分为男和女。如果某一个特征有多个分类，如鸟类下有多种详细的分类，那么该特征的熵值就会很大，不确定性也很大。想想决策树就是将信息一分为二，我该选哪条线将鸟类一分为二？这是很难想的事情，所以这个特征并不好。以上只是举例，在具体项目中不一定对，并且决策树也不一定是二叉树（此处存疑？）。<br>熵的计算公式如下，其中pi是选择该分类的概率，即有20条数据，其中13条是男性，7条是女性。则男性的pi=13/20，女性的pi=7/20。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E5%85%AC%E5%BC%8F.jpg" alt="信息熵的公式"></p>
<h3 id="决策树构建步骤"><a href="#决策树构建步骤" class="headerlink" title="决策树构建步骤"></a>决策树构建步骤</h3><p>决策树构建首先需要一个根节点，这是毋庸置疑的，但是根节点也不是一拍脑门就突然有了的。需要使用一些算法，现在一些常用的算法如：ID3(信息增益)、C4.5(信息增益率，解决ID3的问题，考虑自身熵)、CART(使用GINI系数来当做衡量标准，GINI系数和熵的衡量标准类似，但是计算方式不同)等。<br>以下使用<a href="https://baike.baidu.com/item/ID3%E7%AE%97%E6%B3%95/5522381" target="_blank" rel="noopener">ID3算法</a>，<strong>给出如下训练数据</strong>，来源于唐宇迪机器学习视频中的截图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4%E2%80%94%E2%80%94%E4%B8%BE%E7%9A%84%E4%BE%8B%E5%AD%90%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE.jpg" alt="例子中的数据"></p>
<ol>
<li>首先计算全部数据的熵值，其实就是计算一下给出的数据中的最后一列（最后一列可以当做y）的熵值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E7%9A%84%E7%86%B5%E5%80%BC.jpg" alt="计算原始数据的熵值"></li>
<li>计算其他特征的熵值。解释一下什么叫其他特征的熵值：比如 outlook 这个特征值，如果要计算它，首先需要知道 outlook 这个特征值分为 3 类，sunny、overcast 和 rainy，然后计算依次计算这三种类别的熵值。具体步骤是，比如计算 sunny 的熵值，那就将原数据中 outlook 不是 sunny 类别的数据全部删除，然后使用信息熵的公式计算剩余数据的熵值。其他的类别计算方式以此类推。计算完毕之后就得到了三个熵值，然后按照类别在特征中的比例相加即可。<br>&emsp;&emsp;如下图右侧，当天气为 sunny 时去打球的概率为<script type="math/tex">\frac{2}{5}</script>，不去打球的概率为<script type="math/tex">\frac{3}{5}</script>，利用上面的熵值公式计算结果为 0.971。再分别计算 outlook 其他<strong>分类</strong>：overcast 和 rainy 的熵值，此时得到了三个不同分类的熵值，即 sunny为<script type="math/tex">\frac{5}{14}</script>， overcast为<script type="math/tex">\frac{4}{14}</script>。最终 outlook 的熵值即为<script type="math/tex">\frac{5}{14} * 0.971 + \frac{4}{14} * 0 + \frac{5}{14} * 0.971 = 0.693</script>。最后其他特征值也需要分别计算它们的分类的熵值。<br>&emsp;&emsp;<strong>这里可能会出现困惑</strong>，如果每个特征值都有多个分类，那计算量不是特别大？而且这里的分类是文字，计算量较小，如果分类是数字那么分类几乎不会重复，计算量不是突破天际？我在学决策树时出现了这种疑惑，后来发现算法确实是这样的，并没有错。<br>&emsp;&emsp;分类为文字称为<strong>离散值</strong>，分类为数值称为<strong>连续值</strong>。上面说到决策树碰到连续值计算量会非常大，解决办法：<a href="https://www.baidu.com/s?wd=%E5%86%B3%E7%AD%96%E6%A0%91%E8%BF%9E%E7%BB%AD%E5%80%BC%E5%A4%84%E7%90%86" target="_blank" rel="noopener">决策树连续值处理</a><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E5%85%B6%E4%BB%96%E7%89%B9%E5%BE%81%E7%9A%84%E7%86%B5%E5%80%BC.jpg" alt="计算其他特征的熵值"><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4%E2%80%94%E2%80%94%E5%85%B6%E4%BB%96%E7%89%B9%E5%BE%81%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%88%86%E7%B1%BB.jpg" alt="其他特征以及其分类"></li>
<li>通过1和2对比，选择出熵值最低的特征，并且熵值需要小于原始数据的基本熵值。<br>&emsp;&emsp;上述123步，均可以直接在choose_best_feature_index(dataset)函数中计算获得，顶多加几个封装过的函数，比如calculate_entropy(dataset)，split_dataset(dataset, axis, value)等。</li>
<li>删除已作出决策的数据，并将数据再次执行123步操作。<br>&emsp;&emsp;比如经过一轮筛选发现 outlook 为熵值最低的特征，则按照 outlook 的 3 个类别，决策树将会有 3 条分支，节点与节点连接的边上的值分别是 sunny、overcast 和 rainy。<br>&emsp;&emsp;首先按类别 sunny 继续往下决策，比如所有数据中，类别为 sunny 的数据全部为 yes，即想要出去打球，那么决策树递归结束。因为这个算法的目的就是要判断在这些特征下，这一天去打球是否合适，如果 sunny 下的每条数据都是 yes，就代表我们的目的已经达到了，也就没必要继续决策了。接下来假设类别 overcast 与 sunny 相反，那么就将类别是 overcast 的所有数据从原数据集中删去，再重复 123 步。</li>
<li>构造决策树，决策树可以使用递归算法构建。<br>&emsp;&emsp;新手入门系列，完全可以仅使用python的dict来存储决策树。如：{‘tearRate’: {‘reduced’: ‘no lenses’, ‘normal’: {‘astigmatic’: {‘no’: {‘age’: {‘young’: ‘soft’, ‘pre’: ‘soft’, ‘presbyopic’: {‘prescript’: {‘hyper’: ‘soft’, ‘myope’: ‘no lenses’}}}}, ‘yes’: {‘prescript’: {‘hyper’: {‘age’: {‘young’: ‘hard’, ‘pre’: ‘no lenses’, ‘presbyopic’: ‘no lenses’}}, ‘myope’: ‘hard’}}}}}}</li>
</ol>
<h3 id="举一个简单的例子"><a href="#举一个简单的例子" class="headerlink" title="举一个简单的例子"></a>举一个简单的例子</h3><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/decision_tree/simple_demo" target="_blank" rel="noopener">这里有一个简单例子</a><br>运行结果：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%80%E5%8D%95%E4%B8%BE%E4%BE%8B%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C%E5%9B%BE.png" alt="决策树简单举例最终结果图"></p>
<h3 id="其他还未解决的问题"><a href="#其他还未解决的问题" class="headerlink" title="其他还未解决的问题"></a>其他还未解决的问题</h3><ul>
<li>决策树处理缺失值</li>
<li>决策树处理连续值</li>
<li>剪枝的问题<!-- more --></li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（二）：逻辑回归</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.html</url>
    <content><![CDATA[<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（一）：线性回归</title>
    <url>/%C2%B7zcy/AI/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><ul>
<li>优点：结果易于理解，计算上不复杂。</li>
<li>缺点：对非线性的数据拟合不好。</li>
<li>适用数据类型：数值型和标称型数据。</li>
</ul>
<p>&emsp;&emsp;比如预测汽车的功率，可能会这么计算：<code>horse_power = 0.0015 * annual_salary - 0.99 * hours_listening_to_public_radio</code>。<br>&emsp;&emsp;这就是所谓的<strong>回归方程（regression equation）</strong>，其中 0.0015 和 -0.99 称作<strong>回归系数（regression weights）</strong>，求这些回归系数的过程就是回归。</p>
<blockquote>
<p>回归一词的由来：是由达尔文（Charles Darwin）的表兄弟 Francis Galton 发明的。</p>
</blockquote>
<p>&emsp;&emsp;线性回归的代价函数是：<br><a id="more"></a></p>
<script type="math/tex; mode=display">
cost = \sum^m_{i = 1}(y_i - x^T_i W)^2</script><p>&emsp;&emsp;用矩阵可以表示为 <script type="math/tex">(y - XW)^T(y - XW)</script>，如果对 W 求导就可以得到 <script type="math/tex">X^T(Y - XW)</script>。令其等于 0，解得 W:</p>
<script type="math/tex; mode=display">
\hat{W} = (X^TX)^{-1} X^Ty</script><p>&emsp;&emsp;这公式也可以被称为<strong>正规方程</strong>。需要注意的点是 <script type="math/tex">(X^TX)^{-1}</script> 是一个求逆的过程，但是矩阵求逆在有些情况下是无解的，所以 <script type="math/tex">(X^TX)^{-1}</script> 必须有解才可以使用正规方程，如果无解就不能直接使用正规方程。<br>&emsp;&emsp;上述求解过程被称为最小二乘法（ordinary least squares），简称 OLS。</p>
<h1 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h1><p>&emsp;&emsp;线性回归的一个问题是有可能出现欠拟合现象。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方差。其中一个方法是局部加权线性回归（Locally Weighted Linear Regression, LWLR），<strong>该算法给待预测点附近的每个点赋予一定的权重</strong>。所以正规方程的公式变为：</p>
<script type="math/tex; mode=display">
\theta = (X^TWX)^{-1}X^TWy</script><p>&emsp;&emsp;需要说明一点省得搞混了，刚才我自己都乱了，<em>这里引入的偏差并不是线性方程 y = WX + b 中的偏差</em>。公式中的 W 是一个矩阵，用来给每个数据点赋予权重。LWLR 使用“核”（与支持向量机中的核类似）来<u>对附近的点赋予更高的权重</u>。核的类型可以自由选择，最常用的核就是高斯核，对应权重如下（<strong>那两竖不是求绝对值，而是求模长</strong>）：</p>
<script type="math/tex; mode=display">
w(i, i) = exp(\frac{|x^{(i)} - x|}{-2k^2})</script><p>&emsp;&emsp;这样就构建了一个只含对角的权重矩阵 W，并且点 x 与 <script type="math/tex">x^{(i)}</script> 越近，w(i, i)将会越大。上述公式<strong>只</strong>包含一个需要用户指定的参数 k，它决定了对附近的点赋予多大的权重。对高斯核公式的解释：</p>
<ol>
<li>W 是一个对角矩阵，因为高斯核每次赋值都是在 (i, i)点上赋值。主要说明 <script type="math/tex">|x^{(i)} - x|</script> 部分，其他部分都很好理解。首先说明 x 代表待预测的点，其次开始遍历每一项数据，数据记为t，计算每一个 |t - x| ，这样就得到了一个对角矩阵。意思就是通过待预测点经过一系列运算，为所有数据加上了一个权重。当然这里肯定也计算自己本身，不过自己减自己就是等于 0，e 的 0 次方等于1，所以就等于没有给自己加权重。最后使用公式 <script type="math/tex">(X^TWX)^{-1}X^TWy</script> 求出了解，也就是回归方程的回归系数 W。这里面有两个一模一样的 W，但是意思完全不同，应该可以理解，我就不改了。有了回归系数就可以做预测了，prediction = XW。结束。</li>
<li>这样一来就计算好了第一个数据，注意：假设我们有 200 条数据，上述的步骤仅仅才计算了第一条数据，接着取出第二条数据 x（待预测点），再经过上述的步骤，又得到了回归系数 W，做预测，结束。</li>
<li>如此反复，直到取出第 200 条数据 x（待预测点），再经过上述的步骤，又得到了回归系数 W，做预测，结束。</li>
<li>这部分理解起来比较麻烦，故下面放出代码。<strong>从for 循环开始到 return 前都是上述123步的代码版本</strong>。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LWLR</span><span class="params">(X, Y, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    局部加权线性回归，Locally Weighted Linear Regression</span></span><br><span class="line"><span class="string">    :param X: input</span></span><br><span class="line"><span class="string">    :param Y: label</span></span><br><span class="line"><span class="string">    :param k: 超参数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        prediction: 预测值</span></span><br><span class="line"><span class="string">        W: 回归系数，占个位置，可能以后会有用，现在没卵用</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    X, Y = np.matrix(X), np.matrix(Y)</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(np.dot(X.T, X)) == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"矩阵不可逆"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    weights = np.matrix(np.eye(m))</span><br><span class="line">    prediction = np.empty((m, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 使用第 i 个数据点对包括自己在内的每个数据点赋予权重，对自己就是赋予 1 的权重值，也就是说等于没赋予。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        predict_dot = X[i, :]</span><br><span class="line">        <span class="comment"># 计算权重</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 这是高斯核的公式</span></span><br><span class="line">            difference = X[j, :] - predict_dot</span><br><span class="line">            weights[j, j] = np.exp((difference * difference.T) / (<span class="number">-2</span> * (k ** <span class="number">2</span>)))</span><br><span class="line">        W = (X.T * weights * X).I * (X.T * weights * Y)</span><br><span class="line">        prediction[i] = predict_dot * W</span><br><span class="line">    <span class="keyword">return</span> prediction, np.empty((m, m, m))</span><br></pre></td></tr></table></figure>
<h1 id="岭回归（L2）"><a href="#岭回归（L2）" class="headerlink" title="岭回归（L2）"></a>岭回归（L2）</h1><p>&emsp;&emsp;岭回归中的岭是什么？</p>
<blockquote>
<p>岭回归使用了单位矩阵乘以常量 <script type="math/tex">\lambda</script>，观察单位矩阵 I，发现值 1 贯穿这个对角线，其余元素全是 0。形象地，在 0 构成的平面上有一条由 1 组成的“岭”，这就是岭的由来。</p>
</blockquote>
<p>&emsp;&emsp;简单来说，岭回归就是在矩阵 <script type="math/tex">X^TX</script> 上加上一个 <script type="math/tex">\lambda I</script>从而使得矩阵非奇异。进而能对 <script type="math/tex">X^X + \lambda I</script>求逆，其中 I 是一个单位矩阵，而 <script type="math/tex">\lambda</script> 是用户定义的数值。正规方程变为：</p>
<script type="math/tex; mode=display">
\theta = (X^TX + \lambda I)^{-1}X^Ty</script><p>&emsp;&emsp;这里发现这个公式与加入 L2 正则化一样。（经百度之后，发现还真一样，所以这部分就不细写了）<br>&emsp;&emsp;加入 L2 正则化的代价函数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    cost & = \sum^m_{i = 1}(y_i - x^T_i W)^2 + \lambda ||W||_2\\
    & = \sum^m_{i = 1}(y_i - x^T_i W)^2 + \lambda \sum^m_{j = 1}w^2_j
\end{aligned}</script><p>&emsp;&emsp;<a href="https://www.zhihu.com/question/28221429" target="_blank" rel="noopener">知乎问题</a></p>
<h1 id="lasso-L1"><a href="#lasso-L1" class="headerlink" title="lasso(L1)"></a>lasso(L1)</h1><p>略。<br>补充，参考<a href="https://www.cnblogs.com/mantch/p/10242077.html" target="_blank" rel="noopener">文章</a>，发现岭回归其实就是在<strong>代价函数</strong>（<u>注意是代价函数，不是正规方程</u>）中加入了 L2 正则化，lasso 其实就是在<strong>代价函数</strong>中加入了 L1 正则化。</p>
<h1 id="前向逐步回归"><a href="#前向逐步回归" class="headerlink" title="前向逐步回归"></a>前向逐步回归</h1>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop学习记录（一）：Hadoop集群搭建</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/big-data/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AHadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html</url>
    <content><![CDATA[<h3 id="使用jps，发现没有namenode"><a href="#使用jps，发现没有namenode" class="headerlink" title="使用jps，发现没有namenode"></a>使用jps，发现没有namenode</h3><p>使用命令<code>/opt/hadoop/sbin/start-all.sh</code>启动hadoop，之后使用jsp，一般来讲会出现如下6个线程<br><figure class="highlight basic"><table><tr><td class="code"><pre><span class="line"><span class="symbol">55598 </span>Jps</span><br><span class="line"><span class="symbol">54490 </span>NameNode</span><br><span class="line"><span class="symbol">54684 </span>DataNode</span><br><span class="line"><span class="symbol">54931 </span>SecondaryNameNode</span><br><span class="line"><span class="symbol">55332 </span>NodeManager</span><br><span class="line"><span class="symbol">38251 </span>ResourceManager</span><br></pre></td></tr></table></figure></p>
<p>但是我发现我运行后没有出现namenode。仔细观察中断启动时的日志发现，namenode的日志记录在<strong>/opt/hadoop/logs/hadoop-zhangyu-namenode-a2d8c523dd0b.log</strong>中，打开它，发现在最后居然报了异常。由于Hadoop是java写的，异常很好认（我就是学java出身的）。<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">java<span class="selector-class">.io</span><span class="selector-class">.IOException</span>: There appears to be <span class="selector-tag">a</span> gap <span class="keyword">in</span> the edit log.  We expected txid <span class="number">1</span>, but got txid <span class="number">16</span>.</span><br></pre></td></tr></table></figure></p>
<p>百度之后发现namenode的元数据破坏，需要恢复，具体参考<a href="https://blog.csdn.net/u011478909/article/details/51864071" target="_blank" rel="noopener">某csdn博客</a>。<br>使用<code>hadoop namenode -recover</code>即可恢复。<br><a id="more"></a></p>
<h3 id="使用jps，发现没有datanode"><a href="#使用jps，发现没有datanode" class="headerlink" title="使用jps，发现没有datanode"></a>使用jps，发现没有datanode</h3><p>总结写在前：<br>由于意外hadoop没有创建data文件夹，执行<code>cp -r /data/tmp/hadoop/tmp/dfs/data /data/tmp/hadoop/hdfs</code>，并修改clusterId，clusterId在<strong>/data/tmp/hadoop/hdfs/data/current/VERSION</strong>中。</p>
<p>在执行上面的操作修复namenode之后，发现原来在的datanode居然不见了。还是使用老办法发现日志文件（日志文件名字太长就不写了），异常：<code>All specified directories are failed to load。</code>。经过百度之后发现是datanode和namenode配置文件的VERSION文件中的clusterId不一致。所以只需要将clusterId更改成一致即可。<br>但是我的出现的问题重点不是这个，我在修改clusterId时发现：我的namenode配置在/data/tmp/hadoop/hdfs/name中。但是检查我的datanode配置是否在/data/tmp/hadoop/hdfs/data中时，发现hadoop根本没有创建这个文件夹。于是又进行排查，发现在/data/tmp/hadoop/tmp/dfs中存在data文件夹。于是使用命令<code>cp /data/tmp/hadoop/tmp/dfs/data /data/tmp/hadoop/hdfs</code>，将这份data文件夹拷贝到/data/tmp/hadoop/hdfs，并更改clusterId。<strong>VERSION文件在/data/tmp/hadoop/hdfs/data/current</strong></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>big-data</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda的锅：使用pyquery出现urlopen-error-unknown-url-type-https</title>
    <url>/IT-stuff/python/anaconda%E7%9A%84%E9%94%85%EF%BC%9A%E4%BD%BF%E7%94%A8pyquery%E5%87%BA%E7%8E%B0urlopen-error-unknown-url-type-https.html</url>
    <content><![CDATA[<p>使用pyquery出现urlopen-error-unknown-url-type-https的异常，上网查发现是openssl的问题。话不多说直接说结果。<br>参考文章如下：<br><a href="https://blog.csdn.net/gw85047034/article/details/88039705" target="_blank" rel="noopener">anaconda新建环境在PyCharm执行import ssl失败</a></p>
<p>由于anaconda的python版本是3.7.0，而我在创建虚拟环境时，使用命令<code>conda create -n spider python=3</code>，最后的python=3会选择python最新的版本安装，所以anaconda选择了python3.7.3。于是anaconda的python版本和虚拟环境的python版本冲突了，而我把虚拟环境给PyCharm用了，所以就是anaconda和PyCharm冲突了。只要把虚拟环境的python版本换成和anaconda的python版本一致就可以了。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>error</tag>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫学习记录（一）：正则表达式</title>
    <url>/IT-stuff/python/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html</url>
    <content><![CDATA[<h3 id="正则表达式的使用"><a href="#正则表达式的使用" class="headerlink" title="正则表达式的使用"></a>正则表达式的使用</h3><p>正则表达式的匹配规则网上都有，这里不放出来了，毕竟只是记录学习的博文。<br>以下讲几个一直没搞明白的点。</p>
<h4 id="贪婪与非贪婪"><a href="#贪婪与非贪婪" class="headerlink" title="贪婪与非贪婪"></a>贪婪与非贪婪</h4><p>实现介绍通用匹配，以<code>.*</code>表示。<code>.</code>代表匹配任意字符，<code>*</code>代表匹配0个或多个表达式所以两个连用就代表匹配任意字符。比如字符串<code>&#39;Hello Python and Anaconda&#39;</code>，表达式为<code>&#39;^Hello.*Anaconda$&#39;</code>。代码如下：<br><figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">txt = <span class="string">'Hello Python and Anaconda'</span></span><br><span class="line">result = re.match(<span class="string">"^Hello.*Anaconda$"</span>, txt)</span><br><span class="line"><span class="keyword">print</span>(result.<span class="keyword">group</span>())</span><br></pre></td></tr></table></figure></p>
<p>可以得出我们想要的结果，因为<code>.*</code>代表匹配任意个字符，所以该正则表达式会在<strong><em>Hello</em></strong>之后一直匹配成功，直到遇到<strong><em>Anaconda</em></strong>停止。其中<code>.*</code>其实就是贪婪匹配。但是有时候贪婪匹配会出很大的问题。比如字符串<code>&#39;My tel number is 15012345678&#39;</code>，正则表达式为<code>&#39;^My.*(\d+)$&#39;</code>。具体代码如下：<br><a id="more"></a><br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">txt = <span class="string">'My tel number is 15012345678'</span></span><br><span class="line">result = re.match(<span class="string">"^My.*(\d+)$"</span>, txt)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group()</span></span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group(<span class="number">1</span>)</span></span>)</span><br></pre></td></tr></table></figure></p>
<p>首先这段代码意思是获取字符串中的手机号码，<code>(\d+)</code>代表一个组，这个组里面匹配的东西就是我们需要的结果，而我们的代码也看起来很正常<code>\d+</code>就意味着匹配多个数字。可以看到这段代码<code>print(result.group(1))</code>就是输出手机号。但是结果却只匹配到了8，因为<code>.*</code>代表了贪婪匹配，它会尽可能的匹配到多的字符。因为<code>.*</code>代表匹配任意多的字符串，所以它看到1501234567会一直往下匹配，知道看到8，发现8已经是最后一个数字了，如果这个8不与<code>\d+</code>匹配岂不是出现系统漏洞了，因为<code>\d+</code>就是让你匹配数字，你不匹配不是违反了规则？所以匹配成功。最后将8加入组中，通过result.group(1)获取到这一组。所以贪婪匹配在这种情况是有问题，这就引出了非贪婪匹配：<code>.*?</code>。代码如下：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">txt = <span class="string">'My tel number is 15012345678'</span></span><br><span class="line">result = re.match(<span class="string">"^My.*?(\d+)$"</span>, txt)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group()</span></span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group(<span class="number">1</span>)</span></span>)</span><br></pre></td></tr></table></figure></p>
<p>如果运行这段代码，就会发现匹配成功了。所以以后尽量使用非贪婪模式<code>.*?</code>，而不是贪婪模式<code>.*</code>。</p>
<h4 id="修饰符"><a href="#修饰符" class="headerlink" title="修饰符"></a>修饰符</h4><p>代码<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">txt = <span class="string">'My tel number is 15012345678'</span></span><br><span class="line">result = re.match(<span class="string">"^My.*?(\d+)$"</span>, txt, re.S)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group()</span></span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group(<span class="number">1</span>)</span></span>)</span><br></pre></td></tr></table></figure></p>
<p>这里引用了上面的代码，注意多了个re.S，它代表使.匹配包括换行在内的所有字符。也就是说不加re.S，.其实是匹配不到换行符的。应用场景：网页代码中嵌套标签会经常出现这些换行符，要加入re.S。其他的方法re.search(),re.findall()用法类似。<br>待补充。。。</p>
<h4 id="re模块中的方法"><a href="#re模块中的方法" class="headerlink" title="re模块中的方法"></a>re模块中的方法</h4><ol>
<li>match()<br>这方法从头开始匹配，如果一开始即第一个字符就不匹配，直接就算匹配失败。可以用于校验用户输入的信息是否合乎规范。</li>
<li>search()<br>从头开始匹配，扫描整个字符串，如果碰到一段字符串匹配成功了就立即返回，它只会返回一个结果。</li>
<li>findall()<br>跟它名字一样，扫描整个字符串，获得所有匹配成功的结果。</li>
<li>待补充。。。</li>
</ol>
<h3 id="举一个栗子"><a href="#举一个栗子" class="headerlink" title="举一个栗子"></a>举一个栗子</h3><p>爬取牛客网的技术栈。</p>
<!-- more -->]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda安装问题以及之前安装的Python环境取舍问题</title>
    <url>/IT-stuff/python/anaconda%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%E4%BB%A5%E5%8F%8A%E4%B9%8B%E5%89%8D%E5%AE%89%E8%A3%85%E7%9A%84Python%E7%8E%AF%E5%A2%83%E5%8F%96%E8%88%8D%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>今天安装anaconda时发现了一些问题。</p>
<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>我在安装anaconda之前，已经安装了Python3.6，这个Python环境是很久以前还在学Java时安装的。然后现在安装完anaconda之后，发现anaconda不会自动将原有的Python环境收为己用。所以就出现了原有的Python怎么处理的问题。</p>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>如果原先的Python环境不是很重要，可以直接删了，改用anaconda提供的虚拟环境。当然同时也要删除电脑环境变量中的Python路径。为了保险可以先删除电脑环境变量中的Python路径，然后打开命令行，输入python，发现已经移除后再删除Python。最后还需要将anaconda添加到环境变量中，不过anaconda在安装时有个选项可以直接将其添加进环境变量，如果之前选择了这个选项。可以不用再添加环境变量了。anaconda环境变量总共需要输入三个变量，如下：<br><figure class="highlight taggerscript"><table><tr><td class="code"><pre><span class="line">D:<span class="symbol">\a</span>naconda</span><br><span class="line">D:<span class="symbol">\a</span>naconda<span class="symbol">\S</span>cripts</span><br><span class="line">D:<span class="symbol">\a</span>naconda<span class="symbol">\L</span>ibrary<span class="symbol">\b</span>in</span><br></pre></td></tr></table></figure></p>
<p>注：我所有的编程软件、工具等等都放在D盘。<br>最后参考以下两篇，其中有anaconda安装教程：<br><a id="more"></a></p>
<ul>
<li><a href="https://www.jianshu.com/p/eaee1fadc1e9" target="_blank" rel="noopener">Anaconda完全入门指南</a></li>
<li><a href="https://blog.csdn.net/qq_37025885/article/details/79158153" target="_blank" rel="noopener">如何在已安装Python条件下，安装Anaconda，并将原有Python添加到Anaconda中</a></li>
</ul>
<p>其中，他们还创建了其他的虚拟环境，但是我觉得base环境用用算了，因为我刚学python，还不太懂。<br>至于anaconda可以创建多个虚拟环境，虽然我没用过anaconda，但是我估计理念跟github差不多。想要一个环境直接从别人那拷过来，就不需要自己安装了。<br>注意记得将Pycharm中的python环境改为anaconda的虚拟环境。其他的虚拟环境路径在D:\anaconda\envs。base环境在：D:\anaconda\python.exe<br>如下图，进入设置界面，左上角File进入。点OK就行了。图中有个叫Virtualenv Environment的选项，这个看起来是虚拟环境的意思，其实是python本身自带的。它跟anaconda功能一样也是创建一个虚拟环境，但是这是python提供的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/f095a0ecb23b43dfa99e2a82b720c6ff.jpg" alt="创建Python环境" title="创建Python环境"></p>
<p>最后，在随便一个音乐平台、网站、软件搜索anaconda，歌手Nicki Minaj。不用谢我(ฅωฅ*)。</p>
<!-- more -->]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>error</tag>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>spring boot中使用@ConfigurationProperties注解</title>
    <url>/java/spring%20boot%E4%B8%AD%E4%BD%BF%E7%94%A8%20ConfigurationProperties%E6%B3%A8%E8%A7%A3.html</url>
    <content><![CDATA[<p>在使用@ConfigurationProperties注解前必须引入这个依赖<br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 为了使用@ConfigurationProperties注解，必须加入这个--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-configuration-processor<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>使用方法：<br><a id="more"></a><br><figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="variable">@Component</span></span><br><span class="line"><span class="variable">@ConfigurationProperties</span>(prefix = <span class="string">"io.github.yan624.file"</span>)</span><br><span class="line"><span class="variable">@Data</span></span><br><span class="line">public class PropertiesConfig &#123;</span><br><span class="line">    <span class="selector-tag">private</span> <span class="selector-tag">String</span> <span class="selector-tag">root</span>;</span><br><span class="line">    <span class="selector-tag">private</span> <span class="selector-tag">String</span> <span class="selector-tag">qrPath</span>;</span><br><span class="line">    <span class="selector-tag">private</span> <span class="selector-tag">String</span> <span class="selector-tag">imgPath</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>yml文件：<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">io:</span></span><br><span class="line"><span class="attr">  github:</span></span><br><span class="line"><span class="attr">    yan624:</span></span><br><span class="line"><span class="attr">      file:</span></span><br><span class="line"><span class="attr">        root:</span> <span class="attr">E:/itchat4j/</span></span><br><span class="line"><span class="attr">        qr-path:</span> <span class="string">$&#123;io.github.yan624.file.root&#125;/login</span></span><br><span class="line"><span class="attr">        img-path:</span> <span class="string">$&#123;io.github.yan624.file.root&#125;/img</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure></p>
<p>其中@Data是lombok插件的注解，用于生成get/set方法</p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring注解</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习2017春学习记录及作业：octave实现</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E6%98%A5%E4%BD%9C%E4%B8%9A%EF%BC%9Aoctave%E5%AE%9E%E7%8E%B0.html</url>
    <content><![CDATA[<p>所有代码在<a href="https://github.com/yan624/lihongyi-machine-learning-2017-homework" target="_blank" rel="noopener">lihongyi-machine-learning-2017-homework</a>上</p>
<h3 id="学习情况和做题背景"><a href="#学习情况和做题背景" class="headerlink" title="学习情况和做题背景"></a>学习情况和做题背景</h3><h4 id="学习情况"><a href="#学习情况" class="headerlink" title="学习情况"></a>学习情况</h4><p>先看了<strong>《深度学习》（花书）</strong>这本书，由于我是零基础学习该方向，感觉这本书除了前几章数学方面略看得懂，其他的一脸懵逼。由于本人在学习某一知识之前会预先搜集资料。于是我打开了在知乎收藏的文章，点进了<strong>吴恩达机器学习</strong>网易云课堂。我看了几个算法：线性回归，逻辑回归，神经网络。并且做了线性回归和逻辑回归的练习题。个人感觉吴恩达的视频很适合入门，但是想要仔细理解太难了。我根据吴恩达老师提供的题目，照葫芦画瓢做了一下，做还是做出来了，但是这都是很简单的例子。大都是只有三四个参数，所以感觉只是入门了。<br>这时候我再去看<strong>《深度学习》</strong>这本书，发现有些内容我看得懂了。但是有只是有些而已，往后看还是无法理解。至此，在寒假中我彻底放弃继续看<strong>《深度学习》</strong>。这个时候我又继续搜集资料，我发现了另一个教学视频就是<strong>李宏毅机器学习</strong>，经过自己的查询，最终决定看这个。李宏毅老师讲的略微会难一点，因为他面向的是研究生，这个从他的课程介绍上可以看得出。可以对比一下，</p>
<ul>
<li><strong>吴恩达机器学习</strong>不会涉及任何数学部分，高数、线代、概率论在吴恩达老师的课堂上只会出现名词和推导过程，他不会解释为什么，他假设看这视频的人都不懂数学。因为他的课程面向的是所有人。而且他的课程当中很多知识都是讲解基础知识，深入的东西他会只介绍名词，甚至不会介绍。所以<strong><em>很</em></strong>适合做入门视频，<strong>最重要的是</strong>他的视频都很短，最长的也只有十几分钟。</li>
<li><strong>李宏毅机器学习</strong>讲的比吴恩达的深入许多，他会讲解原理，而且讲的也很清楚，并且数学部分他也不会跳过，而且他都是假设你会这部分的数学知识。另外他讲课是讲例子的，主要的例子是预测神奇宝贝进化后的cp值（大概就是战斗力的意思），还会出现一些游戏如帝国时代、我的世界等。这些都会使得课堂很有意思。在b站有他的视频，不过都很长，一个视频打都有60来分钟，有少数极短。最后他的作业数据量大做起来很有意思。<br>所以我建议先看吴恩达老师的机器学习视频看到神经网络，先入个门要不然看其他人的教学视频可能跟不上节奏，然后看李宏毅老师视频，注意完成他的作业，会很好的帮助自己理解。<a id="more"></a>
</li>
</ul>
<h4 id="做题背景"><a href="#做题背景" class="headerlink" title="做题背景"></a>做题背景</h4><p>做完吴恩达老师布置的作业还以为已经懂了什么是线性回归，没想到做李宏毅老师的作业时发现连开个头都困难。在自己瞎做之后，经过五六天完成了作业，但是期间出现了大量问题，所以写这个博文用来纪录。</p>
<h3 id="hw1-PM2-5预测"><a href="#hw1-PM2-5预测" class="headerlink" title="hw1:PM2.5预测"></a>hw1:PM2.5预测</h3><h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><ol>
<li>李宏毅老师给的数据，没看懂是什么意思。可能是吴恩达老师和李宏毅老师的概念有一些处理，看了很久没理解这个作业的意思。这个没办法，我自己琢磨了一会才搞懂。</li>
<li>开头第一步，完全不知道干什么。<strong>解决</strong>：选函数。吴恩达称之为hypothesis函数，李宏毅称之为模型(Model)，都一个意思，以下称为hypothesis。</li>
<li>hypothesis函数需要一些特征值，即X。当然还有一个theta。但是特征那么多总不能全选吧，而且是几次方的式子也确定不了。<strong>解决</strong>：将每一个特征和需要预测的y，使用画图函数画出来，特征x作为横坐标，待预测y作为纵坐标。然后观察哪些图像关系比较正常，就用哪个特征（比如呈线性关系的就比较正常）。这也是看到其他人的博客才发现特征是这么选的，之前做了几天都是凭感觉选的。。。</li>
<li>做了半天才发现代码写错了。<strong>解决</strong>：做的时候不要太急，把代码好好检查一下。倒不是程序错误，也不是数据分割错误，我是数据划分的时候少划分一大半导致样本量一直很小。</li>
<li>学习速率(alpha)选择多少正常，第一次做心理没底，选了稍微大一点的如0.1，误差大得直接超出浮点型最大值。<strong>解决</strong>：使用adagrad，可以根据迭代次数的增加，自动减少alpha。</li>
<li>梯度下降的迭代次数选择多少正常，第一次做心理没底。<strong>解决</strong>：经过我反复尝试，虽然我还是没有得到肯定的答案，但是我一般将迭代次数设置为4000.</li>
<li>theta初始值设置为多少正常，第一次做心理没底。<strong>解决</strong>：全部为0，包括bais也设置为0。另外吴恩达老师讲过在神经网络中不能这么设置。</li>
<li>怎么选择hypothesis函数的表达式，到底是二次项，三次项，四次项还是五次项等等。<strong>解决</strong>：暂时没百度过，我看吴恩达老师的视频，个人理解应该是最后在1-10次中选择。</li>
<li>除了一个特征的几次方项，那么多个特征的乘积怎么选择，如有特征值x1,x2,x3，x1*x2*x3还是选x1*x3还是选x2*x3，甚至这些乘积里面的特征也可以加上次方。那项的可能就多了去了，到底该怎么选。<strong>解决</strong>：我刚开始是不加这些项的，也就是表达式的一个项中只会有一个特征。后面加也会自己慢慢尝试，基本上就是碰运气。</li>
<li>标准化的lambda怎么设置。<strong>解决</strong>：吴恩达老师有讲怎么调整lambda值以及lambda值调整会出现问题的情况也讲了，所以我都是感觉误差稳定了或者误差太大了才去调的。一般都设为0。</li>
<li>其他必要的变量怎么设置。<strong>解决</strong>：我真的是完全自己调，然后碰运气。。。</li>
<li>怎么看我训练出来的结果如何。<strong>解决</strong>：梯度下降本质是最后得到一组theta。你需要预测数据，首先肯定有X，然后X * theta，就会得到预测结果。接下来是使用代价函数看误差就行了。吴恩达称之为代价函数(Cost function)，李宏毅称之为损失函数(Loss function)，而且算法也有略微不同，不过问题不大。值得注意的是，之前训练的样本被称为train_data，现在预测的被称为cross_validation_data，是完全不同的数据，不要再用train_data去做预测，具体为什么，吴恩达老师讲的很明白了。</li>
<li>误差到多少才算可以。<strong>解决</strong>：吴恩达老师给出了几幅图，但是个人认为这几幅图有几个瑕疵的地方，那就是他没标刻度，可能是故意没标的，因为根本没刻度。所以压根不知道误差到什么程度才算可以。后来经过不断测试和百度，发现网上一个人认为40多的误差他说可以接受。我个人认为，训练误差0-20差不多，交叉验证误差0-40差不多，预测误差感觉40以下差不多。预测误差就是最终做出的预测产生的误差，所以我感觉要再低点，但是我的作业中训练的一直降不下去，最低都是20多。</li>
</ol>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol>
<li>通过分别画出特征值和待输出值的二维图，观察图像来选择特征。</li>
<li>尽可能地把数据处理部分的代码写的完全正确。</li>
<li>选择hypothesis函数，可以从1次方开始尝试。</li>
<li>初始化值。alpha=0.001,iters_num=4000,lambda=0.</li>
<li>开始梯度下降。</li>
<li>出现误差，通过吴恩达老师给出的建议以及李宏毅老师给的补充，可以自行调整。对于我的话，误差太巨大，我一般先调整学习速率alpha(learning rate)和迭代次数iters_num.误差大概到100以内左右才会动特征数量，hypothesis表达式等等这些变量。误差到很小了（这个很小还真不确定，大概0-40左右吧）才会调整lambda。</li>
<li>待补充。</li>
</ol>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>octave添加io包读取一些特定的文件，如csv,xls等</title>
    <url>/octave%E6%B7%BB%E5%8A%A0io%E5%8C%85%E8%AF%BB%E5%8F%96%E4%B8%80%E4%BA%9B%E7%89%B9%E5%AE%9A%E7%9A%84%E6%96%87%E4%BB%B6%EF%BC%8C%E5%A6%82csv%E3%80%81xls%E7%AD%89.html</url>
    <content><![CDATA[<p>输入<code>pkg install -forge io</code>，octave会自动下载，资源在国外会有点卡，需要等一会。<br>过一会会显示</p>
<blockquote>
<p>For information about changes from previous versions of the io package, run ‘news io’.</p>
</blockquote>
<p>这不是说明安装失败了，而是安装成功了。意思是让你仔细了解一下io包个版本的区别。输入news io查询。<br>最后输入<code>pkg load io</code>加载io包。（这句代码不是输入一次永久有效，只要关闭了cli界面，就要重新输入一次，就像java中导包一样）<br>当读取csv文件时，我使用了csv2cell(“文件名”)函数，但是这个函数有问题，它返回的值不是矩阵貌似是字符串。建议使用csvread(“文件名”)函数</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>octave</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world.html</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br><a id="more"></a></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>用idea创建gradle构建的项目</title>
    <url>/IT-stuff/android/%E7%94%A8idea%E5%88%9B%E5%BB%BAgradle%E6%9E%84%E5%BB%BA%E7%9A%84kotlin%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<p>在网上找了找发现没几个人用maven创建kotlin项目，而我以前一直都用maven，没用过gradle，所以只能当学习gradle了。<br>第一步：选择gradle<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/f9fdef1607a94d4788848429af5af5a6.png" alt="选择gradle" title="选择gradle"><br>第二步填写GroupId和ArrtifactId，这个很简单。略。<br>第三步，默认是这样的窗口，但是我选择了User auto-import（自动导包功能），其他都是默认值。<br><a id="more"></a><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/70cb9fa0d58142f094e14a8c8a9f0a79.png" alt="一些配置" title="一些配置"><br>第四步，看看项目的所在文件夹没问题就可以finish了。<br>第五步，发现项目的文件夹有问题，只有个.idea。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/fa8abf0e9fa54d658512abb05b9436ae.png" alt="项目目录只有.idea文件夹" title="项目目录只有.idea文件夹"><br>第六步，我是第一次用gradle，只要等gradle下完就行了。如果下完还是没有，那么重启一下idea就行了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/6acb9a4b777f4c08b60fcce2a8f425eb.png" alt="成功构建" title="成功构建"><br><!-- more --></p>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>android</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>kotlin</tag>
      </tags>
  </entry>
  <entry>
    <title>（十）搭建springcloud：记录项目运行的日志</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E5%8D%81%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E8%AE%B0%E5%BD%95%E9%A1%B9%E7%9B%AE%E8%BF%90%E8%A1%8C%E7%9A%84%E6%97%A5%E5%BF%97.html</url>
    <content><![CDATA[<p>项目运行避免不了出现错误，这是日志就帮了大忙。<br>springboot提供了一个便捷的方法配置日志。在application.yml中加入以下配置，其中com.yan624代表包名，即这个包下的所有类输出的日志都是error以上级别。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">logging:</span></span><br><span class="line"><span class="attr">  file:</span> <span class="string">upms.log</span></span><br><span class="line"><span class="attr">  level:</span></span><br><span class="line">    <span class="string">com.yan624:</span> <span class="string">error</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure></p>
<p>如果如下配置，就代表项目的所有类输出的日志都是error以上级别<br><a id="more"></a><br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">logging:</span></span><br><span class="line"><span class="attr">  file:</span> <span class="string">upms.log</span></span><br><span class="line"><span class="attr">  level:</span></span><br><span class="line"><span class="attr">    root:</span> <span class="string">error</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure></p>
<p>但是这样配置无法做出更灵活的操作，比如将info的日志输出到info.log；将warn的日志输出到warn.log；将error的日志输出到error.log。<br>所以可以删除上面的配置，在classpath路径中新建logback.xml文件，里面填入：<br><figure class="highlight dust"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 修改一下自己想配置的路径，就一个点代表项目根路径--&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_PATH"</span> <span class="attr">value</span>=<span class="string">"."</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- start springboot default configuration of console log pattern --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 彩色日志 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 彩色日志依赖的渲染类 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">conversionRule</span> <span class="attr">conversionWord</span>=<span class="string">"clr"</span> <span class="attr">converterClass</span>=<span class="string">"org.springframework.boot.logging.logback.ColorConverter"</span>/&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">conversionRule</span> <span class="attr">conversionWord</span>=<span class="string">"wex"</span> <span class="attr">converterClass</span>=<span class="string">"org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter"</span>/&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">conversionRule</span> <span class="attr">conversionWord</span>=<span class="string">"wEx"</span> <span class="attr">converterClass</span>=<span class="string">"org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter"</span>/&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 彩色日志格式 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"CONSOLE_LOG_PATTERN"</span></span></span></span><br><span class="line"><span class="xml">              value="$</span><span class="template-variable">&#123;CONSOLE_LOG_PATTERN:-%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;</span><span class="xml">)</span><span class="template-variable">&#123;faint&#125;</span><span class="xml"> %clr($</span><span class="template-variable">&#123;LOG_LEVEL_PATTERN:-%5p&#125;</span><span class="xml">) %clr($</span><span class="template-variable">&#123;PID:- &#125;</span><span class="xml">)</span><span class="template-variable">&#123;magenta&#125;</span><span class="xml"> %clr(---)</span><span class="template-variable">&#123;faint&#125;</span><span class="xml"> %clr([%15.15t])</span><span class="template-variable">&#123;faint&#125;</span><span class="xml"> %clr(%-40.40logger</span><span class="template-variable">&#123;39&#125;</span><span class="xml">)</span><span class="template-variable">&#123;cyan&#125;</span><span class="xml"> %clr(:)</span><span class="template-variable">&#123;faint&#125;</span><span class="xml"> %m%n$</span><span class="template-variable">&#123;LOG_EXCEPTION_CONVERSION_WORD:-%wEx&#125;</span><span class="xml">&#125;"/&gt;</span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"FILE_LOG_PATTERN"</span> <span class="attr">value</span>=<span class="string">"%date %-5level [$</span></span></span><span class="template-variable">&#123;HOSTNAME&#125;</span><span class="xml"><span class="tag"><span class="string"> %thread] %caller</span></span></span><span class="template-variable">&#123;1&#125;</span><span class="xml"><span class="tag"><span class="string">%msg%n"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"CONSOLE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">Pattern</span>&gt;</span>$</span><span class="template-variable">&#123;CONSOLE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">Pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.ThresholdFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>info<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- end springboot default configuration of console log pattern --&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 输出自己项目的日志，我比较喜欢将第三方的日志和自己的日志区分开存放，如果没我这样的强迫症可以不配置 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"MY_PROJECT_LOG_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$</span><span class="template-variable">&#123;LOG_PATH&#125;</span><span class="xml">/my-project.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--日志文件输出格式--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$</span><span class="template-variable">&#123;FILE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>my-project/my-project-%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.%i.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                除按日志记录之外，还配置了日志文件不能超过20M，若超过20M，日志文件会以索引0开始，</span></span><br><span class="line"><span class="xml">                命名日志文件，例如info/info-2019-01-03.0.log</span></span><br><span class="line"><span class="xml">            --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span></span><br><span class="line"><span class="xml">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--日志文件保留天数--&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 此日志文件只记录info级别的 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>info<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$</span><span class="template-variable">&#123;LOG_PATH&#125;</span><span class="xml">/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--日志文件输出格式--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$</span><span class="template-variable">&#123;FILE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>info/info-%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.%i.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                除按日志记录之外，还配置了日志文件不能超过20M，若超过20M，日志文件会以索引0开始，</span></span><br><span class="line"><span class="xml">                命名日志文件，例如info/info-2019-01-03.0.log</span></span><br><span class="line"><span class="xml">            --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span></span><br><span class="line"><span class="xml">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--日志文件保留天数--&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 此日志文件只记录info级别的 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>info<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                以下两个配置代表匹配到日志等级为info则输出，否则拒绝输出。</span></span><br><span class="line"><span class="xml">                如果删除这两行配置，那么这两个配置的默认值为NEUTRAL，意味着从日志等级info开始一层一层向上打印，即info-&gt;warn-&gt;error-&gt;fatal-&gt;off</span></span><br><span class="line"><span class="xml">                再比如上面<span class="tag">&lt;<span class="name">level</span>&gt;</span>配置的是error,那么输出error-&gt;fatal-&gt;off</span></span><br><span class="line"><span class="xml">                如果上面<span class="tag">&lt;<span class="name">level</span>&gt;</span>配置的是off，那么就只输出off等级的日志。</span></span><br><span class="line"><span class="xml">             --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"WARN_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$</span><span class="template-variable">&#123;LOG_PATH&#125;</span><span class="xml">/warn.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--日志文件输出格式--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$</span><span class="template-variable">&#123;FILE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>warn/warn-%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.%i.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                除按日志记录之外，还配置了日志文件不能超过20M，若超过20M，日志文件会以索引0开始，</span></span><br><span class="line"><span class="xml">                命名日志文件，例如warn/warn-2019-01-03.0.log</span></span><br><span class="line"><span class="xml">            --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span></span><br><span class="line"><span class="xml">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--日志文件保留天数--&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 此日志文件只记录warn级别的 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>warn<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$</span><span class="template-variable">&#123;LOG_PATH&#125;</span><span class="xml">/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--日志文件输出格式--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$</span><span class="template-variable">&#123;FILE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>error/error-%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.%i.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                除按日志记录之外，还配置了日志文件不能超过20M，若超过20M，日志文件会以索引0开始，</span></span><br><span class="line"><span class="xml">                命名日志文件，例如error/error-2019-01-03.0.log</span></span><br><span class="line"><span class="xml">            --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span></span><br><span class="line"><span class="xml">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--日志文件保留天数--&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 此日志文件只记录error级别的 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>error<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">springProfile</span> <span class="attr">name</span>=<span class="string">"test"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 控制台输出info级别以上的日志 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"CONSOLE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">springProfile</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">springProfile</span> <span class="attr">name</span>=<span class="string">"prod"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--这里的name属性代表所属包的名称是以io.github.yan624.messageporter开头的类，会按照这个logger的配置打印日志--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"io.github.yan624.messageporter"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"MY_PROJECT_LOG_FILE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 打印warn，error级别的日志 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"warn"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"WARN_FILE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 控制台输出info级别以上的日志 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"CONSOLE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">springProfile</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure></p>
<p>并且在application.yml文件中自己配置<br><figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">spring:</span></span><br><span class="line"><span class="symbol">  profiles:</span></span><br><span class="line"><span class="symbol">    active:</span> prod</span><br></pre></td></tr></table></figure></p>
<p>这样代表了激活了prod的日志配置。对应于logback.xml文件中的<code>&lt;springProfile name=&quot;prod&quot;&gt;。。。&lt;/springProfile&gt;</code>。<br>如果希望把自己某个包中的类的日志输出，在appender-&gt;filter标签下不能写onMacth和onMisMatch具体原因不知，反正我是去掉了就可以输出了。参考springProfile-&gt;logger的name=”com.yan624”。其中MY_PROJECT_FILE的appender我并没有配置onMacth和onMisMatch。<br>名为CONSOLE的appender是springboot的默认输出格式，我在该xml中已经写明注释。<br>其他的应该都很好理解。<br>最后效果就是：在warn.log和error.log文件中分别输出warn和error级别的日志。在控制台输出info<strong>以上</strong>级别的日志。在my-project.log文件中输出我自己项目的info级别<strong>以上</strong>的日志。<br>上面我写了info级别的配置，但是我并没有使用，另外debug级别的配置我没有写，因为我用不上。</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（九）搭建springcloud：在linux上运行springboot项目</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E4%B9%9D%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E5%9C%A8linux%E4%B8%8A%E8%BF%90%E8%A1%8Cspringboot%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<p>具将本地的jar文件拖到服务器上时发现，jar文件只有3kb。果然使用“java -jar 项目名.jar”命令后，显示<strong>no main manifest attribute, in springboot项目名.jar</strong>。<br>解决办法：在项目中（不是聚合父工程中）添加如下代码</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 指定该Main Class为全局的唯一入口 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.yan624.eureka.Eureka_Server7001<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">layout</span>&gt;</span>JAR<span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>repackage<span class="tag">&lt;/<span class="name">goal</span>&gt;</span><span class="comment">&lt;!--可以把依赖的包都打包到生成的Jar包中--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>由于如果直接使用java -jar 项目名.jar启动springboot项目，会造成你启动了这个项目就不用想输入其他命令了，所以使用 “<strong>nohup java -jar springboot项目名.jar &gt;记录日志的文件名.log 2&gt;&amp;1 &amp;</strong>”命令，来让spirngboot项目后台运行，并且记录日志。<br>如果不想输出日志，可以使用“<strong>nohup java -jar springboot项目名.jar &gt;/dev/null 2&gt;&amp;1 &amp;</strong>”</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>springboot启动出现Unable to start EmbeddedWebApplicationContext due to missing EmbeddedServlet</title>
    <url>/java/springboot%E5%90%AF%E5%8A%A8%E5%87%BA%E7%8E%B0Unable-to-start-EmbeddedWebApplicationContext-due-to-missing-EmbeddedServlet.html</url>
    <content><![CDATA[<p>网上的解决办法打都相同，但是我这个问题纯属操作失误。<br>因为我想要启动我的微服务，但是在idea里配置的时候配置错了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/abaa4d89631040ac90ccc023a34ac1d3.png" alt="idea配置" title="idea配置"><br>我把主启动类配错了。可以看到下图主启动类是CMS_BLOG8082，但是手滑选择了CmsBlogApiApplication。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/a61afd34f52f46299eca32ab412f668b.png" alt="主启动类" title="主启动类"><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>error</tag>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>idea启动的项目，打开网页出现部分文字乱码</title>
    <url>/assorted/idea%E5%90%AF%E5%8A%A8%E7%9A%84%E9%A1%B9%E7%9B%AE%EF%BC%8C%E6%89%93%E5%BC%80%E7%BD%91%E9%A1%B5%E5%87%BA%E7%8E%B0%E9%83%A8%E5%88%86%E6%96%87%E5%AD%97%E4%B9%B1%E7%A0%81.html</url>
    <content><![CDATA[<p>起因：<br>在文件中右键点击file encoding，打算更改编码，但是出现一个窗口显示Reloead,Convert,Cancel。由于不懂这几个按键按后的效果是什么所以瞎按，导致文件完全乱码，恢复不回去。<br>之后在打开网页时，发现有部分中文乱码，而且乱码还是出现在前端部分，而且我的文件格式全是utf-8，所以按理说应该没有任何问题。最终也没有解决问题。<br>解决办法：<br>重新配置idea。重置方法详见：<a href="https://jingyan.baidu.com/article/fa4125ac1c613f28ac7092c1.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/fa4125ac1c613f28ac7092c1.html</a><br><em>更新：这个方法治标不治本，导致了部分页面恢复正常了，但是其他页面还是有问题。</em><br><strong>终极解决办法：</strong>找到错误的根源，可能是html，也可能是js。大部分都是js的错误，由于js动态地将中文添加到页面，导致乱码。所以需要将这份js文件，复制出来，用windows的文本文档打开，另存为的时候修改编码为urf-8。再复制回项目即可。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
      </categories>
      <tags>
        <tag>error</tag>
        <tag>印象笔记上的博文</tag>
        <tag>idea</tag>
      </tags>
  </entry>
  <entry>
    <title>（零）搭建springcloud：创建项目</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E9%9B%B6%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<h3 id="创建springboot项目"><a href="#创建springboot项目" class="headerlink" title="创建springboot项目"></a>创建springboot项目</h3><p>右键父工程选择new-&gt;module，创建一个子项目（packaging为jar）。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/33cb351041724b248d26b86121954e93.png" alt="创建module" title="创建module"></p>
<p>点击Module之后会出现如下界面，选择spring Initializr(你如果要创建maven项目的话也选这个)，什么都不要干，点击Next。</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/41f65d0615394aefa53946aa6300d5d2.png" alt="创建springboot项目" title="创建springboot项目"></p>
<p>之后会出现这个界面，填写好信息，注意Packaging是jar，因为springboot是用netty启动的，不需要打成war包，点击Next。<br><a id="more"></a><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/4ad4d8919f1341098719e99ceaee285b.png" alt="配置" title="配置"></p>
<h3 id="创建聚合工程"><a href="#创建聚合工程" class="headerlink" title="创建聚合工程"></a>创建聚合工程</h3><p>右键父工程选择new-&gt;module，创建一个子项目（jpackaging为pom）。<br>选择maven即可。</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a><br><!-- more --></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>ip地址引起的无法单点登录</title>
    <url>/java/ip%E5%9C%B0%E5%9D%80%E5%BC%95%E8%B5%B7%E7%9A%84%E6%97%A0%E6%B3%95%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95.html</url>
    <content><![CDATA[<p>今天碰到一个很奇怪的问题，单点登录明明自认为代码没有任何问题，但是始终登录不进去。<br>最后发现，测试配置的cas-service地址是手动输入的localhost，所以当使用单点登录时在sso系统进行验证完毕时会回调这个地址回到原系统。这一切都符合预期运行。但是当原系统调用backUrl时出问题了。因为我点的是eureka的Application表格的Status列中链接，而这个链接我正好配置显示更详细的ip地址，所以由本来的localhost替换成了我的局域网中的ip地址。<br>综上：我需要登录的地址是localhost，而我需要回调的ip地址是我局域网的地址，显然两个ip不同那么shiro的subject也会不同，自然造成了每次使用isAuthenticated方法都会出问题。当然这只是特例，因为我的上述的两个地址虽然说不同但是实际上都是我一个人的，而上线之后应该不会遇到这种情况了。<br>可以尝试将这几个ip地址映射为一个地址，或者在当用户多次跳转回登录页面时，让它的地址栏强制去除backUrl参数，并给出相应提示。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>（八）搭建springcloud：修改静态文件，如html，js后立即生效</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E5%85%AB%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E4%BF%AE%E6%94%B9%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6%EF%BC%8C%E5%A6%82html%EF%BC%8Cjs%E5%90%8E%E7%AB%8B%E5%8D%B3%E7%94%9F%E6%95%88.html</url>
    <content><![CDATA[<ol>
<li>setting—&gt;Build,Execution,Deployment—&gt;Compiler  找到 build project automatically<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/0e3e14335ea94258adf2616de29ed82d.PNG" alt="idea配置自动构建" title="idea配置自动构建"></li>
<li>按ctrl+shift+alt+/，选择 Registry<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/1ed4835b01e54c82b6d0023db60873ac.jpg" alt="更改注册" title="更改注册"></li>
<li>重启<br>如果还是不行，就用麻烦点的办法，每次修改在idea按ctrl+shift+f9即可<a id="more"></a>
</li>
</ol>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E4%B8%83%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9Aspringboot%E5%A6%82%E4%BD%95%E5%B0%86%E6%89%80%E4%BE%9D%E8%B5%96%E7%9A%84jar%E5%8C%85%E7%9A%84%E7%B1%BB%E6%94%BE%E5%85%A5spring%E5%AE%B9%E5%99%A8.html</url>
    <content><![CDATA[<figure class="highlight dart"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@EnableEurekaClient</span></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Upms8081</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	public <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="built_in">String</span>[] args) &#123;</span><br><span class="line">		<span class="built_in">Object</span>[] objects = <span class="keyword">new</span> <span class="built_in">Object</span>[<span class="number">3</span>];</span><br><span class="line">		objects[<span class="number">0</span>] = Upms8081.<span class="keyword">class</span>;</span><br><span class="line">		objects[<span class="number">1</span>] = UpmsApiAppliction.<span class="keyword">class</span>;</span><br><span class="line">		objects[<span class="number">2</span>] = CommonServiceStarter.<span class="keyword">class</span>;</span><br><span class="line">		SpringApplication.run(objects, args);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其他的class对象分别是所需要初始化类jar包中的springboot启动类<br><a id="more"></a></p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>shiro的filter配置问题，自定义的filter交给spring容器管理会有问题</title>
    <url>/java/shiro%E7%9A%84filter%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%EF%BC%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84filter%E4%BA%A4%E7%BB%99spring%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E4%BC%9A%E6%9C%89%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>此项目基于springboot，放在springmvc中，filter由spring管理不知道为什么就没问题。<br>我配置了/api/v1/<strong>——&gt;roles[]，/</strong>———&gt;authc<br>/api/v1/<strong>一直无法访问，一直调转到authc这个过滤器去验证，我就怀疑是不是/</strong>也匹配到了/api/v1/**，但是以前做的时候一直没问题，所以绕了一大圈。<br>最后发现！！！！！<br>重要的事说三遍，不要将自定义的filter交给spring容器管理！不要将自定义的filter交给spring容器管理！不要将自定义的filter交给spring容器管理！<br><a id="more"></a><br>因为在一个链接的所有配置好的filter执行完毕后，它会接着执行另外一个过滤链，该过滤链由servlet容器提供。也就是说shiro一共会执行两个过滤链，一个是你自己配置好的，第二个是servlet提供的（由于本人没有阅读过spring源码，故不清楚spring注册的bean是怎么进入servlet容器的）。<br>所以当你把自定义的过滤器交给spring管理时，这个过滤器无论如何都会至少执行一次。<br>具体的代码在org.apache.shiro.web.servlet.doFilter(ServletRequest request, ServletResponse response)，第一行就是判断自定义的过滤链是否执行完毕，如果执行完毕就执行servlet提供的过滤链。（注shiro版本为1.4.0）<br>如果出现这个问题只需要将filter的创建由自己完成即可。<br>tips:由于可能在filter中需要注入某些对象。这时只能用springContext获取bean了。</p>
]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E5%85%AD%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E5%9C%A8idea%E5%90%AF%E5%8A%A8%E9%A1%B9%E7%9B%AE%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88hibernate-validator%E4%B8%8Ejavax-validation%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7%E9%97%AE%E9%A2%98%EF%BC%89.html</url>
    <content><![CDATA[<p>由于是从eclipse迁移过来，之前的不同ide开发的问题搞了好久终于搞完了，以为终于成功了，然而又出了一大堆问题。<br>项目无法启动，报了一个奇葩的错：</p>
<blockquote>
<p>Could not initialize class org.hibernate.validator.internal.engine.Configuration</p>
</blockquote>
<p>大致意思貌似就是javax.validation和hibernate-validator版本冲突了（详见<a href="https://stackoverflow.com/questions/14730329/jpa-2-0-exception-to-use-javax-validation-package-in-jpa-2-0）" target="_blank" rel="noopener">stackoverflow JPA 2.0 : Exception to use javax.validation.* package in JPA 2.0</a><br>在无数次更换版本中，终于找到了一个没有问题的版本。hibernate-validator（5.2.4.Final版），我试过很多版本都有问题，包括4.1.3，5.1.6，5.3.6，5.4.1，6.0.1都是没有用的而其中个别版本居然还不会传递依赖，就是hibernate-validator本身是依赖javax.validation（1.1.0.Final版）的，所幸的是5.2.4.Fianl依赖了javax.validation，也省的我再去试javax.validation的版本。<br>所以最后应该将hibernate-validator添加到你需要的工程的pom文件中.spring-boot-starter-web这个jar包它本身是依赖hibernate-validator的，所以我把它直接排除了（exclusions标签）。而springboot默认依赖的版本是5.3.6是有问题的，不知道是不是我电脑的原因，反正我是运行不起来。<br><a id="more"></a></p>
<p>总结：以后使用hibernate-validator要用5.2.4.Final版本。其次，我在eclipse里运行我的程序一点问题都没，移植到idea就各种问题，也不知道是不是idea太严格了。<br><strong><em>补充：由于我的是聚合工程，我搞了两三个小时，<font color="red">发现我在pom文件中改动jar包的版本，虽然idea旁边的maven dependencies中相应的改变了jar包的版本，但是在实际运行过程中，它还是使用的之前版本的jar包，跟没改一样。</font>由于我是聚合工程，我将几乎所有的jar包版本都统一放在parent工程，所以后来发现只需要clean install一下parent工程即可。</em></strong></p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（五）搭建springcloud：在idea启动springboot项目</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E4%BA%94%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E5%9C%A8idea%E5%90%AF%E5%8A%A8springboot%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<p>点击idea右上角 edit configuration<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/4803d077b7534b4fbf2f6f5a915db876.png" alt="idea配置启动类" title="idea配置启动类"><br>点击左上角的<font color="green">绿色“+”</font>然后点击maven就会出现右边窗口，注意要起一个name，然后下面的参数这么配置点击ok<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/e3a99aa12a0b4ad7a5544eaf1510cf4d.png" alt="idea配置maven命令" title="idea配置maven命令"><br>然后再到相同位置就会多了一个你起的名称的一个运行配置。点击即可。<br>最后还有最<strong>重要</strong>的一点，需要在你启动的项目的pom.xml文件中加上配置如下（主要是spring-boot-maven-plugin）：<br><a id="more"></a><br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-surefire-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">skip</span>&gt;</span>true<span class="tag">&lt;/<span class="name">skip</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>更新：此方式启动，无法进行调试，貌似可以通过配置解决。后来我都不用这个方式启动项目，在图二中不选择maven，选择Spring Boot即可运行正常。</strong></p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（三）搭建springcloud：整合mybatis-plus</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E4%B8%89%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E6%95%B4%E5%90%88mybatis-plus.html</url>
    <content><![CDATA[<p>由于一直使用mybatis的example，无法忍受着过长的代码，所以转向了mybatis-plus，配置过于简单，就不演示了。<br>参考官网<a href="http://mp.baomidou.com/#/install" target="_blank" rel="noopener">http://mp.baomidou.com/#/install</a><br>如果看不懂，可以看mybatis-plus提供的整合springboot例子<br>网址：<a href="https://gitee.com/baomidou/mybatisplus-spring-boot" target="_blank" rel="noopener">https://gitee.com/baomidou/mybatisplus-spring-boot</a><br><a id="more"></a><br>如果还看不懂，尼玛别整合了。（手动狗头）<br>提示：<br>Service可以继承mybatis-plus提供的ServiceImpl类<br>mapper层可以继承BaseMapper，这样如果有些mapper比较简单完全可以省去mapper.xml<br>IService可以继承IService</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E5%9B%9B%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9Aeclipse%E8%BF%81%E7%A7%BB%E5%88%B0idea%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88maven-install%E5%91%BD%E4%BB%A4%E5%87%BA%E9%94%99%EF%BC%89.html</url>
    <content><![CDATA[<p>在使用maven的install命令时出现java.lang.ArrayIndexOutOfBoundsException: 10640<br>试了三个小时，一直以为idea有问题，最后没想到是maven版本的问题，我的是3.5.0，只要改成3.5.2以上就可以了（我改成3.5.4）<br>真的奇怪之前在eclipse上用一直好好地<br><a id="more"></a></p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（一）搭建springcloud：springboot整合thymeleaf</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E4%B8%80%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9Aspringboot%E6%95%B4%E5%90%88thymeleaf.html</url>
    <content><![CDATA[<p>thymeleaf的配置类如下，要注意的是它的prefix不是以前的”/WEB-INF/“，而是”classpath:/templates/“。templates文件夹位于resources源文件夹下，该文件夹的名称不是我随便写的，是springboot规定的，用于放置动态文件。类似的还有static用于放置静态文件。<br>其中我分别写了linux（即生产环境）及windows（即开发环境）下的配置，可以根据需要自己删留。（由于一般开发环境为windows，所以我自己偷懒写的加载bean条件，如果开发环境也是linux那么可以直接删掉）其中还可能包含我自己写的工具类，可以直接删去。<br>注：差点忘了，我的thymeleaf是3.0以上的版本，3.0以上的版本变动有些大，如果不是该版本以上，可能无法使用该配置类。springboot默认依赖的版本是2.1.6（貌似），所以需要手动更改，不会的看<a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a id="more"></a><br><figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">@Configuration</span><br><span class="line">@ComponentScan</span><br><span class="line">public <span class="keyword">class</span> ThymeleafConfig &#123;</span><br><span class="line">    <span class="keyword">private</span> static final Logger LOGGER = <span class="module-access"><span class="module"><span class="identifier">LoggerFactory</span>.</span></span>get<span class="constructor">Logger(ThymeleafConfig.<span class="params">class</span>)</span>;</span><br><span class="line">    </span><br><span class="line">    @Bean</span><br><span class="line">    public ThymeleafViewResolver view<span class="constructor">Resolver(SpringTemplateEngine <span class="params">templateEngine</span>)</span>&#123;</span><br><span class="line">        ThymeleafViewResolver viewResolver = <span class="keyword">new</span> <span class="constructor">ThymeleafViewResolver()</span>;</span><br><span class="line">        viewResolver.set<span class="constructor">TemplateEngine(<span class="params">templateEngine</span>)</span>;</span><br><span class="line">        viewResolver.set<span class="constructor">Order(1)</span>;</span><br><span class="line">        viewResolver.set<span class="constructor">CharacterEncoding(<span class="string">"utf-8"</span>)</span>;</span><br><span class="line">        <span class="comment">//不知道这个缓存和下面的模板解析器的缓存是什么关系，但是应该不是同一个意思</span></span><br><span class="line">        <span class="comment">//我在模板解析器设置false，在这行代码之上打印了一下还是true</span></span><br><span class="line">        viewResolver.set<span class="constructor">Cache(<span class="params">false</span>)</span>;</span><br><span class="line">        viewResolver.set<span class="constructor">ViewNames(<span class="params">new</span> String[] &#123;<span class="string">"thymeleaf/*"</span>, <span class="string">".html"</span>, <span class="string">".xhtml"</span>&#125;)</span>;</span><br><span class="line">        return viewResolver;</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 模板解析器</span></span><br><span class="line"><span class="comment">     * @return</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    @<span class="constructor">Conditional(WindowsCondition.<span class="params">class</span>)</span></span><br><span class="line">    @Primary<span class="comment">//springboot貌似内置一个模板解析器，所以将这个bean定义为首选</span></span><br><span class="line">    @Bean</span><br><span class="line">    public SpringResourceTemplateResolver windows<span class="constructor">TemplateResolver()</span>&#123;</span><br><span class="line">        SpringResourceTemplateResolver templateResolver = <span class="keyword">new</span> <span class="constructor">SpringResourceTemplateResolver()</span>;</span><br><span class="line">        templateResolver.set<span class="constructor">Prefix(<span class="string">"classpath:/templates/"</span>)</span>;</span><br><span class="line">        templateResolver.set<span class="constructor">Suffix(<span class="string">".html"</span>)</span>;</span><br><span class="line">        templateResolver.set<span class="constructor">Order(1)</span>;</span><br><span class="line">        templateResolver.set<span class="constructor">TemplateMode(TemplateMode.HTML)</span>;</span><br><span class="line">        <span class="comment">//不缓存页面，即服务器开启时修改thymeleaf页面会有变化。</span></span><br><span class="line">        templateResolver.set<span class="constructor">Cacheable(<span class="params">false</span>)</span>;</span><br><span class="line">        <span class="module-access"><span class="module"><span class="identifier">LOGGER</span>.</span></span>info(<span class="keyword">new</span> <span class="constructor">LogPrintingTemplate()</span></span><br><span class="line">                .<span class="keyword">type</span>(<span class="string">"thymeleaf页面缓存禁用(￣▽￣)~*"</span>)</span><br><span class="line">                .print<span class="literal">()</span>);</span><br><span class="line">        templateResolver.set<span class="constructor">CharacterEncoding(<span class="string">"utf-8"</span>)</span>;</span><br><span class="line">        return templateResolver;</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">    @<span class="constructor">Conditional(LinuxCondition.<span class="params">class</span>)</span></span><br><span class="line">    @Primary<span class="comment">//springboot貌似内置一个模板解析器，所以将这个bean定义为首选</span></span><br><span class="line">    @Bean</span><br><span class="line">    public SpringResourceTemplateResolver linux<span class="constructor">TemplateResolver()</span>&#123;</span><br><span class="line">        SpringResourceTemplateResolver templateResolver = <span class="keyword">new</span> <span class="constructor">SpringResourceTemplateResolver()</span>;</span><br><span class="line">        templateResolver.set<span class="constructor">Prefix(<span class="string">"classpath:/WEB-INF/"</span>)</span>;</span><br><span class="line">        templateResolver.set<span class="constructor">Suffix(<span class="string">".html"</span>)</span>;</span><br><span class="line">        templateResolver.set<span class="constructor">Order(1)</span>;</span><br><span class="line">        templateResolver.set<span class="constructor">TemplateMode(TemplateMode.HTML)</span>;</span><br><span class="line">        <span class="comment">//缓存页面</span></span><br><span class="line">        templateResolver.set<span class="constructor">Cacheable(<span class="params">true</span>)</span>;</span><br><span class="line">        <span class="module-access"><span class="module"><span class="identifier">LOGGER</span>.</span></span>info(<span class="keyword">new</span> <span class="constructor">LogPrintingTemplate()</span></span><br><span class="line">                .<span class="keyword">type</span>(<span class="string">"thymeleaf页面缓存开启(￣▽￣)~*"</span>)</span><br><span class="line">                .print<span class="literal">()</span>);</span><br><span class="line">        templateResolver.set<span class="constructor">CharacterEncoding(<span class="string">"utf-8"</span>)</span>;</span><br><span class="line">        return templateResolver;</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 模版引擎</span></span><br><span class="line"><span class="comment">     * @return</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    @Bean</span><br><span class="line">    public SpringTemplateEngine template<span class="constructor">Engine(ITemplateResolver <span class="params">templateResolver</span>)</span>&#123;</span><br><span class="line">        SpringTemplateEngine templateEngine = <span class="keyword">new</span> <span class="constructor">SpringTemplateEngine()</span>;</span><br><span class="line">        templateEngine.set<span class="constructor">TemplateResolver(<span class="params">templateResolver</span>)</span>;</span><br><span class="line">        templateEngine.set<span class="constructor">EnableSpringELCompiler(<span class="params">true</span>)</span>;</span><br><span class="line">        return templateEngine;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Conditional里的类如下，linux的类似</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowsCondition</span> <span class="keyword">implements</span> <span class="title">Condition</span></span>&#123;</span><br><span class="line">       </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">matches</span><span class="params">(ConditionContext context, AnnotatedTypeMetadata metadata)</span> </span>&#123;</span><br><span class="line">        Environment environment = context.getEnvironment();</span><br><span class="line">        String osName = environment.getProperty(<span class="string">"os.name"</span>);</span><br><span class="line">        <span class="keyword">boolean</span> isWindows = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">if</span>(osName.toLowerCase().contains(<span class="string">"windows"</span>))   isWindows = <span class="keyword">true</span>;</span><br><span class="line">         </span><br><span class="line">        <span class="keyword">return</span> isWindows;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结：</p>
<ol>
<li>resources源文件夹下，templates文件夹放置动态文件，static文件夹下放置静态文件，其下的子文件夹可以随意创建。</li>
<li>thymeleaf的html文件建议放置在templates/thymeleaf文件夹下（我的习惯）</li>
<li>static文件夹下的资源是可以通过浏览器直接访问的，类似于传统web工程的webapp下的文件；而templates文件夹下的文件必须写上映射，即创建controller层，然后跟以前一样写mapping，这样就可以通过浏览器访问，然后springmvc视图解析，将视图返回给浏览器。类似于以前的WEB-INF文件夹。<br>4.<em> 更新：由于今天在配置thymeleaf时给我产生了疑惑，所以在此特地重申：templates和static文件夹的名字不是我起的，是springboot的规定。</em></li>
</ol>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（二）搭建springcloud：修改thymeleaf版本</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%EF%BC%88%E4%BA%8C%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E4%BF%AE%E6%94%B9thymeleaf%E7%89%88%E6%9C%AC.html</url>
    <content><![CDATA[<p>起因：springboot整合thymeleaf默认依赖的版本是3.0以下的版本，而我一直是用的3.0以上的版本，而且貌似3.0以上的版本性能更好，所以果断自己做了版本的更改。但是在更改是有一点问题。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- thymeleaf --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-thymeleaf<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>由于我想排除依赖，所以我在其中排除了thymeleaf-spring4的依赖，但是我发现thymeleaf核心包的版本还是3.0以下，具体解决过程不写了，解决如下：<br>上面的依赖不要删，在下面再添加依赖，父工程中添加版本，可以选择3.0以上的版本（我的是3.0.9）<br><a id="more"></a><br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>这样就可以了，但是最好再多做一步，将第一个依赖改为这样：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- thymeleaf --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-thymeleaf<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>nz.net.ultraq.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf-layout-dialect<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>nz.net.ultraq.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf-layout-dialect<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>由于thymeleaf版本变迁，所以上面的这个dialect版本必须也要跟着改变（我使用2.0.0，建议使用2.0.0以上版本）。<br>最后附上父工程配置</p>
<figure class="highlight dust"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="comment">&lt;!-- 视图层，在springboot工程中二者缺一不可，因为springboot默认依赖thymeleaf2.0 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">thymeleaf.version</span>&gt;</span>3.0.9.RELEASE<span class="tag">&lt;/<span class="name">thymeleaf.version</span>&gt;</span><span class="comment">&lt;!-- thymeleaf版本 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">thymeleaf-layout-dialect.version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">thymeleaf-layout-dialect.version</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.thymeleaf/thymeleaf-spring4 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf-spring4<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;thymeleaf.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>nz.net.ultraq.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf-layout-dialect<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;thymeleaf-layout-dialect.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a><br><!-- more --></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云安装mysql</title>
    <url>/IT-stuff/linux/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%AE%89%E8%A3%85mysql.html</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/itor/p/6339505.html" target="_blank" rel="noopener">linux下mysql-5.6忘记root密码，重置root密码详细过程</a><br><a href="https://www.cnblogs.com/BenWong/p/4322085.html" target="_blank" rel="noopener">Linux 下MySql 重置密码</a><br>这两篇结合着看就大致装完了，但是会有一个问题，就是外网连不上mysql，这是由于mysql默认不允许外网连接，所以需要做一点小变动。</p>
<p>当执行第三行时可能会出现：Duplicate entry ‘%-root’ for key ‘PRIMARY’的错误提示信息，但是不需要管它</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">mysql&gt;use mysql;</span><br><span class="line">mysql&gt;<span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> host = <span class="string">'%'</span> <span class="keyword">where</span> <span class="keyword">user</span> = <span class="string">'root'</span>;</span><br><span class="line">mysql&gt;<span class="keyword">select</span> host, <span class="keyword">user</span> <span class="keyword">from</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure>
<p>最后再执行<code>flush privileges;</code>即可<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>linux下安装redis</title>
    <url>/IT-stuff/linux/linux%E4%B8%8B%E5%AE%89%E8%A3%85redis.html</url>
    <content><![CDATA[<h3 id="下载redis"><a href="#下载redis" class="headerlink" title="下载redis"></a>下载redis</h3><p>首先下载redis，由于是linux下安装，所以下载tar.gz压缩包，官网地址<a href="https://redis.io/" target="_blank" rel="noopener">https://redis.io/</a><br>然后使用工具把下载下来的压缩包上传到服务器上（我用的是腾讯云），我放在了/usr/local/redis下</p>
<h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p><code>tar zxvf  redis-4.0.10.tar.gz</code></p>
<h3 id="进入解压后的文件夹"><a href="#进入解压后的文件夹" class="headerlink" title="进入解压后的文件夹"></a>进入解压后的文件夹</h3><p><code>cd redis-4.0.10</code></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight gauss"><table><tr><td class="code"><pre><span class="line"><span class="built_in">make</span></span><br><span class="line">cd src(下一步需要有gcc编译redis源文件，如果没有，运行yum install gcc-c++)</span><br><span class="line"><span class="built_in">make</span> install PREFIX=/usr/<span class="keyword">local</span>/redis</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="移动配置文件到安装目录"><a href="#移动配置文件到安装目录" class="headerlink" title="移动配置文件到安装目录"></a>移动配置文件到安装目录</h3><figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cd</span> ..</span><br><span class="line"><span class="keyword">mkdir</span> /usr/<span class="keyword">local</span>/redis/etc </span><br><span class="line">mv redis.<span class="keyword">conf</span> /usr/<span class="keyword">local</span>/redis/etc</span><br></pre></td></tr></table></figure>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight jboss-cli"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cd</span> bin</span><br><span class="line"><span class="string">./redis-server</span></span><br></pre></td></tr></table></figure>
<p>这是你会发现你的控制台无法再输入命令了，因为你启动了redis，如果想启动了redis还想干其他事，那么需要将redis设置后后台运行</p>
<h3 id="关闭redis服务并编辑刚才的配置文件"><a href="#关闭redis服务并编辑刚才的配置文件" class="headerlink" title="关闭redis服务并编辑刚才的配置文件"></a>关闭redis服务并编辑刚才的配置文件</h3><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">ctrl</span> <span class="string">c</span></span><br><span class="line"><span class="attr">cd</span> <span class="string">etc</span></span><br><span class="line"><span class="attr">vim</span> <span class="string">redis.conf</span></span><br></pre></td></tr></table></figure>
<p>将daemonize设置为yes。<br>如果找不到daemonize在哪，在命令模式下按”/“输入要查找的字，回车进行查找，”n”继续查找。</p>
<p><strong>注：如果改了之后还是没有反应，那么可以在启动redis时指定配置文件，如：./redis-server ../etc/redis.conf</strong><br>然后可以使用netstat -lntp|grep 6379看看6379端口是不是被监听了。6379是redis默认端口。<br><!-- more --></p>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>安装与部署</tag>
      </tags>
  </entry>
  <entry>
    <title>thymeleaf在@{}中无法使用其他服务器链接的问题</title>
    <url>/java/thymeleaf%E5%9C%A8-%E4%B8%AD%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E5%85%B6%E4%BB%96%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%93%BE%E6%8E%A5%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>以下是官网原话</p>
<ul>
<li>Absolute URLs: <a href="http://www.thymeleaf.org" target="_blank" rel="noopener">http://www.thymeleaf.org</a></li>
<li>Relative URLs, which can be:<ul>
<li>Page-relative: user/login.html</li>
<li>Context-relative: /itemdetails?id=3 (context name in server will be added automatically)</li>
<li>Server-relative: ~/billing/processInvoice (allows calling URLs in another context (= application) in the same server.</li>
<li>Protocol-relative URLs: //code.jquery.com/jquery-2.0.3.min.js</li>
</ul>
</li>
</ul>
<p>中文版</p>
<ul>
<li>绝对网址： <a href="http://www.thymeleaf.org" target="_blank" rel="noopener">http://www.thymeleaf.org</a></li>
<li>相对URL，可以是：<ul>
<li>页面相对： user/login.html</li>
<li>与上下文相关:( /itemdetails?id=3服务器中的上下文名称将自动添加）</li>
<li>服务器相对:( ~/billing/processInvoice允许在同一服务器中的另一个上下文（=应用程序）中调用URL。</li>
<li>与协议相关的网址： //code.jquery.com/jquery-2.0.3.min.js<a id="more"></a>
</li>
</ul>
</li>
</ul>
<p>它就提供这几种方式，但是我的服务器比如是baidu.com，使用<code>th:href=&quot;@{http://sina.com/index.css}&quot;</code>这样是可以的，但是<code>th:href=&quot;@{${sina_static}/index.css}&quot;</code>这样却不行，也就是说静态资源无法使用变量直接取出来，那么以后改动这些资源将十分麻烦，因为只能写死了。thymeleaf不让动态的获取。<br>thymeleaf提供的几种方式没有一种可以显示不同服务器的地址。<br>后来发现还有另外一种方式，就是投机取巧的使用url参数。<br><code>th:href=&quot;@{(由于博客系统原因删除这句话，包括小括号){variable}/index.css(variable=${value})}&quot;</code>这样即可。<br><!-- more --></p>
]]></content>
      <categories>
        <category>java</category>
        <category>thymeleaf</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>bootstrap-switch常用操作/属性</title>
    <url>/IT-stuff/%E5%89%8D%E7%AB%AF/bootstrap-switch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C-%E5%B1%9E%E6%80%A7.html</url>
    <content><![CDATA[<h3 id="引入css、js"><a href="#引入css、js" class="headerlink" title="引入css、js"></a>引入css、js</h3><p><a href="http://www.bootcss.com/p/bootstrap-switch/" target="_blank" rel="noopener">bootstrap-switch中文官网</a></p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&lt;link <span class="attribute">rel</span>=<span class="string">"stylesheet"</span> <span class="attribute">href</span>=<span class="string">"<span class="variable">$&#123;CONTEXT &#125;</span>/css/bootstrap-switch.min.css"</span>&gt;</span><br><span class="line">&lt;script <span class="attribute">src</span>=<span class="string">"<span class="variable">$&#123;CONTEXT &#125;</span>/js/bootstrap/bootstrap-switch.min.js"</span>&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure>
<p><strong>注：</strong>貌似还有一个叫做bootstrapSwitch.js的插件，不要引入错了。</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"input-group"</span>&gt;</span><br><span class="line">&lt;label <span class="keyword">for</span>=<span class="string">"open"</span>&gt;操作&amp;nbsp;&lt;/label&gt;</span><br><span class="line">&lt;input <span class="built_in">id</span>=<span class="string">"open"</span> checked type=<span class="string">"checkbox"</span> data-<span class="keyword">on</span>-<span class="built_in">text</span>=<span class="string">"开"</span> data-off-<span class="built_in">text</span>=<span class="string">"关"</span> data-<span class="keyword">on</span>-color=<span class="string">"success"</span> /&gt;</span><br><span class="line">&lt;/<span class="keyword">div</span>&gt;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="常用属性"><a href="#常用属性" class="headerlink" title="常用属性"></a>常用属性</h3><ul>
<li>data-on-text=”开” data-off-text=”关”，代表开启/关闭时按钮上的文字。</li>
<li>checked代表默认选中，不填代表不选中。</li>
<li>data-on-color=”success”，代表选中时的颜色。</li>
</ul>
<h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><ul>
<li>$(‘#open’).bootstrapSwitch();，开始自动生成一个按钮。注：#open是input的id。</li>
<li>$(‘#open’).on(‘switchChange.bootstrapSwitch’, function (event,state) { }，switch触发的事件，所以不用自己再写click事件了。注：state代表按钮当前选中状态。</li>
<li>$(‘#toggle-state-switch’).bootstrapSwitch(‘toggleState’);，来回切换。</li>
<li>$(‘#toggle-state-switch’).bootstrapSwitch(‘setState’, false);，设置按钮状态，false代表未选中。</li>
</ul>
<p>另外不要写官网上的那个格式，官网上的格式是误导人的，它那个写法：把input放在div里，然后div加一个switch的class，这样其实是代表按钮自动生成后的dom节点。所以按照我这个写就行了。</p>
<!-- more -->]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>front-end</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>bootstrap-switch</tag>
      </tags>
  </entry>
  <entry>
    <title>基于shiro的单点退出功能</title>
    <url>/java/%E5%9F%BA%E4%BA%8Eshiro%E7%9A%84%E5%8D%95%E7%82%B9%E9%80%80%E5%87%BA%E5%8A%9F%E8%83%BD.html</url>
    <content><![CDATA[<p><strong>2019.01.24更新：发现其他sso系统的实现思路跟我的不一样，我也不知道这样对不对，但是功能是能实现的。</strong></p>
<hr>
<p>由于非单点登录系统在检测用户是否已经单点退出时有点复杂。<br>分两种情况：</p>
<ol>
<li>在每次访问链接时，重定向到单点登录系统进行验证，这是可以做到的，但是用户体验差（这样做或许也可以）。</li>
<li>发送一个http请求，这样可以在用户不知情的情况下进行验证，但是发起http请求，请求的用于不再是当前用户了，即session 不是同一个了。</li>
</ol>
<p>而shiro需要获取subject才能退出登录（我找了很久没有找到其他的办法），所以需要做一个折中的方法，在单点退出的过滤器那，设置session过期，session失效，然后在authc过滤器，判断当前session是否失效，如果失效则执行退出。<br><a id="more"></a><br>2018-5-4更新：<br>最终实现：其他系统发出退出登录请求，upms接收到直接将该用户注销，同时全局session从redis中自动删除，并且在删除之前写入一个名为logout的属性，设置为true（由于session瞬间被删除了，但是并没有多余，因为以后可能加入某些机制，使得session过一会才被删除，所以有必要提前做好防患）。<br>由于upms系统中退出登录，但是没有办法通知其他子系统，所以只能被迫让其他子系统自己来得到通知。具体实现如下：</p>
<ol>
<li>用户在登录时，会在全局session中添加一个属性名为logout，设置为false。</li>
<li>这样当用户在子系统退出后，当前子系统是知道用户已经退出。</li>
<li>但是其他子系统并不知情，所以用户再次访问当前子系统是直接跳转到登录页面。</li>
<li>而当用户访问其他子系统时，子系统需要先跳转到upms系统进行查看，该用户是否退出登录。</li>
<li>规则很简单，upms只需要判断logout属性是否为空或者是否为false即可。</li>
</ol>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>solr使用自动建议搜索出现No suggester named default was configured</title>
    <url>/java/solr%E4%BD%BF%E7%94%A8%E8%87%AA%E5%8A%A8%E5%BB%BA%E8%AE%AE%E6%90%9C%E7%B4%A2%E5%87%BA%E7%8E%B0No-suggester-named-default-was-configured.html</url>
    <content><![CDATA[<p>solr的默认配置文件中，有一个路径为/suggest的requestHandler，它里面没有配置suggest.dictionary，这个意思就是建议的字典。而默认值可能就是default，所以出现这个错误，只需要配置一下字典就行了。我碰到的时候我也想不明白，用全文搜索都没搜到default。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/b093167a98b9425ca7318393fb6ed37a.PNG" alt="solr配置" title="solr配置"><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>error</tag>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>Solr中文分词以及拼写纠错</title>
    <url>/java/Solr%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E4%BB%A5%E5%8F%8A%E6%8B%BC%E5%86%99%E7%BA%A0%E9%94%99.html</url>
    <content><![CDATA[<p>由于在更改配置文件后，重启tomcat，solr的web页面可以直接看到变化，所以我都懒得使用dataimport了。<br>我之前一直配置中文分词以及拼写纠错失败，我把配置中默认查询字段就改了一个名字，把text改为spell，每一处都替换掉，居然会产生不一样的结果。我搞了一个多小时，发现怎么换都是这样，前后结果完全不同。要么可以分词要么可以拼写纠错，二者没有同时出现过。最后要睡觉的时候，dataimport了一下。发现居然可以了。<br>我猜测可能在建立索引时把text作为键，于是我更改了text的名字，自然是无法查询到任何东西。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>二、将mysql的数据导入到solr中</title>
    <url>/java/%E4%BA%8C%E3%80%81%E5%B0%86mysql%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0solr%E4%B8%AD.html</url>
    <content><![CDATA[<h3 id="一、solr部署于tomcat"><a href="#一、solr部署于tomcat" class="headerlink" title="一、solr部署于tomcat"></a>一、solr部署于tomcat</h3><p><a href="https://yan624.github.io/一、solr部署于tomcat">上一篇</a></p>
<h3 id="二、将mysql的数据导入到solr中"><a href="#二、将mysql的数据导入到solr中" class="headerlink" title="二、将mysql的数据导入到solr中"></a>二、将mysql的数据导入到solr中</h3><p>tomcat同级目录中的solr被称为solr_home<br>tomcat的webapps文件夹中的solr是solr的web工程<br>将从apache上下下来的solr<font color="red">根</font>文件夹中的/contrib和/dist，复制到solr_home的lib文件夹中，如果没有这个文件夹可以自己建一个。如下图:<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/1375f60bca34409d952810bc12beb13c.PNG" alt="solr lib" title="solr lib"><br>这两个文件夹中全是jar包。现在只是复制过来了，solr还不能用它们，所以需要配置一下。<br>solr_home中应该有一个collection1文件夹，这是solr的一个示例，如果不放心可以拷贝一份，改一个名字。注意如果拷贝了一份，需要进入你拷贝的这份文件夹，找到core.properties，将里面的name属性改为你的文件夹名字。<br>我复制了一份，取名为order，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/a193f1c3fea84613b0b048c0791dd1b2.PNG" alt="solr官方示例" title="solr官方示例"><br>进入order文件夹，里面有一个conf文件夹，再进入，找到solrconfig.xml文件，打开编辑。前几十行很多注释，往下大概80行的样子，有几个lib标签，这些标签就是代表导入之前的jar包。但是我们的路径发生了改变，所以这里需要配置一下。<br><a id="more"></a><br><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/dataimporthandler/lib"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-dataimporthandler-\d.*\.jar"</span> /&gt;</span><br><span class="line">   </span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/extraction/lib"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-cell-\d.*\.jar"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/clustering/lib/"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-clustering-\d.*\.jar"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/langid/lib/"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-langid-\d.*\.jar"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/velocity/lib"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-velocity-\d.*\.jar"</span> /&gt;</span><br></pre></td></tr></table></figure></p>
<p>lib标签dir属性的根目录位于conf文件夹的同级目录。需要注意的是前两个标签，不出意外的话，这两行你们是没有的，这里需要加上这两行。因为它们关系到导入数据。<br>还是在这个文件中，往下拉，大概找到requestHandler标签，其实不找到也行，但是为将相同的标签放在一起便于管理，所以需要找到并在其中一个位置加上下面的代码：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">requestHandler</span> <span class="attr">name</span>=<span class="string">"/dataimport"</span> <span class="attr">class</span>=<span class="string">"solr.DataImportHandler"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">lst</span> <span class="attr">name</span>=<span class="string">"defaults"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">str</span> <span class="attr">name</span>=<span class="string">"config"</span>&gt;</span>data-config.xml<span class="tag">&lt;/<span class="name">str</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">lst</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">requestHandler</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>这个就是代表了一个请求路径，看英文名代表了数据导入。这里需要注意一点，class是solr.DataImportHandler，而不是其它的，我就是因为懒得自己写，顺手改了一个注释掉的标签，然后忘记改class属性，导致数据一直导入失败。里面的config标签就是代表了数据的配置。接下来需要配置这份文件。<br>与solrconfig.xml同级目录中<font color="red">新建</font>一个（如果不存在这个文件的话）data-config.xml。配置如下：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">&lt;dataConfig&gt;</span><br><span class="line">    &lt;dataSource <span class="keyword">type</span>="JdbcDataSource"</span><br><span class="line">        driver="com.mysql.jdbc.Driver" url="jdbc:mysql://localhost:3306/order"</span><br><span class="line">        <span class="keyword">user</span>="root" <span class="keyword">password</span>="password" batchSize="-1" /&gt;</span><br><span class="line">    &lt;document&gt;</span><br><span class="line">        &lt;entity <span class="type">name</span>="tb_businessman" query="select * from tb_businessman"&gt;</span><br><span class="line">            &lt;field column="businessman_id" <span class="type">name</span>="businessman_id" /&gt;</span><br><span class="line">            &lt;field column="businessman_name" nam="businessman_name" /&gt;</span><br><span class="line">            &lt;field column="introduction" <span class="type">name</span>="introduction" /&gt;</span><br><span class="line">            &lt;field column="business_hours" <span class="type">name</span>="business_hours" /&gt;</span><br><span class="line">            &lt;field column="start_price" <span class="type">name</span>="start_price" /&gt;</span><br><span class="line">            &lt;field column="lunch_box_fee" <span class="type">name</span>="lunch_box_fee" /&gt;</span><br><span class="line">            &lt;field column="merchant_store_thumbnail" <span class="type">name</span>="merchant_store_thumbnail" /&gt;</span><br><span class="line">            &lt;field column="monthly_sales" <span class="type">name</span>="monthly_sales" /&gt;</span><br><span class="line">            &lt;field column="average_score" <span class="type">name</span>="average_score" /&gt;</span><br><span class="line">            &lt;field column="is_open" <span class="type">name</span>="is_open" /&gt;</span><br><span class="line">            &lt;field column="QR_code" <span class="type">name</span>="QR_code" /&gt;</span><br><span class="line">        &lt;/entity&gt;</span><br><span class="line">    &lt;/document&gt;</span><br><span class="line">&lt;/dataConfig&gt;</span><br></pre></td></tr></table></figure>
<p>dataSource不做讲解，document中的entity标签的name属性指的是类似于一个id，代表在solr的web应用的网页里面的一个select的一个option（目前我是这么理解的）。query就是查数据。column就是数据库列名，name是接下来配置的。<br>接下来配置最后一步，schema.xml文件，也是在conf目录下。<br>里面solr已经配置很多了，所以看起来很麻烦。找到fields标签，这个标签下面就是配置各个字段，就是上面说的name。其中</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"_version_"</span> <span class="attribute">type</span>=<span class="string">"long"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line"></span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"_root_"</span> <span class="attribute">type</span>=<span class="string">"string"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"false"</span>/&gt;</span><br></pre></td></tr></table></figure>
<p>这两个不要删，然后把fields下的所有标签全删了，改成如下：</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"businessman_id"</span> <span class="attribute">type</span>=<span class="string">"string"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span> <span class="attribute">required</span>=<span class="string">"true"</span> <span class="attribute">multiValued</span>=<span class="string">"false"</span> /&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"businessman_name"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span> <span class="attribute">multiValued</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"introduction"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"business_hours"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"start_price"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"lunch_box_fee"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"merchant_store_thumbnail"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"monthly_sales"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"average_score"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"is_open"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"QR_code"</span> <span class="attribute">type</span>=<span class="string">"string"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span> <span class="attribute">multiValued</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"text"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span> <span class="attribute">multiValued</span>=<span class="string">"true"</span>/&gt;</span><br></pre></td></tr></table></figure>
<p>篇幅有限，不做太多解释，接下来，在fields标签下一个标签应该是<uniquekey>，把里面的值，改为你的id，我的是businessman_id。注意上面的text_cn这是某个solr的域类型，现在是初步测试，就不写这个了，可以缓存solr已经配置好的text_general，这样虽然我们的数据是中文的，但是完全可以进行简单的处理了。<br>最后还需要一个mysql的jar包，把它放到WEB-INF的lib目录下，就可以启动tomcat访问了。如下图，进行导入数据，点击菜单栏中Query进行查询。（注：4,5两步中间需要等一会，不要马上点，如果成功了，“显示”那会出现绿色字体。）Query中其他参数可自行学习。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/0820f706eae3497599ffd4d1b6783ab2.PNG" alt="步骤" title="步骤"><br><!-- more --></uniquekey></p>
]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
      </tags>
  </entry>
  <entry>
    <title>一、solr部署于tomcat</title>
    <url>/java/%E4%B8%80%E3%80%81solr%E9%83%A8%E7%BD%B2%E4%BA%8Etomcat.html</url>
    <content><![CDATA[<h3 id="一、solr部署于tomcat"><a href="#一、solr部署于tomcat" class="headerlink" title="一、solr部署于tomcat"></a>一、solr部署于tomcat</h3><p>solr版本：solr-4.7.1<br>操作时间：2018-4-11<br>tomcat版本：tomcat9</p>
<p>由于《solr实战》是用solr4.*，所以我就下了一个版本4的。但是其实已经出到版本7了。<br>下载地址：<a href="http://archive.apache.org/dist/lucene/solr/4.7.1/" target="_blank" rel="noopener">http://archive.apache.org/dist/lucene/solr/4.7.1/</a></p>
<p>我是先照着《solr实战》中基础部分，在jetty中练习了大概一两个小时再移到tomcat中玩的，如果以前没玩过solr建议先知道哪些文档有哪些作用，再移到tomcat。因为移到tomcat和直接在jetty中玩，文件路径有点不一样，而且配置也有点不一样。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/b0e4d6cd5265431fae04c44c4debf8a1.PNG" alt="solr文件路径" title="solr文件路径"></p>
<ol>
<li>将我上图中选中的solr文件夹移动到tomat<font color="red">同级</font>目录下（或者其他目录，这个路径问题是可以自己配置的，下面会说。），该文件夹被称呼为solr_home。<a id="more"></a>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/22852cec88614c66ad0613cf657d6dda.PNG" alt="solr home" title="solr home"></li>
<li>然后将第一个图中<font color="red">webapps文件夹</font>中的一个名为solr.war的war包，移动到<font color="red">tomcat中的webapps文件夹</font>中，就是部署war包。</li>
<li>启动tomcat，让tomcat解压war包。这时webapps下多了一个名为solr文件夹（与上面的solr_home不一样），它的WEB-INF/lib下有很多jar包，但是还是缺少了一些。还是第一个图，进入lib文件夹，里面会有一个ext文件夹把里面的jar包全部复制到tomcat的solr项目的lib文件夹中。这样就差不多完成了。最后结果如下图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/0dca7561a59340c2b3b404508487c60e.PNG" alt="tomcat中最后结果" title="tomcat中最后结果"></li>
<li>然后之前说solr_home放在了tomcat同级目录下，而solr这个项目并不能自己发现这个solr_home，所以需要配置一下。进入tomcat中的solr项目更改WEB-INF文件夹中的web.xml文件。用ctrl+f的快捷键搜索一下env-entry这个标签。更改为：</li>
</ol>
<figure class="highlight fortran"><table><tr><td class="code"><pre><span class="line">&lt;env-<span class="built_in">entry</span>&gt;</span><br><span class="line">    &lt;env-<span class="built_in">entry</span>-<span class="keyword">name</span>&gt;solr/home&lt;/env-<span class="built_in">entry</span>-<span class="keyword">name</span>&gt;</span><br><span class="line">    &lt;env-<span class="built_in">entry</span>-<span class="keyword">value</span>&gt;D:\tomcat\solr&lt;/env-<span class="built_in">entry</span>-<span class="keyword">value</span>&gt;</span><br><span class="line">    &lt;env-<span class="built_in">entry</span>-<span class="keyword">type</span>&gt;java.lang.String&lt;/env-<span class="built_in">entry</span>-<span class="keyword">type</span>&gt;</span><br><span class="line">&lt;/env-<span class="built_in">entry</span>&gt;</span><br></pre></td></tr></table></figure>
<p>注意其中env-entry-value是我solr_home的绝对路径。这样solr项目就可以找到solr_home了。<br>这个时候启动tomcat应该就可以访问了。项目地址是<code>http://localhost:8080/solr</code>，到现在只是完成了部署，下篇介绍如何导入mysql中的数据。</p>
<h3 id="二、将mysql的数据导入到solr中"><a href="#二、将mysql的数据导入到solr中" class="headerlink" title="二、将mysql的数据导入到solr中"></a>二、将mysql的数据导入到solr中</h3><p><a href="https://yan624.github.io/二、将mysql的数据导入到solr中">下一篇</a><br><!-- more --></p>
]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>安装与部署</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>访问browse页面solr出现lazy loading error</title>
    <url>/java/%E8%AE%BF%E9%97%AEbrowse%E9%A1%B5%E9%9D%A2solr%E5%87%BA%E7%8E%B0lazy-loading-error.html</url>
    <content><![CDATA[<p>如图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/4494aa77feea482388e892e7e6793b8a.PNG" alt="lazy loading error" title="lazy loading error"><br>如果你将下载下来的solr自己部署到tomcat下，然后启动无异常，但是访问browse路径下时，出现该异常。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/c84bd8f2e8b1496284c705b515bbc522.PNG" alt="solr配置" title="solr配置"><br>看选中的那行，参数名wt它的值时velocity，意识是让solr返回以velocity模版引擎构建的页面。而velocity实际上也是一个配置。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/b1c20a04a208423ba9a5cb73aef2caf2.PNG" alt="solr velocity配置" title="solr velocity配置"><br>同一份配置文件，往最下面拉，看见这个配置，原因是没有这个类，<code>class=&quot;solr.VelocityResponseWriter&quot;</code>，这里有问题。所以只需要导入该包，或者直接在访问browse时跟一个参数，wt=xml或者wt=json等等。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>thymeleaf遍历列表出现的问题</title>
    <url>/java/thymeleaf%E9%81%8D%E5%8E%86%E5%88%97%E8%A1%A8%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>今天做一个功能，找了好久，也看了thymeleaf的官网好久，发现th遍历集合居然不能根据索引获取对象。还要后台自己整理结构，真的麻烦。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>thymeleaf</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>吐槽</tag>
      </tags>
  </entry>
  <entry>
    <title>swagger的初步运用</title>
    <url>/java/swagger%E7%9A%84%E5%88%9D%E6%AD%A5%E8%BF%90%E7%94%A8.html</url>
    <content><![CDATA[<p>网上的介绍很多，我主要描述网上不常见的：<br><code>@ApiImplicitParam</code><br>这个注解有一个属性：paramType。它的有效值仅有5中，分别为：path，query，form，body，header。<br>但是这几个值，我感觉用起来特别麻烦。</p>
<h3 id="path（配合-PathVariable）"><a href="#path（配合-PathVariable）" class="headerlink" title="path（配合@PathVariable）"></a>path（配合@PathVariable）</h3><p>这是最简单的，如/user/{userId}，如果使用这种形式，就可以使用path；</p>
<h3 id="query（配合-RequestParam）"><a href="#query（配合-RequestParam）" class="headerlink" title="query（配合@RequestParam）"></a>query（配合@RequestParam）</h3><p>这个我认为是有bug的，如果在swagger2的配置中这样写：<br><a id="more"></a><br><figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="constructor">Docket(DocumentationType.SWAGGER_2)</span></span><br><span class="line">    .api<span class="constructor">Info(<span class="params">new</span> ApiInfoBuilder()</span></span><br><span class="line">        .title(<span class="string">"订餐系统_接口文档"</span>)</span><br><span class="line">        .description(<span class="string">"描述：用于订餐管理系统其他模块的数据调用"</span>)</span><br><span class="line">        .version(<span class="string">"版本号:1.0"</span>)</span><br><span class="line">        .build<span class="literal">()</span></span><br><span class="line">    )</span><br><span class="line">    .select<span class="literal">()</span></span><br><span class="line">    .apis(<span class="module-access"><span class="module"><span class="identifier">RequestHandlerSelectors</span>.</span></span>any<span class="literal">()</span>)</span><br><span class="line">    .paths(<span class="module-access"><span class="module"><span class="identifier">PathSelectors</span>.</span></span>any<span class="literal">()</span>)</span><br><span class="line">    .build<span class="literal">()</span></span><br><span class="line">    .path<span class="constructor">Mapping(<span class="string">"/"</span>)</span></span><br><span class="line">    .direct<span class="constructor">ModelSubstitute(LocalDate.<span class="params">class</span>, String.<span class="params">class</span>)</span></span><br><span class="line">    .generic<span class="constructor">ModelSubstitutes(ResponseEntity.<span class="params">class</span>)</span></span><br><span class="line">    .use<span class="constructor">DefaultResponseMessages(<span class="params">false</span>)</span></span><br><span class="line">    .enable<span class="constructor">UrlTemplating(<span class="params">true</span>)</span></span><br></pre></td></tr></table></figure></p>
<p>看最后一个方法，这就代表你的文档最后生成是这样的：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/ee28b7c4e8f84f33938d2d2c1deced0a.PNG" alt="swagger post的接口样例截图" title="swagger post的接口样例截图"><br>我为什么要说有bug？这看起来很美好，但是当你在网站上测试时，如下<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/737c0b56dad0405cb8a5b0e330ee8380.PNG" alt="url模板bug" title="url模板bug"><br>swagger提供了，在网站上直接测试的功能，但是请看黄色部分，/modify-password后面跟了什么，看起来那只是说明用的{?newPassword…}，直接加在了链接上，导致报了404，找不到该url。<br>就这玩意搞了我好几个小时，后来还是无意中发现的。但是如果把那个方法删了那么就会变成下面的样子。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/e98acd4958754ee5b5876623b758a493.PNG" alt="正常的post请求url模板" title="正常的post请求url模板"><br>这样就是正常的，就是普通的get请求，但是这里注意一点，post请求就不要用query了，因为它是直接在url后跟参数的。说是post其实被它改成了get（ps：仅在网页上测试时很坑爹，如果只是给其他开发者使用，那是无所谓的，顶多不要在线测试这项功能）。</p>
<h3 id="form"><a href="#form" class="headerlink" title="form"></a>form</h3><p>这个也很坑爹，我也是后来发现的。常见的表单提交，可以f12看到是Query String Parameters，但是这个使用了坑爹的Request Plyload（并没有说它不好）。哪坑爹呢？后台的controller方法里我试了无数个注解，无数个方法接受不到值，要么是null，要么http错误。<br>我最会想到了，也是因为前段时间正好用过—-》我看到swagger在网页上提示是formData，玩过前端的人应该知道js有一个对象叫做：FormData（感兴趣的自己百度）。坑爹的就在这，FormData是用来提交二进制数据的（我是这么理解的），比如一张图片，一份文件。所以我抱着尝试的心态，在Controller的方法中加入了一个参数MultipartFile对象，尼玛，居然正常接收到了参数。<br>综上，如果用了form，想使用在线测试功能，就加个MultipartFile参数。</p>
<h3 id="body"><a href="#body" class="headerlink" title="body"></a>body</h3><p>这个还行，因为如果使用了这个swagger会提交json数据（但是不知道是字符串还是对象，初步估计应该是对象），所以如果使用body，那么@ApiImplicitParam就不用写很多了，比如说，你想接受用户的账号（account），密码（password），那么不需要这样写：</p>
<figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="variable">@ApiImplicitParams</span>(</span><br><span class="line">    &#123;<span class="variable">@ApiImplicitParam</span>(name=<span class="string">"account"</span>,value=<span class="string">"账号"</span>,dataType=<span class="string">"String"</span>, paramType = <span class="string">"body"</span>), </span><br><span class="line">    <span class="variable">@ApiImplicitParam</span>(name=<span class="string">"password"</span>,value=<span class="string">"密码"</span>,dataType=<span class="string">"String"</span>, paramType = <span class="string">"body"</span>)&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>只需要这样：<code>@ApiImplicitParam(name=&quot;user&quot;,value=&quot;用户信息&quot;,dataType=&quot;User&quot;, paramType = &quot;body&quot;)</code><br>就可以了，User就是实体类，在网页上显示就是这样：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/327d110778174272982d99f219254f7e.PNG" alt="swagger中将参数类型设置body后的结果" title="swagger中将参数类型设置body后的结果"><br>它会让你提交json数据，只有body会让你提交json数据，很好区分。<br>但是我为什么要说它坑爹。因为swagger吧User的所有字段都放进去了，虽然可以一个个删掉，但是每次浏览网页都要做一遍，麻烦死了。所以需要使用如下方法：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/b9b79737da504931b2a940c1a201330f.PNG" alt="用户实体类" title="用户实体类"><br>设置为只读，这样就不会在网页上显示了，我们可以把除了account（图中的tel）和password的字段全部设置为只读。这里放心我测试过了，只读只是在网页看不见，其他的功能暂时没有受到影响。<br><strong>注：</strong>@RequestBody和@ModelAttribute不能一起用<br><strong>更新：</strong>今天测试的时候发现，如果其他客户端使用代码的方式调用会出现不小的问题。这里把我解决的部分写出：content-type必须设置为“application/json”，传递参数时，不需要写键，只需要传一个json字符串即可，并且json字符串必须使用双引号，而不能是单引号（测试于fastxml jackson）。<br>我列出我的代码（使用RestTemplate）<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/ebcbfdb2368e43be85252b69a65dce96.PNG" alt="使用body的问题" title="使用body的问题"></p>
<h3 id="header"><a href="#header" class="headerlink" title="header"></a>header</h3><p>没有用过，但是是配合@RequestHeader用的<br><!-- more --></p>
]]></content>
      <categories>
        <category>java</category>
        <category>swagger</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>shiro角色和权限的用法</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/shiro%E8%A7%92%E8%89%B2%E5%92%8C%E6%9D%83%E9%99%90%E7%9A%84%E7%94%A8%E6%B3%95.html</url>
    <content><![CDATA[<h3 id="shiro通配符的理解"><a href="#shiro通配符的理解" class="headerlink" title="shiro通配符的理解"></a>shiro通配符的理解</h3><p>shiro提供通配符，例如<code>*:*:*</code>的形式<br>意味着：第一部分代表一个域，第二部分代表操作，第三部分代表实例。<br>举个例子，现在有一个权限管理系统，称为<code>:upms</code>，可以写出权限<code>upms:update:coment</code>,这个权限值代表可以在upms这个域中对coment进行update操作。但是也不要思维定死，“域就是一个系统”。<br>再比如说一个权限值<code>upms:update:user</code>,代表可以更新一个用户的信息，这里的域的确是upms，但是并不是所有的用户都用这个域（事实上也不能用）。比如普通的用户也有权限更新自己的信息，那么可以再创建一个权限值比如“user:update:15900000000”,意味着一个用户可以更新15900000000这个账号的信息。</p>
<h3 id="shiro角色和权限的理解"><a href="#shiro角色和权限的理解" class="headerlink" title="shiro角色和权限的理解"></a>shiro角色和权限的理解</h3><p>之前一直想角色和权限的用法差不多，为什么还要分角色和权限？<br><a id="more"></a><br>然后看了<a href="http://shiro.apache.org/permissions.html" target="_blank" rel="noopener">shiro官网的文档</a>貌似懂了一些。<br>官网说shiro一共有三种权限粒度，分为：资源级别，实例级别，属性级别（ <a href="http://shiro.apache.org/java-authorization-guide.html#levels-of-permission-granularity" target="_blank" rel="noopener">这三种粒度的描述页面</a>）<br>角色可以控制对资源的访问，比如对一个页面的访问。如果程序中已经设置了隐式角色，那么直接使用注解的形式可以很快速的控制这个网页的浏览权限。<br>而权限是对角色更细粒度的划分。比如现在有一个角色名为“admin”，他拥有对系统中所有权限的CRUD操作，现在需要临时给另外一个很普通的用户（比如只是一个user）查看权限的操作（R）。如果没有shiro提供的权限控制，<font color="red">那么只能给这个用户一个临时的“admin”角色，但是这样他的权限就太大了，因为“admin”是可以查看权限，但是他同样可以删除权限。</font></p>
<p>上述是个人理解。</p>
<p>又看了一遍才看懂了shiro官网的意思。</p>
<ol>
<li>资源级别指的是一个人可以干什么</li>
<li>实例级别指的是一个人可以对什么干什么</li>
<li>属性级别指的是一个人可以对什么上的什么干什么</li>
</ol>
<p>以我的博客网站为例，1代表一个人可以管理整个博客后台，2代表一个人可以对博客类别进行某些操作，3代表一个人可以对博客类别的某一个属性（比如博客类别的状态）进行某些操作</p>
<p>2018-3-17更新，红色部分有错误，如果只给一个用户赋予权限而部赋予角色，用户还是访问不了该链接，因为shiro会执行角色过滤器和权限过滤器，所以用户必须同时满足角色和权限的需求。<br>所以给一个用户赋予临时权限的思路是错的。</p>
<p>2018-3-17再次更新，突然角色这条思路不是全错，因为没有必要为一个链接同时赋予角色和权限，因为一个角色下囊括了多个权限，为何还要多次一举呢？我觉得如果一个链接需要同时赋予角色以及权限，那么肯定是因为该链接非常重要，不允许非该角色的用户操作。<br>综上，如果一个链接很重要，比如删除整个系统的功能，那么肯定是同时需要角色和权限的，而如果是一个很普通的操作，那么可以只赋予权限，而不赋予角色。</p>
<p>但是权限的细分可以快速的为一个角色增加或删除权限，如给一个角色新增一个权限，那么该角色就可以多访问一个链接。</p>
]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>linux安装nginx</title>
    <url>/IT-stuff/linux/linux%E5%AE%89%E8%A3%85nginx.html</url>
    <content><![CDATA[<p>参照<a href="https://www.linuxidc.com/Linux/2016-08/134110.htm" target="_blank" rel="noopener">Linux中Nginx安装与配置详解</a><br>但是出了点小问题，按照步骤往下直到 配置nginx的时候出现了找不到openssl的错误（我的是阿里云的服务器）。<br>然后自己去下了一个openssl的包去安装死活不行。 最后执行<code>yum -y install openssl openssl-devel</code>就可以了。。<br>其次该博文中提示需要在某个文件中加入一句话，并重启防火墙，但是这样做我之前安装的tomcat也访问不了了，所以只要把防火墙关了就可以暂时解决了。</p>
<p>注：博文中的方法是为了开启防火墙，并且只运行80端口访问。我的服务器的防火墙不是iptables，而是firewall，所以不应该用博文中的方法，应该为firewall添加允许80端口访问<br>详见<a href="http://blog.csdn.net/codepen/article/details/52738906" target="_blank" rel="noopener">centos7 Firewall防火墙开启80端口</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>安装与部署</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云部署javaweb项目</title>
    <url>/IT-stuff/linux/%E9%98%BF%E9%87%8C%E4%BA%91%E9%83%A8%E7%BD%B2javaweb%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<h3 id="配置java运行环境"><a href="#配置java运行环境" class="headerlink" title="配置java运行环境"></a>配置java运行环境</h3><h4 id="查看服务器系统版本"><a href="#查看服务器系统版本" class="headerlink" title="查看服务器系统版本"></a>查看服务器系统版本</h4><p><code>#getconf LONG_BIT</code></p>
<blockquote>
<p>64</p>
</blockquote>
<p>一般都是64了吧</p>
<h4 id="下载jdk"><a href="#下载jdk" class="headerlink" title="下载jdk"></a>下载jdk</h4><p>下载地址：<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">jdk下载地址</a><br>版本：jdk-8u151-linux-x64.tar.gz（我下的版本）<br>使用WinSCP将jdk移动到linux中（有很多类似的软件，使用起来很简单）<br>在使用WinSCP连接时，输入公网IP，并输入帐号密码即可，不需要改变端口号。<br>至于将jdk放在哪个目录，可以看这篇<a href="http://blog.csdn.net/ubuntu64fan/article/details/8289335" target="_blank" rel="noopener">Linux下JDK到底应该安装在哪儿</a><br>我选择放在/usr/local/java下</p>
<h4 id="查看当前系统是否有JDK"><a href="#查看当前系统是否有JDK" class="headerlink" title="查看当前系统是否有JDK"></a>查看当前系统是否有JDK</h4><p>使用rpm -qa |grep jdk<br>如果有就移除<br><a id="more"></a></p>
<h4 id="解压JDK"><a href="#解压JDK" class="headerlink" title="解压JDK"></a>解压JDK</h4><p><code>tar zxvf 文件名</code></p>
<h4 id="配置java环境"><a href="#配置java环境" class="headerlink" title="配置java环境"></a>配置java环境</h4><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#vim /etc/profile       </span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/local/java/jdk1.8.0_91   </span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">CLASSPATH</span>=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar   </span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=<span class="variable">$PATH</span>:$JAVA_HOME/bin   </span><br><span class="line"><span class="builtin-name">export</span> JAVA_HOME CLASSPATH PATH</span><br></pre></td></tr></table></figure>
<p>按esc-&gt;输入“:wq”退出</p>
<h4 id="查看java版本，检验是否成功"><a href="#查看java版本，检验是否成功" class="headerlink" title="查看java版本，检验是否成功"></a>查看java版本，检验是否成功</h4><p>在此之前先运行source /etc/profile命令，或者重启机器<br>重启命令sudo shutdown -r now<br>不出意外就可以了</p>
<hr>
<p><strong>java运行环境到此配置成功，接下来安装、配置tomcat</strong></p>
<hr>
<h3 id="下载tomcat"><a href="#下载tomcat" class="headerlink" title="下载tomcat"></a>下载tomcat</h3><p><a href="http://tomcat.apache.org/" target="_blank" rel="noopener">tomcat官网</a><br>我下载的是<a href="https://tomcat.apache.org/download-90.cgi" target="_blank" rel="noopener">tomcat9</a></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/d6a2325465794dd88baf548bc89131a0.PNG" alt="tomcat官网截图" title="tomcat官网截图"></p>
<p>选择tar.gz即可，后面括号里的按了貌似没反应（我不知道干嘛用的=.=）<br>还是使用WinSCP，将tomcat放到/usr/local/tomcat下</p>
<h3 id="解压tomcat"><a href="#解压tomcat" class="headerlink" title="解压tomcat"></a>解压tomcat</h3><p><code>tar -zxvf apache-tomcat-9.0.2.tar.gz</code><br>之后就是跟在windows系统下一样部署web项目，具体我也在摸索…..=.=</p>
<h3 id="配置完成后依然无法访问"><a href="#配置完成后依然无法访问" class="headerlink" title="配置完成后依然无法访问"></a>配置完成后依然无法访问</h3><p>如果无法通过8080访问，其他原因暂且不说（如防火墙，8080被占用），有一个阿里云的问题，需要配置安全组</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/4b0bf9c8aa524d1f8bcf85b5506d1433.PNG" alt="阿里云安全组" title="阿里云安全组"></p>
<p>如上图端口并没有配置8080，而tomcat默认端口号是8080，所以无法访问到，点击“添加安全组规则”添加即可。</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/8ae0fd824bc0431499b37a9fd4e7c0f3.PNG" alt="添加安全组" title="添加安全组"></p>
<p>如上图所示，添加成功。<br>手太快了，无法对比，之前无法访问的网页忘记截图了，但是我截了访问成功的图片。这里可以看到可以通过公网ip访问了。</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/898416f96b5643de87e73a3579aba6ec.PNG" alt="成功访问tomcat主页" title="成功访问tomcat主页"></p>
<h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><p><img src="http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/attach/52685/cn_zh/1496913639790/%E6%96%B0%E6%89%8B%E5%85%A5%E9%97%A8_%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%89%88_V2.pdf" alt="阿里云的教程"><br>这篇讲的很详细</p>
<h3 id="使用navicat连接"><a href="#使用navicat连接" class="headerlink" title="使用navicat连接"></a>使用navicat连接</h3><p><a href="https://help.aliyun.com/knowledge_detail/37855.html?spm=5176.11065259.1996646101.searchclickresult.7755a429KB6TbE" target="_blank" rel="noopener">Navicat for SQL Server 连接 RDS For SQL Server 数据库</a><br>这篇用sql server连得，但是也差不多。<br>只要复制mysql的外网连接地址去连就行了。</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/8db46e497396457e970db85ff37b756e.PNG" alt="navicat链接成功" title="navicat链接成功"></p>
<h3 id="在阿里云控制台登录数据库"><a href="#在阿里云控制台登录数据库" class="headerlink" title="在阿里云控制台登录数据库"></a>在阿里云控制台登录数据库</h3><p><a href="https://help.aliyun.com/document_detail/26138.html?spm=5176.7741843.2.4.m1jXSQ" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/26138.html?spm=5176.7741843.2.4.m1jXSQ</a><br>这篇文章讲的很简单。</p>
<!-- more -->]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>安装与部署</tag>
      </tags>
  </entry>
  <entry>
    <title>关于shiro的session总结</title>
    <url>/java/%E5%85%B3%E4%BA%8Eshiro%E7%9A%84session%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<ul>
<li>关闭浏览器，使用shiro记住我功能时，session失效，即重新打开浏览器需要重新登录。其实并没有失效，而是再次打开浏览器时，shiro又重新创建了一个session，所以导致之前的session与现在的session不一致。所以造成了session失效的假象.</li>
<li>使用浏览器，在地址栏输入访问的链接但是不按回车时（意思就是只在地址栏输入一个链接不访问网页），系统会产生一个session，但是马上（一秒不到）自动删掉。</li>
<li>使用RememberMe时，断开会话后会重新生成一个session，并且sessionId以及里面原来保存的值全都没有了，但是不需要重新登录（此适用于user级别的过滤链）。</li>
<li>使用RememberMe时，断开会话后会重新生成一个session，并且sessionId以及里面原来保存的值全都没有了，需要重新登录，但是由于是重新登录，本质上session中的值是没有的，实质上登陆一次后又在session中存入了所有属性。（此适用于authc级别的过滤链）</li>
<li>可以看到断开会话后会重新产生一个session，所以之前的session没用了，但是没有自动删掉。而由于user级别的用户不需要重新登录，session中的值全没了（比如用户个人信息：头像等），所以网页中呈现出来肯定很怪。可以创建一个filter将之前会话的值全部复制到当前session中，然后清除之前的session（由于authc级别是不需要复制的，所以只需要清除之前的session即可，但是因为保不准用户在操作时，session中会存入一些其他的值，所以最好也复制一遍）。<a id="more"></a></li>
</ul>
]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>在shiro的realm中注入service引发的惨案</title>
    <url>/java/%E5%9C%A8shiro%E7%9A%84realm%E4%B8%AD%E6%B3%A8%E5%85%A5service%E5%BC%95%E5%8F%91%E7%9A%84%E6%83%A8%E6%A1%88.html</url>
    <content><![CDATA[<p><strong><em>2019.01.24更新，由于发现了mybatis-plus插件已经实现了我要实现的功能，我自己写的肯定没别人研发的框架写得好，所以此方法已经废弃。</em></strong></p>
<hr>
<p>环境介绍一下：由于service层的代码重复太多，于是我想直接把一些重复的代码抽出来。<br>比如说这个方法，它可以实现根据条件查询，框架是mybatis。而注入的mapper是有spring提供的方法获取到的，问题就出在这里了。<br><figure class="highlight monkey"><table><tr><td class="code"><pre><span class="line">@SuppressWarnings(<span class="string">"unchecked"</span>)</span><br><span class="line">@Override</span><br><span class="line"><span class="keyword">public</span> List&lt;Record&gt; selectByExample(Example example) &#123;</span><br><span class="line">    <span class="class"><span class="keyword">Class</span>&lt;? <span class="keyword">extends</span> <span class="title">Object</span>&gt; <span class="title">mapperClazz</span> = <span class="title">mapper</span>.<span class="title">getClass</span>();</span></span><br><span class="line">    List&lt;Record&gt; res = <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="function"><span class="keyword">Method</span> <span class="title">method</span> =</span> mapperClazz.getDeclaredMethod(<span class="string">"selectByExample"</span>, example.getClass());</span><br><span class="line">        res = (List&lt;Record&gt;) <span class="function"><span class="keyword">method</span>.<span class="title">invoke</span>(</span>mapper, example);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalArgumentException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NoSuchMethodException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (SecurityException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>这个方法获取到了mapper的类型，即CLass对象。</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SuppressWarnings(<span class="meta-string">"unchecked"</span>)</span></span><br><span class="line"><span class="keyword">public</span> Class&lt;Mapper&gt; getMapperType() &#123;</span><br><span class="line">    ParameterizedType genericSuperclass = (ParameterizedType) <span class="keyword">this</span>.getClass().getGenericSuperclass();</span><br><span class="line">    <span class="keyword">return</span> (Class&lt;Mapper&gt;) genericSuperclass.getActualTypeArguments()[<span class="number">2</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这里使用了我自定义的SpringContextUtil类获取了mapper这个bean。注意我这里使用了构造器初始化，问题就出在这里。</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">public <span class="constructor">BaseServiceImpl()</span> &#123;</span><br><span class="line">    Class&lt;Mapper&gt; mapperType = get<span class="constructor">MapperType()</span>;</span><br><span class="line">    this.mapper = (Mapper) <span class="module-access"><span class="module"><span class="identifier">SpringContextUtil</span>.</span></span>get<span class="constructor">Bean(<span class="params">mapperType</span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>getBean()方法就不提供了，可以自行百度。主要是这个类SpringContextUtil。<br>要想使用spring提供的以java代码方法获取bean有5种方式（大致差不多，我是用了第四种）：实现ApplicationContextAware接口。只需要在实现类中定义private static ApplicationContext applicationContext;变量，再设置set方法，即可使用这个对象获取到bean。<br>到这里一切都没问题，可以当启动tomcat时，问题出来了。<br>它报了一个异常，必须初始化applicationContext。我纳闷了，我之前在Controller层注入service对象用的好好的，为什么在shiro的realm中注入就出事了？<br>我看了我的applicationContext.xml文件，我是这么写的：</p>
<figure class="highlight xl"><table><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:springContextConfig.xml"</span> /&gt;</span><br><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:spring/upms-dao.xml"</span> /&gt;</span><br><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:spring/upms-service.xml"</span> /&gt;</span><br><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:spring/upms-tx.xml"</span> /&gt;</span><br><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:shiro/shiro.xml"</span> /&gt;</span><br></pre></td></tr></table></figure>
<p>然后看了web.xml文件</p>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">&lt;context-<span class="built_in">param</span>&gt;</span><br><span class="line">	&lt;<span class="built_in">param</span>-name&gt;contextConfigLocation&lt;/<span class="built_in">param</span>-name&gt;</span><br><span class="line">	&lt;<span class="built_in">param</span>-<span class="built_in">value</span>&gt;classpath:applicationContext.xml&lt;/<span class="built_in">param</span>-<span class="built_in">value</span>&gt;</span><br><span class="line">&lt;/context-<span class="built_in">param</span>&gt;</span><br></pre></td></tr></table></figure>
<p>好像稍微有点头绪了<br>我在初始化spring上下文的时候，直接初始化了shiro.xml，这没问题。但是在applicationContext.xml中第一行的<code>&lt;import resource=&quot;classpath:springContextConfig.xml&quot; /&gt;</code>里面注册了SPringContextUtil这个工具类。（这个工具类不会一下子注册进去成为bean，它会在spring上下文初始化完毕之后才行）。<br>所以我需要在realm中注册AdminService——-&gt;AdminService继承了BaseServiceImpl——-&gt;BaseServiceImpl在构造器中使用了applicationContext.getBean()。这个步骤在spring上下文初始化还没完成的时候执行，但是你要想使用applicationContext，spring上下文必须初始化完毕，所以冲突了。<br>那我就想直接把shiro.xml托到DispatcherServlet里面初始化就完事了。因为在DispatcherServlet加载的配置文件中，我扫描了Controller层，而在Controller层使用是没问题的（我之前还不懂的时候就这么做的，没报异常）。然后我这么做了，但是又报异常了——-&gt;No Bean Named “shiroFilter”。这不坑爹吗？<br>我先缕一缕，<code>&lt;context-param&gt;</code>最先执行，其次是<code>&lt;filter&gt;</code>，最后是<code>&lt;servlet&gt;</code>（不一定完全正确，但是可以这么理解）。所以说shiro.xml放在DispatcherServlet中不行！因为filter比servlet先执行，它必须要shiro.xml中的bean。所以怎么办？我必须把shiro.xml放在context-param和filter的中间。这根本不可能（起码我做不到）。<br>后来我想到了懒加载（我印象中有听说过这个概念），我一百度还真有。那我应该把懒加载放在哪个bean上呢？我决定放在AdminService上。但是没用，因为Realm这个类不能懒加载，因为它马上就被初始化了。而realm中注入了AdminService，所以即使是懒加载也没屌用。<br>最后终于解决了。<br>既然你要初始化bean，要用构造器，我就把调用applicationContext的代码不放在构造器里。我写了一个initMapper方法。</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void init<span class="constructor">Mapper()</span> &#123;</span><br><span class="line">    Class&lt;Mapper&gt; mapperType = get<span class="constructor">MapperType()</span>;</span><br><span class="line">    this.mapper = (Mapper) <span class="module-access"><span class="module"><span class="identifier">SpringContextUtil</span>.</span></span>get<span class="constructor">Bean(<span class="params">mapperType</span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意@Override注解。<br>那么现在问题就是，怎么调用这个方法。<br>经过度娘之后，终于解决了。贴上类，我就不解释了。附上链接<a href="https://www.cnblogs.com/rollenholt/p/3612440.html" target="_blank" rel="noopener">https://www.cnblogs.com/rollenholt/p/3612440.html</a></p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">public <span class="keyword">class</span> MapperInit implements ApplicationListener&lt;ContextRefreshedEvent&gt; &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void on<span class="constructor">ApplicationEvent(ContextRefreshedEvent <span class="params">event</span>)</span> &#123;</span><br><span class="line">        <span class="comment">//下面的方法执行就行了，这个方法会执行两次，我们不需要这么做</span></span><br><span class="line">    &#125;</span><br><span class="line">       </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 需要执行的逻辑代码，当spring容器初始化完成后就会执行该方法。</span></span><br><span class="line"><span class="comment">     * 由于加了@PostConstruct注解所以这个方法跟上面的onApplicationEvent一样。</span></span><br><span class="line"><span class="comment">     * 但是这个方法只会在root application context环境中执行，不会在 projectName-servlet context中执行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    @<span class="constructor">SuppressWarnings(<span class="string">"rawtypes"</span>)</span></span><br><span class="line">    @PostConstruct</span><br><span class="line">    <span class="keyword">private</span> void init<span class="constructor">Mapper()</span> &#123;</span><br><span class="line">        Map&lt;String, BaseService&gt; beansOfType = <span class="module-access"><span class="module"><span class="identifier">SpringContextUtil</span>.</span></span>get<span class="constructor">ApplicationContext()</span>.get<span class="constructor">BeansOfType(BaseService.<span class="params">class</span>)</span>;</span><br><span class="line">        for(Map.Entry&lt;String, BaseService&gt; entrySet : beansOfType.entry<span class="constructor">Set()</span>) &#123;</span><br><span class="line">            BaseService service = entrySet.get<span class="constructor">Value()</span>;</span><br><span class="line">            service.init<span class="constructor">Mapper()</span>;</span><br><span class="line">            <span class="module-access"><span class="module"><span class="identifier">System</span>.</span></span>out.println(service);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>vo,dto,do,po的理解</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/vo-dto-do-po%E7%9A%84%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<ul>
<li>po:持久化对象，没啥好说的，数据库里一张表对应一个po。</li>
<li>vo:视图对象，是指某个指定页面的所有数据，注意是<font color="red">所有数据</font>。</li>
<li>dto:跟vo差不多，甚至可以代替vo，但是二者理论上是不一样的。vo用于数据显示，dto用于数据传输(展示层与业务层之间)。如性别在数据库里用01表示，在传输时dto直接定义Integer，但是在展示时总不能展示0和1，并且男女也有不同称谓(帅哥美女，先生女士等等)，所以需要另建一个vo用于处理这些逻辑。但是如果没有特殊需求，其实vo就是dto，dto是将业务层数据传输出去，在视图层展示时将以数字表示的性别转换为文字。如果客户端没有要求，那么不传输出数字，直接写上男女也没问题，如果确是有要求，在业务层就不能传出文字，而只能传出数字了。</li>
<li>do:无法里给<a id="more"></a>
以上出自:<a href="https://www.cnblogs.com/qixuejia/p/4390086.html" target="_blank" rel="noopener">博客园博主</a><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/e3592ba9de284f128b9183a2434ae018.jpg" alt="领域模型" title="领域模型"><!-- more --></li>
</ul>
]]></content>
      <categories>
        <category>java</category>
        <category>javaee</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>js一些常用的属性和函数</title>
    <url>/IT-stuff/%E5%89%8D%E7%AB%AF/js%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84%E5%B1%9E%E6%80%A7%E5%92%8C%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<p>由于老是忘记js和jq的函数，这里记一下。</p>
<h3 id="javascript"><a href="#javascript" class="headerlink" title="javascript"></a>javascript</h3><ol>
<li>string.replace(//, “”)<br>第一个参数是正则，表达式在//两个之间写，第二个参数是需要替换成什么。</li>
<li>string.substr()<br>用法和一样</li>
<li>new Date(date).toLocaleString()<br>将毫秒数转成本地的时间，如中国：2017/9/4 下午9:42:41</li>
<li>(a/b).toFixed(2)<br>相除保留两位小数，如700/100等于7.00。要不然就是只等于7（java里整数相除只得到整数部分）<a id="more"></a>
</li>
</ol>
<h3 id="Jquery"><a href="#Jquery" class="headerlink" title="Jquery"></a>Jquery</h3><ol>
<li>$(“#id”).attr()<br>为一个节点添加属性，一个属性:attr(“name”, “file”)。多个属性:attr({“name”:”file”, “type”,”button”})</li>
<li>$(“#id”).remove()<br>移除节点</li>
<li>$(“#id”).removeAttr(“”)<br>移除一个节点的属性</li>
<li>$(“#id”).children(“:first”)<br>一个节点的子节点</li>
<li>$(“#id”).offset()<br>一个节点离网页顶部的top值和left值，不是离浏览器的top值和left值（以整个网页的高度做计算）。<br>$(“#id”).offset().top代表节点的top值，left值同理。</li>
<li>$(“body,html”).animate({<br> scrollTop:scroll_offset.top //让body的scrollTop等于pos的top，就实现了滚动<br>},0);<br>滚动监听</li>
<li>window.location.search<br>获得当前url，如：<a href="http://localhost:8080/contextpath/index.html" target="_blank" rel="noopener">http://localhost:8080/contextpath/index.html</a></li>
<li>$.parseJSON(string)<br>将一个字符串转为json对象，但是不要在用ajax的时候，data:$.parseJSON(string)这样用，我搞了2个小时，没查出错，原来是这里错了。<br>string json = ‘{“comment”:”abcdrf”,”name”:”xiaowang”}’;<br>我在用$.ajax()的时候，把data赋值$.parseJSON(json)，出错了！！！data:json。后台接受的很成功。<br>但是后台传来一个json字符串，用这个方法转成json对象，可以直接通过.取值挺方便的。</li>
<li>new FormData().append()<br>使用 FormData对象获取到form中的数据后，可以继续append自己想要添加的数据。<!-- more --></li>
</ol>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>front-end</category>
      </categories>
      <tags>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>再探String</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%E5%86%8D%E6%8E%A2String.html</url>
    <content><![CDATA[<p>估计是最终版了。</p>
<figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line"><span class="keyword">String</span> s1 = <span class="keyword">new</span> <span class="keyword">String</span>(<span class="string">"张三"</span>);</span><br><span class="line"><span class="keyword">String</span> s2 = <span class="keyword">new</span> <span class="keyword">String</span>(<span class="string">"张三"</span>);</span><br><span class="line"><span class="keyword">String</span> s3 = <span class="string">"张三"</span>;</span><br><span class="line"><span class="keyword">String</span> s4 = <span class="string">"张三"</span>;</span><br></pre></td></tr></table></figure>
<p>我们都知道<code>String s3 = &quot;张三&quot;;</code>这句代码的”张三”字符串位 于常量池中，而s3==s4这种明显就是小儿科，true！<br>我们来解析一下<code>String s3 = &quot;张三&quot;;</code>这句代码。</p>
<ol>
<li>首先s3进入栈中，”张三”进入常量池中，”张三”返回一个地址，s3拿到这个地址，所以我们就可以拿着这个地址做一些事情。这应该都很熟悉了，那么问题来了：“String s1 = new String(“张三”);”这句代码是怎么执行的？<a id="more"></a></li>
<li><p>首先我们要知道一个定义：成员变量和局部变量。<br>s3是一个局部变量，当然是相对来说，比如说上面的4句代码都位于一个方法中，那么s3是一个局部变量。</p>
</li>
<li><p>接下来我们解析<code>String s1 = new String(&quot;张三&quot;);</code>这句代码。<br>s1还是一个局部变量所以进入栈中，new String会在堆中创建一个String实例对象（OK很简单），那么这个<br>“张三”怎么办？注意了！String对象是如何存储字符串的？实际上就是一个char数组（同时也解释了一个中文可以用一个char来存储），所以”张三”在String中实际上是这样的：value[0]=’张’，value[1]=”三”。<br>这个value是一个char数组，而它优势String的成员变量也就是说，value这个变量不会进入栈，而是跟着String在堆内存中（实际上不是，我先打个比喻）。<br>String对象的构造器还没完！它还会初始化一个hash码，这个是String自己初始化的，不用你来控制，所以hash码这个变量也是跟着String进入堆中。</p>
</li>
</ol>
<p>现在我们来缕一缕———-&gt;String在堆中开辟一个空间，这个空间里有value数组，hash变量（其他的我就不说了，我上面也说过了，实际上value并不在String中）。</p>
<p>知道上面的一切之后，就可以分析了。</p>
<p>是不是”张三”这种形式的写法，字符串就进入了常量池？比如：<code>String s1 = &quot;张三&quot;;String s1 = new String(&quot;张三&quot;);StringBuilder s3 = new StringBuilder(&quot;张三&quot;);StringBuffer s4 = new StringBuffer(&quot;张三&quot;);</code></p>
<p>其实我们有办法做到，观察是否进入了。<br>但是之前先考虑，<code>String s1 = new String(&quot;张三&quot;);</code>和<code>String s3 = &quot;张三&quot;;</code>这两个”张三”是同一个字符串吗？<br>其实看我上面的某些描述，可能也推测出来了，这两个”张三”就是同一个（指同一个地址）。下面讲如何做到观察是否为同一个。</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">String <span class="built_in">s1</span> = new String(<span class="string">"abc"</span>)<span class="comment">;</span></span><br><span class="line">String <span class="built_in">s2</span> = new String(<span class="string">"abc"</span>)<span class="comment">;</span></span><br><span class="line">Class&lt;?&gt; clazz1 = <span class="built_in">s1</span>.getClass()<span class="comment">;</span></span><br><span class="line">Class&lt;?&gt; clazz2 = <span class="built_in">s2</span>.getClass()<span class="comment">;</span></span><br><span class="line">Field declaredField1 = clazz1.getDeclaredField(<span class="string">"value"</span>)<span class="comment">;</span></span><br><span class="line">Field declaredField2 = clazz2.getDeclaredField(<span class="string">"value"</span>)<span class="comment">;</span></span><br><span class="line">declaredField1.setAccessible(true)<span class="comment">;</span></span><br><span class="line">declaredField2.setAccessible(true)<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField1.get(<span class="built_in">s1</span>) == declaredField2.get(<span class="built_in">s2</span>))<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField1.get(<span class="built_in">s1</span>))<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField2.get(<span class="built_in">s2</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>上面的代码结果如何，显然已经解释的很清楚了。</p>
<p>解</p>
<p>首先”张三”实际上是一个char数组，而它是String的成员变量，所以它硬该在堆内存中，但是事实是在常量池。<br>常量池：看名字就知道是存常量的。<br>在网络上的熏陶，我们知道String name = “李四”；”李四”在常量池，所以name是一个常量（无语ing，别被带偏了，我在今天之前也是这么认为的）。<br>被final修饰的才是常量。现在你们大概猜出什么了。去看String的源码吧，看看value这个成员变量的前面是用什么修饰的！！！自己去看才会记忆深刻，我就不贴源码了。</p>
<p>还没完<br>是不是”张三”这种形式的写法，字符串就进入了常量池？<br>我之前问了这个问题。答：不是。<br>StringBuilder name1 = new StringBuilder(“张三”);<br>String name2 = “张三”;<br>这时候又不一样了。<br>这两个”张三”不是同一个（也就是说地址不一样）。话不多说上代码。</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">String <span class="built_in">s1</span> = new String(<span class="string">"abc"</span>)<span class="comment">;</span></span><br><span class="line">StringBuilder <span class="built_in">s2</span> = new StringBuilder(<span class="string">"abc"</span>)<span class="comment">;</span></span><br><span class="line">Class&lt;?&gt; clazz1 = <span class="built_in">s1</span>.getClass()<span class="comment">;</span></span><br><span class="line">Class&lt;?&gt; clazz2 = <span class="built_in">s2</span>.getClass()<span class="comment">;</span></span><br><span class="line">clazz2 = clazz2.getSuperclass()<span class="comment">;</span></span><br><span class="line">Field declaredField1 = clazz1.getDeclaredField(<span class="string">"value"</span>)<span class="comment">;</span></span><br><span class="line">Field declaredField2 = clazz2.getDeclaredField(<span class="string">"value"</span>)<span class="comment">;</span></span><br><span class="line">declaredField1.setAccessible(true)<span class="comment">;</span></span><br><span class="line">declaredField2.setAccessible(true)<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField1.get(<span class="built_in">s1</span>) == declaredField2.get(<span class="built_in">s2</span>))<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField1.get(<span class="built_in">s1</span>))<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField2.get(<span class="built_in">s2</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>上面的结果有没有出乎意料？地址打印出来都不一样。你们也猜到了什么吧。<br>去看StringBuilder的源码吧。StringBuffer同理。<br>注：StringBuilder中没有value，但是它的父类AbstractStringBuilder有，去看看value前面是什么修饰的。</p>
<hr>
<p>2018.9.6更新<br>java1.7之后貌似有很大变化。上面的理论可能不太适用</p>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>解决li标签使用inline-block时产生的4px</title>
    <url>/IT-stuff/%E5%89%8D%E7%AB%AF/%E8%A7%A3%E5%86%B3li%E6%A0%87%E7%AD%BE%E4%BD%BF%E7%94%A8inline-block%E6%97%B6%E4%BA%A7%E7%94%9F%E7%9A%844px.html</url>
    <content><![CDATA[<p>完美兼容的方法<br>父元素（比如ul）设置font-size:0;<br>使用letter-spacing:-Npx;<br>li标签重新设置字体大小<br>原文出处：解决inline-block出现的间隙<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>IT-stuff</category>
        <category>front-end</category>
      </categories>
      <tags>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>理解接口和抽象类</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%E7%90%86%E8%A7%A3%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8A%BD%E8%B1%A1%E7%B1%BB.html</url>
    <content><![CDATA[<p>这篇只是自学java基础时的个人理解而已。</p>
<p>初学abstract和interface时会感到很疑惑，明明这两个没有具体实现的方法，为什么还要继承或者实现他们呢？明明可以自己在自己编写的类中写一个方法，而不去继承或实现。现在看来大概是因为一种规范，也是为了让别人更好的看懂，如果自己写方法，自己去用，当然不需要多此一举。如果编写一个Run的接口，让别人去用，别人一看就知道这个接口是用来让某种东西跑起来。</p>
<ul>
<li>抽象类<br>它只有声明，而没有具体的实现。抽象方法必须加上abstract关键字。如果一个类含有抽象方法即这个类是抽象类，但是抽象类也可以不含有抽象方法，不过这样定义一个抽象类就毫无意义。<br>抽象类中也可以拥有成员变量和普通的成员变量。<br>抽象类与普通类主要有三种区别：</li>
</ul>
<ol>
<li>抽象方法必须为public或者protected，缺省情况下为public</li>
<li>抽象类不能被用来创建对象</li>
<li>如果继承一个抽象类则必须实现它的抽象方法，如果不实现则必须将这个方法也加上abstract。<a id="more"></a></li>
</ol>
<ul>
<li>接口<br>在软件工程中，接口泛指供别人调用的方法或者函数。它是对行为的抽象。<br>接口可以含有变量和方法，但是它们都会被自动加上public static final关键字，如果没有加上，系统会自动添加。<br>一个类可以实现多个接口。<br>接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法；</li>
<li>区别<br>抽象类是“是不是”的关系，定义一个animal类，再定义一个Dog类，animals extends Dog，是一种“是不是”的关系，即狗是不是动物。<br>接口是“有没有”的关系，定义一个interface Eat，实现它，Dog implements Eat，即Dog类实现Eat接口，狗有没有吃这种功能。<br>具体内容引自 <a href="http://www.cnblogs.com/dolphin0520/p/3811437.html" target="_blank" rel="noopener">http://www.cnblogs.com/dolphin0520/p/3811437.html</a><br>这篇只是为了帮助自己理解二者的区别。</li>
</ul>
<!-- more -->]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>自动装箱和自动拆箱功能</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%E8%87%AA%E5%8A%A8%E8%A3%85%E7%AE%B1%E5%92%8C%E8%87%AA%E5%8A%A8%E6%8B%86%E7%AE%B1%E5%8A%9F%E8%83%BD.html</url>
    <content><![CDATA[<p><code>Integer a = new Integer(1000);</code><br>在jdk5.0之后走了自动装箱功能<br><code>Integer a = 1000;</code>即与上面的等式相同。<br>在编译阶段，编译器帮助我们改进代码，将<code>Integer a = new Integer(1000);</code><br>自动改为<code>Integer a = 1000;</code></p>
<p><code>int c = new Integer(1500);</code>进行自动拆箱。<br>编译器将上述代码自动改为<code>int c = new Integer(1500).intValue();</code></p>
<p>但是在–128到127之间的数依然会当做基本数据类型处理。<br><figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">Integer <span class="built_in">a1</span>=<span class="number">100</span><span class="comment">;</span></span><br><span class="line">Integer <span class="built_in">a2</span>=<span class="number">100</span><span class="comment">;</span></span><br></pre></td></tr></table></figure></p>
<p>a1==a2返回的照理说应该是false，但是在处理时，它只是基本数据类型，所以返回true。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>StringBuilder和StringBuffer的区别</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/StringBuilder%E5%92%8CStringBuffer%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<p>StringBuilder线程不安全，效率高。<br>DtringBuffer线程安全，效率低。<br>都是可变字符序列。<br><code>StringBuilder s = new StringBuilder()</code>字符数组长度初始化为16<br><code>StringBuilder s = new StringBuilder(32)</code>字符数组长度初始化为32<br>其中的append方法最终返回this，可以实现方法链，<code>s.append(&quot;ab&quot;).append(&quot;cd&quot;)</code></p>
<figure class="highlight isbl"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">StringBuilder</span>(<span class="variable">String</span> <span class="variable">s</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="function"><span class="title">super</span>(<span class="variable">s</span>.length() + <span class="number">16</span>);</span></span><br><span class="line"><span class="function">	<span class="title">append</span>(<span class="variable">s</span>);</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
<p>StringBuilder类的其中一个构造器，建立一个初始化内容的长度加上16的数组<br>如果传入的一个数组比初始化的大，则会进行数组扩容。<br>新容量 = 旧容量 * 2+2<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>区分“==”和equals</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/%E5%8C%BA%E5%88%86%E7%AD%89%E5%8F%B7%E2%80%9C==%E2%80%9D%E5%92%8Cequals.html</url>
    <content><![CDATA[<p>比如</p>
<figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line"><span class="keyword">String</span> s1=<span class="keyword">new</span> <span class="keyword">String</span>(<span class="string">"abc"</span>);</span><br><span class="line"><span class="keyword">String</span> s2=<span class="keyword">new</span> <span class="keyword">String</span>(<span class="string">"abc"</span>);</span><br></pre></td></tr></table></figure>
<p>s1==s2返回的是faluse，因为s1指向的是String类的value数组，而s2指向的是另一个String类的value数组，对象不同。<br>而s1.equals(s2)返回true。因为eauals比较的是内容，即value数组。<br><a id="more"></a><br><figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">String </span><span class="built_in">s3</span>=<span class="string">"abc"</span><span class="comment">;</span></span><br><span class="line"><span class="keyword">String </span><span class="built_in">s4</span>=<span class="string">"abc"</span><span class="comment">;</span></span><br></pre></td></tr></table></figure></p>
<p>这个初始化的，s3==s4和s3.equals(s4)返回的都是true，因为s3和s4指向的都是方法区的常量池</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>new String(abc)与String s=abc的区别</title>
    <url>/%C2%B7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java/new-String-abc-%E4%B8%8EString-s-abc%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<p>String类是不可变字符序列。<br>比如<code>String s=&quot;abc&quot;;</code><br>原理是将字符串”abc”存入value数组，而value数组是final，所以value数组是不能改变的，但是String产生的s对象，s这个指针是可变的。<br>局部变量s在栈中产生，指向在堆(方法区的常量池)中的”abc”,然后如果想在字符串后面添加字符，则会诞生一个新的字符串(比如”abcd”)，然后将s指针指向”abcd”。<br>而<code>String s=new String(&quot;abc&quot;);</code><br>同理s产生在栈中，字符串”abc”产生在堆(方法区的常量池)中，但是s不直接指向”abc”。<br>new String在堆中产生一个对象，由常量池中的”abc”对它进行初始化，而s对象指向的是new String这个对象。<br>s指向String，String指向”abc”。<br><a id="more"></a></p>
<p>总结:new String()比String多创建了一个对象，浪费内存。<br>用<code>String s=new String()</code>创建了在栈中的s对象以及String类对象。<br>用String s仅仅创建了s对象</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
</search>
