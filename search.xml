<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>tips</title>
    <url>/tips.html</url>
    <content><![CDATA[<blockquote>
<p><strong>提示：先按住ctrl再点击超链接可以在新窗口打开</strong></p>
</blockquote>
<a id="more"></a>]]></content>
  </entry>
  <entry>
    <title>一些快捷键</title>
    <url>/%E4%B8%80%E4%BA%9B%E5%BF%AB%E6%8D%B7%E9%94%AE.html</url>
    <content><![CDATA[<p>最近老是用到win10的快捷键，但是用过之后过几天就忘了，所以记录下。<br>还有有谁知道怎么用快捷键打开高级系统设置，设置环境变量老是要打开这个，烦得很。</p>
<h1 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h1><p>按 V 进入 Visual Mode，然后可以上下左右选择行数。按 y 复制，d 剪切，p 粘贴。</p>
<h1 id="win10"><a href="#win10" class="headerlink" title="win10"></a>win10</h1><ol>
<li>win+E 打开我的电脑</li>
<li>win+R 打开运行</li>
<li>win+L 锁屏</li>
<li>fn+ESC 打开/关闭功能键。f5是刷新键，但是有时候发现按f5无效，其实是因为功能键打开了，只需要按fn+ESC关闭就可。</li>
<li>四指在触摸板向左/向右滑动，切换桌面。</li>
<li>三指向上滑动将当前的任务以小窗口显示在桌面，三指向下滑动隐藏。</li>
<li>win+Tab类似6</li>
<li>win+d最小化所有打开的窗口</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
      </categories>
      <tags>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title>SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning</title>
    <url>/%E8%AE%BA%E6%96%87/48%E3%80%81SQLNet%EF%BC%9AGenerating%20Structured%20Queries%20From%20Natural%20Language%20Without%20Reinforcement%20Learning.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;主要描述一下该论文 column attention。</p>
<h1 id="Sequence-to-set"><a href="#Sequence-to-set" class="headerlink" title="Sequence-to-set"></a>Sequence-to-set</h1><p>&emsp;&emsp;直观地说，<strong>where 子句</strong>中出现的列名是<strong>所有列名</strong>的子集。因此，我们可以仅仅预测子集中的列名，而不是生成列名序列（<strong>博主注</strong>：他的意思可能是，不要将 sql 语句中 “select Column A, Column B, Column C…” 的 “Column A, Column B, Column C…” 当做<strong>生成序列</strong>的任务，而是将其当做 slot filling）。我们把这个想法称为 <em>sequence-to-set</em> 的预测。<br>&emsp;&emsp;尤其是我们计算 <script type="math/tex">P_{wherecol}(col|Q)</script>，其中 col 是列名，q 是自然语言问题。为此，将计算 <script type="math/tex">P_{wherecol}(col|Q)</script> 表达为</p>
<script type="math/tex; mode=display">
    P_{wherecol}(col|Q) = \sigma(u^T_c E_{col} + u^T_q E_Q)</script><p>&emsp;&emsp;其中 <script type="math/tex">\sigma</script> 是 sigmoid 函数，<script type="math/tex">E_{col}</script> 和 <script type="math/tex">E_Q</script> 分别是 column name 和自然语言问题的嵌入， <script type="math/tex">u_c</script> 和 <script type="math/tex">u_q</script> 是两个可训练的列向量。。。（<em>后面还有一大段话省略了，主要看 column attention</em>）</p>
<h1 id="Column-attention"><a href="#Column-attention" class="headerlink" title="Column attention"></a>Column attention</h1><p>&emsp;&emsp;上一节的 <script type="math/tex">P_{wherecol}(col|Q)</script> 的计算公式在使用 <script type="math/tex">E_Q</script> 上有一个问题，因为只计算了自然语言语句的隐藏状态，它也许不能记住在预测<strong>特定</strong>列名时有用的<strong>特定</strong>信息。。。（后面举了个例子）<br>&emsp;&emsp;为了融入这一直觉，我们设计了 column attention 机制去计算 <script type="math/tex">E_{Q|col}</script> 来代替 <script type="math/tex">E_Q</script>。假定 <script type="math/tex">H_Q</script> 是 dxL 的矩阵，L 代表自然语言问题的长度。<script type="math/tex">H_Q</script> 的第 i 列代表问题中对应的第 i 个 token 的 LSTM 的隐藏状态输出。<br>&emsp;&emsp;我们对<strong>问题中的每一个 token</strong> 都计算 attention weight w，w 是 L 维的列向量（博主注：此处应该指的是在计算一个 token 的情况下，论文中未详细指明，仅为猜测）。计算公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    w & = softmax(v) \\
    v_i & = (E_{col})^T W H^i_Q \qquad \forall i \in {1, \dots, L} \\ 
\end{aligned}</script><p>&emsp;&emsp;<script type="math/tex">v_i</script> 表示 v 的第 i 维，<script type="math/tex">H^i_Q</script> 表示 <script type="math/tex">H_Q</script> 的第 i 列，W 是一个 dxd 大小的可训练矩阵。<br>&emsp;&emsp;在 attention weights w 被计算出来之后，我们可以基于 w 计算 <script type="math/tex">E_{Q|col}</script>，作为每个 token 的隐藏输出的加权和，此处的隐藏状态指 LSTM 上的（这句话极其的绕，懒得解释了。只需要注意一点，这句话是在计算一个单词的情况下，而并非计算整个 question）。</p>
<script type="math/tex; mode=display">
E_{Q|col} = H_Q w</script><p>&emsp;&emsp;在 <strong>Sequence-to-set</strong> 中的表达式，我们可以将 <script type="math/tex">E_Q</script> 替换为 <script type="math/tex">E_{Q|col}</script>，以获得 column attention model。</p>
<script type="math/tex; mode=display">
    P_{wherecol}(col|Q) = \sigma(u^T_c E_{col} + u^T_q E_{Q|col})</script><p>&emsp;&emsp;（下面的略）</p>
<h1 id="博主注"><a href="#博主注" class="headerlink" title="博主注"></a>博主注</h1><p>&emsp;&emsp;论文中似乎没有很明确地说明 column 的 embedding 是如何训练的。但是这一环很重要，因为这篇论文是关于 wikiSQL 数据集，训练 seq2sql 的比较前的论文，也就是说后面的论文有一些就是使用这篇论文的方法。所以这一点没搞懂，导致也看不懂其他的论文。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>【读书笔记】：《自然语言处理综论》（第二版）</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%90%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%91%EF%BC%9A%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%BB%BC%E8%AE%BA%E3%80%8B%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88%EF%BC%89.html</url>
    <content><![CDATA[<h1 id="正则表达式与自动机"><a href="#正则表达式与自动机" class="headerlink" title="正则表达式与自动机"></a>正则表达式与自动机</h1><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p>&emsp;&emsp;正则表达式首先由 Kleene（1956）研制的。</p>
<h1 id="N元语法"><a href="#N元语法" class="headerlink" title="N元语法"></a>N元语法</h1><p>&emsp;&emsp;猜测哪一个单词最可能跟在下面的句子片段后面。“Please turn your homework…”。其中最可能的单词是“in”，或者是“over”，但不可能是“the”。<br>&emsp;&emsp;我们把这种<strong>猜测单词</strong>的问题采用诸如 N 元语法模型（N-gram model）这样的概率模型来形式化地加以描述。N 元语法模型根据前面出现的 N - 1 个单词猜测下面一个单词。一个 N 元语法是包含 N 个单词的序列：2 元语法一般称为 bigram，如 please turn，turn your，your homework；3 元语法一般称为 trigram，如 please turn your，turn your homework。这种单词序列的概率模型又称为<strong>语言模型</strong>（Language Models，ML）。<br>&emsp;&emsp;</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
  </entry>
  <entry>
    <title>论文笔记：TypeSQL: Knowledge-based Type-Aware Neural Text-to-SQL Generation</title>
    <url>/%E8%AE%BA%E6%96%87/38%E3%80%81TypeSQL%EF%BC%9AKnowledge-based%20Type-Aware%20Neural%20Text-to-SQL%20Generation.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.gg363.site/pdf/1804.09769.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2018 年。</p><ul><li>所用数据集<ul><li>WikiSQL</li></ul></li></ul>
          </div>
<p>&emsp;&emsp;对 text-sql 任务提出了 TypeSQL 模型，将问题视为 slot filiing task，使用 type（详见下面内容） 信息以更好的理解输入中稀有实体和和数字。<br>&emsp;&emsp;对关系型数据库构建一个自然语言接口是一个重要且具有挑战的问题（(Li and Jagadish, 2014; Pasupat and Liang, 2015; Yin et al., 2016; Zhong et al., 2017; Yaghmazadeh et al., 2017; Xu et al., 2017; Wang et al., 2017a）。本论文使用 WikiSQL，它是 <strong>text-to-SQL</strong> 问题的一个巨大的<strong>基准数据集</strong>。对于该任务，具体来说是给定一个关于数据表的自然语言问题及其协议，系统需要生成与该问题对应的 SQL 查询。<br>&emsp;&emsp;本文基于之前的 state-of-the-art SQLNet（<a href="https://openreview.net/pdf?id=SkYibHlRb" target="_blank" rel="noopener">Xu et al., 2017: Sqlnet: Generating structured queries from natural language without reinforcement learning</a>），TYPESQL 使用一个 <strong>sketch-based</strong> 方法，并将此任务视为 slot filing 问题。<br>&emsp;&emsp;进一步，特定于一个数据库的情况下，自然语言问题通常会包含不常见的实体和数字。之前的一些工作 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.4408&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Agrawal and Srikant, 2003: Searching with numbers</a> 已经展示了这些词汇对许多下游任务起着重要作用，但是在预训练词嵌入模型中，大部分词汇缺乏准确的 embeddings。为了解决这一问题，无论单词来自知识图谱、数据库的列还是数字，TYPESQL 为每一个单词分配一个 type。例如，在图 1 中，我们将“mort drucker”作为 PERSON，对应于我们的知识图谱；将“spoofed title”，“artist”和“issue” 作为 COLUMN，因为它们是数据的列名；最后将 “88.5” 作为 FLOAT。结合这一发明，TYPESQL 进一步提高了 WiKiSQL 上的性能。<br>&emsp;&emsp;此外，先前大部分工作假定用户的查询包含准确的列名和实体，但是这是不切实际的。为了解决这一问题，……。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>&emsp;&emsp;类似 SQLNet，我们使用了 sketch-based 方法，并且将该任务视为 slot filling。首先预处理问题输入，识别 type。然后使用两层 bi-directional LSTMs 去 encode 问题的单词，encode 时分别利用了 type 和列名（数据库）。最后用 LSTMs 输出的隐藏状态预测 SQL sketch 中 slot 的值。</p>
<h2 id="Type-Recognition-for-Input-Preprocessing"><a href="#Type-Recognition-for-Input-Preprocessing" class="headerlink" title="Type Recognition for Input Preprocessing"></a>Type Recognition for Input Preprocessing</h2><p>&emsp;&emsp;<strong>首先将每个问题分为长度 2-6 的 n-gram 语法，然后在 table scheme 中使用它们进行搜索</strong>（这步很关键，但是我觉得用 n-gram 语法可能会错过一些列名吧），并且将问题中出现的任意列名打上 COLUMN 标签。其他类别做类似操作，转换如下所示（以下 type 均来自 Freebase）：</p>
<ol>
<li>问题中出现的列名 -&gt; COLUMN</li>
<li>问题中的数字和日期 -&gt; INTEGER, FLOAT, DATE, and YEAR</li>
<li>命名体 -&gt; PERSON, PLACE, COUNTRY, ORGANIZATION, and SPORT</li>
</ol>
<p>&emsp;&emsp;五种类别的命名体以及涵盖了数据集中的大部分实体，因此不再使用 Freebase 提供的其他实体类型。</p>
<h2 id="Input-Encoder"><a href="#Input-Encoder" class="headerlink" title="Input Encoder"></a>Input Encoder</h2><p>&emsp;&emsp;如图 1 所示，我们的 input encoder 由 bi-LSTM 组成，分别为：<script type="math/tex">\text{Bi-LSTM}^{QT}</script> 和 <script type="math/tex">\text{Bi-LSTM}^{COL}</script>。为了编码问题中的一对 word 和 type，<strong>我们将 word 和对应的 type 的嵌入拼接起来，然后将它们输入进 <script type="math/tex">\text{Bi-LSTM}^{QT}</script></strong>。最后分别输出隐藏状态 <script type="math/tex">\text{H}_{QT}</script> 和 <script type="math/tex">\text{H}_{COL}</script>。<br>&emsp;&emsp;为了<strong>编码列名</strong>，SQLNet 使用 Bi-LSTM 对每一个列名编码。我们首先平均具有 COLUMN 类型的单词的嵌入，然后只使用<strong>一个</strong> <script type="math/tex">\text{Bi-LSTM}^{COL}</script> 编码。这样的编码方法提高了 1.5% 的性能，并且使得时间减半。<em>我感觉这篇论文写得好乱，有点读不懂这部分</em>。可能需要看一下 SQLNet</p>
<h2 id="Slot-Filling-Model"><a href="#Slot-Filling-Model" class="headerlink" title="Slot-Filling Model"></a>Slot-Filling Model</h2><p>&emsp;&emsp;接下来，我们预测 SQL sketch 中 slots 的值。<br>&emsp;&emsp;文章<strong>沿用</strong>了 SQLNet 的 <strong>Column Attention</strong> 机制，即将 question 输入 Bi-LSTM 后得到的 <script type="math/tex">H_{QT}</script> 和 column 的 <script type="math/tex">H_{COL}</script> 做 Attention。关于列的编码部分，上面说了看不懂。计算过程为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \alpha_{QT/COL} & = softmax(H_{COL} W_{ct} H^T_{QT}) \\
    H_{QT/COL} & = \alpha_{QT/COL} H_{QT} \\
\end{aligned}</script><p>&emsp;&emsp;最后我们就得到了 <script type="math/tex">H_{QT/COL}</script> 隐藏状态。然后使用这个隐藏状态进行预测。具体公式为<br><strong>MODEL COL-$SELECT COL</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    s & = V^{sel} tanh(W^{sel}_c H^T_{COL} + W^{sel}_{qt} H^T_{QT/COL}) \\
    P_{sel_col} & = softmax(s) \\
\end{aligned}</script><p>&emsp;&emsp;<script type="math/tex">P_{sel_col}</script> 就是每个单词的概率，我们可以使用 argmax() 函数得到最大概率的索引。</p>
<p><strong>MODEL COL-$COND#</strong>：<br>&emsp;&emsp;。。。略</p>
<p>&emsp;&emsp;对于不同的模型，论文中都有说明，就不一一记录了。<br><div class="note info">
            <p>&emsp;&emsp;之前理解错了，还以为跟 encoder-decoder 一模一样，现在才知道原来 slot filling 是这样的。<br>&emsp;&emsp;跟语义解析来比较，就是说大致的生成语句已经给你写好了，剩下的几个空，用 attention 的方法来填充，可以理解为不需要 decoder 部分了。</p>
          </div><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/%E8%AE%BA%E6%96%87/paper_result.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>论文笔记：Neural Semantic Parsing over Multiple Knowledge-bases</title>
    <url>/%E8%AE%BA%E6%96%87/39%E3%80%81Neural%20semantic%20parsing%20over%20multiple%20knowledge-bases.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.gg363.site/pdf/1702.01569.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;思想：将不同领域的数据集合并，以提高训练集大小。<br>&emsp;&emsp;语义分析被认为是将语言语句翻译为可执行的逻辑形式的技术。要做到普遍使用语义分析的一个基本阻碍是<strong>在新领域标注逻辑形式的代价太大</strong>。为了解决这一问题，先前工作的策略有从 denotations、paraphrases、m declarative sentences 训练。<br>&emsp;&emsp;本论文提出一个正交解：将来自不同域中的多个数据集的样本合并到一起，每个数据集对应一个单独的知识库（KB），并在所有示例上训练模型。这次方法由于观察到知识库在实体和属性上有所不同，但语言组合的结构在领域之间重复，所以由此启发而来。例如，语言中的“最大”对应于“argmax”，动词后跟一个名词通常表示连接操作。与仅在单个领域上训练的模型相比，跨域共享信息的模型可以提高泛化能力。<br>&emsp;&emsp;最近 <a href="https://arxiv.gg363.site/pdf/1606.03622.pdf" target="_blank" rel="noopener">Jia and Liang, 2016: Data recombination for neural semantic parsing</a> 以及 <a href="https://arxiv.gg363.site/pdf/1601.01280.pdf" target="_blank" rel="noopener">Dong and Lapata, 2016: Language to logical form with neural attention</a> 提出了用于语义分析的 seq2seq 模型。将语言和逻辑形式简单地表示为向量形式，这些神经网络模型大致上能促进信息共享。我们以他们的工作为基础，研究了在语言编码和逻辑形式解码过程中跨领域共享表示的模型（即研究语言和逻辑形式在不同领域如何表示，如医学和旅游业）。我们最终发现，<strong>通过向解码器提供领域的表示，我们可以在多个领域上训练单个领域，并且与在每个领域上分别训练的模型相比，大大提高了准确性</strong>。在 Overnight 数据集上提高了性能，并减少了网络参数。</p>
<h1 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h1><p>&emsp;&emsp;seq2seq + attention.</p>
<h1 id="多个-KB-上的模型"><a href="#多个-KB-上的模型" class="headerlink" title="多个 KB 上的模型"></a>多个 KB 上的模型</h1><p>&emsp;&emsp;本文，我们强调一项设置：我们访问来自不同领域的训练集 K，每个领域对应不同的 KB。所有领域的输入都是自然语句，标签都是逻辑形式（<strong>我们假定被标注逻辑形式可以被转换为单个形如 lambda-DCS 的形式语言</strong>）。虽然从单词到 KB 常量的映射在每个域都是特定的，但是我们期望语言所表达的意义可以跨域共享。下面开始描述模型架构。</p>
<h2 id="One-to-one-model"><a href="#One-to-one-model" class="headerlink" title="One-to-one model"></a>One-to-one model</h2><p>&emsp;&emsp;此模型类似于 <strong>Section 2</strong> 所描述的模型（Jia and Liang, 2016），如 Figure 2 所示。它由一个 encoder 和一个 decoder 组成，可以用于生成所有领域的输出。因此，模型所有参数由所有领域共享，并且模型从所有样本中训练。</p>
<h2 id="Many-to-many-model"><a href="#Many-to-many-model" class="headerlink" title="Many-to-many model"></a>Many-to-many model</h2><p>&emsp;&emsp;</p>
<h2 id="One-to-many-model"><a href="#One-to-many-model" class="headerlink" title="One-to-many model"></a>One-to-many model</h2><p>&emsp;&emsp;单个 encoder 共享，但是为每个领域设置一个独立的 decoder。共享的 encoder 捕获每个领域输入的英语单词序列的事实，特定领域的 decoder 学习来自正确领域下词表的输出标记（tokens）。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><ul>
<li>数据集：Overnight</li>
</ul>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>&emsp;&emsp;复制了 Jia and Liang, 2016 的实验配置，使用相同的超参数。。。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Neural network-based question answering over knowledge graphs on word and character level</title>
    <url>/%E8%AE%BA%E6%96%87/36%E3%80%81Neural%20network-based%20question%20answering%20over%20knowledge%20graphs%20on%20word%20and%20character%20level.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://www.researchgate.net/publication/315769814_Neural_Network-based_Question_Answering_over_Knowledge_Graphs_on_Word_and_Character_Level" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;基于向量建模的方法。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1506.02075.pdf" target="_blank" rel="noopener">A. Bordes, 2015: Large-scale simple question answering with memory networks</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1604.00727.pdf" target="_blank" rel="noopener">D. Golub and X. He, 2016: Character-level question answering with attention</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1606.03391.pdf" target="_blank" rel="noopener">W. Yin, 2016: Simple question answering by attentive convolutional neural network</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1606.01994.pdf" target="_blank" rel="noopener">Z. Dai, 2016: Cfo: Conditional focused neural question answering with large-scale knowledge bases</a><br>&emsp;&emsp;<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>【NLP算法】（三）条件随机场CRF</title>
    <url>/zcy/%E3%80%90NLP%E7%AE%97%E6%B3%95%E3%80%91%EF%BC%88%E4%B8%89%EF%BC%89%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF.html</url>
    <content><![CDATA[<p>&emsp;&emsp;<a href="https://zhuanlan.zhihu.com/p/44042528" target="_blank" rel="noopener">一篇不错的 CRF 入门文章</a>，但是后面求所有路径的分数的算法，说得有点模糊，推荐看<a href="https://www.bilibili.com/video/av52626653/?p=4" target="_blank" rel="noopener">HMM与CRF隐形马尔可夫链与条件随机场</a>，虽然这个视频播放量不高，但足以看懂 CRF 最后的那个算法。<br>&emsp;&emsp;<a href="https://zhuanlan.zhihu.com/p/27338210" target="_blank" rel="noopener">Bi-LSTM-CRF for Sequence Labeling</a><br>&emsp;&emsp;<a href="https://blog.csdn.net/cuihuijun1hao/article/details/79405740" target="_blank" rel="noopener">Pytorch Bi-LSTM + CRF 代码详解</a></p>
<h1 id="马尔科夫链——Markov-Chain"><a href="#马尔科夫链——Markov-Chain" class="headerlink" title="马尔科夫链——Markov Chain"></a>马尔科夫链——Markov Chain</h1><p>&emsp;&emsp;有时也称为显马尔科夫模型（observed Markov model）。<br>&emsp;&emsp;是一个随机过程模型，它表述了一系列可能的事件，在这个系列当中每一个事件的概率仅依赖于前一个事件。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/【NLP算法】（三）条件随机场CRF/马尔科夫链.jpg" alt="马尔科夫链"></p>
<h1 id="隐马尔科夫模型——HMM"><a href="#隐马尔科夫模型——HMM" class="headerlink" title="隐马尔科夫模型——HMM"></a>隐马尔科夫模型——HMM</h1><h2 id="概率模型"><a href="#概率模型" class="headerlink" title="概率模型"></a>概率模型</h2><p>&emsp;&emsp;概率模型就是将学习任务归结于计算变量的概率分布的模型。<br>&emsp;&emsp;在生活中，我们经常会根据一些已经观察到的现象来推测和估计未知的东西——这种需求，恰恰是概率模型的推断（Inference）行为所作的事情。<br>&emsp;&emsp;推断的本质是：利用可观测变量来预测未知变量的条件分布。<br>&emsp;&emsp;概率模型可以分为两类：生成模型（generative model）和判别模型（discriminative model）。<br>&emsp;&emsp;我们将可观测变量的集合命名为 O，我们感兴趣的未知变量的集合命名为 Y。<br>&emsp;&emsp;生成模型学习出来的是 O 与 Y 的联合概率分布 P(O,Y)，而判别模型学习的是条件概率分布 P(Y|O)。<br>&emsp;&emsp;例如朴素贝叶斯模型就是生成模型，而逻辑回归就是判别模型。</p>
<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><p>&emsp;&emsp;<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>zcy</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
    <url>/%E8%AE%BA%E6%96%87/35%E3%80%81Seq2sql%EF%BC%9AGenerating%20structured%20queries%20from%20natural%20language%20using.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1709.00103.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;本文主要做出两项贡献：1）提出 Seq2SQL，将自然语言问题翻译为其对应的 SQL queries。2）发布 WikiSQL 语料库，其包含 80654 个人工标注的自然语言问题实例， SQL queries 以及从 24241 张 HTML 网页中提取的 SQL 表（网页来自 Wikipedia）。<em>WikiSQL 比以前提供给 logical forms 和自然语句的语义分析数据集大一个数量级</em>。发布 WikiSQL 的同时，我们还发布了一个此数据库的查询引擎（query execution engine）<br><div class="note primary">
            <p>&emsp;&emsp;本论文将自然语言转为 SQL，关系型数据库。而知识图谱是非关系型数据库存储的。</p>
          </div><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文复现：Language to Logical Form with Neural Attention</title>
    <url>/%E8%AE%BA%E6%96%87%E4%BB%A3%E7%A0%81/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%EF%BC%9ALanguage%20to%20Logical%20Form%20with%20Neural%20Attention.html</url>
    <content><![CDATA[<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><p>&emsp;&emsp;论文的地址<a href="https://arxiv.org/pdf/1601.01280.pdf" target="_blank" rel="noopener">在此</a>，作者使用了 Lua 语言实现，代码地址<a href="https://github.com/donglixp/lang2logic" target="_blank" rel="noopener">在这</a>，然而我不会 Lua 语言，于是找了找是否有 Python 的实现版本。还真有，Python 版本代码地址<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch" target="_blank" rel="noopener">在这</a>。<br>&emsp;&emsp;但是 Python 版本的代码<strong>篇幅太长</strong>，且<strong>几乎没有注释</strong>，于是我将其重写了一遍，一些工具类是直接复制别人的，但是核心代码我改写了一下，并添加了一些注释。<br><div class="note info">
            <p>&emsp;&emsp;我将里面的数据获取模块移除了。</p>
          </div></p>
<h1 id="论文实现"><a href="#论文实现" class="headerlink" title="论文实现"></a>论文实现</h1><p>&emsp;&emsp;论文共用了两个办法：1）普通 seq2seq 模型；2）作者自创的 seq2tree 模型。其中每个模型又分别有 <strong>lstm 实现</strong>和 <strong>lstm + attention 实现</strong>两种版本。虽然两个版本使用的技术不同，但是说到底也只是同一个模型。以下讲解原理。</p>
<h2 id="seq2seq-模型"><a href="#seq2seq-模型" class="headerlink" title="seq2seq 模型"></a>seq2seq 模型</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>&emsp;&emsp;论文中使用 LSTM 实现 seq2seq 模型，训练之后，accuracy 大约在 70%。</p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>&emsp;&emsp;我自己用了 Transformer 改写了一下，并且调了几天的参数，发现效果出奇的差，accuracy 最好只有 18%。然后我还发现，对于短句子几乎是百分比预测正确，对于长句子百分比预测错误。所以<strong>我怀疑是否是位置编码那产生的问题，考虑到一个逻辑形式它并不是纯粹的线性结构，它的内部是由很多括号的</strong>。<br>&emsp;&emsp;经调参后得到最好的一组参数如下：</p>
<ol>
<li>learning rate: 0.001</li>
<li>dim_feedforward: 随意（我设置为 256）</li>
<li>h_model: 256</li>
<li>nhead: 4</li>
<li>encoder_layer/decoder_layer: 1</li>
<li>dropout: 0.4</li>
<li>batch_size: 32（16 的效果可能更好）</li>
<li>epoch: 95（epoch 可以进一步修改）</li>
<li>src_mask: False</li>
<li>tgt_mask: True</li>
<li>memory_mask: False</li>
</ol>
<h2 id="seq2tree-模型"><a href="#seq2tree-模型" class="headerlink" title="seq2tree 模型"></a>seq2tree 模型</h2><p>&emsp;&emsp;</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch踩坑总结</title>
    <url>/pytorch%E8%B8%A9%E5%9D%91%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<ol>
<li><code>tensor.view()</code> 相当于numpy中resize（）的功能，但是用法可能不太一样。</li>
<li>关于 nn.LSTM 的用法：如果不想手动初始化隐藏状态，而是想让 pytorch 帮你初始化，那么就只需要填入输入值即可。举个例子，下面的代码输入向量所对应的自然语句是 [‘how are you’, ‘i’m fine’]。<br>其中输入值 x 的形状为 (seq_len, batch_size, input_size)，比如上面的例子就是 (3, 2, 300)，代表序列长度为 3（因为上面的两句话的最大长度为 3），批量大小为 2，词向量维度为 300。注意一点：<strong>pytorch 会根据你输入的序列长度</strong>（输入的张量必须是一样长的，参差不齐的张量无法输入，当然了，你也无法创建出这样的张量）<strong>自动帮你做时间步上的计算，比如你输入的序列长度为 20，那么它会自动在神经网络的第一层计算 20 次，再返回结果给你。</strong>如果看不懂这里是什么意思，说明你不会 LSTM。可以再看看 LSTM 是怎么计算的。<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cell</span> = nn<span class="variable">.LSTM</span>(input_size, hidden_size, num_layers)</span><br><span class="line"><span class="keyword">output</span>, (a, m) = <span class="keyword">cell</span>(x)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习的下一步</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5.html</url>
    <content><![CDATA[<ul>
<li>机器学习能不能知道“我不知道”<br>机器学习的 classifier 可以判断一张图片是不是猫，但是能不能判断出“我不知道这是什么”？这项技术叫做 <strong>Anomaly Detection</strong>。</li>
<li>机器说出为什么“我知道”<ul>
<li>神马汉斯的例子</li>
<li>马辨识器的例子。机器只是辨识了英文字母<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/机器学习的下一步/说出为什么“我知道”.jpg" alt="说出为什么“我知道”"></li>
</ul>
</li>
<li>机器的错觉？<ul>
<li>adversarial attack。感觉是 CV 里的技术<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/机器学习的下一步/机器的错觉.jpg" alt="机器的错觉"></li>
</ul>
</li>
<li>终身学习（Life-long Learning）<br>机器能否终身学习。现在模型一般只能对应一个任务，如果让一个模型去学习下围棋，之后再让它去学习玩星海。那么它就不会下围棋了。这被为 <strong>Catastrophic Forgetting</strong>。</li>
<li>学习如何学习<br>如何写一个<strong>能够写出具有学习能力的程序</strong>的程序。这被称为 <strong>Meta-learning/Learn to learn</strong>。</li>
<li>一定需要很多训练数据吗？<ul>
<li>Few-shot learning</li>
<li>Zero-shot learning</li>
</ul>
</li>
<li>Reinforcement learning</li>
<li>神经网络压缩（Network Compression）<ul>
<li>把大神经网络路缩小</li>
<li>参数二元化<ul>
<li>所有的参数都变成 +1 或 -1</li>
</ul>
</li>
</ul>
</li>
<li>如果训练数据和测试数据长得不一样<ul>
<li>对于 CV 来说，训练数据和测试数据长得差不多，比如手写体识别。但是如果在真实场景中，测试数据是彩色的，可能会出现准确率骤降的情况。那么如何解决呢？<a id="more"></a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>【机器学习算法】半监督学习</title>
    <url>/zcy/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E3%80%91%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>&emsp;&emsp;半监督学习就是在已有的带标签的数据之后，还有一组不带标签的数据。一般来说，在做无监督学习时，unlabeled data 远大于 labeled data。<br>&emsp;&emsp;半监督学习一般分为两种：</p>
<ul>
<li>Transductive learning：unlabeled data 就是你的 testing sdata</li>
<li>Inductive learning：unlabeled data 不是你的 testing sdata</li>
</ul>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/【机器学习算法】半监督学习/导读.jpg" alt="导读"></p>
<h2 id="为什么做半监督学习"><a href="#为什么做半监督学习" class="headerlink" title="为什么做半监督学习"></a>为什么做半监督学习</h2><p>&emsp;&emsp;有人说机器学习训练数据很少，其实不完全对。因为只是 labeled data 少，unlabeled data 随处可见。所以如果能将这些 unlabeled data 运用进去就好了。原因如下：</p>
<ul>
<li>搜集数据很简单，但是搜集 labeled data 代价昂贵</li>
<li>生活中，我们自己也在做半监督学习</li>
</ul>
<h1 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h1><h1 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h1><h1 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h1><h1 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h1>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>zcy</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Coarse-to-Fine Decoding for Neural Semantic Parsing</title>
    <url>/%E8%AE%BA%E6%96%87/34%E3%80%81Coarse-to-Fine%20Decoding%20for%20Neural%20Semantic%20Parsing.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;提出一个结构感知的神经架构，将语义解析过程分解为如下两个步骤：给定一个输入语句，1）首先生成它含义的粗略草图（a rough sketch of its meaning），其中低级信息<strong>被掩盖</strong>（如<strong>变量名和参数</strong>）。2）然后考虑输入本身和草图来填充丢失的细节。<br>&emsp;&emsp;RNN 在多种 NLP 任务中的成功应用对 seq2seq 的语义解析产生了强大的冲击力，如<a href="https://arxiv.org/pdf/1606.03622.pdf" title="Data recombination for neural semantic parsing" target="_blank" rel="noopener">Jia and Liang, 2016</a>; Dong and Lapata, 2016; <a href="https://arxiv.org/pdf/1603.06744.pdf" title="Latent predictor networks for code generation" target="_blank" rel="noopener">Ling et al., 2016</a>。<br>&emsp;&emsp;我们认为，这种方法至少有三个优点。首先，分解步骤<strong>将高级语义信息与低级语义信息分离开来</strong>，使译码器能够在不同的粒度级别对语义进行建模。其次，模型可以明确地为具有相同草图（即基本含义）的示例共享粗糙结构的知识，即使它们的实际含义表示不同（例如，由于不同的细节）。第三，在生成草图后，解码器知道语句的基本含义是什么，<strong>模型可以将其作为全局上下文来改进对最终细节的预测</strong>。<br>&emsp;&emsp;使用如下数据集：</p>
<ol>
<li>GEO</li>
<li>ATIS</li>
<li>DJANGO</li>
<li>WikiSQL</li>
</ol>
<h1 id="问题阐释"><a href="#问题阐释" class="headerlink" title="问题阐释"></a>问题阐释</h1><p>&emsp;&emsp;定义 <script type="math/tex">x = x_1 \dots x_{|x|}</script> 为自然语句，<script type="math/tex">y = y_1 \dots y_{|y|}</script>为意义表示，<script type="math/tex">a = a_1 \dots a_{|a|}</script>为 sketch 表示。<del>注意 <strong>sketch 的定义为</strong>：一个中间变量，如将自然语句转化为 Logical Form，Source Code，SQL，noSQL，SPARQL等表示，这些表示都算是一个 sketch。</del>下图论文架构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Coarse-to-Fine Decoding for Neural Semantic Parsing/Coarse2Fine架构.jpg" alt="Coarse2Fine架构"></p>
<h2 id="Sketch-Generation"><a href="#Sketch-Generation" class="headerlink" title="Sketch Generation"></a>Sketch Generation</h2><p>&emsp;&emsp;encoder 将自然语句编码为向量，decoder 去计算 <script type="math/tex">p(a|x)</script> 从而通过encoding 向量 生成 sketch a。具体来讲，<strong>Input Decoder</strong> 将字转为词向量，并使用 Bi-LSTM 训练。<strong>Coarse Meaning Decoder</strong> 生成 sketch a，也使用 LSTM 并且加上 attention 机制。</p>
<h2 id="Meaning-Representation-Generation"><a href="#Meaning-Representation-Generation" class="headerlink" title="Meaning Representation Generation"></a>Meaning Representation Generation</h2><p>&emsp;&emsp;Meaning representation 由输入 x 以及生成的 sketch a 预测产生，具体就是计算 <script type="math/tex">p(y|x,a)</script>。<strong>Sketch Encoder</strong> 与 Input Decoder 类似，使用 Bi-LSTM 并将 sketch a 映射为词向量。<strong>Fine Meaning Decoder</strong> 与 Coarse Meaning Decoder 类似。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;总的来说就是先用自然语句生成 coarse sketch，然后再用 coarse sketch 生成 fine sketch。一共使用了两个 encoder-deocder 模型，但是将这两个模型连起来用。</p>
<h1 id="三个语义分析任务"><a href="#三个语义分析任务" class="headerlink" title="三个语义分析任务"></a>三个语义分析任务</h1><p>&emsp;&emsp;为了证明我们的框架适用于跨域和意义表示，我们为三个任务开发了模型，即将自然语言解析为逻辑形式、Python 源代码和 SQL 查询。对于每一个任务，我们都描述了使用的数据集以及 sketch 提取的提取步骤。</p>
<h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><p>&emsp;&emsp;</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习算法（四）：Transformer</title>
    <url>/zcy/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ATransformer.html</url>
    <content><![CDATA[<h1 id="Transformer位置信息"><a href="#Transformer位置信息" class="headerlink" title="Transformer位置信息"></a>Transformer位置信息</h1><p>&emsp;&emsp;引用自<a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></p>
<blockquote>
<p>&emsp;&emsp;NLP 句子中单词之间的相对位置是包含很多信息的，上面提过，RNN 因为结构就是线性序列的，所以天然会将位置信息编码进模型；而CNN的卷积层其实也是保留了位置相对信息的，所以什么也不做问题也不大。但是对于 Transformer 来说，为了能够保留输入句子单词之间的相对位置信息，必须要做点什么。为啥它必须要做点什么呢？因为输入的第一层网络是 Muli-head self attention 层，我们知道，Self attention 会让当前输入单词和句子中任意单词发生关系，然后集成到一个 embedding 向量里，但是当所有信息到了 embedding 后，位置信息并没有被编码进去。所以，Transformer 不像 RNN 或 CNN，必须明确的在输入端将 Positon 信息编码，Transformer 是用<strong>位置函数</strong>来进行位置编码的，而 Bert 等模型则给每个单词一个 <strong>Position embedding</strong>。将单词 embedding 和单词对应的 position embedding 加起来形成单词的输入 embedding，类似上文讲的 ConvS2S 的做法。</p>
</blockquote>
<h1 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h1><p>&emsp;&emsp;从<a href="https://zhuanlan.zhihu.com/p/59629215" target="_blank" rel="noopener">文章</a>做的总结，此文为另一篇英文文章的翻译版，英文原文地址为 <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>。此外另一篇<a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">文章</a>也做了翻译。<br>&emsp;&emsp;Transformer 不同于 RNN，它的输入是独立计算的，输入序列的某个时间步并不依赖其它的时间步。也就是说它可以<strong>并行运算</strong>。</p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>&emsp;&emsp;在第一节中讲到，Transformer 天然地不具有句子的<strong>位置</strong>属性，所以我们需要使用一种办法让它拥有句子的位置属性。<br>&emsp;&emsp;为解决这个问题，Transformer 为每个输入的词嵌入增加了一个向量。<br><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png" alt="To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern."></p>
<p>&emsp;&emsp;如果假定词嵌入维度为4，那真实的位置编码如下：<br><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png" alt="A real example of positional encoding with a toy embedding size of 4"></p>
<p>&emsp;&emsp;位置编码的生成方法在原论文的 section 3.5 中有描述。你可以在 <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py" target="_blank" rel="noopener">get_timing_signal_1d()</a> 函数中看到用于生成位置编码的代码。这并不是生成位置编码的唯一方式。然而，它的优点在于可以扩展到看不见的序列长度（eg. 如果要翻译的句子的长度远长于训练集中最长的句子）。</p>
<h2 id="self-Attention"><a href="#self-Attention" class="headerlink" title="self-Attention"></a>self-Attention</h2><p><img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png" alt="Transformer计算机制"></p>
<blockquote>
<p>注意，这些新创建的向量的维度小于词嵌入向量(embedding vector)。它们（新创建的向量）的维度是 <strong>64</strong>，而词嵌入和编码器的输入输出向量的维度是 <strong>512</strong>。它们不必更小，这是一种架构选择，可以使多头注意力(multiheaded attention)计算不变。 </p>
</blockquote>
<ol>
<li>需要从每个编码器的输入向量创建三个向量，即 <strong>Query</strong> 向量，<strong>Key</strong> 向量和 <strong>Value</strong> 向量；<ul>
<li>那么，究竟什么是“query”，“key”，“value”向量呢？（以下为自己的猜测）<ul>
<li>q 代表当前需要计算 score 的向量，此向量查询 key 中的权重，使得查询到自己需要的 score</li>
<li>k 代表当前单词的权重</li>
<li>v 代表单词的词向量</li>
</ul>
</li>
</ul>
</li>
<li>计算得分（score 权重）。这里的分数是通过将 query 向量与我们正在评分的单词的 key 向量做点积来得到。所以如果我们计算位置 #1 处的单词的 self-attention，第一个得分就是就是 <script type="math/tex">q_1</script> 和 <script type="math/tex">k_1</script> 的点积。第二个得分是 <script type="math/tex">q_1</script> 和 <script type="math/tex">k_2</script> 的点积。<br><img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png" alt="计算 score"></li>
<li>将分数除以8（论文中使用 Key 向量维数的平方根—-64。这可以有更稳定的梯度。实际上还可以有其他可能的值，这里使用默认值）<br><img src="https://jalammar.github.io/images/t/self-attention_softmax.png" alt="计算 score"></li>
<li>然后经过一个softmax操作后输出结果。Softmax可以将分数归一化，这样使得结果都是正数并且加起来等于1<br><img src="https://jalammar.github.io/images/t/self-attention-output.png" alt=""></li>
<li>将每个 value 向量乘以 softmax 得分（准备将他们相加）</li>
<li>对加权值向量求和，这样就产生了在这个位置的self-attention的输出（对于第一个单词）</li>
</ol>
<p>&emsp;&emsp;上述为 self-attention 单个单词的计算步骤，其实它可以并行计算。<br><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" alt="并行计算第一步"></p>
<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" alt="并行计算第二步"></p>
<h2 id="multi-headed-attention"><a href="#multi-headed-attention" class="headerlink" title="multi-headed attention"></a>multi-headed attention</h2><p>&emsp;&emsp;看英文名感觉“高大上”，其实很简单。就是将上述的 self-attention 做多次，具体做几次你可以自行选择，Transformer 选择了 8 次。这样就产生了 8 个向量，但是我们训练时其实只要一个向量就够了，所以我们将这 8 个向量<strong>拼接</strong>起来，这样就形成了一个巨长无比的向量。那我们怎么得到我们所需长度的向量呢？很简单，只需要再乘一个权重矩阵就行了。<br><img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png" alt=""></p>
<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png" alt=""></p>
<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png" alt=""></p>
<p>&emsp;&emsp;全过程：<br><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="全过程"></p>
<h2 id="The-Residuals-与-layer-normalization"><a href="#The-Residuals-与-layer-normalization" class="headerlink" title="The Residuals 与 layer-normalization"></a>The Residuals 与 layer-normalization</h2><p>&emsp;&emsp;残差连接说白了就是跳过一层，输入到下一层（比较直白，不一定对）。<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png" alt=""></p>
<p>&emsp;&emsp;layer-normalization：<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" alt=""></p>
<p>&emsp;&emsp;此步骤在解码层中是类似的操作。将内部件画全，就是如下所示一般（图中的 encoder、decoder 各有两个）：<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" alt=""></p>
<p>&emsp;&emsp;在 self_attention 之后要做一次 normalization，此均值归一化步骤的具体算法，详见：</p>
<ul>
<li><a href="https://www.cnblogs.com/hellcat/p/9735041.html#_label3_0" target="_blank" rel="noopener">『计算机视觉』各种Normalization层辨析</a> </li>
<li><a href="https://www.jianshu.com/p/c357c5717a60" target="_blank" rel="noopener">layer normalization 简单总结</a></li>
</ul>
<h1 id="decodedr"><a href="#decodedr" class="headerlink" title="decodedr"></a>decodedr</h1><p>&emsp;&emsp;在讲解 decoder 之前，看一张动图，了解一下 Transformer 是如何运作的。首先下图中生成了第一个字母 I。<br><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" alt=""></p>
<p>&emsp;&emsp;编码器开始处理输入序列，然后将顶部编码器的输出变换为一组注意力向量 <strong>K</strong> 和 <strong>V</strong>，这些将在每个解码器的“encoder-decoder attention” 层使用，这有助于解码器集中注意力在输入序列的合适位置。解释一下 <strong>K</strong> 和 <strong>V</strong> 的具体用法，<strong>在 encoder 中的 self-attention 会使用到 <script type="math/tex">Q_{self-attention}</script>、<script type="math/tex">K_{self-attention}</script>、<script type="math/tex">V_{self-attention}</script> 三个向量，但是它们实际上是输入值 x 的三份拷贝再乘上各自不同的权重矩阵得来。对于 decoder 的 self-attention 与 encoder 的如出一撤，但是对于 encoder-decoder attention 却有点不一样。<script type="math/tex">Q_{encoder-decoder \, attention}</script> 还是 x 的拷贝乘上一个权重矩阵，但是 <script type="math/tex">K_{encoder-decoder \, attention}</script> 和 <script type="math/tex">V_{encoder-decoder \, attention}</script> 分别是 K 和 V 乘上各自的权重矩阵。</strong>以上所有向量所乘的权重矩阵均可以由你自己随机初始化产生。<br>&emsp;&emsp;接着下面的 gif 完成了余下的步骤。Transformer 将之前输出的 I 当做下一个时间步的输入，又走了一遍 decoder。以此循环往复，直到输出一个结束标记 <code>&lt;EOS&gt;</code> 才结束循环。<strong>博主注</strong>：这样一来似乎对于 decoder 来说并不能实现并行运算。<strong>博主注2</strong>：后来发现其实也可以。<br><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt=""></p>
<p>&emsp;&emsp;其中 decoder 中的 attention 与 encoder 的 attenntion 有所区别，具体的区别就是上上段黑体的内容。除了那个区别之外，没有其他区别了。</p>
<h1 id="Generator-The-Final-Linear-and-Softmax-Layer"><a href="#Generator-The-Final-Linear-and-Softmax-Layer" class="headerlink" title="Generator:The Final Linear and Softmax Layer"></a>Generator:The Final Linear and Softmax Layer</h1><p>&emsp;&emsp;如何将其转换为一个单词？这就是最后 Softmax 层和线性层的工作了。<br>&emsp;&emsp;线性层是一个简单的全连接神经网络，它将解码器堆叠(decoder stack)产生的向量映射到一个巨大的向量（词汇表的大小，原来的向量大小是词嵌入的大小）中去，这个向量称为 logits 向量。<br>&emsp;&emsp;Softmax 层将这些分数转化为概率(全部为正数，加起来为 1.0)。选择具有最高概率的单元，并将与其相关的单词作为本时间步的输出。</p>
<h1 id="padding-mask-sequence-mask"><a href="#padding-mask-sequence-mask" class="headerlink" title="padding mask/sequence mask"></a>padding mask/sequence mask</h1><p>&emsp;&emsp;无论 encoder 还是 decoder 都要做 mask（很多对 Transformer 的总结文章都只提到了 decoder 部分的 Masked Multihead Self-Attention，实际上 encoder 也要做一次）。mask 一共分为两种，寻常所见的 decoder 中的 mask 指的是 <strong>sequence mask</strong>，encoder 中的 mask 指的是 <strong>padding mask</strong>。详见：</p>
<ul>
<li><a href="https://luozhouyang.github.io/transformer/" target="_blank" rel="noopener">Transformer模型的PyTorch实现</a></li>
<li><a href="https://www.jianshu.com/p/405bc8d041e0" target="_blank" rel="noopener">The Transformer</a></li>
</ul>
<p>&emsp;&emsp;<strong>对于 padding mask</strong>：在 encoder 中的每次 scaled dot-product 都要做一次 sequence mask。由于我们要让序列的长度相等以便做向量化操作，所以必不可少地需要对输入序列进行<strong>截断</strong>或<strong>补零</strong>操作。所以 sequence mask 的<strong>主要目的</strong>是使得我们的 self-attention 不要过多的关注向量中的 0。<strong>具体操作是</strong>：将序列中补零位置的值置为 -INF，使得序列经过 scaled dot-product 后的 softmax 层时，对应位置会得到<strong>概率 0</strong>。<br>&emsp;&emsp;<strong>对于 sequence mask</strong>：使得 decoder 无法看见未来的信息，decoder 的 attention 只能关注解码单词之前的输出单词，而不能依赖后面未解码出来的单词。</p>
<h1 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h1><p>&emsp;&emsp;在最后的 softmax 层直接输出了概率最大的位置的单词，这叫做<strong>贪婪解码——greedy decoding</strong>。<br>&emsp;&emsp;另一种更合理的解码方式叫做 <strong>Beam Search</strong>。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>【NLP算法】（零）NLP基础算法</title>
    <url>/zcy/%E3%80%90NLP%E7%AE%97%E6%B3%95%E3%80%91%EF%BC%88%E9%9B%B6%EF%BC%89NLP%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html</url>
    <content><![CDATA[<h1 id="one-hot"><a href="#one-hot" class="headerlink" title="one hot"></a>one hot</h1><h1 id="N-gram-算法"><a href="#N-gram-算法" class="headerlink" title="N-gram 算法"></a>N-gram 算法</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/baimafujinji/article/details/51281816" target="_blank" rel="noopener">参考文章</a></p>
<ul>
<li><strong>基本算法</strong><br>有句子：老王吃过了吗？<br>该句子可表示为 <script type="math/tex">S = w_1w_2w_3w_4w_5w_6</script><br>那么该句子的概率为 p(S)=p(w1w2w3w4w5w6) = <script type="math/tex">\prod</script>p(wt|w1w2…wt-1) = p(w1)p(w2|w1)p(w3|w1w2)…<br>由于该算法太复杂，所以就产生了其他的算法。马尔科夫模型，该模型认为，一个词的出现仅仅依赖于它前面出现的几个词。可理解为是n-gram。</li>
<li><strong>n-gram</strong><br>n-gram只根据前n-1个单词来预测第n个单词的概率。<br>比如n=2时，也被称为bi-gram。公式为： p(wt|wt-1) = p(wtwt-1)/p(wt)<br>意思就是计算wt-1后面出现wt的概率<br>比如t等于1，则就是计算“老”后面出现“王”的概率<br><strong>剩下的问题就是如何计算一个字出现的概率</strong>。</li>
<li><strong>如何计算概率</strong><br>可以做一个统计，将所拥有的数据计算出每个字出现的次数。并且再统计两个字连在一起的次数。比如：p(王|老) = p(老王)/p(王) = 600/10000 = 6%<br>在神经网络模型中，计算概率是通过 softmax 函数计算的，即将 one hot 编码的向量经过矩阵运算后传入 softmax 函数中。从而得到一个类似概率的值。</li>
<li><strong>用途</strong><br><a href="http://blog.sciencenet.cn/blog-713101-797384.html" target="_blank" rel="noopener">参考</a><br>20世纪80年代至90年代初,n-gram技术被广泛地用来进行文本压缩,检查拼写错误,加速字符串查找,文献语种识别。90年代,该技术又在自然语言处理自动化领域得到新的应用,如自动分类,自动索引,超链的自动生成,文献检索,无分隔符语言文本的切分等。<br>目前N-gram最为有用的就是自然语言的自动分类功能。基于n-gram的自动分类方法有两大类,一类是人工干预的分类(Classification),又称分类;一类是无人工干预的分类(Clustering),又称聚类。</li>
</ul>
<h1 id="词袋模型——BOW"><a href="#词袋模型——BOW" class="headerlink" title="词袋模型——BOW"></a>词袋模型——BOW</h1><p>&emsp;&emsp;例如，有文档：你好棒棒</p>
<ol>
<li>建立字典<br>{‘你’:1, ‘好’:2, ‘棒’:3}</li>
<li>bow模型<br>(1, 1)(2, 1)(3, 2)<br>其中第一个元素代表该单词在字典中的id，第二个元素代表该单词在所在文档中出现的次数。</li>
</ol>
]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>zcy</tag>
      </tags>
  </entry>
  <entry>
    <title>【知识图谱】（一）从概念到实战</title>
    <url>/zcy/%E3%80%90%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E3%80%91%EF%BC%88%E4%B8%80%EF%BC%89%E4%BB%8E%E6%A6%82%E5%BF%B5%E5%88%B0%E5%AE%9E%E6%88%98.html</url>
    <content><![CDATA[<h1 id="知识图谱描述"><a href="#知识图谱描述" class="headerlink" title="知识图谱描述"></a>知识图谱描述</h1><p>&emsp;&emsp;知识图谱是一种新型的<strong>数据库</strong>，是一种基于图的数据结构。每个节点表示现实世界中存在的“实体”，每条边为实体与实体之间的“关系”。以下为知识图谱的几点作用：</p>
<ul>
<li>从“关系”分析问题</li>
<li>把不同种类的信息连接在一起</li>
<li>一个关系网络</li>
</ul>
<p>&emsp;&emsp;学习知识图谱首先得掌握以下几种技能：</p>
<ol>
<li><strong>基础知识</strong>：自然语言处理、图数据库操作知识、基本编程能力：Python、SQL；</li>
<li><strong>领域知识</strong>：知识图谱构建方法、知识图谱推理方法；</li>
<li><strong>行业知识</strong></li>
</ol>
<h1 id="知识图谱的构建步骤"><a href="#知识图谱的构建步骤" class="headerlink" title="知识图谱的构建步骤"></a>知识图谱的构建步骤</h1><ol>
<li>数据收集(持续收集与更新)（<strong>关键词抽取</strong>、<strong>命名体识别</strong>、<strong>关系抽取</strong>、<strong>事件抽取</strong>）<ol>
<li>原始数据，通常可能是一篇文章<ol>
<li>爬虫技术<ol>
<li>垂直爬虫</li>
<li>搜索引擎相关的爬虫</li>
</ol>
</li>
</ol>
</li>
<li>语料数据，通常词库，词典，同义词</li>
<li>开源的第三方知识图谱，例如搜狗人物关系图</li>
<li>开源的训练好的词向量(word2vec)模型,tfidf</li>
</ol>
</li>
<li>图谱设计<ol>
<li>实体定义(本体)<br>实体：实体类型<ol>
<li>属性<br>例如,手(长度，面积)，类别：身体器官</li>
</ol>
</li>
<li>属性定义</li>
<li>关系定义<ol>
<li>关系也需要定义类别</li>
<li>需要评估关系可以覆盖的数据量，一般服从28 原则，20%的关系，覆盖80%数据</li>
</ol>
</li>
</ol>
</li>
<li>知识清洗<ol>
<li><strong>实体消歧</strong></li>
<li><strong>实体统一</strong></li>
</ol>
</li>
<li>知识融合(实体链接)<ol>
<li>实体与关系的融合</li>
<li>实体扩充(融合外部知识图谱或者数据)（<strong>知识合并</strong>）</li>
</ol>
</li>
<li><strong>知识存储</strong>-图数据库</li>
</ol>
<h2 id="知识图谱的架构与设计"><a href="#知识图谱的架构与设计" class="headerlink" title="知识图谱的架构与设计"></a>知识图谱的架构与设计</h2><p>&emsp;&emsp;略</p>
<h2 id="知识源数据的获取"><a href="#知识源数据的获取" class="headerlink" title="知识源数据的获取"></a>知识源数据的获取</h2><p>&emsp;&emsp;略。可以使用爬虫等技术，或者直接网上搜现成的数据。</p>
<h2 id="信息抽取"><a href="#信息抽取" class="headerlink" title="信息抽取"></a>信息抽取</h2><p>&emsp;&emsp;包括关键词抽取、命名体识别、关系抽取，事件抽取等技术。</p>
<h3 id="关键词抽取"><a href="#关键词抽取" class="headerlink" title="关键词抽取"></a>关键词抽取</h3><h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>&emsp;&emsp;分类算法中的流程：<br>&emsp;&emsp;分词—&gt;(自然语言处理,与,知识,图谱,知识图谱)—&gt;去停词—&gt;(自然语言处理,知识,图谱,知识图谱)—&gt;建立索引—&gt;(1,2,3,432,66)—&gt;one hot—&gt;word2vec—&gt;</p>
<h4 id="语料库"><a href="#语料库" class="headerlink" title="语料库"></a>语料库</h4><p>&emsp;&emsp;jieba 分词同时基于一些<strong>语料库</strong>和手写的<strong>规则</strong>（如隐马尔科夫模型）。<br>&emsp;&emsp;如果想要加入自己的语料库可以使用下面的代码，语料库的格式可在 github jieba 上找到。<br><figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">'/home/python/dictionary.txt'</span>)</span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span>(<span class="string">' '</span>.<span class="keyword">join</span>(seg_list))</span><br></pre></td></tr></table></figure></p>
<ol>
<li>词库<ul>
<li>医药知识图谱<ul>
<li>语料库（网上有现成的，不用自己爬，如：医药行业专业词典）<ul>
<li>医院的名称</li>
<li>疾病的名称</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h4><p>&emsp;&emsp;文本数据的表示模型：</p>
<ul>
<li>布尔模型（boolean model）</li>
<li>向量空间模型（vector space model）</li>
<li>概率模型（probabilistic model）</li>
<li>图空间模型（graph space model）等</li>
</ul>
<p>&emsp;&emsp;以下为几种主要的模型，它们的目标都是：建立文档的向量（矩阵）模型。加粗代表是现在常用的模型</p>
<ol>
<li><strong>TF-IDF</strong></li>
<li>LDA</li>
<li>LSA/LSI</li>
<li><strong>Word2Vec</strong></li>
<li>one-hot</li>
<li>BERT</li>
<li>…</li>
</ol>
<h5 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h5><p>&emsp;&emsp;TF：词频<br>&emsp;&emsp;IDF：逆文档频率。<br>&emsp;&emsp;权重 = TF * IDF<br>&emsp;&emsp;TF-IDF 可能会漏掉一些词。比如一篇文章只出现一次“周杰伦”，但是它已经表示了这篇文章的主旨。可是 TF-IDF 无法为该词分配较高的权重。<br>&emsp;&emsp;另外 jieba 中其实可以直接使用 TF-IDF。导入<code>jieba.analyse</code>即可使用。（TF-IDF 其实就是提取句子的标签）<br><figure class="highlight clean"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> ja</span><br><span class="line">ja.extract_tags(sentence, topK=<span class="number">3</span>,withWeight=<span class="literal">False</span>, allowPOS=())</span><br></pre></td></tr></table></figure></p>
<h5 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h5><p>&emsp;&emsp;TF-IDF 只考虑单个文字，忽略了句子中的上下文信息。而word2vec 考虑了上下文，输入值为某个单词的前几个单词、后几个单词和其本身。<br>&emsp;&emsp;word2vec 现成的工具包有：1)gensim；2)tensorflow；3)keras。<br>&emsp;&emsp;另外 QA 系统等应用可能不适合使用 word2vec 训练出来的单词。因为它训练出来的词向量没有捕获到<strong>太多</strong>的上下文信息。众所众知，QA 系统和对话系统等应用需要经常使用到很多上下文信息。</p>
<h3 id="命名体识别——NER"><a href="#命名体识别——NER" class="headerlink" title="命名体识别——NER"></a>命名体识别——NER</h3><p>&emsp;&emsp;所谓的命名体（named entity）就是人名、机构名、地名以及其他所有以名称为标识的实体。更广泛的实体还包括数字、日期、货币、地址等等。<br>&emsp;&emsp;难点：1)<strong>同义词、歧义词等</strong>；2)<strong>未登录词判定</strong>。<br>&emsp;&emsp;一般流程：1)<strong>基于规则的方法</strong>；2)<strong>基于模型的方法</strong>，常见的序列标注模型包括 <strong>HMM</strong>（Hidden Markov Model）、<strong>CRF</strong>（Conditional random field）、<strong>RNN</strong>。不过虽然基于模型的方法技术比较新颖，但是由于太过复杂以及太难解释，所以公司还是用基于规则的方法比较多。</p>
<h4 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h4><ul>
<li>基于HMM</li>
<li>基于CRF<div class="note danger">
            <p>&emsp;&emsp;上课的时候没听明白。</p>
          </div></li>
<li>基于RNN<br>&emsp;&emsp;要做命名体识别，首先要做序列标注的任务。目前有以下几种公认的标注体系：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/标注体系.png" alt="标注体系"></li>
</ul>
<h3 id="关系抽取（特征工程）"><a href="#关系抽取（特征工程）" class="headerlink" title="关系抽取（特征工程）"></a>关系抽取（特征工程）</h3><ol>
<li><strong>文本特征提取</strong>，采用 tf-idf</li>
<li><strong>关键字抽取</strong>，比如转让，收购，整合等等</li>
<li><strong>句法特征提取</strong>，主要是与核心词之间的关系，包括企业实体本身和前后词与核心词之间的关系，距离等。即抽取（实体，关系，实体）<strong>三要素</strong>特征<ul>
<li>依存句法分析<ul>
<li>依存树，<a href="http://ltp.ai/demo.html" target="_blank" rel="noopener">demo</a></li>
<li>CCG</li>
</ul>
</li>
<li>分类器</li>
</ul>
</li>
<li>如果使用 NN 训练，可以拼接三要素和 tf-idf 特征</li>
</ol>
<h3 id="事件抽取"><a href="#事件抽取" class="headerlink" title="事件抽取"></a>事件抽取</h3><p>&emsp;&emsp;略，培训中未提到，估计跟关系抽取差不多。</p>
<h2 id="知识融合"><a href="#知识融合" class="headerlink" title="知识融合"></a>知识融合</h2><h3 id="实体链接"><a href="#实体链接" class="headerlink" title="实体链接"></a>实体链接</h3><h4 id="实体统一-实体对齐"><a href="#实体统一-实体对齐" class="headerlink" title="实体统一/实体对齐"></a>实体统一/实体对齐</h4><p>&emsp;&emsp;<strong>注：另一种说法是实体统一和实体对齐并不是同一件事。此处姑且当它们是同一件事。</strong><br>&emsp;&emsp;对同一实体具有多个名称的情况进行实体统一，将多个名称统一替换成一个命名实体。比如，“河北银行股份有限公司”和“河北银行”可以统一成“河北银行”。<br>&emsp;&emsp;大致来说这个应用是使用规则来做实体统一。目前（2019 年 7 月）来说，基于规则的做法大概能解决 70% 左右的问题。还可以使用余弦相似度，分类等算法进行融合使用。</p>
<ul>
<li>分离出地名，比如河北，北京</li>
<li>去除后缀，比如有限公司，集团</li>
<li>提取经营范围，比如医疗，化学</li>
<li>剩余部分为中间字段</li>
<li>最后选择以上四个部分的某些部分进行拼接，成为一个唯一的命名实体，如果有中间字段，则仅使用中间字段即可，并对某些特殊的经营范围做补充，比如银行；否则，优先使用地名加经营范围，其次是地名加后缀。</li>
</ul>
<p>&emsp;&emsp;<strong>更新命名体：在做完实体统一之后，将原数据中的实体进行替换即可</strong>。</p>
<h4 id="实体消歧"><a href="#实体消歧" class="headerlink" title="实体消歧"></a>实体消歧</h4><p>&emsp;&emsp;与实体统一不同。实体统一是将两个不一样名称的实体统一起来，而实体消歧是将同一个名称的实体在不同语境下区分开来，比如：苹果在不同的语境下分别有水果和手机的意思。<br>&emsp;&emsp;中文的不怎么好做，主要运用规则。</p>
<h3 id="知识合并"><a href="#知识合并" class="headerlink" title="知识合并"></a>知识合并</h3><p>&emsp;&emsp;<a href="https://102.alibaba.com/downloadFile.do?file=1518508273059/CoLink An Unsupervised Framework for User Identity Linkage.pdf" target="_blank" rel="noopener">阿里巴巴实体合并框架</a></p>
<h2 id="知识加工"><a href="#知识加工" class="headerlink" title="知识加工"></a>知识加工</h2><h2 id="知识存储与检索"><a href="#知识存储与检索" class="headerlink" title="知识存储与检索"></a>知识存储与检索</h2><h2 id="知识应用"><a href="#知识应用" class="headerlink" title="知识应用"></a>知识应用</h2><h1 id="汉语处理的难点"><a href="#汉语处理的难点" class="headerlink" title="汉语处理的难点"></a>汉语处理的难点</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/汉语处理的难点.jpg" alt="汉语处理的难点"></p>
<h1 id="NLP-工具包"><a href="#NLP-工具包" class="headerlink" title="NLP 工具包"></a>NLP 工具包</h1><p>&emsp;&emsp;略。详见此<a href="https://yan624.github.io/普开培训.html#NLP-工具包">博客</a></p>
<h1 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h1><p>&emsp;&emsp;以上为知识图谱的大致概述，以下以几个例子大致地将构建步骤串联起来。首先给出知识图谱的总结<strong>思维导图</strong>，可以按照图中的内容自行对应查找知识点。思维导图的阅读顺序是<strong>从上至下</strong>，<strong>从右至左</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/知识图谱总结.png" alt="知识图谱总结"></p>
<h2 id="医疗命名体识别"><a href="#医疗命名体识别" class="headerlink" title="医疗命名体识别"></a>医疗命名体识别</h2><p>&emsp;&emsp;<a href="https://github.com/liuhuanyong/MedicalNamedEntityRecognition" target="_blank" rel="noopener">项目地址</a>，使用了基于字向量的<strong>四层双向 LSTM</strong> 与 <strong>CRF 模型</strong>的网络。<br>&emsp;&emsp;本项目大致使用了<strong>信息抽取</strong>-&gt;<strong>命名体识别</strong>的技术。项目中有一个名为 data_origin 的文件夹，其结构为：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">data_origin</span><br><span class="line">  ├─一般项目</span><br><span class="line">  │  ├─一般项目-<span class="number">1</span>.txt</span><br><span class="line">  │  ├─一般项目-<span class="number">1</span><span class="selector-class">.txtoriginal</span><span class="selector-class">.txt</span></span><br><span class="line">  │  └─。。。</span><br><span class="line">  ├─出院情况</span><br><span class="line">  ├─病史特点</span><br><span class="line">  └─诊疗经过</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;<em>一般项目-1.txt</em> 文件包含了由<strong>人工标注</strong>过的数据，<em>一般项目-1.txtoriginal.txt</em> 包含了原始数据，即未经过任何处理的数据。类似以下的格式。第 2 列和第 3 列代表该命名体在原始数据中的开始和结束的索引。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/原始数据.jpg" alt="原始数据"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/人工标注后的数据.jpg" alt="人工标注后的数据"></p>
<p>&emsp;&emsp;以上的数据为项目的原数据（那个由人工标注过的数据也算原数据），我们需要使用一套标注体系（本项目使用 BIO 体系）来将原数据处理一下，以下是处理结果。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/使用 BIO 标注后的数据.jpg" alt="使用 BIO 标注后的数据"></p>
<p>&emsp;&emsp;你可能会疑惑 DISEASE-* 之类的东西是什么意思，以及它是怎么出来的。其实十分简单，如下所示，都是预先定义好的。以 B 结尾，代表一个命名体的开始，以 I 结尾，代表一个命名体的结束。而产生数据的过程也只是写死的一套逻辑，使用 if else 进行判断罢了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/标签字典.jpg" alt="标签字典"></p>
<h3 id="训练之前的准备工作"><a href="#训练之前的准备工作" class="headerlink" title="训练之前的准备工作"></a>训练之前的准备工作</h3><ol>
<li>定义标签</li>
<li>人工将数据一条一条地标注命名体、起始位置以及标签</li>
<li>选择一套标注体系</li>
<li>将<strong>每一份</strong>原数据使用标注体系处理后，存入<strong>一份</strong>文件</li>
</ol>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>&emsp;&emsp;代码中以 <code>[&#39;。&#39;,&#39;?&#39;,&#39;!&#39;,&#39;！&#39;,&#39;？&#39;]</code> 符号作为一份病历的结束。然后将训练数据重新拆分成多分训练样本。我倒是认为在原数据处理完毕合并时，就做一些处理不行吗？如果以那些符号作为判断条件，可能有些不太准。</p>
<ol>
<li>加载字向量；</li>
<li>以 <code>[&#39;。&#39;,&#39;?&#39;,&#39;!&#39;,&#39;！&#39;,&#39;？&#39;]</code> 符号作为一份病历的结束，重新切分数据为多份训练样本；</li>
<li>每一个字都有一个标注，比如训练样本：[感, 染, 风, 寒]和标注：[CHECK-B, CHECK-I, DISEASE-B, DISEASE-I]—转换为—&gt;[32, 8454, 676, 934]和[7, 8, 10, 9]；</li>
<li>程序定义有 150 个时间步，第一层 BiLSTM 为 128 维，第二层的 BiLSTM 为 64 维，各层之间的 Dropout 取 0.5；</li>
<li>将训练样本输入 RNN，RNN 的输出输入 CRF，CRF 输出一个 11 维的向量，即每一个字都会输出一个 11 维的向量。所以可以看做是一个 11 元分类模型，即判断一个字属于哪一类的标注，也就是序列标注的含义——为字标注属性；</li>
<li>训练结束，就完成了一个序列标注模型。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>命名体识别</strong>的功能，使用了 <strong>LSTM</strong> 以及 <strong>CRF</strong> 的技术，原数据采用了<strong>人工标注</strong>的处理方式，原数据转为训练样本采用了<strong>规则模版</strong>的方式。总的来说，没有太大难度。对于此项目，我们需要理解 LSTM 和 CRF 的算法，整个过程的难点就在人工标注上，费时费力。</p>
<h2 id="中文人物关系知识图谱"><a href="#中文人物关系知识图谱" class="headerlink" title="中文人物关系知识图谱"></a>中文人物关系知识图谱</h2><p>&emsp;&emsp;<a href="https://github.com/liuhuanyong/PersonRelationKnowledgeGraph" target="_blank" rel="noopener">项目地址</a>。此项目代码结构有点复杂，涉及了很多爬虫，我对爬虫不是很了解。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>关系抽取</strong>的功能，具体使用了什么技术<strong>未知</strong>。</p>
<h2 id="判断两个企业实体是否存在投资关系"><a href="#判断两个企业实体是否存在投资关系" class="headerlink" title="判断两个企业实体是否存在投资关系"></a>判断两个企业实体是否存在投资关系</h2><p>&emsp;&emsp;<a href="https://github.com/rlistengr/Entity-relationship-extraction" target="_blank" rel="noopener">项目地址</a>。</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;此项目实现了<strong>关系抽取</strong>和<strong>实体统一</strong>的功能，基本上使用了人工模版去判断两个企业是否<strong>统一</strong>。是否存在投资关系也是用规则判断的。</p>
<h2 id="金融问答项目"><a href="#金融问答项目" class="headerlink" title="金融问答项目"></a>金融问答项目</h2><p>&emsp;&emsp;此项目（<strong>实验21-1-FinancialKGQA</strong>）实现了一个简单的金融问答项目，前提项目为<strong>实验19-neo4j构建简单的金融知识图谱</strong>，旨在使用爬虫技术构建一个金融知识图谱。数据和代码已经由 2019.6.27 普开知识图谱培训机构提供。</p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p>&emsp;&emsp;从下图可以看出，只是简单的关键词匹配。然后通过 neo4j 的 CQL 语句进行查询。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/[知识图谱]（一）概述/金融问答示例代码.jpg" alt="金融问答示例代码"></p>
<h2 id="企业经营退出风险预测"><a href="#企业经营退出风险预测" class="headerlink" title="企业经营退出风险预测"></a>企业经营退出风险预测</h2><p>&emsp;&emsp;<a href="https://github.com/xiaorancs/business-exit-risk-forecast" target="_blank" rel="noopener">项目地址</a>，还没研究过。同一个项目，<a href="https://github.com/ShawnyXiao/2017-CCF-BDCI-Enterprise" target="_blank" rel="noopener">另一个人的项目地址</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>2019 普开培训</title>
    <url>/2019%20%E6%99%AE%E5%BC%80%E5%9F%B9%E8%AE%AD.html</url>
    <content><![CDATA[<h1 id="第一天"><a href="#第一天" class="headerlink" title="第一天"></a>第一天</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>&emsp;&emsp;知识图谱是一种新型的<strong>数据库</strong>，是一种基于图的数据结构。每个节点表示现实世界中存在的“实体”，每条边为实体与实体之间的“关系”。以下为知识图谱的几点作用：</p>
<ul>
<li>从“关系”分析问题</li>
<li>把不同种类的信息连接在一起</li>
<li>一个关系网络</li>
</ul>
<p>&emsp;&emsp;学习知识图谱首先得掌握以下几种技能：</p>
<ol>
<li><strong>基础知识</strong>：自然语言处理、图数据库操作知识、基本编程能力：Python、SQL；</li>
<li><strong>领域知识</strong>：知识图谱构建方法、知识图谱推理方法；</li>
<li><strong>行业知识</strong></li>
</ol>
<p>&emsp;&emsp;现在知识图谱领域中比较火热的是：风控。<strong>企查查</strong>可以查询企业的状态。<br>&emsp;&emsp;知识图谱<strong>核心技术</strong>可分为（大致就是一本书的目录）：</p>
<ol>
<li>知识图谱的架构与设计</li>
<li>知识图谱核心技术-<strong>知识源数据的获取</strong></li>
<li>知识图谱核心技术-信息抽取-<strong>关键词抽取</strong>(属性与数值)</li>
<li>知识图谱核心技术-信息抽取-<strong>实体识别</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-信息抽取-<strong>关系抽取</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-信息抽取-<strong>事件抽取</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-知识融合概述</li>
<li>知识图谱核心技术-知识融合-实体链接-<strong>实体统一</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-知识融合-实体链接-<strong>实体消岐</strong>（深度学习+经典方案）</li>
<li>知识图谱核心技术-知识融合-<strong>知识合并</strong></li>
<li>知识图谱核心技术-知识加工概述</li>
<li>知识图谱核心技术-知识加工-<strong>本体构建</strong></li>
<li>知识图谱核心技术-<strong>知识存储与检索</strong></li>
<li>知识图谱核心技术-知识加工-<strong>知识推理</strong></li>
<li>知识应用-智能问答，风控，营销….</li>
<li>知识图谱核心技术-知识加工-知识更新</li>
<li>知识图谱核心技术-知识加工-质量评估</li>
</ol>
<p>&emsp;&emsp;<strong>基本任务</strong>和<strong>主要研究方向</strong>：</p>
<ul>
<li>机器翻译</li>
<li>自动摘要</li>
<li>文本分类与信息过滤</li>
<li>信息检索</li>
<li>信息抽取与文本挖掘<ul>
<li>实体抽取：命名体识别</li>
<li>关系抽取：关系抽取算法</li>
<li>事件抽取<ul>
<li>地区、时间、过程</li>
<li>文本分类（为事件分类）</li>
</ul>
</li>
</ul>
</li>
<li>情感分析</li>
<li>自动问答</li>
<li>……</li>
</ul>
<p>&emsp;&emsp;自然语言处理与知识图谱的<strong>处理步骤</strong>：</p>
<ol>
<li>分词、语料库、文本分类、文本聚类、文本词性分析。。。</li>
<li>信息抽取</li>
<li>知识融合阶段<ul>
<li>实体统一</li>
<li>实体消歧</li>
</ul>
</li>
</ol>
<h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>&emsp;&emsp;分类算法中的流程：<br>(自然语言处理与知识图谱)—&gt;分词—&gt;(自然语言处理,与,知识,图谱,知识图谱)—&gt;去停词—&gt;(自然语言处理,知识,图谱,知识图谱)—&gt;建立索引—&gt;(1,2,3,432,66)—&gt;one hot—&gt;word2vec—&gt;</p>
<h3 id="语料库"><a href="#语料库" class="headerlink" title="语料库"></a>语料库</h3><p>&emsp;&emsp;jieba 分词同时基于一些<strong>语料库</strong>和手写的<strong>规则</strong>（如隐马尔科夫模型）。<br>&emsp;&emsp;如果想要加入自己的语料库可以使用下面的代码，语料库的格式可在 github jieba 上找到。<br><figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">jieba.load_userdict(<span class="string">'/home/python/dictionary.txt'</span>)</span><br><span class="line">seg_list = jieba.cut(text, cut_all=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span>(<span class="string">' '</span>.<span class="keyword">join</span>(seg_list))</span><br></pre></td></tr></table></figure></p>
<ol>
<li>词库<ul>
<li>医药知识图谱<ul>
<li>语料库（网上有现成的，不用自己爬，如：医药行业专业词典）<ul>
<li>医院的名称</li>
<li>疾病的名称</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h3><p>&emsp;&emsp;文本数据的表示模型：</p>
<ul>
<li>布尔模型（boolean model）</li>
<li>向量空间模型（vector space model）</li>
<li>概率模型（probabilistic model）</li>
<li>图空间模型（graph space model）等</li>
</ul>
<p>&emsp;&emsp;以下为几种主要的模型，它们的目标都是：建立文档的向量（矩阵）模型。加粗代表是现在常用的模型</p>
<ol>
<li><strong>TF-IDF</strong></li>
<li>LDA</li>
<li>LSA/LSI</li>
<li><strong>Word2Vec</strong></li>
<li>one-hot</li>
<li>BERT</li>
<li>…</li>
</ol>
<h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>&emsp;&emsp;TF：词频<br>&emsp;&emsp;IDF：逆文档频率。<br>&emsp;&emsp;权重 = TF * IDF<br>&emsp;&emsp;TF-IDF 可能会漏掉一些词。比如一篇文章只出现一次“周杰伦”，但是它已经表示了这篇文章的主旨。可是 TF-IDF 无法为该词分配较高的权重。<br>&emsp;&emsp;另外 jieba 中其实可以直接使用 TF-IDF。导入<code>jieba.analyse</code>即可使用。（TF-IDF 其实就是提取句子的标签）<br><figure class="highlight clean"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> ja</span><br><span class="line">ja.extract_tags(sentence, topK=<span class="number">3</span>,withWeight=<span class="literal">False</span>, allowPOS=())</span><br></pre></td></tr></table></figure></p>
<h4 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h4><p>&emsp;&emsp;TF-IDF 只考虑单个文字，忽略了句子中的上下文信息。而word2vec 考虑了上下文，输入值为某个单词的前几个单词、后几个单词和其本身。<br>&emsp;&emsp;word2vec 现成的工具包有：1)gensim；2)tensorflow；3)keras。<br>&emsp;&emsp;另外 QA 系统等应用可能不适合使用 word2vec 训练出来的单词。因为它训练出来的词向量没有捕获到<strong>太多</strong>的上下文信息。众所众知，QA 系统和对话系统等应用需要经常使用到很多上下文信息。</p>
<h3 id="汉语处理的难点"><a href="#汉语处理的难点" class="headerlink" title="汉语处理的难点"></a>汉语处理的难点</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/汉语处理的难点.jpg" alt="汉语处理的难点"></p>
<h2 id="NLP-工具包"><a href="#NLP-工具包" class="headerlink" title="NLP 工具包"></a>NLP 工具包</h2><ul>
<li>中文分词工具（粗体推荐使用，其他随意）<ul>
<li><strong>jieba</strong>：<a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">下载地址</a>。分词、ti-idf、标注。。。<ul>
<li>全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；</li>
<li>精确模式：试图将句子最精确地切开，适合文本分析；</li>
<li>搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</li>
</ul>
</li>
<li>snownlp：<a href="https://github.com/isnowfy/snownlp" target="_blank" rel="noopener">下载地址</a>。这个有点慢</li>
<li><strong>Hanlp</strong>：<a href="https://github.com/hankcs/pyhanlp" target="_blank" rel="noopener">下载地址</a>。功能较多，比如：<ul>
<li>中文分词</li>
<li>词性标注（pos）</li>
<li>命名实体识别（ner）</li>
<li>关键词提取</li>
<li>自动摘要</li>
<li>短语提取</li>
<li>拼音转换</li>
<li>简繁转换</li>
<li>依存句法分析</li>
<li>word2vec</li>
</ul>
</li>
<li>pkuseg：<a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">下载地址</a>。支持细领域分词，比如海洋、新闻、医药等。MIT 许可证，所以不可商用</li>
<li>THULAC：<a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="noopener">下载地址</a></li>
<li>fudannlp：不怎么更新了<ul>
<li>fastNLP：复旦新开发的一个工具，做了很多模型的集成，如 BERT。</li>
</ul>
</li>
</ul>
</li>
<li>英文分词工具<ul>
<li>gensim：分词、主题分析等</li>
<li><strong>spaCy</strong>：<a href="https://spacy.io/usage/models" target="_blank" rel="noopener">文档</a></li>
</ul>
</li>
</ul>
<h2 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h2><p>&emsp;&emsp;所谓的命名体（named entity）就是人名、机构名、地名以及其他所有以名称为标识的实体。更广泛的实体还包括数字、日期、货币、地址等等。<br>&emsp;&emsp;难点：1)<strong>同义词、歧义词等</strong>；2)<strong>未登录词判定</strong>。<br>&emsp;&emsp;一般流程：1)<strong>基于规则的方法</strong>；2)<strong>基于模型的方法</strong>，常见的序列标注模型包括 <strong>HMM</strong>（Hidden Markov Model）、<strong>CRF</strong>（Conditional random field）、<strong>RNN</strong>。不过虽然基于模型的方法技术比较新颖，但是由于太过复杂以及太难解释，所以公司还是用基于规则的方法比较多。</p>
<h3 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h3><p>&emsp;&emsp;要做命名体识别，首先要做序列标注的任务。目前国家有以下几种标注体系：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/标注体系.png" alt="标注体系"></p>
<ul>
<li>基于HMM</li>
<li>基于CRF<div class="note danger">
            <p>&emsp;&emsp;上课的时候没听明白。</p>
          </div></li>
<li>基于RNN</li>
</ul>
<h1 id="第二天"><a href="#第二天" class="headerlink" title="第二天"></a>第二天</h1><h2 id="医疗命名体识别"><a href="#医疗命名体识别" class="headerlink" title="医疗命名体识别"></a>医疗命名体识别</h2><p>&emsp;&emsp;使用 BIO 标注体系。<br>&emsp;&emsp;命名体识别模型训练步骤：</p>
<ol>
<li>准备数据：<strong>原始数据，即自然语言语句</strong><ol>
<li><strong>定义大类</strong>，如 BODY、SIGN、DISEASE。使用数据抽样的方法，2-3 周</li>
</ol>
</li>
<li>对原始数据进行标注：<strong>对原始数据进行人工标注，如<code>右髋部    21    23    身体部位</code>、<code>疼痛    27    28    症状和体征</code></strong>。data_orign 文件夹中有 *.txt 和 *.txtoriginal.txt 文件。其中 *.txtoriginal.txt 文件中是医生诊断的原始数据，*.txt 中是将原始数据中的特征标注出来（此步骤是人工操作。不过如果有很多数据，其实可以偷个懒，因为<strong>有些特征差不多，在一份病历中标注一次就够了</strong>。比如风寒会出现很多次，其实只要在一份病历中标注一次，之后就可以被程序识别到了，当然多标注几份也行），<strong>如果已经有字典，比如网上下载的，可以不进行此步</strong>。</li>
<li>设置标注格式：如 IO、BIO、BMEWO 等体系。</li>
<li>编写转换程序：<strong>将所有标注的病历数据按标注体系转换，并且合并在一份文件中。如：<code>肺    DISEASE-B</code>、<code>炎    DISEASE-I</code>。详见：transfer_data.py</strong>。在 data/train.txt中。<strong>注：此标注方法不需要进行分词，因为它以字为级别</strong>。</li>
<li>算法模型：LSTM 和 CRF 如何结合？请看下图。不管多大项目，词向量一般选 300 维<ul>
<li>LSTM</li>
<li>CRF<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/BiLSTM + CRF.png" alt="BiLSTM + CRF"><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/LSTM+CRF例子.jpg" alt="LSTM+CRF例子"></li>
</ul>
</li>
<li>算法预测：预测结果较差，可能是因为数据较少。</li>
</ol>
<p>&emsp;&emsp;CRF 并不擅长提取属性，比如病人的受伤面积，可以使用正则表达式。<br>&emsp;&emsp;端到端训练就是一个模型到另一个模型的训练，比如LSTM + CRF。</p>
<h2 id="中国人物关系图谱"><a href="#中国人物关系图谱" class="headerlink" title="中国人物关系图谱"></a>中国人物关系图谱</h2><h2 id="讲到的技术"><a href="#讲到的技术" class="headerlink" title="讲到的技术"></a>讲到的技术</h2><ul>
<li>命名体识别</li>
<li>关系抽取</li>
</ul>
<h1 id="第三天"><a href="#第三天" class="headerlink" title="第三天"></a>第三天</h1><h2 id="实体分词"><a href="#实体分词" class="headerlink" title="实体分词"></a>实体分词</h2><p>&emsp;&emsp;重要内容提示:●交易简要内容:中海(海南)海盛船务股份有限公司将散货船“百花山”轮作为废钢船出售给江门市银湖拆船有限公司，出售价格为人民币17，183，633.49元。<br>&emsp;&emsp;——分词为——&gt;<br>&emsp;&emsp;重要内容, 提示, 交易, 简要内容, 中海, 海南, 海盛船务股份有限公司, 。。。</p>
<h2 id="命名体识别"><a href="#命名体识别" class="headerlink" title="命名体识别"></a>命名体识别</h2><p>&emsp;&emsp;识别所有命名体。</p>
<h3 id="企业实体识别"><a href="#企业实体识别" class="headerlink" title="企业实体识别"></a>企业实体识别</h3><p>&emsp;&emsp;利用 foolnltk 工具包，对每个新闻做命名实体识别，并对企业命名实体做实体统一，最后将每个新闻中的企业实体替换为统一的企业实体。</p>
<h2 id="实体统一-实体对齐"><a href="#实体统一-实体对齐" class="headerlink" title="实体统一/实体对齐"></a>实体统一/实体对齐</h2><p>&emsp;&emsp;对同一实体具有多个名称的情况进行实体统一，将多个名称统一替换成一个命名实体。比如，“河北银行股份有限公司”和“河北银行”可以统一成“河北银行”。<br>&emsp;&emsp;大致来说这个应用是使用规则来做实体统一。目前（2019 年 7 月）来说，基于规则的做法大概能解决 70% 左右的问题。还可以使用余弦相似度，分类等算法进行融合使用。</p>
<ul>
<li>分离出地名，比如河北，北京</li>
<li>去除后缀，比如有限公司，集团</li>
<li>提取经营范围，比如医疗，化学</li>
<li>剩余部分为中间字段</li>
<li>最后选择以上四个部分的某些部分进行拼接，成为一个唯一的命名实体，如果有中间字段，则仅使用中间字段即可，并对某些特殊的经营范围做补充，比如银行；否则，优先使用地名加经营范围，其次是地名加后缀。</li>
</ul>
<h3 id="更新命名体"><a href="#更新命名体" class="headerlink" title="更新命名体"></a>更新命名体</h3><p>&emsp;&emsp;<strong>在做完实体统一之后，将原数据中的实体进行替换即可</strong>。</p>
<h2 id="特征工程（关系抽取）"><a href="#特征工程（关系抽取）" class="headerlink" title="特征工程（关系抽取）"></a>特征工程（关系抽取）</h2><ol>
<li><strong>文本特征提取</strong>，采用 tf-idf</li>
<li><strong>关键字抽取</strong>，比如转让，收购，整合等等</li>
<li><strong>句法特征提取</strong>，主要是与核心词之间的关系，包括企业实体本身和前后词与核心词之间的关系，距离等。即抽取（实体，关系，实体）三要素特征<ul>
<li>依存句法分析<ul>
<li>依存树，<a href="http://ltp.ai/demo.html" target="_blank" rel="noopener">demo</a></li>
<li>CCG</li>
</ul>
</li>
<li>分类器</li>
</ul>
</li>
<li>拼接三要素 + tf-idf 特征</li>
</ol>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>&emsp;&emsp;将特征工程提取到的特征做 onehot 编码（不一定要是 onehot），利用随机森林进行模型拟合。使用贝叶斯超参数调优，调优参数为【决策树数量，决策树的最大深度，随机数生成器】。或者可以使用深度学习的算法，如神经网络。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="实体消歧"><a href="#实体消歧" class="headerlink" title="实体消歧"></a>实体消歧</h3><p>&emsp;&emsp;中文的不怎么好做，主要运用规则。</p>
<h3 id="知识融合"><a href="#知识融合" class="headerlink" title="知识融合"></a>知识融合</h3><p>&emsp;&emsp;实体扩充(融合外部知识图谱或者数据)。<a href="https://102.alibaba.com/downloadFile.do?file=1518508273059/CoLink An Unsupervised Framework for User Identity Linkage.pdf" target="_blank" rel="noopener">阿里巴巴实体链接框架</a></p>
<h2 id="知识图谱构建步骤总结"><a href="#知识图谱构建步骤总结" class="headerlink" title="知识图谱构建步骤总结"></a>知识图谱构建步骤总结</h2><ol>
<li>数据收集(持续收集与更新)（<strong>关键词抽取</strong>、<strong>命名体识别</strong>、<strong>关系抽取</strong>、<strong>事件抽取</strong>）<ol>
<li>原始数据，通常可能是一篇文章<ol>
<li>爬虫技术<ol>
<li>垂直爬虫</li>
<li>搜索引擎相关的爬虫</li>
</ol>
</li>
</ol>
</li>
<li>语料数据，通常词库，词典，同义词</li>
<li>开源的第三方知识图谱，例如搜狗人物关系图</li>
<li>开源的训练好的词向量(word2vec)模型,tfidf</li>
</ol>
</li>
<li>图谱设计<ol>
<li>实体定义(本体)<br>实体：实体类型<ol>
<li>属性<br>例如,手(长度，面积)，类别：身体器官</li>
</ol>
</li>
<li>属性定义</li>
<li>关系定义<ol>
<li>关系也需要定义类别</li>
<li>需要评估关系可以覆盖的数据量，一般服从28 原则，20%的关系，覆盖80%数据</li>
</ol>
</li>
</ol>
</li>
<li>知识清洗<ol>
<li><strong>实体消歧</strong></li>
<li><strong>实体统一</strong></li>
</ol>
</li>
<li>知识融合(实体链接)<ol>
<li>实体与关系的融合</li>
<li>实体扩充(融合外部知识图谱或者数据)（<strong>知识合并</strong>）</li>
</ol>
</li>
<li><strong>知识存储</strong>-图数据库</li>
</ol>
<h1 id="前三天的总结"><a href="#前三天的总结" class="headerlink" title="前三天的总结"></a>前三天的总结</h1><ol>
<li>知识图谱的架构与设计</li>
<li>知识图谱核心技术-知识源数据的获取</li>
<li>知识图谱核心技术-信息抽取-关键词抽取(属性与数值)</li>
<li>知识图谱核心技术-信息抽取-实体识别（深度学习+经典方案）<ol>
<li>目的：抽取数据中的实体信息，例如人名</li>
<li>方法：<ol>
<li>规则：（正则等）</li>
<li>模型：传统方法CRF，深度学习BiLSTM+CRF</li>
</ol>
</li>
<li>过程：<ol>
<li>按照CRF要求定义好实体的分类与标注体系</li>
<li>标注训练数据</li>
<li>编写BiLSTM+CRF模型</li>
<li>使用模型预测</li>
<li>组合预测的结果</li>
<li>纠错预测的结果</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-信息抽取-关系抽取（深度学习+经典方案）<ol>
<li>目的：抽取实体与实体间的关系，例如：出生于</li>
<li>方法：<ol>
<li>规则，例如：通过关键词，进行匹配</li>
<li>模型<ol>
<li>传统 <ol>
<li>分类<ol>
<li>基于CRF+LSTM，需要将实体标签变成关系类别的标签，进行预测</li>
</ol>
</li>
<li>基于语法树<ol>
<li>依托于语法规则，识别关系属于哪两个实体，要求是句子结构要短一点，如果很长，规则不好定义</li>
</ol>
</li>
<li>BootStrapping</li>
</ol>
</li>
<li>深度学习</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-信息抽取-事件抽取（深度学习+经典方案）<ol>
<li>目的：抽取内容中的事件，以及他们的关系</li>
<li>事 件关系的类型：<ol>
<li>因果事件  某一事件导致某一事件发生  A导致B  </li>
<li>事件预警  因果溯源 由因求果 &lt;地震,房屋倒塌&gt; 条件事件  某事件条件下另一事件发生  如果A那么B  </li>
<li>事件预警  时机判定  &lt;限制放宽,立即增产&gt; 反转事件  某事件与另一事件形成对立  虽然A但是B  预防不测  反面教材  &lt;起步晚,发展快&gt;</li>
<li>顺承事件  某事件紧接着另一事件发生  A接着B  事件演化  未来意图识别  &lt;去旅游,买火车票&gt; </li>
</ol>
</li>
<li>主要的方法：<ol>
<li>规则</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识融合概述<ol>
<li>目的：将信息抽取中，抽取的实体与关系，进行融合<br>例如，(曹操，父子，曹丕) （曹操，父子，曹植）</li>
<li>融合的层次-实体链接<ol>
<li>实体与实体的融合</li>
<li>实体与外部数据的融合</li>
<li>知识图谱与知识图谱的融合</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识融合-实体链接-实体统一（深度学习+经典方案）<ol>
<li>目的：统一实体的名称，例如杭州阿里巴巴集团，阿里巴巴</li>
<li>统一的方法：<ol>
<li>规则：例如去掉杭州阿里巴巴集团的集团，与地区，比较与简称的差距</li>
<li>基于模型：如入A与B，判断是否为一个实体</li>
<li>基于文本相似度：例如使用余弦定理，</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识融合-实体链接-实体消岐（深度学习+经典方案）<ol>
<li>目的：消除实体间的歧义</li>
<li>方法：<ol>
<li>结合语境， 例如该文章类别如果是3c数码类文章，那么小米指的是小米<br>然后进行实体补全</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识融合-知识合并<ol>
<li>实体与实体的融合</li>
<li>实体与外部数据的融合</li>
<li>知识图谱与知识图谱的融合</li>
</ol>
</li>
<li>知识图谱核心技术-知识加工概述</li>
<li>知识图谱核心技术-知识加工-本体构建</li>
<li>知识图谱核心技术-知识存储与检索 - Neo4j create<ul>
<li>(:Movie {title:”驴得水”,released:2016})  return p; </li>
<li>(p1:Person {name:’Alice’}) -[:KNOWS][-&gt;(p2:Person {name:’Bob’})</li>
</ul>
</li>
<li>知识图谱核心技术-知识加工-知识推理</li>
<li>知识应用-智能问答，风控，营销….<ol>
<li>智能问答应用<ol>
<li>如何实现基于知识图谱的智能问答？</li>
<li>对用户输入的问题进行语义分析<ol>
<li>问题分类<ol>
<li>问题的类型分类：例如，冬天下雨怎么办，是咨询类问题</li>
<li>问题的问形式上的分类，例如，怎么办，如何办，去哪办</li>
</ol>
</li>
<li>问句解析<ol>
<li>实体提取，例如，中国移动真不错，提取了中国移动实体</li>
<li>意图的预测，例如，万达怎么去，预测客户是想买东西</li>
<li>问题补全，例如，周末去哪吃比较好——&gt;周末去哪(万达附近)吃饭比较好</li>
<li>其他重要词汇识别</li>
</ol>
</li>
<li>将解析过的语句，转换成：cql等图数据查询语句</li>
<li>将查到的结果，结合之前的问题分类与模版，进行模板填充，反馈给客户</li>
</ol>
</li>
<li>风控</li>
<li>营销<ol>
<li>亲人圈发现</li>
<li>朋友圈发现</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>知识图谱核心技术-知识加工-知识更新</li>
<li>知识图谱核心技术-知识加工-质量评估</li>
</ol>
<h1 id="第四天"><a href="#第四天" class="headerlink" title="第四天"></a>第四天</h1><p>&emsp;&emsp;等于没学。搞了一天的环境配置。</p>
<h1 id="第五天"><a href="#第五天" class="headerlink" title="第五天"></a>第五天</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/普开培训/图谱构建流程概览.png" alt="图谱构建流程概览"><br>&emsp;&emsp;图谱构建流程在第十五讲 PPT。</p>
<h1 id="第六天"><a href="#第六天" class="headerlink" title="第六天"></a>第六天</h1><p>&emsp;&emsp;第十六讲实验步骤。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>conference</category>
      </categories>
  </entry>
  <entry>
    <title>2. 两数相加</title>
    <url>/leetcode/2.%20%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0.html</url>
    <content><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><h1 id="有问题的解法1"><a href="#有问题的解法1" class="headerlink" title="有问题的解法1"></a>有问题的解法1</h1><p>代码<br><figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * public class ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) &#123; val = x; &#125;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123;</span><br><span class="line">        <span class="keyword">int</span> nextNodeCarryValue = <span class="number">0</span>;</span><br><span class="line">        ListNode <span class="keyword">sum</span> = <span class="keyword">new</span> ListNode(nextNodeCarryValue);</span><br><span class="line">        ListNode s = <span class="keyword">sum</span>;</span><br><span class="line">        </span><br><span class="line">        ListNode t1 = l1, t2 = l2;</span><br><span class="line">        <span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(t1 != <span class="keyword">null</span> || t2 != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">// 计算和</span></span><br><span class="line">            <span class="keyword">int</span> res = t1.val + t2.val;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">int</span> nodeValue = res % <span class="number">10</span>;</span><br><span class="line">            nextNodeCarryValue = res / <span class="number">10</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 先创建下一个节点</span></span><br><span class="line">            ListNode nextNode = <span class="keyword">new</span> ListNode(nextNodeCarryValue);</span><br><span class="line">            </span><br><span class="line">            s.val += nodeValue;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 移动节点</span></span><br><span class="line">            t1 = t1.next;</span><br><span class="line">            t2 = t2.next;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 顺便处理最后一个节点，这里的代码十分难理解</span></span><br><span class="line">            <span class="keyword">if</span>((t1 != <span class="keyword">null</span> || t2 != <span class="keyword">null</span>) || nextNodeCarryValue != <span class="number">0</span>)&#123;</span><br><span class="line">                s.next = nextNode;</span><br><span class="line">                s = s.next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 进最后一位</span></span><br><span class="line">        <span class="comment">// if(nextNodeCarryValue != 0)&#123;</span></span><br><span class="line">        <span class="comment">//     s.next = new ListNode(nextNodeCarryValue); </span></span><br><span class="line">        <span class="comment">// &#125;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//处理两个 ListNode 不等长的情况</span></span><br><span class="line">        ListNode t = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span>(t1 != <span class="keyword">null</span>)&#123;</span><br><span class="line">            t = t1;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            t =t2;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(t != <span class="keyword">null</span>)&#123;</span><br><span class="line">            s.next = <span class="keyword">new</span> ListNode(t.val);</span><br><span class="line">            t = t.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">sum</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>1. 两数之和</title>
    <url>/leetcode/1.%20%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C.html</url>
    <content><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。<br><strong>示例</strong>：</p>
<blockquote>
<p>给定 nums = [2, 7, 11, 15], target = 9<br>因为 nums[0] + nums[1] = 2 + 7 = 9<br>所以返回 [0, 1]</p>
</blockquote>
<h1 id="暴力破解"><a href="#暴力破解" class="headerlink" title="暴力破解"></a>暴力破解</h1><ul>
<li>时间复杂度：O(<script type="math/tex">n^2</script>)</li>
<li>空间复杂度：O(1)</li>
<li>用时 39ms</li>
</ul>
<p>首先想到了暴力破解的方法，但是后来发现其实没必要遍历整个数组，内部的循环从 0 开始遍历会浪费时间，与将 <code>for(int j = 0; j &lt; len; j++)</code> 改为了 <code>for(int j = i + 1; j &lt; len; j++)</code>。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">    public int[] twoSum(int[] nums, int target) &#123;</span><br><span class="line">        int len = nums.<span class="built_in">length</span>;</span><br><span class="line">        int[] res =&#123;<span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span>(int <span class="built_in">i</span> = <span class="number">0</span>; <span class="built_in">i</span> &lt; len; <span class="built_in">i</span>++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(int <span class="built_in">j</span> = <span class="built_in">i</span> + <span class="number">1</span>; <span class="built_in">j</span> &lt; len; <span class="built_in">j</span>++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[<span class="built_in">i</span>] + nums[<span class="built_in">j</span>] == target)&#123;</span><br><span class="line">                    res[<span class="number">0</span>] = <span class="built_in">i</span>; res[<span class="number">1</span>] = <span class="built_in">j</span>;</span><br><span class="line">                    <span class="keyword">return</span> res;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="两遍哈希表"><a href="#两遍哈希表" class="headerlink" title="两遍哈希表"></a>两遍哈希表</h1><p>暴力破解的办法时间复杂度较高，还有一种方法可以减少时间复杂度，但是会增加空间复杂度。创建一个 Map 来暂存数据。</p>
<ul>
<li>时间复杂度：O(n)</li>
<li>空间复杂度：O(n)</li>
<li>用时 10ms</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        Map&lt;Integer, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">            <span class="built_in">map</span>.put(nums[i], i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> diff = target - nums[i];</span><br><span class="line">            Integer j = <span class="built_in">map</span>.get(diff);</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">map</span>.containsKey(diff) &amp;&amp; j != i)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;i, j&#125;;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ArithmeticException(<span class="string">"无解"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="一遍哈希表"><a href="#一遍哈希表" class="headerlink" title="一遍哈希表"></a>一遍哈希表</h1><ul>
<li>时间复杂度：O(n)</li>
<li>空间复杂度：O(n)</li>
<li>用时 6ms</li>
</ul>
<p>一遍就能做完题目看似不可能，因为看似无法遍历完所有组合，但是实际上可以，只需要仔细思考一下。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        Map&lt;Integer, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> diff = target - nums[i];</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">map</span>.containsKey(diff))&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[] &#123;i, <span class="built_in">map</span>.get(diff)&#125;;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">map</span>.put(nums[i], i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ArithmeticException(<span class="string">"无解"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph</title>
    <url>/%E8%AE%BA%E6%96%87/30%E3%80%81Complex%20Sequential%20Question%20Answering%EF%BC%9ATowards%20Learning%20to%20Converse%20Over%20Linked%20Question%20Answer%20Pairs%20with%20a%20Knowledge%20Graph.html</url>
    <content><![CDATA[<h1 id="注"><a href="#注" class="headerlink" title="注"></a>注</h1><p>&emsp;&emsp;<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17181/15750" target="_blank" rel="noopener">论文地址</a>。<br>&emsp;&emsp;<strong>凉了。读完论文，发现论文中的实验是使用 python2 写的，而且由于没有 VPN 无法下载实验附带的数据，训练数据有 17G，我都不想下了。</strong></p>
<h1 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h1><p>&emsp;&emsp;人机对话时，人们通常会提出许多问题，其中大部分都可以通过大规模的 KG 回答。为此，我们提出了 Comples Sequential QA（CSQA） 任务，它由以下两种任组成：</p>
<ol>
<li>在拥有百万个实体的 KG 上进行复杂的推理从而回答事实性问题；</li>
<li>通过一系列连贯的链接问答对去学习交谈。</li>
</ol>
<p>&emsp;&emsp;接着还让工作人员创建了一个数据集，包括总共 1.6M 轮的 200k 的对话数据。我们还要求数据集含有<strong>逻辑推理</strong>（logical），<strong>定量推理</strong>（quantitative）以及<strong>比较推理</strong>（comparative ）的能力（此三种能力下面有详解）。因此这就迫使我们的模型要做到：</p>
<ol>
<li>解析复杂的自然语言问题；</li>
<li>使用对话上下文解析表达中的<strong>共指</strong>（coreferences ）、<strong>省略</strong>（ellipsis ）问题；</li>
<li>要求理清<strong>含糊不清</strong>（ambiguous ）的问题；</li>
<li>检索相关的 KG 的子图去回答这些问题。</li>
</ol>
<p>&emsp;&emsp;说明：</p>
<ul>
<li><strong>共指问题</strong>（coreferences）：就是说一个代词指向多个对象，机器人无法理解具体指向哪个</li>
<li><strong>省略问题</strong>（ellipsis）：表达没问题，但是表达中省略了一部分信息，需要人自己去上文中推测</li>
<li><strong>含糊不清的问题</strong>（ambiguous）：（与 1 类似，请看 1）</li>
</ul>
<h1 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h1><ol>
<li>引入 CSQA 的概念；</li>
<li>展示一流的 QA 和对话系统的处理方法在解决这些任务时的不足之处；</li>
<li>对 CSQA 提出了一个模型，由一流的 hierarchical conversation model（<strong>HRED</strong>）（<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11957/12160" target="_blank" rel="noopener">Serban 2016a</a>，<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14567/14219" target="_blank" rel="noopener">Serban 2017</a>） 和 key value（<strong>KV</strong>） based memory network model（<a href="https://arxiv.org/pdf/1606.03126.pdf" target="_blank" rel="noopener">Miller 2016</a>） 组成。</li>
</ol>
<h1 id="数据集创建"><a href="#数据集创建" class="headerlink" title="数据集创建"></a>数据集创建</h1><p>&emsp;&emsp;论文花了很大的篇幅描写了数据集是如何创建的。使用 14-Nov-2016 的 wiki data 创建，其中包含了 5.2k 的 relation（谓语），12.8M entity（主语），52.3M facts（宾语）。但是省略了像“ISO 3166-1 alpha-2 code”、“NDL Auth ID”等 relation，因为不期望用户会问这些模糊的问题。接着论文分别描述了 Simple Questions、Complex Questions 和 Linked Sequential QA 是如何创建的。</p>
<h2 id="Simple-Questions"><a href="#Simple-Questions" class="headerlink" title="Simple Questions"></a>Simple Questions</h2><p>&emsp;&emsp;为了发现问题，我们要求 annotators 自己提出问题并用 KG 中的<strong>单个</strong>三元组进行回答。后来 annotators 认为对于个三元组，主要有三种类型的问题：</p>
<ol>
<li>基于宾语（object）的问题，问题中包含三元组中的主语和关系，答案包含三元组中的宾语；</li>
<li>基于主语（subject）的问题，问题中包含三元组中的宾语和关系，答案包含三元组中的主语；</li>
<li>基于关系（relation）（理解成谓语也可以）的问题。后来在创建的数据集中发现，此类问题没有多大的意义。比如，数据集中有人问了一个很不自然的问题“Q:How is Himalayas related to India? A:located in”。<strong>所以论文只关注前两个问题</strong>。</li>
</ol>
<h2 id="Complex-Questions"><a href="#Complex-Questions" class="headerlink" title="Complex Questions"></a>Complex Questions</h2><p>&emsp;&emsp;接下来要求 annotators 建立一些逻辑推理（Logical Reasoning）、定量推理（Quantitative Reasoning）、比较推理（Comparative Reasoning）类型的问题。</p>
<ol>
<li>逻辑推理：考虑问题“哪些河流流经中国和印度？”，为了回答这个问题首先需要创建两组集合i){flowthrough, India, river}，ii){flowthrough, China, river}。最后求交集。此类问题可由 Simple Questions 修改得到，如 <strong>AND</strong> 操作：<strong>“哪些河流流经印度”</strong>修改为<strong>“哪些河流流经印度”+“和中国”</strong>；<strong>OR</strong>操作：<strong>“哪些河流流经印度”</strong>修改为<strong>“哪些河流流经印度”+“或中国”</strong>。全部的操作包括以下三种：<ul>
<li>AND</li>
<li>OR</li>
<li>NOT</li>
</ul>
</li>
<li>定量推理：如遇到max、min、count、at least/almost/approxmately/equal to N 等问题需要做定量推理。中文类似。</li>
<li>比较推理：基于某一个关系的问题需要做推理。如：“哪个国家拥有的河流比印度多？”</li>
</ol>
<h2 id="Linked-Sequential-QA"><a href="#Linked-Sequential-QA" class="headerlink" title="Linked Sequential QA"></a>Linked Sequential QA</h2><p>&emsp;&emsp;现在开始通过上述的 QA 对创建连续的对话，简单来说，如果两个问题共享一个 relation 或者 entity，那么就将两个问题放在一起。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>&emsp;&emsp;CSQA 由对话和 QA 组成，我们提出的模型由 <strong>HRED</strong> 模型和 <strong>key value memory network</strong> 模型组合而成。其中 HRED 模型是对话系统中的一流模型，key value memory network 模型是 QA 系统中的一流模型。我们的模型由以下组件构成：</p>
<ol>
<li><strong>Hierarchical Encoder</strong>：</li>
<li><strong>Handling Large Vocabulary</strong></li>
<li><strong>Candidate generation</strong></li>
<li><strong>Key Value Memory Network</strong></li>
<li><strong>Decoder</strong></li>
</ol>
<p>其中 1234 是 encoder，5是 decoder。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/复杂的连续问答：使用知识图谱在关联的问答对上学习交谈能力/提出的模型架构.jpg" alt="提出的模型架构"></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>&emsp;&emsp;使用 <strong>Adam</strong> 算法作为优化算法。<br>&emsp;&emsp;然后调整以下超参数：<strong>learning rate</strong> <script type="math/tex">\in</script> {1e-3, 4e-4}， <strong>RNN hidden unit size</strong>、 <strong>word embeddingsize</strong>、 <strong>KG embedding size</strong> <script type="math/tex">\in</script> {256, 512}，<strong>batch size</strong> <script type="math/tex">\in</script> {32, 64}，<strong>dialog context size</strong> as 2。<br>&emsp;&emsp;使用 Precision 和 Recall 作为评估指标。对于验证和计数的问题我们使用 accuracy 作为评估指标，此类问题会产生 YES/NO 或者 counts 的结果。最后对于需要阐明（clarification）的问题，系统产生自然语言回应，这通常是 KG 实体和非 KG 单词的序列，因此使用 Precision/Recall 作为 KG 实体的预测，使用 BLEU 作为语义相似度的衡量指标。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/复杂的连续问答：使用知识图谱在关联的问答对上学习交谈能力/实验结果.jpg" alt="实验结果"></p>
<h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>根据表 4 中的结果，我们讨论了现有方法的一些缺点，并提出了未来研究的领域。</p>
<ol>
<li><strong>Simple v/s Complex Questions</strong>：很明显在我们的模型上，与简单问题相比，复杂问题的性能非常差。改进点有很多，i)不确定现在逻辑函数是否可以处理定量、比较和逻辑推理问题；ii)不清楚现有的 encoder（HRED + key value memory network）是否能够有效地解析复杂问题并为 encoder 提供良好的表示。</li>
<li><strong>Direct v/s Indirect Questions</strong>：用表 4 中的第 3、4 行跟第 2 行比较，发现在处理不完整的问题时，模型性能有所下降，这些问题都需要依赖上下文才能解决共指、省略等难点。即使现在的对话系统（HRED）确实捕捉到了上下文，也没有什么作用。因为其中的一个<em>关键点</em>是<strong>对于我们创建的数据集有一个巨大的挑战</strong>：数据集里的 <strong>named entities</strong> 和 <strong>relations</strong> 比上下文中<strong>其他单词更重要</strong>，所以我们需要一个更好的模型，可以在训练时标出 relations 和 entities 的重要性（例如：<strong>注意力机制</strong>）。</li>
<li><strong>Candidate Generation</strong>：</li>
<li><strong>Better organization of the memory</strong>：对于某些问题，特别是设计多个实体和逻辑操作的复杂问题，不可避免地需要使用大量的内存存储元组。大约有 15% 的问题需要超过 100k 个候选元组。这会使 GPU 超负荷，并且也会使 softmax 的计算开销巨大，所以需要i)更好的内存组织方式，ii)SoftMax 函数的近似方法。<a id="more"></a></li>
</ol>
]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（八）：Adaboost</title>
    <url>/zcy/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9AAdaboost.html</url>
    <content><![CDATA[<h1 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h1><p>&emsp;&emsp;分类算法有很多，比如逻辑回归、kNN 算法、决策树、朴素贝叶斯算法、支持向量机等，它们各有优缺点。我们自然可以<strong>将不同的分类器组合起来</strong>，而这种组合结果则被称为<strong>集成方法</strong>（ensemble method）或者元算法（meta-algorithm）。使用形式多种多样，可以是不同算法的集成，还可以是相同算法不同配置的集成，也可以自行发挥。</p>
<h1 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h1><p>&emsp;&emsp;自举汇聚法（bootstrap aggregating），也称为 bagging 方法。是从原始数据集选择 S 次后得到 S 个新数据集的一种技术。新数据集与原数据集的大小相等。<br>&emsp;&emsp;在 S 个数据集建好之后，将某个学习算法分别作用域每个数据集就得到了 S 个分类器。使用这 S 个分类器进行分类，然后将结果中最多的类别作为最后的分类结果。<br>&emsp;&emsp;当然还有一些更先进的 bagging 方法，比如随机森林（random forest）。</p>
<h1 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h1><p>&emsp;&emsp;boosting 方法拥有多个版本，这里只关注其中一个最流行的版本 AdaBoost。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>【NLP算法】（一）word2vec</title>
    <url>/zcy/%E3%80%90NLP%E7%AE%97%E6%B3%95%E3%80%91%EF%BC%88%E4%B8%80%EF%BC%89word2vec.html</url>
    <content><![CDATA[<h1 id="词嵌入模型"><a href="#词嵌入模型" class="headerlink" title="词嵌入模型"></a>词嵌入模型</h1><p>&emsp;&emsp;以前在训练语言模型的时候使用one-hot编码，现在使用的是word embedding。但是这样理解其实稍微有点问题，因为如果这样理解，就难以解释embedding层是用来干嘛的了。<br>&emsp;&emsp;正解：<br>&emsp;&emsp;<strong>其实现在使用的还是one-hot</strong>，在keras框架中输入的就是one-hot编码。我以前一直是上面的那种理解，所以我一直不懂为什么要输入one-hot，而不直接输入word embedding。众所周知，one-hot编码乘上词嵌入矩阵可以得到词向量，这步操作看起来有点像查表。但其实这里用到了<strong>预训练</strong>。<br>&emsp;&emsp;词嵌入矩阵是预先训练好的，其实就是一个权重矩阵，而我们做的就是初始化了输入层的权重矩阵，只不过以前是随机初始化，而现在是直接初始化为词嵌入矩阵。embedding层只用了一步就获得了词的特征，但以前需要训练。所以我们算是使用了预训练的方法，直接获得了embedding层，但是相比于cv领域，目前我们还无法初始化高层的权重。<br>&emsp;&emsp;<strong>总结一下就是词嵌入矩阵是input层到embedding层的权重矩阵。</strong></p>
<ul>
<li><strong>缺陷</strong><br>多义词问题无法解决，所以导致word embedding一直效果不好。<br>以下引用自<a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">专栏</a><br>Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。</li>
</ul>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p><a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="noopener">word2vec 数学原理目录</a><br><a href="https://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="noopener">word2vec 数学原理</a><br><a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="noopener">word2vec原理推导与代码分析</a></p>
<h1 id="训练词向量得到的-accuracy"><a href="#训练词向量得到的-accuracy" class="headerlink" title="训练词向量得到的 accuracy"></a>训练词向量得到的 accuracy</h1><p>word2vec 的 accuracy 貌似没卵用。我反正用 keras 搭的 CBOW 模型，accuracy 极低，只有 1%不到。<br><a href="https://www.zhihu.com/question/271782463" target="_blank" rel="noopener">参考</a>  </p>
<h1 id="自己训练-word2vec-还是直接用别人的"><a href="#自己训练-word2vec-还是直接用别人的" class="headerlink" title="自己训练 word2vec 还是直接用别人的"></a>自己训练 word2vec 还是直接用别人的</h1><p><a href="https://bbs.csdn.net/topics/392144812?list=4625705" target="_blank" rel="noopener">参考</a><br>可能还是自己训练 word2vec 比较好，用别人的 word2vec 不太行。</p>
<h1 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h1><a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>zcy</tag>
      </tags>
  </entry>
  <entry>
    <title>代价函数</title>
    <url>/zcy/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><div class="table-container">
<table>
<thead>
<tr>
<th>代价函数</th>
<th>选择</th>
</tr>
</thead>
<tbody>
<tr>
<td>binary cross entropy</td>
<td>典型选择：二元分类</td>
</tr>
<tr>
<td>cross entropy</td>
<td>典型选择：多元分类</td>
</tr>
<tr>
<td>mse</td>
<td>典型选择：线性回归</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Softmax-Loss"><a href="#Softmax-Loss" class="headerlink" title="Softmax Loss"></a>Softmax Loss</h1><p>&emsp;&emsp;Softmax Loss 是由 softmax 和 cross entropy loss 组合而成，在 pytorch，caffe，tensorflow 等开源框架的实现中，直接将二者合并在一层。如在 pytorch 中不需要再输出层加上 softmax 层用于分类，直接使用 cross entropy loss 即可。</p>
<h1 id="各类代价函数"><a href="#各类代价函数" class="headerlink" title="各类代价函数"></a>各类代价函数</h1><p><a href="https://blog.csdn.net/cqfdcw/article/details/78173839" target="_blank" rel="noopener">方差、协方差、标准差、均方差、均方根值、均方误差、均方根误差</a><br><a href="https://www.cnblogs.com/shujuxiong/p/9339916.html" target="_blank" rel="noopener">L1正则和L2正则的比较分析详解</a></p>
<h1 id="各类距离公式"><a href="#各类距离公式" class="headerlink" title="各类距离公式"></a>各类距离公式</h1><p><a href="https://blog.csdn.net/guojingjuan/article/details/50396254" target="_blank" rel="noopener">python 各类距离公式实现</a></p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>英文</th>
<th>公式</th>
<th>别称</th>
</tr>
</thead>
<tbody>
<tr>
<td>残差平方和 SSE</td>
<td>Sum of Squares for Error</td>
<td>SSE = <script type="math/tex">\sum^m_{i=1}(y_i - \hat{y}_i)^2</script></td>
<td>剩余平方和 RSS</td>
</tr>
<tr>
<td>回归平方和 SSR</td>
<td>Sum of Squares for Regression</td>
<td>SSR = <script type="math/tex">\sum^m_{i=1}(\hat{y}_i - \bar{y})^2</script></td>
<td>解释平方和 ESS</td>
</tr>
<tr>
<td>总离差平方和 SST</td>
<td>Sum of Squares for Total</td>
<td>SST = <script type="math/tex">\sum^m_{i=1}(y_i - \bar{y})^2</script></td>
<td>总离差平方和 TSS</td>
</tr>
</tbody>
</table>
</div>
<p>三者之间的关系是 SST = SSR + SSE</p>
<script type="math/tex; mode=display">R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}</script><a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title>管理hexo中的自定义代码</title>
    <url>/%E7%AE%A1%E7%90%86hexo%E4%B8%AD%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E4%BB%A3%E7%A0%81.html</url>
    <content><![CDATA[<h1 id="管理代码"><a href="#管理代码" class="headerlink" title="管理代码"></a>管理代码</h1><p>&emsp;&emsp;最近一直用 next 主题，也在里面加入了许多自定义代码，但是怕以后更新把我的代码覆盖掉。所以打算把代码集中处理一下。<br>&emsp;&emsp;next 主题下的 layout 文件夹存放了大部分布局代码。下面是next主题的 layout 文件夹目录。<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">layout</span><br><span class="line">  ├─_custom</span><br><span class="line">  │  ├─head.swig</span><br><span class="line">  │  ├─<span class="selector-tag">header</span>.swig</span><br><span class="line">  │  └─sidebar.swig</span><br><span class="line">  ├─_macro</span><br><span class="line">  │  ├─menu</span><br><span class="line">  │  └─sidebar.swig</span><br><span class="line">  ├─_partials</span><br><span class="line">  │  ├─head</span><br><span class="line">  │  ├─header</span><br><span class="line">  │  ├─page</span><br><span class="line">  │  ├─post</span><br><span class="line">  │  ├─search</span><br><span class="line">  │  └─share</span><br><span class="line">  ├─_scripts</span><br><span class="line">  └─_third-party</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;其中需要注意的是 <strong>_custom</strong> 文件夹，因为我们要把自定义文件放在这里，其次我们还要注意下 <strong>_partials</strong> 文件夹，里面存放的都是主要的布局代码。由于我们经常会遇到要添加 css、js 文件的需求，所以有必要认识一下 <strong>_partials</strong> 文件夹下的 <strong>head</strong> 和 <strong>header</strong> 文件夹，其中 head 文件夹代表存放一些申明性质的代码，如果我说<code>&lt;head&gt;&lt;/head&gt;</code>这个标签大家应该就都懂了，而 header 文件夹存放的则是网页的头部信息布局，同理<code>&lt;header&gt;&lt;/header&gt;</code>标签就是指网页布局中的头部。<br>&emsp;&emsp;打开 <strong>_custom</strong> 文件夹会发现里面一共有三个文件，分别是：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">_custom</span><br><span class="line">  ├─head.swig</span><br><span class="line">  ├─<span class="selector-tag">header</span>.swig</span><br><span class="line">  ├─siderbar.swig</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这三部分是网页中最重要的三部分，这些文件里面全是空白的，也就是可以供用户自定义。而且这些文件已经在代码中加载了，如果你写上了代码，不需要手动调用，程序会自动调用。程序具体在哪引用了就不说了，太过复杂，有兴趣可以自己去找。<br>&emsp;&emsp;有了这三个自定义文件已经可以解决绝大部分的问题了。</p>
<ul>
<li>如果你想要增加一些 css、js 文件，你可以把代码放入 head.swig，你可能会想我想要在不同的网页加载不同的脚本文件怎么办？其实没关系，你完全可以加入 if 判断语句加载；</li>
<li>如果你想改变 header 的布局或其他属性，可以在 header.swig 中加入代码，；</li>
<li>如果你想改变 siderbar 的布局或其他属性，可以在 siderbar.swig 中加入代码。<em>sidebar 的主要代码在 _macro 文件夹中的 siderbar.swig 中</em>。</li>
<li>如果想要在网页的任意位置增加代码，可以自己创建一份文件，如：my_scripts.swig，然后在想要引用的地方使用代码<code>[% include &#39;_custom/my_scripts.swig&#39; %}</code></li>
</ul>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>假如你自己写代码，代码中使用了 next 中已存在的变量，比如 post 变量中具有文章的大部分属性。那么你是无法在自定义的 swig 文件中使用该变量的，你必须手动地将此变量传入进 swig 文件。例如：<br>定义文件<code>note-tips.swig</code>，使用了 post 变量。在引用此文件时，要加上一点代码：<br><figure class="highlight twig"><table><tr><td class="code"><pre><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="keyword">set</span></span> foo = &#123;post: post&#125; %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="keyword">include</span></span> '../_custom/note-tips.swig' with foo %&#125;</span><span class="xml"></span></span><br></pre></td></tr></table></figure></p>
<p>可能会有点奇怪，为什么不直接写<code>with post</code>？我也不知道为什么，我试过了，只有这样才行。with 的意思是带上这个变量一起传到 swig 模版里。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>Neural Enquire: Learning to Query Tables in Natural Language</title>
    <url>/%E8%AE%BA%E6%96%87/29%E3%80%81Neural%20Enquirer%EF%BC%9ALearning%20to%20Query%20Tables%20in%20Natural%20Language.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;我们提出使用一种神经网络结构结合知识库来回答自然语言（NL）问题。与之前端到端的语义解析器不同，NEURAL ENQUIRER 是完全“神经化”的：它提供查询和 KB 表的分布式表示，并通过一系列可微的操作执行查询。该模型可以通过 end-to-end 和 step-by-step 的监督进行梯度下降训练。在训练期间，查询和 KB 表的表示将与查询执逻辑（query execution logic）一起进行优化。实验表明，该模型可以学习对结构丰富的 KB 表执行复杂的 NL 查询。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
  </entry>
  <entry>
    <title>Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision</title>
    <url>/%E8%AE%BA%E6%96%87/28%E3%80%81Neural%20Symbolic%20Machines%EF%BC%9ALearning%20Semantic%20Parsers%20on%20Freebase%20with%20Weak%20Supervision.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>本文介绍了一种神经符号机（Neural Symbolic Machine, NSM）</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
  </entry>
  <entry>
    <title>论文笔记：Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing</title>
    <url>/%E8%AE%BA%E6%96%87/27%E3%80%81Sequence-to-Action%EF%BC%9AEnd-to-End%20Semantic%20Graph%20Generation%20for%20Semantic%20Parsing.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://mp.weixin.qq.com/s?__biz=MzI2NjkyNDQ3Mw==&amp;mid=2247486979&amp;idx=2&amp;sn=2d95556630820c853f2ca9b2855dd60a&amp;chksm=ea87f6d5ddf07fc3cc8477d3a0cd5142e9191d91ff3161847524c37539b372b306a8f9b700a8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">某篇解析</a>；<a href="http://tongtianta.site/paper/11795" target="_blank" rel="noopener">某篇解析</a><br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1809.00773.pdf" target="_blank" rel="noopener">论文地址</a>。<br>&emsp;&emsp;本文提出一种神经语义分析方法——Sequence-to-Action，将语义分析当做一个端到端的<strong>语义图生成</strong>的过程。我们同时使用了最近语义分析两个有前途的方向，<strong>首先</strong>我们的模型使用了一个语义图来表示一个句子的含义，该语义图与知识库紧密相关（博主注：<strong>即可以将语义图看作是知识库的一个子图</strong>）。<strong>其次</strong>，利用神经网络强大的表示学习和预测能力，提出一种 RNN 模型，能够有效的将句子映射到动作序列，从而生成语义图（博主注：<strong>此动作序列就是指生成语义图的动作，将这些动作看作是一个序列</strong>）。实验表明该方法在 OVERNIGHT 数据集上展现了一流的性能，在 GEO 以及 ATIS 数据集上得到了有一定竞争力的性能。<br>&emsp;&emsp;语义分析旨在<strong>将自然语言句子映射为逻辑形式</strong>（Zelle andMooney, 1996; Zettlemoyer and Collins, 2005;Wong and Mooney, 2007; Lu et al., 2008;Kwiatkowski et al., 2013）。例如“Which states border Texas?”将会被映射为 <em>answer (A, (state (A),nextto (A, stateid ( texas ))))</em>。<br>&emsp;&emsp;语义分析器需要两个函数，一个处理结构预测，另一个处理语义基础。传统的语义解析器通常基于复合语法，如 CCG（Zettlemoyer and Collins, <a href="https://arxiv.org/pdf/1207.1420" target="_blank" rel="noopener">2005</a>, <a href="https://www.aclweb.org/anthology/D07-1071" target="_blank" rel="noopener">2007</a>），DCS（<a href="https://www.aclweb.org/anthology/P11-1060" target="_blank" rel="noopener">Liang et al., 2011</a>）等。不幸的是，设计语法和学习精确的词汇仍是一个挑战，特别是在开放域。而且设计有效的特性往往很困难，它的学习过程也不是端到端的。为了解决上述问题，本文提出了两种有前途的研究方向：<strong>基于语义图</strong>的方法和<strong>基于 seq2seq</strong> 方法。<br>&emsp;&emsp;基于语义图的方法(Reddy et al.,2014, 2016; Bast and Haussmann, 2015; Yih et al.,2015)将句子的含义表示为语义图（即知识库的子图，参考图 1 中的例子）并<strong>将语义分析视为语义图匹配/生成过程</strong>。<strong>与逻辑形式相比，语义图与知识库有着紧密的关系</strong>(Yih et al., 2015), ，与句法结构有许多共性（Reddy et al.,2014）。基于语义图的句法分析的主要挑战是如何有效地构造句子的语义图，目前语义图是通过与模式匹配（Bast and Haussmann, 2015），从依赖树转换（Reddy et al., 2014, 2016），或者通过 staged heuristic search algorithm（Yih et al.,2015）构建的。这些方法都是基于人工设计的构造过程，它们很难处理开放/复杂的情况。<br>&emsp;&emsp;近年来，得益于 RNN 模型有较强的表示能力和预测能力，其在 Seq2Seq 模型上取得了成功，比如机器翻译。许多 Seq2Seq 模型也用于语义分析（Xiaoet al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016），不需要高质量的词典、人工构建的语法和特性。这些模型通过端到端的训练，利用注意力机制（Bahdanauet al., 2014; Luong et al., 2015）学习句子和逻辑形式之间的软对齐。<br>&emsp;&emsp;本文提出了一种新的神经语义分析框架——Sequence-to-Action。它可以同时利用语义图表示的优点和 seq2seq 模型强大的预测能力。具体来说，我们将语义分析建模为一个端到端的语义图生成过程。例如，在图 1 中，我们的模型将通过生成一系列变量[add variable:a，addtype:state，…]来解析“which states border Texas”这句话。为了实现上述目标，我们首先设计了一个动作集，对语义图的生成过程进行编码（包括节点动作：add variable,add entity,add type，边动作：add edg 以及操作动作：argmin,argmax,count,sum 等）然后我们设计了一个 RNN 模型，该模型可以生成一个动作序列来构造句子的语义图。最后，我们在解码过程中合并结构和语义约束来进一步增强解析。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Language to Logical Form with Neural Attention</title>
    <url>/%E8%AE%BA%E6%96%87/26%E3%80%81Language%20to%20Logical%20Form%20with%20Neural%20Attention.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1601.01280.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2016 年。</p>
          </div>
<p>&emsp;&emsp;语义分析的目的是将自然语言映射到机器可解释的有意义表示。传统的方法依赖于高质量的词汇、人工构建的模板以及特定领域或表示的语言特性，本文提出了一种注意力增强的 encoder-decoder 通用模型。将输入的话表示为向量形式，并通过调节输出序列或者树生成逻辑形式（总结来说，就是<strong>将话语转为逻辑形式</strong>，详情请看图 1）。<br>&emsp;&emsp;下图将一句话转为了逻辑形式，不同于以前的方法，它是通过神经网络生成的，而以前的方法依赖于手写的规则。图片取自 <a href="https://www.aclweb.org/anthology/W00-1317" title="Automated construction ofdatabase interfaces: Intergrating statistical and rela-tional learning for semantic parsing" target="_blank" rel="noopener">Tang and Mooney200</a>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/语句转为逻辑形式.jpg" alt="语句转为逻辑形式"></p>
<p>&emsp;&emsp;基于 RNN 的 encoder-decoder 已成功应用于各种 NLP 任务，图 1 中使用了 LSTM，我们的做法是提出了两个变体模型。<strong>第一个模型</strong>将语义解析视为普通的序列转换任务，<strong>第二个模型</strong>配备了层次树解码器，该解码器明确地捕获逻辑形式的组合结构。我们还引入了<strong>注意力机制</strong>，并提出一个识别步骤来<strong>识别很少提到的实体</strong>和<strong>数字</strong>。<br>&emsp;&emsp;对<strong>四个数据集</strong>的实验结果表明，我们的方法在不使用人工设计特征的情况下具有竞争力，并且易于迁移。<br>&emsp;&emsp;我们的工作综合了两种标准研究，即<strong>语义分析</strong>和 <strong>encoder-decoder 架构的神经网络</strong>。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;学习语义解析器的问题引起了广泛的关注，可以追溯到 Woods（1973年）。。。。</p>
<h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>&emsp;&emsp;我们的目标是学习一个模型，将<u><strong>自然语言输入 <script type="math/tex">q = x_1 \dots x_{|q|}</script></strong></u> 映射为其含义的<u><strong>逻辑形式（logical form）表示 <script type="math/tex">a = y_1 \dots y_{|a|}</script></strong></u>。条件概率被分解为：</p>
<script type="math/tex; mode=display">
\begin{align}
    p(a|q) & = \prod^{|a|}_{t=1} p(y_t|y_{<t},q) \tag 1\\
    y_{<t} & = y_1 \dots y_{t-1}
\end{align}</script><p>&emsp;&emsp;我们的模型包含一个编码器和一个解码器，编码器负责将输入的自然语言 q 编码成向量，解码器负责生成 <script type="math/tex">y_1 \dots y_{|a|}</script>。下面将仔细描述。</p>
<h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>&emsp;&emsp;对于普通的 Seq2Seq 任务，使用 LSTM 来计算，如下图所示。<script type="math/tex">h^l_t</script> 代表第 l 层的第 t 个时间步的隐藏层，公式为：</p>
<script type="math/tex; mode=display">
\begin{align}
    h^l_t = \text{LSTM}(h^l_{t-1},h^{l-1}_t) \tag 2
\end{align}</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/Seq2Seq.jpg" alt="Seq2Seq"><br>&emsp;&emsp;在实验中，遵循 <a href="https://arxiv.org/pdf/1409.2329.pdf" title="RECURRENT NEURAL NETWORK REGULARIZATION" target="_blank" rel="noopener">Zaremba et al. 2015</a> 提出的架构。不过，使用其他类型的门控激活函数也是可以的（例如<a href="https://arxiv.org/pdf/1406.1078.pdf" title="Learning phrase representations using RNN encoder-decoder for statistical machine translation" target="_blank" rel="noopener">Cho et al. 2014</a>）。<strong>对于 encoder</strong>，<script type="math/tex">h^0_t = W_qe(x_t)</script>（注：此公式是第 0 层的运算步骤，即输入层）是 RNN 中输入的词向量，<script type="math/tex">W_q \in \mathbb{R}^{n \times |V_q|}</script> 代表输入层的权重值矩阵，e(·) 代表对应 token 的索引。<strong>对于 decoder</strong>，<script type="math/tex">h^0_t = W_ae(y_{t-1})</script> 代表前一个预测词的词向量，其中 <script type="math/tex">W_a \in \mathbb{R}^{n \times |V_a|}</script>。接下来，最后的 LSTM <script type="math/tex">h^L_t</script> 被用于预测 <script type="math/tex">t</script>-th 输出 token，计算公式为：</p>
<script type="math/tex; mode=display">
\begin{align}
    p(y_t|y_t,q) = softmax(W_oh^L_t)^T e(y_t) \tag 3
\end{align}</script><p>&emsp;&emsp;<strong>该公式用于预测每一个 token</strong>。另外补充一点，增加了 “start-of-sequence” <code>&lt;s&gt;</code> 和 “end-of-sequence” <code>&lt;/s&gt;</code>。<br>&emsp;&emsp;该模型总的来说，就是 LSTM 的计算方法，也没什么好说的。</p>
<h2 id="Seq2Tree"><a href="#Seq2Tree" class="headerlink" title="Seq2Tree"></a>Seq2Tree</h2><p>&emsp;&emsp;Seq2Seq 模型有一个<strong>缺点</strong>就是它<strong>忽略了逻辑形式的层次结构</strong>。所以，要改良的话，它需要记住各种辅助信息（比如括号对），以此生成格式良好的输出。如下图 3 所示，是一个层次树 decoder：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/Seq2Tree模型.jpg" alt="Seq2Tree模型"></p>
<p>&emsp;&emsp;Seq2Tree 与 Seq2Seq 的编码器类似，不同的是解码器。Seq2Tree 以自上而下的方式生成逻辑，为了定义树结构，我们定义了一个表示子树的 “nonterminal” <code>&lt;n&gt;</code> 标记。如图 3 所示，将<strong><em>逻辑形式 “lambda $0 e (and (&gt;(departure_time $0) 1600:ti) (from $0 dallas:ci))”</em></strong>预处理为树，方法是<strong>用 nonterminal 替换括号对之间的标记</strong>（token）。<strong>特殊记号 <code>&lt;s&gt;</code> 和 <code>&lt;(&gt;</code> 分别表示序列和 nonterminal 序列的开头</strong>（由于缺少空间，图3中省略了），<strong>记号 <code>&lt;/s&gt;</code> 代表序列结束</strong>。具体步骤是：</p>
<ol>
<li>编码输入值 q；</li>
<li>层次树解码器使用 RNN 在逻辑形式 a（在<strong>任务定义</strong>中已经说明了 q 和 a 的含义）的对应部分的子树中生成 tokens（注意这里的 token 带了 s）；</li>
<li>如果预测的 token 为 <code>&lt;n&gt;</code>，则通过调节 nonterminal 的隐藏向量来解码序列。（博主注：举个例子理解一下：看图 3 的第一层，先是使用 encoder 进行编码，接着开始对逻辑形式进行解码，逻辑形式就是上面的斜体部分。接下来预测到了 token<code>&lt;n&gt;</code> 于是调用 nonterminal 的隐藏向量来进行解码，即生成一棵子树。以此类推，碰到 toekn <code>&lt;n&gt;</code> 就开始解码）</li>
<li>与 Seq2Seq 解码器不同，当前的隐藏状态不仅仅取决于上一个时间步，为了更好地利用 parent nonterminal 的信息，我们引入了一个 parent-feeding 的连接，其中 parent nonterminal 的隐藏向量与输入连接（concatenated）并喂入 LSTM。</li>
</ol>
<p>&emsp;&emsp;再举个例子帮助理解一下，如图 4 所示。逻辑形式为 <strong><em>A B (C)</em></strong>，其中 <script type="math/tex">y_1 \dots y_6</script> 代表不同的时间步，<strong><em>(C)</em></strong> 对应子树。解码一共有<strong>两个步骤</strong>：一旦输入值 q 被编码，首先在深度为 1 处生成 <script type="math/tex">y_1 \dots y_4</script>，直到 token <code>&lt;/s&gt;</code> 被预测到；接下来通过调节 nonterminal <script type="math/tex">t_3</script> 的隐藏向量来生成 <script type="math/tex">y_5, y_6</script>，<script type="math/tex">p(a|q)</script> 的概率是这<strong>两个序列解码步骤</strong>的乘积：</p>
<script type="math/tex; mode=display">
p(a|q) = p(y_1 y_2 y_3 y_4 | q) p(y_5 y_6 | y_{\leq 3},q)</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/一个简单的Seq2Tree的例子.jpg" alt="一个简单的Seq2Tree的例子"></p>
<h2 id="Attention-机制"><a href="#Attention-机制" class="headerlink" title="Attention 机制"></a>Attention 机制</h2><p>&emsp;&emsp;<strong><em>Attention 的原理</em></strong>。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>&emsp;&emsp;我们的目标是<strong>最大化</strong>由自然语言语句作为输入时产生的逻辑形式的可能性，所以目标函数为：</p>
<script type="math/tex; mode=display">
\text{minimize} - \sum_{(q,a) \in D} logp(a|q)</script><p>&emsp;&emsp;其中 <script type="math/tex">D</script> 是所有自然语言逻辑形式训练对的集合，<script type="math/tex">p(a|q)</script> 按式（1）计算。采用 <strong>RMSProp</strong> 算法解决了这一非凸优化问题。此外，使用 <strong>Dropout</strong> 进行正则化。</p>
<h2 id="推论"><a href="#推论" class="headerlink" title="推论"></a>推论</h2><p>&emsp;&emsp;暂时略。</p>
<h2 id="参数识别"><a href="#参数识别" class="headerlink" title="参数识别"></a>参数识别</h2><p>&emsp;&emsp;大多数的语义分析数据集都是为问答开发的。在经典的系统中，问题被映射乘逻辑形式，并在知识库中获取答案。由于问答任务的性质，许多自然语言的语句都包含实体或数字，它们通常被解析为逻辑形式的参数。其中不可避免地会有一些罕见或者根本不会出现在数据集中的实体或数字（对于小规模数据集尤其如此）。传统的序列编码器只是简单地用一个特殊的位置单词符号替换稀有单词（<a href="https://arxiv.org/pdf/1410.8206.pdf" title="Addressing the Rare Word Problem in Neural Machine Translation" target="_blank" rel="noopener">Luong et al. 2015a</a>; <a href="https://arxiv.org/pdf/1412.2007.pdf" title="On Using Very Large Target Vocabulary for Neural Machine Translation" target="_blank" rel="noopener">Jean et al. 2015</a>），这对语义分析是有害的。<br>&emsp;&emsp;为此开发了一个简单的参数识别程序。具体来说就是在输入的问题中标识实体和数字，并用它们的<strong>类型</strong>和<strong>唯一 id</strong> 替换它们。例如，将训练样本“<em>jobs with a salary of 40000</em>”及其逻辑形式“job(ANS), salary_greater_than(ANS,40000, year)”预处理为“jobs with a salary of <em><script type="math/tex">num_0</script></em>”和“job(ANS), salary_greater_than(<em>ANS</em>,<em><script type="math/tex">num_0</script></em>,<em>year</em>)”。一旦解码完毕，后处理步骤就会将所有标记 <script type="math/tex">type_i</script> 恢复到它们以前的实体或数字。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;我们将我们的方法在四个数据集上分别与以前的多个系统进行比较，下面将描述这些数据集。代码可在此处获得<a href="https://github.com/donglixp/lang2logic" target="_blank" rel="noopener">https://github.com/donglixp/lang2logic</a>（lua 版，官方），<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch" target="_blank" rel="noopener">https://github.com/Alex-Fabbri/lang2logic-PyTorch</a>（python 版，非官方）。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>&emsp;&emsp;<strong>JOBS</strong>    工作<br>&emsp;&emsp;<strong>GEO</strong>        Gene Expression Omnibus（基因表达综合）<br>&emsp;&emsp;<strong>ATIS</strong>    Airline Travel Information System（航空旅行信息系统）<br>&emsp;&emsp;<strong>IFTTT</strong>    if this then that（<a href="https://ifttt.com/" target="_blank" rel="noopener">地址</a> <a href="https://baike.baidu.com/item/ifttt/8378533" target="_blank" rel="noopener">百度百科介绍</a>），<a href="https://www.aclweb.org/anthology/P15-1085" target="_blank" rel="noopener">Quirk et al.2015</a> 从 IFTTT 网站提取大量的 if-this-then-that 来创建此数据库<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Language to Logical Form with Neural Attention/数据集介绍.jpg" alt="数据集介绍"></p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>&emsp;&emsp;自然语言语句是小写的，并且使用基于维基百科的常见拼写错误列表来纠正拼写错误。使用 NLTK 来限制词汇（[Bird et al.2009] Natural Language Processing with Python. O’Reilly Media.），对于 IFTTT 过滤了在训练集中出现少于五次的 token，channels 和 functions。对于其他数据集，过滤了在训练集中至少两次没有出现的输入词，但保留了逻辑形式中的所有 token。并且使用了<strong>参数识别</strong>，当然也可以使用更复杂的办法。<br>&emsp;&emsp;超参数在 JOBS 和 GEO 上使用了交叉验证，使用了 ATIS 和 IFTTT 作为标准开发集（就是验证集，不同的叫法而已 development/validation）。</p>
<ul>
<li><strong>RMSProp</strong>：batch size = 20；parameter = 0.95；</li>
<li><strong>梯度修剪</strong>为 5 以缓解梯度爆炸（<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">Pascanu et al.2013</a>）；</li>
<li><strong>参数</strong>从均匀分布 <script type="math/tex">U(-0.08, 0.08)</script> 中随机初始化；</li>
<li>两层 <strong>LSTM</strong> 用于 IFTTT，单层 LSTM 用于其他数据集；</li>
<li><strong>dropout</strong> <script type="math/tex">\in</script> {0.2,0.3,0.4,0.5}；</li>
<li>隐藏向量和词嵌入<strong>维度</strong>从 {150, 200, 250} 选择；</li>
<li><strong>early stopping</strong>；</li>
<li>输入句子在进入编码器之前被反转（<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever et al.2014</a>）；</li>
<li><strong>贪婪搜索</strong>生成逻辑形式；</li>
<li><strong>softmax</strong> 用于分类。</li>
</ul>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>&emsp;&emsp;Attention 机制可以提高性能，对于小数据集<strong>参数识别</strong>至关重要。</p>
<h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><h1 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h1><p>&emsp;&emsp;<a href="http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf" title="Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars" target="_blank" rel="noopener">论文</a>使用 Logical Form 对基准数据集做实验，数据集包括 Geo880 和 Jobs640，论文中使用的是 Logical Form 的其中一种表示——<strong>PCCG</strong>，它是 CCG 的改进版。他们将数据集分割为训练集和测试集，Language to Logical Form with Neural Attention 沿用了此分割方式（比如说将 GEO 分割为 680 个训练样本，200 个测试样本），并且采用此论文的思想，即：将自然语言映射为 Logical Form。<br>&emsp;&emsp;虽然 PCCG 的 Logical Form 效果不错，但是作者没有使用他，而是使用了 <a href="https://www.aclweb.org/anthology/D11-1140" target="_blank" rel="noopener">lambda-caculus</a>。<strong>作者将 Geo880 等数据集改写为了 lambda-calculus 的形式</strong>。<em>在<a href="https://github.com/yuxuan1995liu/Semantic-Parsing-Data-Pre-Processing" target="_blank" rel="noopener">此处</a>可找到全部数据，但是这里面的格式不是 lambda-calculus。我有点搞不懂他提供的数据到底是什么意思</em>。<strong>19.09.16 补充</strong>：经过多方查找，终于找到了 geo880 最初的<a href="https://link_springer.gg363.site/content/pdf/10.1007/3-540-44795-4_40.pdf" title="Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing" target="_blank" rel="noopener">论文</a>，在<a href="https://www.cs.utexas.edu/~ml/publications/year/2001" target="_blank" rel="noopener">此处</a>找到的。GEO880 最初版本并不是 lambda calculus。<br>&emsp;&emsp;作者将数据改写为 lambda-calcullus 形式是我估计的。因为全文找不到数据的来源，格式转换的说明也找不到。只是在 Section 4.1 Datasets 中说到：</p>
<blockquote>
<p>&emsp;&emsp;<strong>GEO 有 880 个示例，将其分割为 680 个训练样本以及 200 个测试样本（Zettlemoyer and Collins, 2005）， 我们使用了基于 lambda-calculus 的具有相同含义的表示</strong>。</p>
</blockquote>
<p>&emsp;&emsp;所以我推测作者应该是将原本的 PCCG 表示的 GEO 改成了 lambda-calculus 表示。<br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1805.04793" title="Coarse-to-Fine Decoding for Neural Semantic Parsing" target="_blank" rel="noopener">论文</a>是作者对 Language to Logical Form with Neural Attention 的改进版。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Large-scale Simple Question Answering with Memory Network</title>
    <url>/%E8%AE%BA%E6%96%87/5%E3%80%81Large%20scale%20Simple%20Question%20Answering%20with%20Memory%20Network.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1506.02075v1.pdf" target="_blank" rel="noopener">论文地址</a>，发表于 2015 年。<br>&emsp;&emsp;开放域问答系统的目的是在不受域限制的情况下，为用自然语言表达的问题提供准确的答案。问答系统有很长的历史，它们搜索文本文档或在网络上提取答案（see e.g.(Voorhees and Tice, 2000; Dumais et al., 2002)）。最近在公开的大型知识库（KBS）方面也取得了进展，如 Freebase 知识库。然而，尽管最近大家都在关注设计一个具有推理能力的系统，它可以检索并使用 KB 中的<strong>多重事实</strong>进行问答。但是其实只涉及 <strong>KB 中单个事实的简单问答</strong>都还没被解决，本论文中将其称为 Simple Question Answering。<br>&emsp;&emsp;KBQA 现存的方法：1）将 question 转为结构化的 KB 查询语句（Berant et al. 2013）；或者 2）学习将 question 以及 facts 嵌入到低维向量空间中，然后在这些向量中通过计算相似度检索答案（<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a</a>）。<br>&emsp;&emsp;本文贡献有二：</p>
<ul>
<li>其一，为了<strong>研究现有系统</strong>以及<strong>通过多任务学习在不同数据源上同时训练</strong>成为可能，我们收集了第一个基于知识库的大规模的问答数据集，称为 SimpleQuestions。包含了人类编写和 Freebase facts 相关的超过 10 万个问题，另外现有的基准数据集 WebQuestions 包含的问题少于 6 千个，这些问题是使用 google suggest api 自动创建的。</li>
<li>其二，提出了一种基于词嵌入的问答系统，在 Memory Networks (MemNNs)（<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Weston et al., 2015</a>;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">Sukhbaatar et al., 2015</a>） 框架下开发而成。</li>
</ul>
<p>&emsp;&emsp;虽然我们的模型与之前的 QA 嵌入模型（<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a</a>;<a href="https://arxiv.org/pdf/1404.4326.pdf" target="_blank" rel="noopener">Bordes et al., 2014b</a>）相似，但使用 MemNNs 的框架为未来工作中更复杂的推理方案提供了思路，因为 MemNNs 在复杂的推理问答任务上表现了很好的性能（<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="noopener">Weston et al., 2015</a>）。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><ol>
<li>Sections 3, 4：介绍了基于词嵌入的问答系统；</li>
<li>Section 5：相关工作；</li>
<li>Section 6：实验结果。</li>
</ol>
<h1 id="Memory-Network-for-Simple-QA"><a href="#Memory-Network-for-Simple-QA" class="headerlink" title="Memory Network for Simple QA"></a>Memory Network for Simple QA</h1><p>&emsp;&emsp;Memory network 由一个 memory（一个索引对象数组）和一个神经网络组成。神经网络由 Input map(I), Generalization(G), Output map(O) and Response(R) 构成。其工作流如下所示：</p>
<ol>
<li>Storing Freebase：第一阶段。解析 Freebase（可以是 FB2M 或 FB5M，取决于配置）并且将它存进 memory。它使用 Input module 去预处理数据；</li>
<li>Training：第二阶段。训练 MemNN 去回答问题。此处使用 Input, Output and Response modules，训练主要关注核心 Output module 嵌入模型的参数；</li>
<li>Connecting Reverb：第三阶段。将来自 Reverb 的 new facts 添加到 memory 中。这是在训练完毕后进行的，为了测试 MemNNs 在不需要重新训练的情况下处理 new facts 的能力。它使用 Input module 去预处理 Reverb facts 并且使用 Generalization module 将它们和已经被存储的 facts 连接。</li>
</ol>
<h2 id="Input-module"><a href="#Input-module" class="headerlink" title="Input module"></a>Input module</h2><p>&emsp;&emsp;此组件预处理 3 种类型的数据，它们会被输入进神经网络：</p>
<ol>
<li>Freebase facts：用于填充 memory；</li>
<li>questions：系统需要回答的问题；</li>
<li>Reverb facts：在 workflow 第二阶段中，我们用它扩展 memory。</li>
</ol>
<h3 id="preprocessing-Freebase"><a href="#preprocessing-Freebase" class="headerlink" title="preprocessing Freebase"></a>preprocessing Freebase</h3><p>&emsp;&emsp;Freebase 数据最初存储原子 facts，包括将单个实体作为主语或者宾语，再在它们之间加上一个联系（即谓语）。<strong>但是这样的存储需要从两个方面与 QA 任务适应</strong>。</p>
<ol>
<li>为了回答不止有一个答案的问题，我们将 fact 重新定义为一个三元组，其包含 subject，relationship 以及通过 relationship 连接至 subject 的一组 objetcs 。这个分组过程将 atomic facts 转为 grouped facts，以下将其简单的称为 facts。Table 2 显示了这样分组可以减少 facts 的数量。</li>
<li></li>
</ol>
<h3 id="Preprocessing-Freebase-facts"><a href="#Preprocessing-Freebase-facts" class="headerlink" title="Preprocessing Freebase facts"></a>Preprocessing Freebase facts</h3><p>&emsp;&emsp;</p>
<h3 id="Preprocessing-questions"><a href="#Preprocessing-questions" class="headerlink" title="Preprocessing questions"></a>Preprocessing questions</h3><h3 id="Preprocessing-Reverb-facts"><a href="#Preprocessing-Reverb-facts" class="headerlink" title="Preprocessing Reverb facts"></a>Preprocessing Reverb facts</h3><h2 id="Generalization-module"><a href="#Generalization-module" class="headerlink" title="Generalization module"></a>Generalization module</h2><p>&emsp;&emsp;此模块负责将新的元素增加到 memory 中。在我们的例子中，memory 具有一个 multigraph 结构，其中每个节点都是 Freebase 的一个实体，multigraph 中被标记的 arcs 是 Freebase 中的 relationships：预处理之后，所有 Freebase 的 facts 都使用此结构存储。<br>&emsp;&emsp;<br>&emsp;&emsp;为了将 Reverb 的 subject 和 object 链接到 Freebase 实体，我们使用 precomputed entity links (<a href="https://www.aclweb.org/anthology/W12-3016.pdf" target="_blank" rel="noopener">Lin et al., 2012</a>)。。。。</p>
<h2 id="Output-module"><a href="#Output-module" class="headerlink" title="Output module"></a>Output module</h2><p>&emsp;&emsp;Output 模块通过给定 input ，在 memory 中执行查表（lookup）操作，返回该问题的 supporting facts。在我们的 simple QA 例子中，此模块只返回一个 supporting fact。为了避免为所有存储的 facts 评分（即为了避免时间代价太大），我们先执行一步<em>近似实体链接</em>（proximate entity linking），以生成一个小的候选 facts 集合。最后， supporting fact 指的是与嵌入模型中的问题最相似的候选 fact。</p>
<h3 id="Candidate-generation"><a href="#Candidate-generation" class="headerlink" title="Candidate generation"></a>Candidate generation</h3><p>&emsp;&emsp;略。</p>
<h3 id="Scoring"><a href="#Scoring" class="headerlink" title="Scoring"></a>Scoring</h3><p>&emsp;&emsp;略。</p>
<h2 id="Response-module"><a href="#Response-module" class="headerlink" title="Response module"></a>Response module</h2><p>&emsp;&emsp;在 memory network 中，Response 模块对 Output 模块的结果进行后处理操作，以计算预期的答案。在我们的例子中，它返回被挑选出来的 supporting fact 的对象集（博主注：这个对象集我猜测是 KG 中的三元组）。<br>&emsp;&emsp;注：不必纠结 Response 模块的具体功能，可以自己定制，必然在<a href="https://zhuanlan.zhihu.com/p/29590286" target="_blank" rel="noopener">记忆网络之Memory Networks</a>中写到最初的 memory network 的 response 模块只是简单地将向量转成单词。</p>
<h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/liuchonge/article/details/78128238" target="_blank" rel="noopener">记忆网络之open-domain QA 应用</a>，csdn 的一篇博客，也对此论文的训练方法做了总结。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge</title>
    <url>/%E8%AE%BA%E6%96%87/24%E3%80%81An%20End-to-End%20Model%20for%20Question%20Answering%20over%20Knowledge%20Base%20with%20Cross-Attention%20Combining%20Global%20Knowledge.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><div class="note info">
            <p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P17-1021" target="_blank" rel="noopener">论文地址</a>，发表于 2017 年。</p>
          </div>
<p>&emsp;&emsp;随着知识库数量的增加，人们越来越希望寻找到一些有效的方法来获取这些资源。现在有几种专门为<strong>查询 KBs</strong> 设计的<strong>语言</strong>：SPARQL（<a href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/" target="_blank" rel="noopener">rudhommeaux and Seaborne, 2008</a>）。但要使用这些语言，用户不仅需要熟悉它们，还要了解 KBs 的体系结构。相比之下，<strong>以自然语言为查询语言</strong>的 KB-QA 是一种更友好的方案，近年来已成为研究热点。这项任务<strong>以前</strong>有两个主流的研究方向：</p>
<ol>
<li>基于语义解析（semantic parsing-base, SP-based）</li>
<li>基于信息检索（information  retrieval-based, IR-based）</li>
</ol>
<p>&emsp;&emsp;<strong>现在</strong>随着神经网络方法的发展，基于神经网络的 KB-QA 已经取得了令人瞩目的成果。其中至关重要的步骤就是计算<strong>问题和候选答案</strong>之间的相似性分数，这一步骤的<strong>关键一点</strong>就是学习它们的表示。然而以往的研究更注重<strong>答案的学习表示</strong>。例如，<a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al. 2014a</a> 考虑候选答案子图的重要性，<a href="https://www.aclweb.org/anthology/P15-1026" target="_blank" rel="noopener">Dong et al. 2015</a>利用上下文和答案的类型。无论如何，<strong>问题的表示</strong>终究还是表达不全。现有的方法 <a href="https://arxiv.org/pdf/1406.3676.pdf" target="_blank" rel="noopener">Bordes et al., 2014a,</a> <a href="https://arxiv.org/pdf/1404.4326.pdf" target="_blank" rel="noopener">b</a> 使用 bag-of-word 模型将问题表示为一个向量，但是这样<strong>问题与答案的关联性</strong>还是被忽视了。我们认为一个问题应该根据回答时不同的侧重面来表示（注：<em>其实就是想用注意力机制</em>，回答的侧重面可以是答案实体本身、答案类型、答案上下文等）。<br>&emsp;&emsp;因此本文提出了一个端到端的神经网络模型，通过 <strong>cross-attention</strong> 机制，根据不同的候选答案动态地表示问题及对应的分数。此外还利用了 KB 中的全部知识，旨在将 KB 中丰富的知识集成到答案中，以此缓解 out-of-vocabulary(<strong>OOV</strong>) 的问题，从而帮助 cross-attention 更精确地表示问题。最后实验结果表明了该方法确实有效。<br>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P15-1026" title="Question Answering over Freebase wit hMulti-Column Convolutional Neural Networks" target="_blank" rel="noopener">论文</a>（<a href="https://yan624.github.io/论文/23、Question Answering over Freebase with Multi-Column Convolutional Neural Networks.html">论文笔记地址</a>）中的方法很有启发性，但是由于简单地选择三个独立的 CNN ，因此过于机械化。所以我们使用了基于 cross-attention 的神经网络模型。<br>&emsp;&emsp;模型架构如下，步骤与之前的论文的步骤类似。<strong>1)</strong>先找到问题的主题（main entity/topic entity）；<strong>2)</strong>然后在知识库中找到主题相连的节点作为候选答案，<strong>3)</strong>最后送入 score layer 进行评分，排序分数选出分数最高的候选答案作为正确答案。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge/MCCNN总览.jpg" alt="MCCNN总览"></p>
<p>&emsp;&emsp;为了方便描述，我们将任何一种基本元素称为资源（resource），无论是实体还是关系。比如 (/m/0f8l9c,location.country.capital,/m/05qtj) 的描述是法国的首都是巴黎，其中的 <em>/m/0f8l9c</em> 和 <em>/m/05qtj</em> 分别代表法国和巴黎，<em>location.country.capital</em> 是一种关系。</p>
<h1 id="我们的方法"><a href="#我们的方法" class="headerlink" title="我们的方法"></a>我们的方法</h1><h2 id="候选者生成"><a href="#候选者生成" class="headerlink" title="候选者生成"></a>候选者生成</h2><p>&emsp;&emsp;略，我已经写过无数遍了。使用 Freebase API 构建的。</p>
<h2 id="The-Neural-Cross-Attention-Model"><a href="#The-Neural-Cross-Attention-Model" class="headerlink" title="The Neural Cross-Attention Model"></a>The Neural Cross-Attention Model</h2><p>&emsp;&emsp;下图是模型的架构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge/MCCNN架构.jpg" alt="MCCNN架构"></p>
<ul>
<li>问题表示（图 2 中左侧部分显示了处理步骤）<ol>
<li>使用向量表示问题中的每个单词，这跟其他 NLP 任务差不多，不过它是随机初始化的词嵌入矩阵 <script type="math/tex">E_w \in \mathbb{R}^{d \text{x} v_w}</script>，然后取出对应单词的词向量。d 代表词向量的维度，<script type="math/tex">v_w</script> 代表词表的大小。</li>
<li>将词向量送入 LSTM，值得注意的是我们没有使用单向 LSTM，因为这样一个单词表示只会捕获到之前的单词的信息而不会包含之后的单词。为此我们使用了双向 LSTM 外加 Bahdanau（<a href="https://arxiv.org/pdf/1409.0473.pdf" title="Neural machine translation by jointly learning to align and translate" target="_blank" rel="noopener">Bahdanau, 2014</a>） attention 的处理；</li>
<li>这样就会获得两个表示 <script type="math/tex">(\overrightarrow{h_1}, \overrightarrow{h_2}, \dots, \overrightarrow{h_n})</script> 以及 <script type="math/tex">(\overleftarrow{h_1}, \overleftarrow{h_2}, \dots, \overleftarrow{h_n})</script>，然后将两个表示拼接起来组成 [<script type="math/tex">\overrightarrow{h_i};\overleftarrow{h_i}</script>]，正反向 LSTM 单元的大小都是 <script type="math/tex">\frac{d}{2}</script>。</li>
</ol>
</li>
<li>回答的不同侧面表示（图 2 中右侧下方部分）<ol>
<li>直接使用 KB 的嵌入矩阵 <script type="math/tex">E_k \in \mathbb{R}^{d \text{x} v_k}</script>，其中 <script type="math/tex">v_k</script> 代表知识库中资源的大小，该嵌入矩阵随机初始化并在训练时学习表示，使用全局信息对表示的进一步提高将在 3.3 节 Combining Global Knowledge（原论文）描述。具体来说我们使用回答的四个方面：问答实体 <script type="math/tex">a_e</script>，回答关系 <script type="math/tex">a_r</script>，回答类型 <script type="math/tex">a_t</script>，回答上下文 <script type="math/tex">a_c</script>。它们的嵌入被分别表示为 <script type="math/tex">e_e</script>, <script type="math/tex">e_r</script>, <script type="math/tex">e_t</script>, <script type="math/tex">e_c</script>；</li>
<li>值得注意的是问答上下文由多个 KB 资源组成，我们将它们定义为 (<script type="math/tex">c_1, c_2, \dots, c_m</script>)，首先获得它们的嵌入 (<script type="math/tex">e_{c_1}, e_{c_2}, \dots, e_{c_m}</script>)，然后计算它们的平均值 <script type="math/tex">e_c = \frac{1}{m} \sum^m_{i=1} e_{c_i}</script></li>
</ol>
</li>
<li>Cross-Attention model（图 2 中右侧上方部分以及最上方部分），详见 3.2.3 Cross-Attention model</li>
</ul>
<h2 id="Combining-Global-Knowledge"><a href="#Combining-Global-Knowledge" class="headerlink" title="Combining Global Knowledge"></a>Combining Global Knowledge</h2><p>&emsp;&emsp;Combining Global Knowledg，利用TransE得到knowledge embedding。</p>
<h1 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h1><ol>
<li>使用了 Bahdanau Attention 处理；</li>
<li>使用了双向 LSTM，会得到两个向量，最后将这两个向量拼接在一起，就是 BiLSTM 这层的最终向量。另外正反的 LSTM 的长度都是 <script type="math/tex">\frac{d}{2}</script>；</li>
<li>回答通过问答实体 <script type="math/tex">a_e</script>，回答关系 <script type="math/tex">a_r</script>，回答类型 <script type="math/tex">a_t</script>，回答上下文 <script type="math/tex">a_c</script> 四个方面来表示，其中 ac 是所有词向量的平均值。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li><a href="https://arxiv.org/pdf/1404.4326" title="Open question answering with weakly supervised embedding models" target="_blank" rel="noopener">Antoine Bordes 等 2014b</a>；</li>
<li><a href="https://arxiv.org/pdf/1406.3676" title="Question Answering with Subgraph Embeddings" target="_blank" rel="noopener">Antoine Bordes 等 2014a</a>；</li>
<li><a href="https://www.aclweb.org/anthology/P14-2105" title="Semantic Parsing for Single-Relation Question Answering" target="_blank" rel="noopener">Yih W 等 2014</a>，实际上是基于语义解析的，但是用了词向量；</li>
<li><a href="https://www.aclweb.org/anthology/D14-1071" title="Joint relational embeddings for knowledge-based question answering" target="_blank" rel="noopener">Min-Chul Yang 等 2014</a>，实际上是基于语义解析的但是用了词向量；</li>
<li><a href="https://www.aclweb.org/anthology/P15-1026" title="Question Answering over Freebase with Multi-Column Convolutional Neural Networks" target="_blank" rel="noopener">Dong 等 2015</a>，这篇是跟我们的文章最相近的（使用了 CNN 而非 RNN + Attention）；</li>
<li><a href="https://www.aclweb.org/anthology/C16-1226" title="Hybrid Question Answering over Knowledge Base and Free Text" target="_blank" rel="noopener">Kun Xu 等 2016b</a>；<a href="https://arxiv.org/pdf/1603.00957.pdf" title="Question Answering on Freebase via Relation Extraction and Textual Evidence" target="_blank" rel="noopener">Xu K 等 2016a</a>。</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Question Answering over Freebase with Multi-Column Convolutional Neural Networks</title>
    <url>/%E8%AE%BA%E6%96%87/23%E3%80%81Question%20Answering%20over%20Freebase%20with%20Multi-Column%20Convolutional%20Neural%20Networks.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/P15-1026" target="_blank" rel="noopener">论文地址</a>，发表于 2015 年。<br>&emsp;&emsp;大多数现有的系统通常依靠人工制作的特性和规则来进行<em>问题理解</em>以及<em>答案排序</em>。此外，一些方法（<a href="https://arxiv.org/pdf/1406.3676.pdf" title="Question Answering with Subgraph Embeddings" target="_blank" rel="noopener">Bordes et al., 2014a</a>; <a href="https://arxiv.org/pdf/1404.4326.pdf" title="Open Question Answering with Weakly Supervised Embedding Models" target="_blank" rel="noopener">Bordeset al., 2014b</a>）使用问题的词嵌入的总和来表示问题，但是这忽略了<strong>词序信息</strong>，无法处理复杂问题，例如 who killed A 和 who A killed 两个问题的表示是一样的。本文介绍了 multi-column convolutional neural networks (MCCNNs)，从三个方面（<strong>回答路径（Answer Type），回答上下文（Answer Context），回答类型（Answer Path）</strong>）理解问题。使用 Freebase 作为知识库，在 WebQuestions 数据集上进行了广泛的实验。最终表明，此方法拥有更好的性能。<br>&emsp;&emsp;神经网络训练步骤：</p>
<ol>
<li>MCCNNs 从输入的问题中使用不同 column networks 去提取<strong>回答路径，回答上下文，回答类型</strong>。跟 Bordes 的论文一样，该论文知识库（本文就是 FreeBase）中的实体和关系也由向量表示。</li>
<li>然后评分层（score layer）根据问题和候选答案的表示进行排序（点积）。</li>
</ol>
<h1 id="处理步骤"><a href="#处理步骤" class="headerlink" title="处理步骤"></a>处理步骤</h1><p>&emsp;&emsp;给定一个自然语言问题 <script type="math/tex">q = w_1 \dots w_n</script>，从 FreeBase 中检索相应的实体和属性，然后将它们作为候选答案 <script type="math/tex">C_q</script>。比如，问题 <em>when did Avatar release in UK</em> （阿凡达在英国的发行时间）的答案是 <em>2009-12-17</em>。需要注意的是对于该问题也许有一系列的正确答案。以下数据将被使用到：<strong>WebQuestions</strong>，<strong>FreeBase</strong>，<strong>WikiAnswers</strong>。<br>&emsp;&emsp;MCCNN 概览如图 1 所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Question Answering over Freebase with Multi-Column Convolutional Neural Networks/MCCNN概览.jpg" alt="MCCNN概览"></p>
<p>&emsp;&emsp;比如说，对于问题 whendid Avatar release in UK，从 FreeBase 中查询 <strong>Avatar</strong>（可以称为 <strong>main entity</strong> 或者 <strong>topic entity</strong>） 的<strong>相连节点</strong>（related nodes），这些相连节点被认为是候选答案（<script type="math/tex">C_q</script>）。然后对于每个候选答案 a，模型将会预测一个分数 S(q,a) 以判断 a 是否为正确答案。<br>&emsp;&emsp;对于问题的三个侧面的向量表示分别以 <script type="math/tex">f_1(q)</script> <script type="math/tex">f_2(q)</script> <script type="math/tex">f_3(q)</script> 表示，同理答案的三个侧面分别以 <script type="math/tex">g_1(a)</script> <script type="math/tex">g_2(a)</script> <script type="math/tex">g_3(a)</script> 表示。<script type="math/tex">f_i(q)</script> 和 <script type="math/tex">g_i(a)</script>拥有相同的维度。使用这些问答的表示，我们可以计算问答对 (q,a) 的分数。具体来说，评分函数 S(q,a) 定义为（如图 1 所示，评分层计算分数并将其加起来）：</p>
<script type="math/tex; mode=display">
S(q,a) = \underbrace{f_1(q)^Tg_1(a)}_{\text{answer path}} + \underbrace{f_2(q)^Tg_2(a)}_{\text{answer context}} + \underbrace{f_3(q)^Tg_3(a)}_{\text{answer type}}</script><h2 id="候选者生成"><a href="#候选者生成" class="headerlink" title="候选者生成"></a>候选者生成</h2><p>&emsp;&emsp;训练神经网络的<strong>第一步是</strong>从 FreeBase 中为问题检索候选答案。用户提出的问题应该包含一个<strong>可识别</strong>的实体，该实体与知识库相连。我们使用 <strong>Freebase Search API</strong>（<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf" title="Freebase: a collaboratively created graph database for structuringhuman knowledge" target="_blank" rel="noopener">Bollacker et al., 2008)</a>） 查询问题中的命名体。如果没有任何命名体，则查询名词短语，我们使用调用 API 返回的列表中的第一个实体。这个实体解决办法也被 <a href="https://www.aclweb.org/anthology/P14-1090" title="Information Extraction over Structured Data: Question Answering with Freebase" target="_blank" rel="noopener">Yao and Van Durme, 2014)</a> 使用，还可以研发更好的办法，但不是本论文的关注点。<strong>最后关联实体的所有 2-hops（应该是周围的意思，我没有查到是什么意思，但是在<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html#论文总结">博客笔记</a>中有所总结） 节点被认为是候选答案</strong>。并把问题 q 的候选答案集合称为 <script type="math/tex">C_q</script>。</p>
<h2 id="MCCNNs-for-Question-Understanding"><a href="#MCCNNs-for-Question-Understanding" class="headerlink" title="MCCNNs for Question Understanding"></a>MCCNNs for Question Understanding</h2><p>&emsp;&emsp;MCCNNs 使用多列（<strong>列</strong>指的是图 1 中左侧那三片）卷积网络从字嵌入中学习不同方面。使用 <a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf" title="Natural Language Processing (Almost) from Scratch" target="_blank" rel="noopener">Collobert R 等 2011</a> 的方法解决语言长度不一的问题。具体的做法可参考原论文 <strong>4.2 MCCNNs for Question Understanding</strong>。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&emsp;&emsp;未来的探索方向：</p>
<ol>
<li>整合更多的外部知识源，如clueweb；</li>
<li>以多任务学习方式训练MCCNN；</li>
<li>由于我们的模型能够检测到问题中最重要的单词，因此使用结果挖掘有效的问题模式将是非常有趣的。</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Joint Relational Embeddings for Knowledge-based Question Answering</title>
    <url>/%E8%AE%BA%E6%96%87/22%E3%80%81Joint%20Relational%20Embeddings%20for%20Knowledge%20based%20Question%20Answering.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.aclweb.org/anthology/D14-1071" target="_blank" rel="noopener">论文地址</a>，发表于 2014 年。<br>&emsp;&emsp;将自然语言（natural language，NL）问题转换为对应的逻辑形式（logical form，LF）是基于知识库问答（KB-QA）任务的核心任务，转换问题也被称作语义分析。<del>在 KB-QA 任务领域，与以往（Mooney, 2007; Liang et al., 2011;Cai and Yates, 2013; Fader et al., 2013; Berant etal., 2013; Bao et al., 2014）<u>基于词汇化短语（lexicalized phrases）和逻辑谓语（logical predicates）之间的映射作为词汇触发器（lexical trigger）来执行语义分析中的转换任务</u>不同（其中 Fader 2013 提出的论文在<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html">论文笔记1</a>和<a href="https://yan624.github.io/论文/21、Question Answering with Subgraph Embeddings.html">论文笔记2</a>中具有提及，ctrl f 之后搜索 <em>Paraphrase-Driven Learning for Open Question Answering</em> 或者 <em>Fader</em> 即可找到对应位置）</del>，本论文进一步提出了一种<strong>将 NL 问题映射到 LFs 中</strong>的新的<strong>嵌入式</strong>方法，其利用<strong>词汇表达</strong>与 <strong>KB 中的属性</strong>在隐含空间中的语义关联来实现。实验表明，在两个公开的 QA 数据集上，该方法优于其他三种 KB-QA 的基线方法。<br>&emsp;&emsp;先前工作必须处理以下两种限制：</p>
<ol>
<li>由于逻辑谓语的含义通常具有不同的自然语言表达（natural language expression，NLE）形式，因此从谓语提取的词汇触发器可能有时会受到大小限制；</li>
<li>由于命名体识别（named entity recognition，NER）组件检测到的实体将用于与逻辑谓语一起组成逻辑形式，因此它们的类型也应该与谓语一致。然而，现有的 KB-QA 系统使用的 NER 组件大都独立于 NLE 到谓语的映射步骤。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;一如既往地（我为什么要说一如既往？因为前两篇论文笔记都记录了）说明<strong>语义分析</strong>有多糟糕，需要使用大量的人力，继而只能被限制在特定的领域（以后关于这些劣势都不写了）。<br>&emsp;&emsp;一如既往地描述了 FreeBase。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Question Answering with Subgraph Embeddings</title>
    <url>/%E8%AE%BA%E6%96%87/21%E3%80%81Question%20Answering%20with%20Subgraph%20Embeddings.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1406.3676" target="_blank" rel="noopener">论文地址</a>，发表于 2014 年。<br>&emsp;&emsp;本文的作者在同年发表了另一篇论文，将上一篇论文称为 A，此论文称为 B，对于 A 论文我也做了<a href="https://yan624.github.io/论文/20、Open Question Answering with Weakly supervised Embedding Models.html">论文笔记</a>，本论文是上一篇论文的改进版。A 只是对简单问题进行研究，B 研究如何改进模型并回答更复杂的问题。<br>&emsp;&emsp;开放域问答中的一流技术大致可以分为两大类：1)基于信息检索；2)基于语义解析。<strong>信息检索</strong>系统首先通过 KBs 的搜索 API（转换方式估计是手写模版，论文中未细说） <strong>将问题转换为有效的查询语句</strong>（比如 neo4j 数据库的 CQL）以此检索到大量的候选答案，然后再仔细地识别准确的答案（<a href="https://www.sciencedirect.com/science/article/pii/S0020025511003860" title="A survey on question answering technology from an information retrieval perspective" target="_blank" rel="noopener">Kolomiyets O 等 2011</a>，<a href="https://www2012.universite-lyon.fr/proceedings/proceedings/p639.pdf" title="Template-based Question Answering over RDF Data" target="_blank" rel="noopener">Unger C 等 2012</a>，<a href="https://www.aclweb.org/anthology/P14-1090" title="Information Extraction over Structured Data: Question Answering with Freebase" target="_blank" rel="noopener">Yao X 等 2014</a>）。<strong>语义解析</strong>旨在通过语义分析系统正确<strong>解释</strong>问题的含义，<strong>解释步骤</strong>的做法是把问题转换为数据库查询语句（这里的查询语句应该是逻辑形式，比如<strong>组合范畴法</strong>），以此查询到正确的答案。尽管这两种方法有能力去处理大规模知识库，但是需要专家手动的创建词汇、语法以及 KB 协议才能有所成效。且<strong>没有通用性</strong>，<strong>无法方便地扩展到</strong>具有其他模式、更广泛词汇或英语以外语言的<strong>新数据库</strong>。<br>&emsp;&emsp;相反，<a href="https://www.aclweb.org/anthology/P13-1158" target="_blank" rel="noopener">Paraphrase-Driven Learning for Open Question Answering</a> 提出了一个几乎不需要人工注释的开放域 QA 框架，虽然这是一种有趣的方法，但是它被其他方法超越了。即第二段提到的论文 A。<br>&emsp;&emsp;相比于论文 A，作者作出了以下几点<strong>改进</strong>：1）对于候选答案，考虑更多更长的路径（之前只考虑了 main entity 周围的节点）；2）对候选答案进行更有意义的表示：答案的表示包含问答路径以及周围的子图。</p>
<h1 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h1><p>&emsp;&emsp;假设所有潜在的答案都是 KB 中的实体，当 KB 中不存在该实体时，可以使用一些方法解决（论文中具体没说，只是说了一种极简单的方式：<em>When this entity is not given, plain string matching is used to perform entity resolution</em>）。<br>&emsp;&emsp;此外 N 代表词典的大小，其中 <script type="math/tex">N = N_W + N_S</script>，<script type="math/tex">N_W</script> 代表词嵌入的大小，<script type="math/tex">N_S</script> 代表实体和关系的数量。</p>
<h1 id="改进：考虑多维度的信息"><a href="#改进：考虑多维度的信息" class="headerlink" title="改进：考虑多维度的信息"></a>改进：考虑多维度的信息</h1><p>&emsp;&emsp;以下描述一个候选答案的特征表示，论文将以三个角度进行表示：</p>
<ol>
<li>Single Entity：此表示方式与上一篇论文一样，没什么讲究。就是 Freebase 中的一个实体，<script type="math/tex">\psi(a)</script> 代表答案的 1-of-<script type="math/tex">N_S</script>（one hot）表示；</li>
<li>Path Representation：答案被认为是一条 path，该 path 从<strong>问题中被提及的实体</strong>到<strong>答案实体</strong>。此实验中，考虑 1-hop 或者 2-hops 级别的 path。比如，(barack obama, people.person.place of birth, honolulu) 是 1-hop path，(barack obama, people.person.place of birth, location. location.containedby, hawaii) 是 2-hop path。这导致了 <script type="math/tex">\psi(a)</script> 代表 3-of-<script type="math/tex">N_S</script> 或者 4-of-<script type="math/tex">N_S</script> 的向量，至于为什么是 *-of-<script type="math/tex">N_S</script>，显而易见。</li>
<li>Subgraph Representation：我们将 2 中的 <strong>Path</strong> 和连接候选答案的整个<strong>子图</strong>进行编码。<em>具体看论文，写的有点看不懂</em>。</li>
</ol>
<p>&emsp;&emsp;我们的假想是将所有的信息都编码进表示以提高结果，但是这不大可能。所以还是采用将子图编码进表示的方法。下图即为实验的模型，右下角显示了编码方式。<br><img src="https://img-blog.csdn.net/20171101002818501?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTEFXXzEzMDYyNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="架构图" title="架构图"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;与论文 A 差不多，多了一个多任务训练，其他的细枝末节没仔细看。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Open Question Answering with Weakly supervised Embedding Models</title>
    <url>/%E8%AE%BA%E6%96%87/20%E3%80%81Open%20Question%20Answering%20with%20Weakly%20supervised%20Embedding%20Models.html</url>
    <content><![CDATA[<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1404.4326" target="_blank" rel="noopener">论文地址</a>，论文发表于 2014 年。<br>&emsp;&emsp;建立一个能够回答任何问题的计算机是人工智能的一个长期目标。这一领域一个重要的发展时大规模知识库的建立，如 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Freebase</a> 和 <a href="https://content.iospress.com/download/semantic-web/sw134?id=semantic-web%2Fsw134" target="_blank" rel="noopener">DBPedia</a>，它们存储了大量的通用信息。它们由三元组的形式构成一个数据库，通过各种关系和格式连接成实体对。那么回答问题被定义为<strong>给定一个用自然语言表达的查询语句</strong>（一个查询语句的例子：中国的首都在哪？）<strong>从知识库中检索正确的实体或实体集的任务</strong>。<br>&emsp;&emsp;最近，通过将问题映射为<strong>逻辑形式</strong>或者类似<strong>数据库查询</strong>的方法取得了富有希望的进展。虽然这种方法可能有效，但是缺点是要采用大量的人为标记的数据或者需要工作人员定义词汇表和语法。<br>&emsp;&emsp;本文采用一种激进的学习方式，将问题映射为向量（无法人为解释）的特征表示。并且将重点放在回答一些基于比较宽泛的主题的简单事实性问题。这项任务的难点来自词汇的多样性，而不是句法的复杂性。<br>&emsp;&emsp;该方法采用随机梯度下降，然后使用 fine-tuning 进行训练。经验表明该模型能够捕获一些有意义的信号，且这是唯一一种能够在弱标记数据上训练的方法。</p>
<h1 id="论文内容介绍"><a href="#论文内容介绍" class="headerlink" title="论文内容介绍"></a>论文内容介绍</h1><ol>
<li>Section 2：讨论了之前的工作；</li>
<li>Section 3：介绍了开放域问答的问题；</li>
<li>Section 4：给出了模型；</li>
<li>Section 5：实验结果。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li>大规模的问答历史悠久，主要由 TREC tracks（<a href="https://arxiv.org/pdf/cs/0110053.pdf" target="_blank" rel="noopener">Voorhees 2000</a>） 发起，这是第一个成功地<strong>将问题转换为查询</strong>的问答系统。将问题转换为查询之后，又<strong>将查询提供给 web 搜索引擎</strong>，然后<strong>从返回的页面或片段中取出答案</strong>（<a href="http://aiweb.cs.washington.edu/research/projects/ai3/mulder/mulder-www10.pdf" target="_blank" rel="noopener">Kwok 2001</a>, <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-06/SS02-06-002.pdf" target="_blank" rel="noopener">Banko 2002</a>）。这种方法需要大量的人工操作来处理查询，然后解析和搜索结果。</li>
<li>大型 KBs 的出现，如 FreeBase 和 DBPedia（论文地址已在第一章给出），改变了上述状况，但是也带来巨大的挑战。语言的多样性以及 KBs 规模的庞大，使得需要通过监督学习来处理大量的<strong>带标签</strong>的数据。最早的方法是基于手写模板的 KBs 开放问答，然而对于日新月异 KBs（增加/删除三元组和实体） 还不够成熟。之后开始尝试使用较少的监督情况下<strong>学习 KBs 和自然语言之间的联系</strong>，但是这项工作实际上在解决<strong>信息提取</strong>的问题（<a href="https://www.aclweb.org/anthology/P09-1113" title="Distant supervision for relation extraction without labeled data" target="_blank" rel="noopener">Mintz M 等 2009</a>，<a href="https://www.aclweb.org/anthology/P11-1055" title="Knowledge-Based Weak Supervision for Information Extractionof Overlapping Relation" target="_blank" rel="noopener">Hoffmann R 等 2011</a>，<a href="https://www.aclweb.org/anthology/D12-1093" title="Reading The Web with Learned Syntactic-Semantic Inference Rules" target="_blank" rel="noopener">Lao N 等 2012</a>，<a href="https://www.aclweb.org/anthology/N13-1008" title="Relation Extraction with Matrix Factorization and Universal Schemas" target="_blank" rel="noopener">Riedel S 等 2013</a>）。以上以及本文未提及到的这些通过直接或者间接的监督机器学习来获得更多表现力的解决办法实际上是为了避开标签数据过多的问题。</li>
<li>近年来，有一种基于语义解析器（<a href="https://www.aclweb.org/anthology/P13-1042" title="Large-scale Semantic Parsing via Schema Matching and Lexicon Extension" target="_blank" rel="noopener">Cai Q 等 2013</a>，<a href="https://www.aclweb.org/anthology/D13-1160" title="Semantic Parsing on Freebase from Question-Answer Pairs" target="_blank" rel="noopener">Berant J 等 2013</a>，<a href="https://www.aclweb.org/anthology/D13-1161" title="Scaling Semantic Parsers with On-the-fly Ontology Matching" target="_blank" rel="noopener">Kwiatkowski T 等 2013</a>）的新的问答系统被提出，它只具有少量标记数据。但仍需要耗费大量精力去仔细设计词汇，语法和知识库。</li>
<li>所以本文（2014 年）提出了基于嵌入式的问答模型。据我们所知，这是以前从未尝试过的。</li>
</ol>
<h1 id="开放域问答"><a href="#开放域问答" class="headerlink" title="开放域问答"></a>开放域问答</h1><p>&emsp;&emsp;本文使用 <a href="https://www.aclweb.org/anthology/P13-1158" target="_blank" rel="noopener">Fader 2013</a> 的问答框架，并使用了相同的数据。</p>
<h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>&emsp;&emsp;我们将回答问题的任务看作为：给定一个问题 q，对应的答案由 KB 中的三元组 t 给出。这意味着我们的问题由<strong>一组三元组 t</strong> 提供对问题及其答案的解释，例如：</p>
<blockquote>
<p>q: What environment does a dodo live in?（渡渡鸟生活在什么样的环境中？）<br>t: (dodo.e, live-in.r, makassar.e)<br>q: What are the symbols for Hannukah?（光明节的象征是什么？）<br>t: (menorah.e, be-for.r, hannukah.e)<br>q: What is a laser used for?（极光可以用来做什么？）<br>t: (hologram.e,be-produce-with.r,laser.e)</p>
</blockquote>
<p>&emsp;&emsp;这里每个问题我们只给出一个 t，但是实际上它可以有很多，所以上文说是一组三元组。本文其余部分，<strong>使用 <script type="math/tex">\kappa</script>（读作 kappa） 代表 KB ，使用 <script type="math/tex">\epsilon</script> 代表 KB 中的实体或者关系。问题的词表用 V 表示，<script type="math/tex">n_V</script> <script type="math/tex">n_{\epsilon}</script>分别表示 V 和 <script type="math/tex">\epsilon</script> 的大小</strong>。<br>&emsp;&emsp;我们的模型在于<strong>函数 S(·)</strong>，它可以为 question-answer triple pairs (q,t) 打分。因此，找到问题 q 的 top-ranked 的答案 <script type="math/tex">\hat{t}</script>(q) 直接由以下公式得出：</p>
<script type="math/tex; mode=display">
\hat{t}(q) = arg \max_{t' \in \kappa}S(q, t')</script><p>&emsp;&emsp;为了处理多个答案，我们将结果呈现为排完序的列表并对其评分，而不是直接采用最前面的预测结果。<br>&emsp;&emsp;使用评分函数可以直接查询 KB，而不需要在<strong>语义分析系统</strong>中一样为问题定义一个中间的结构化逻辑表示。我们的目标是学习 S(·)，余下将讲述用于训练的数据的创建步骤。</p>
<h2 id="用于训练的数据"><a href="#用于训练的数据" class="headerlink" title="用于训练的数据"></a>用于训练的数据</h2><div class="note info">
            <p>待续</p>
          </div>
<h1 id="Embedding-based-model"><a href="#Embedding-based-model" class="headerlink" title="Embedding-based model"></a>Embedding-based model</h1><p>&emsp;&emsp;模型使用了词嵌入（2019 年了，应该谁都知道了，不做解释）。</p>
<h2 id="Question-KB-Triple-Scoring"><a href="#Question-KB-Triple-Scoring" class="headerlink" title="Question-KB Triple Scoring"></a>Question-KB Triple Scoring</h2><p>&emsp;&emsp;我们的框架关注的是函数 S(q,t) 的学习，该函数的目的是对一个<strong>问题 q</strong> 和 一个<strong>来自 <script type="math/tex">\kappa</script> 的三元组 t</strong> 进行打分。该评分方法受到了先前工作 labeling images withwords 的启发（<a href="https://link.springer.com/content/pdf/10.1007/s10994-010-5198-3.pdf" target="_blank" rel="noopener">Weston 2013</a>），我们采用该方法将图片和标签替换成了问题和三元组。直观来讲就是：<br>有点难翻译，故给出原文：</p>
<blockquote>
<p>&emsp;&emsp;Intuitively, it consists of projecting questions, treated as a bag of words(and possibly n-grams as well), on the one hand, and triples on the other hand,into a shared embedding space and then computing a similarity measure (the dot  product  in  this  paper)  between  both  projections.<br>&emsp;&emsp;大致意思，将问题和三元组使用词袋模型（也可以是 n-gram 模型）投射到共享的嵌入空间，然后计算二者的相似度（本文使用点积的方式）。</p>
</blockquote>
<p>&emsp;&emsp;那么评分函数为:</p>
<script type="math/tex; mode=display">
S(q,t) = f(q)^Tg(t)</script><p>&emsp;&emsp;<strong>其中 f(·) 将问题中的单词映射到 <script type="math/tex">\mathbb{R}^{\kappa}</script>，<script type="math/tex">f(q) = V^T \Theta(q)</script>。V 是关于 <script type="math/tex">\mathbb{R}^{n_v \times \kappa}</script> 包含所有词嵌入 v 的矩阵。<script type="math/tex">\Theta(q)</script>是 q（<script type="math/tex">\in \{0,1\}^{n_v}</script>） 的二进制（稀疏）表示。同样，g(·) 将 KB 三元组中的实体和关系映射到 <script type="math/tex">\mathbb{R}^{\kappa}</script>，<script type="math/tex">g(t) = W^T\Psi(t)</script>，W 是关于 <script type="math/tex">\mathbb{R}^{n_e \times \kappa}</script> 包含所有实体和关系的嵌入 w 的矩阵，<script type="math/tex">\Psi(t)</script> 是 t（<script type="math/tex">\in \{0,1\}^{n_e}</script>） 的二进制（稀疏）表示。</strong><br><div class="note info">
            <p>&emsp;&emsp;注：上一段太长了，解释一下。f(q) 就是词向量，g(t) 就是实体和关系的向量（下一段原文写到 g(t) 是将三元组中的嵌入全部相加）。</p>
          </div></p>
<p>&emsp;&emsp;将单词表示为词袋模型似乎有一点局限性，但是由于我们特定的设置，语法都很简单，因此含有的信息十分有限，所以词袋模型应该也能带来不错的性能。当然也有反例，比如 <em>What are cats afraid of ?vs.What are afraid of cats ?</em> 这将会有不同的答案。不过这种情况十分罕见。未来考虑将 parse tree features 或者 semantic role labels 作为输入放入嵌入模型中。<br>&emsp;&emsp;与以前的工作（<a href="https://arxiv.org/pdf/1307.7973" target="_blank" rel="noopener">Weston 2013</a>）不同的是，在我们的模型中，实体出现三元组的不同侧面（左右侧）时，实体并非拥有相同的嵌入。KB 中的关系并不是对称的，所以会出现三元组中左侧和右侧的实体是不同的情况。<strong>由于 g(·) 是将三元组中的所有成分相加，所以每一个实体我们都需要两个嵌入</strong>。<br>&emsp;&emsp;这样就可以很容易地对任何三元组进行评分：</p>
<script type="math/tex; mode=display">
\hat{t}(q) = arg \max_{t' \in \kappa}S(q, t') = arg \max_{t' \in \kappa}(f(q)^Tg(t'))</script><p>&emsp;&emsp;接下来花了好几段讲怎么训练。</p>
<h2 id="Fine-tuning-the-Similarity-between-Embeddings"><a href="#Fine-tuning-the-Similarity-between-Embeddings" class="headerlink" title="Fine-tuning the Similarity between Embeddings"></a>Fine-tuning the Similarity between Embeddings</h2><p>&emsp;&emsp;由于受到数据大小的限制，需要使用微调来改进性能。</p>
<h1 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h1><p>&emsp;&emsp;通读论文之后还是有点搞不清论文是怎么训练的，后来看了一下 CCF ADL100 刘康老师的 PPT ，感觉有点理解了，以下是训练步骤：</p>
<ol>
<li>输入自然语言表达的问题，比如：姚明的老婆的是哪里人？</li>
<li>使用 entity linking（论文中貌似没有这步，我在看 PPT 时也是一知半解，好在前几天我刚好在一篇论文中看到了这个 entity linking！<a href="https://yan624.github.io/论文/19、Semantic Parsing via Staged Query Graph Generation：Question Answering with Knowledge Base.html#链接主题实体">博客地址</a>，entity linking 源于<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）找到 main entity，main entity 周围的 entity 均是候选 entity。如下图，姚明是 main entity，姚明周围的实体都算作候选 entity，比如叶莉、火箭队、上海等。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/论文笔记：Open Question Answering with Weakly supervised Embedding Models/姚明的老婆是谁的知识图谱的子图.jpg" alt="姚明的老婆是谁的知识图谱的子图"></li>
<li>计算问题和候选 entity 的相似度，其中问题由词向量表示，候选 entity 是一个三元组的形式，难以直接用词向量表示，方法是将三元组中的三个对象分别用词向量表示，然后将三个词向量相加。这样就得到了问题的词向量和 entity 的词向量，点乘获得相似度。</li>
<li>由于候选 entity 不一定只有一个，所以可以获得多个相似度。进行排序即可获得最相似的候选 entity。</li>
</ol>
<div class="note danger">
            <p>&emsp;&emsp;以上的训练步骤并不是论文中的训练步骤，只是我为了给自己加深映像写的，具体的训练步骤在原论文第 4 节，具体在 4.1。</p>
          </div>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base</title>
    <url>/%E8%AE%BA%E6%96%87/19%E3%80%81Semantic%20Parsing%20via%20Staged%20Query%20Graph%20Generation%EF%BC%9AQuestion%20Answering%20with%20Knowledge%20Base.html</url>
    <content><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>&emsp;&emsp;此论文为 2015 年的论文。<br>&emsp;&emsp;本文会出现一个名为<strong>谓语序列（predicate sequence）</strong>的名词，论文中没有详细说明。但是估计就是：一个实体至另一个实体的有向路径上的所有谓语的连接形式。如下文第一张图 Family Guy-&gt;cvt1-&gt;Mila Kunis 的谓语序列就是 cast-actor。</p>
<h1 id="论文概要"><a href="#论文概要" class="headerlink" title="论文概要"></a>论文概要</h1><p>&emsp;&emsp;<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;节选自摘要部分：</p>
<blockquote>
<p>&emsp;&emsp;论文提出了一个基于知识库问答的新的语义解析（semantic parsing）框架。首先定义一个类似于知识库的<strong>子图（subgraph）</strong>的查询图（query graph），可以直接映射到一个语义的逻辑形式（如<script type="math/tex">\lambda</script>-calculus）。所以<strong>语义分析简化为查询图的生成</strong>，并将其表示为一个阶段性搜索问题。然后通过使用先进的实体链接系统（<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）以及深度卷积网络来实现问题与谓语序列之间的匹配。在 WEBQUESTIONS 的数据集上，F1 指标达到了 52.5% 的水平，高于以前的方法。</p>
</blockquote>
<p>&emsp;&emsp;以下大型知识库已经成为支持开放领域问答的重要资源：</p>
<ul>
<li>DBPedia</li>
<li>Freebase</li>
</ul>
<p>&emsp;&emsp;最先进的 KB-QA 方法都是基于<strong>语义解析</strong>的，在语义解析中一个问题或者一种表达被映射到它具有一定意义的表示上（如逻辑形式，具体来说可以是 <script type="math/tex">\lambda</script>-calculus），即将自然语言映射为表达式，然后被翻译为一个 <strong>KB 查询</strong>。最后，只需要执行查询就可以检索问题的答案。<strong>但是大多数<u>传统的</u>语义解析方法在很大程度上都<u>脱离</u>知识库</strong>。由于没有前人的贡献累积，因此 QA 问题面临着一系列的挑战。例如：</p>
<ul>
<li>当在逻辑形式中使用与知识库中的谓语不同的谓语时，可能需要用到本体匹配（ontology matching）的问题（Kwiatkowski et al., 2013）。</li>
<li>即使表示语言与知识库的模式接近，从知识库中的大量词汇表中寻找正确的谓语与语句的描述相关联仍然是一个难题（Berant and Liang, 2014）。</li>
</ul>
<p>&emsp;&emsp;由（Yao and Van Durme, 2014; Bao etal., 2014）的启发，该论文提出了一个语义解析框架，定义一个查询图可以直接地映射到由 <script type="math/tex">\lambda</script>-calculus 表达的逻辑形式。从语义上来讲，与 <script type="math/tex">\lambda</script>-DCS（Liang, 2013）十分接近。将解析行为分为 3 步：</p>
<ol>
<li>定位问题中的主题实体；</li>
<li>找到回答与主题实体之间的主要关联；</li>
<li>（通过额外的约束扩大查询图，约束即回答需要附加的额外属性，如最早时间等）或者（答案与其他实体之间的关联）。</li>
</ol>
<p>&emsp;&emsp;至此将一个语义解析问题划分成了一系列的子问题。例如 entity linking 和 relation matching。</p>
<h2 id="文章内容介绍"><a href="#文章内容介绍" class="headerlink" title="文章内容介绍"></a>文章内容介绍</h2><ol>
<li>Sec. 2: 介绍了图知识库（估计就是知识图谱）的概念和查询图的设计；</li>
<li>Sec. 3: 介绍了基于搜索方法的查询图生成；</li>
<li>Sec. 4: 实验结果；</li>
<li>Sec. 5: 论文中的方法和其他相关工作的比较；</li>
<li>Sec. 6: 总结。</li>
</ol>
<h1 id="Knowledge-Base"><a href="#Knowledge-Base" class="headerlink" title="Knowledge Base"></a>Knowledge Base</h1><p>&emsp;&emsp;论文中的知识库 K 是一个包含主语、谓语、宾语的三元组（e1, p, e2）的集合，其中 e1 和 e2<script type="math/tex">\in</script>E，是一个实体。p<script type="math/tex">\in</script>P，是一个谓语。这种形式的知识库通常称为知识图谱。每一个实体是一个节点，两个相关联的实体由谓语标记的有向边连接，边的方向是从主语实体到宾语实体。如下图就是一个 Freebase 的子图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Freebase subgraph of Family Guy.jpg" alt="Freebase subgraph of Family Guy"></p>
<div class="note info">
            <p>&emsp;&emsp;Freebase 中有一个叫 <a href="https://developers.google.com/freebase/guide/basic_concepts#cvts" target="_blank" rel="noopener">CVT</a>（此链接需要翻墙访问） 的特殊实体类型，它不是一个真正的实体，而是用于收集事件或特殊的关联的多个字段。</p>
          </div>
<h1 id="Query-graph"><a href="#Query-graph" class="headerlink" title="Query graph"></a>Query graph</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>&emsp;&emsp;给定一个知识图谱。执行逻辑形式的查询等价于寻找一个子图，该子图的表现形式可以映射到查询动作。之后解析绑定的变量。<br><div class="note warning">
            <p>&emsp;&emsp;接下来，以实体这个属性来表示真实世界的实体和 CVT 实体以及日期或高度等属性，这些实体之间的区别对于论文中的方法来说并不重要。</p>
          </div><br>&emsp;&emsp;就像知识图谱一样，查询图中的相关节点也是通过有向边连接，并用 K 中的谓语标记。查询图由四中类型的节点组成：</p>
<ol>
<li>grounded entity：圆角矩形表示。grounded entity 是在知识库 K 中已存的实体。</li>
<li>existential variable：圆形表示。existential variable 是 un-grounded entity。</li>
<li>lambda variable：阴影圆形表示。lambda variable 是 un-grounded entity。尤其，该论文表示希望<strong>检索</strong>能够映射到 lambda variable 的所有实体<strong>作为</strong>最终答案。其也被称为<strong>answer 节点</strong>。</li>
<li>aggregation function：菱形表示。aggregation function 被用于操作特定的实体，该实体通常具有一些数值属性。</li>
</ol>
<p>&emsp;&emsp;下图展示了一个查询图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Query graph that represents the question “Who first voiced Meg on Family Guy？”.jpg" alt="Query graph that represents the question “Who first voiced Meg on Family Guy？”"></p>
<p>&emsp;&emsp;上图是“谁第一次为 Family Guy 中的 Meg 配音？”的问题。MegGriffin 和 FamilyGuy 由圆角矩形表示，圆圈节点 y 表示应该存在一个实体来描述扮演关系，比如角色、演员和开始饰演此角色的时间。阴影圆圈节点也被称为 <strong>answer 节点</strong>。菱形节点 argmin 限制答案必须是扮演此角色的最早的演员。同样不含聚合函数的<script type="math/tex">\lambda-calculus</script>逻辑形式查询为<script type="math/tex">\lambda x.\exists y.cast(FamilyGuy,y) \Lambda actor(y,x) \Lambda character(y,MegGriffin)</script>。在使用聚合函数之前，对 K 运行此查询图会匹配 LaceyChabert 以及 MilaKunis，请看第一张图。但是只有 LaceyChabert 是正确答案，因为是她最早开始扮演这个角色。<br><div class="note info">
            <p>查询图的设计灵感来源于（Reddyet al., 2014），但是他的查询图是从问题的 CCG 解析中映射出来的，在映射到子图前还需要进一步的转换。从语义上来说，该论文的查询图更像简单的 <script type="math/tex">\lambda-DCS</script>。</p>
          </div></p>
<h2 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h2><p>&emsp;&emsp;<strong>首先</strong>树图（tree graph）的根由一个实体节点组成，称为主题实体（topic entity）。<strong>其次</strong>，只有一个 lambda 变量 x 作为答案节点，从根到 x 有一个定向路径，其中含有 0 个或多个 existential variables。论文中将此路径称为图的核心推理链，因为它描述了答案和主题实体之间的主要关系。这个链除了根节点外只有变量节点。<strong>最后</strong>，可以将 0 个或多个实体或者聚合函数节点附加到每个变量节点，包括 answer 节点。例如，上图 Family Guy 是根，而 Family Guy-&gt;y-&gt;x 是核心推理链，分支 y-&gt;MegGriffin 阐述了角色，而 y-&gt;argmin 限制答案必须是该角色最早的参与者。<br>&emsp;&emsp;定义状态（state）集合<script type="math/tex">S = \{\phi, S_e, S_p, S_c\}</script>，其中每个状态可以是一个空的图（<script type="math/tex">\phi</script>），一个主题实体的单节点图（<script type="math/tex">S_e</script>），一个核心推理链（<script type="math/tex">S_p</script>）或者带有额外约束的更复杂的查询图（<script type="math/tex">S_c</script>）。<br>&emsp;&emsp;定义动作（action）集合<script type="math/tex">A = \{A_e, A_p, A_c, A_a\}</script>，其中<script type="math/tex">A_e</script>选取实体节点，<script type="math/tex">A_p</script>确定核心推理链，<script type="math/tex">A_c</script>和<script type="math/tex">A_a</script>分别约束和聚合节点。<br>&emsp;&emsp;给出一个示例<script type="math/tex">q_{ex}</script> = “Who first voiced Meg of Family Guy?”。</p>
<h3 id="链接主题实体"><a href="#链接主题实体" class="headerlink" title="链接主题实体"></a>链接主题实体</h3><p>&emsp;&emsp;从初始状态<script type="math/tex">S_0</script>开始，正确的操作是创建一个与给定问题中的主题实体相对应单节点图。例如，<script type="math/tex">q_{ex}</script>中可能的主题实体是 Family Guy 和 MegGriffin，如下图所示。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Two possible topic entity linking actionsapplied to an empty graph, for question “Who firstvoiced[Meg]on[Family Guy]？”.jpg" alt="Two possible topic entity linking actionsapplied to an empty graph, for question “Who firstvoiced[Meg]on[Family Guy]？”"></p>
<p>&emsp;&emsp;使用的<strong>实体链接系统</strong>是专为短且有噪声的文本设计的，源于（<a href="https://arxiv.org/pdf/1609.08075.pdf" target="_blank" rel="noopener">Yang and Chang, 2015</a>）。具体不做赘述，详情可参考相关论文。</p>
<h3 id="确定核心推理链"><a href="#确定核心推理链" class="headerlink" title="确定核心推理链"></a>确定核心推理链</h3><p>&emsp;&emsp;给定与主题实体 e 对应的单节点图的状态 s，扩展该图的正确操作是确定核心推理链，即主题实体和答案之间的关系。下图展示了扩展<script type="math/tex">s_1</script>中的单节点图的三个可能的链。具体做法是，当中间的 existential variable 链接 CVT 时，探索长度为 2 的所有路径，如果没有链接，则探索长度为 1 的路径。<br><div class="note primary">
            <p>&emsp;&emsp;本节主要描述了如何确定核心推理链，不过上文一段先描述了如何确定候选的核心推理链。具体做法上一段也已经给出，但是由于原论文讲的也有点不清楚，此处加以说明，以下只是推测。</p><ol><li>扩展主题节点 Family Guy 的三个可能的核心推理链，应该是从知识库 K 中入手。请看第一张图，它是知识库 K 中的一张子图。从 Family Guy 中开始可以看到有三条边，两条边上是 cast，一条边上是 writer。由于两条边相同，于是就融为了一条推理链。至于最后一条推理链的谓语是 genre，可能是第一张图的子图中没有标出造成的。总而言之，那三条推理链就是从知识库 K 中获取。</li><li>existential variable 即 y，lambda variable 即 x。可以把知识库 K 中的 CVT 节点看作是 y，答案看作是 x。</li></ol>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/Candidate core inferential chains start from the entity FamilyGuy.jpg" alt="Candidate core inferential chains start from the entity FamilyGuy"></p>
<p>&emsp;&emsp;这样做的目的是将自然表达映射到正确的谓语序列上。对于问题“Who first voiced Meg on [Family Guy]?”，需要衡量的是在{cast-actor, writer-start, genre}中每个序列（<em>注：这个元组就是上图的三个候选核心推理链上的谓语</em>）正确捕捉 Family Guy 和 Who 之间关系的可能性。因此将这个问题简化为使用神经网络测量语义相似度。</p>
<h4 id="Deep-Convolutional-Neural-Networks"><a href="#Deep-Convolutional-Neural-Networks" class="headerlink" title="Deep Convolutional Neural Networks"></a>Deep Convolutional Neural Networks</h4><p>&emsp;&emsp;虽然是陈述一个相同的问题，但是以语义等价的方式来重新表达该问题仍旧拥有巨大的多样性。并且还存在自然语言表达与知识库中的谓语不匹配的情况。<strong>为了处理上述两个问题</strong>，论文建议使用 Siamese neural networks（<a href="http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf" target="_blank" rel="noopener">Bromley et al., 1993</a>）来识别核心推理链（暹（xiān）罗神经网络，也可以叫连体神经网络。看见这个中文就很好理解了。Siamese neural networks 可以进行语义相似度分析，QA 的匹配等操作。详情可以先看看<a href="https://www.jianshu.com/p/92d7f6eaacf5" target="_blank" rel="noopener">这篇</a>博客）。注：由于上图可以得知一个问题可以获得几个候选得到核心推理链，这就是因为语言的多样性造成的，所以需要一个方法来识别一条最核心的推理链。<br>&emsp;&emsp;例如，将一个问题映射到一种<strong>模式</strong>上，方法是将实体替换为通用符号 &lt;e&gt;，然后将其与<strong>候选链</strong>比较。比如问题“who first voiced meg on &lt;e&gt;”和 cast-actor。该模型由两个神经网络组成，一个处理<strong>模式</strong>，一个处理<strong>核心推理链</strong>（这个模型说白了就是 Siamese neural networks）。两个神经网络都映射到 k 维向量作为网络的输出，最后使用距离函数（如余弦相似度）计算语义相似度。<br><div class="note info">
            <p>&emsp;&emsp;该论文处理<strong>匹配问题</strong>使用了 CNN 模型。你可能会有点疑惑<strong>匹配问题</strong>是什么问题，前面压根就没提到过。是的，论文里也没说过，我只能猜测，这里的 CNN 其实就是上述模型的两个神经网络的具体实现。处理模型和处理核心推理链可能都用了 CNN 模型。另外论文中也没有说如何将核心推理链送入 CNN 中。论文中倒是稍微提了一下如何将问题送入 CNN 中，使用 word hashing 技术（<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf" target="_blank" rel="noopener">Huang et al., 2013</a>）。</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/CNN架构.jpg" alt="CNN架构"></p>
<h3 id="增加约束和聚合函数"><a href="#增加约束和聚合函数" class="headerlink" title="增加约束和聚合函数"></a>增加约束和聚合函数</h3><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>&emsp;&emsp;<strong>Topic Entity</strong>：由实体链接系统返回的分数直接作为特征。<br>&emsp;&emsp;<strong>Core Inferential Chain</strong>：使用不同的 CNN 模型的相似度分数来衡量核心推理链的质量，以下为 3 个模型。</p>
<ul>
<li><strong>PatChain</strong>：比较模式和谓语序列。</li>
<li><strong>QuesEP</strong>：将主题实体的名称与谓语序列拼接完成之后，将其与原问题比较。</li>
<li><strong>ClueWeb</strong>：使用 ClueWeb 语料库的 Freebase 注释训练 ClueWeb 模型</li>
</ul>
<p>&emsp;&emsp;<strong>Constraints &amp; Aggregations</strong>：当查询图中有约束节点，使用一些简单的特征来检查问题中是否存在单词可以与约束实体或者属性相关联。相似地，也可以使用一些预定义的关键字，比如“first”、“current”或者“latest”作为 argmin 节点的特征。<br>&emsp;&emsp;<strong>Overall</strong>：回答节点的个数和总节点个数也都作为特征。<br>&emsp;&emsp;比如下图，（1）属于 Topic Entity，（2）（3）（4）属于 Core Inferential Chain，（5）（6）（7）属于 Constraints &amp; Aggregations，（8）（9）属于 Overall：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base/特征举例.jpg" alt="特征举例"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&emsp;&emsp;使用 WEBQUESTIONS 数据集，评价指标有：precision，recall 和 F1。其中 F1 的平均值作为主要的评价指标。</p>
<h1 id="其他参考资料"><a href="#其他参考资料" class="headerlink" title="其他参考资料"></a>其他参考资料</h1><p>&emsp;&emsp;在浏览此篇论文时，发现还有其他人也看过这篇论文并且留下了笔记（中文）。<br>&emsp;&emsp;<a href="https://bigquant.com/community/t/topic/121147" target="_blank" rel="noopener">笔记1</a><br>&emsp;&emsp;<a href="https://blog.csdn.net/qq_32782771/article/details/82773048" target="_blank" rel="noopener">笔记2</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>paper</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>在hexo中添加绘制流程图及其他图的功能</title>
    <url>/%E5%9C%A8hexo%E4%B8%AD%E6%B7%BB%E5%8A%A0%E7%BB%98%E5%88%B6%E6%B5%81%E7%A8%8B%E5%9B%BE%E5%8F%8A%E5%85%B6%E4%BB%96%E5%9B%BE%E7%9A%84%E5%8A%9F%E8%83%BD.html</url>
    <content><![CDATA[<p>hexo 本身不支持绘制流程图，但是可以使用以下命令安装插件来实现此功能。<br><figure class="highlight processing"><table><tr><td class="code"><pre><span class="line">npm install --<span class="built_in">save</span> hexo-<span class="built_in">filter</span>-flowchart</span><br></pre></td></tr></table></figure></p>
<p><a href="https://github.com/bubkoo/hexo-filter-flowchart" target="_blank" rel="noopener">插件地址</a><br>语法可以<a href="https://cloud.tencent.com/developer/article/1142260" target="_blank" rel="noopener">在这</a>找<br>一个简单的例子<br>(`乘3)flow<br>st=&gt;start: Start|past:&gt;<a href="http://www.google.com[blank" target="_blank" rel="noopener">http://www.google.com[blank</a>]<br>e=&gt;end: End:&gt;<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br>op1=&gt;operation: My Operation|past<br>op2=&gt;operation: Stuff|current<br>sub1=&gt;subroutine: My Subroutine|invalid<br>cond=&gt;condition: Yes<br>or No?|approved:&gt;<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br>c2=&gt;condition: Good idea|rejected<br>io=&gt;inputoutput: catch something…|request</p>
<p>st-&gt;op1(right)-&gt;cond<br>cond(yes, right)-&gt;c2<br>cond(no)-&gt;sub1(left)-&gt;op1<br>c2(yes)-&gt;io-&gt;e<br>c2(no)-&gt;op2-&gt;e<br>(`乘3)<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习算法（七）：K-NN</title>
    <url>/zcy/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9AK-NN.html</url>
    <content><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p>K-NN 算法采用测量不同特征值之间的距离的方法进行分类。<br>工作原理：<br>存在一个<strong>样本数据集</strong>，也称作训练样本集，并且样本集中每个数据都存在标签。输入<strong>没有标签的新数据</strong>后，将<strong>新数据</strong>的每个特征与<strong>样本集</strong>中的数据对应特征进行比较，然后算法提取样本集中特征最相似的数据（最邻近）的分类<strong>标签</strong>。一般来说，只选择样本数据集中前 k 个最相似的数据，这就是 k-NN 算法中 k 的出处，通常 k 是不大于 20 的整数。<br>最后选择在 k 个最相似的数据中出现次数最多的分类，作为新数据的分类。</p>
</blockquote>
<div id="flowchart-0" class="flow-chart"></div>

<p>简单来说，K-NN 算法使用了一种计算特征之间的距离的公式，然后选择距离前 k 近的数据，获取这些数据的标签。通过一个简单的统计，获取这 k 项数据中最多的类别。最后我们将新数据看作是这个类别。</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    group = np.array([[<span class="number">1.0</span>, <span class="number">1.1</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.1</span>]])</span><br><span class="line">    labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(input, dataset, labels, k)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    K-NN 分类</span></span><br><span class="line"><span class="string">    :param input: 输入数据，即待分类的数据</span></span><br><span class="line"><span class="string">    :param dataset: 训练数据集</span></span><br><span class="line"><span class="string">    :param labels: dataset 对应的标签</span></span><br><span class="line"><span class="string">    :param k: 显而易见</span></span><br><span class="line"><span class="string">    :return: input 的类别</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算特征之间的距离，只是一个很简单的算法</span></span><br><span class="line">    <span class="comment"># 先算差，再平方，然后将一个项数据的所有特征累加，最后开方</span></span><br><span class="line">    all_distances = np.sqrt(np.sum(np.power((input - dataset), <span class="number">2</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 对距离进行逆序排序</span></span><br><span class="line">    sorted_distance_indices = all_distances.argsort()</span><br><span class="line">    <span class="comment"># 对类别进行计数</span></span><br><span class="line">    class_count = &#123;&#125;</span><br><span class="line">    <span class="comment"># 选取前 k 项数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        <span class="comment"># 第 i 项数据的标签</span></span><br><span class="line">        label = labels[sorted_distance_indices[i]]</span><br><span class="line">        <span class="comment"># 标签存在则加 1，不存在就默认是 0 再加 1</span></span><br><span class="line">        class_count[label] = class_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据标签的数量排序</span></span><br><span class="line">    sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_class_count[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    X, Y = create_dataset()</span><br><span class="line">    res = classify([<span class="number">0</span>, <span class="number">0</span>], X, Y, <span class="number">3</span>)</span><br><span class="line">    print(res)</span><br></pre></td></tr></table></figure>
<h2 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h2><p>由于有些数据范围波动较大，可以进行均值归一化处理。</p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>引用《机器学习实战》中的应用。</p>
<ol>
<li>可以分类电影的类别，已知数据：打斗镜头、接吻镜头、<strong>电影的类别</strong>。如果给定一部新电影，则可以根据该电影的打斗镜头、接吻镜头来计算此部电影属于哪种类别。</li>
<li>改进约会网站配对效果。已知数据：每年获得的飞行常客里程数、玩视频游戏所耗时间百分比、每周消费的冰淇淋公升数、<strong>用户交往对象的类别</strong>。其中<strong>用户交往对象的类别</strong>指：<ul>
<li>不喜欢的人</li>
<li>魅力一般的人</li>
<li>极具魅力的人<br>则可以输入一个新的约会对象的数据，从而判断此人属于哪种类别，如果属于不喜欢的人的类别，那么用户可以提前得知，并且决定不去约会。</li>
</ul>
</li>
<li>甚至可以识别手写数字。将图片转换成 0 1 表示，即数字部分用 1 表示，其他部分用 0 表示。组成一个 32 x 32 数字矩阵，然后将矩阵转换为 1 x 1024 的向量。其中的每一维度的值可以看作为一个特征。算法类似。</li>
</ol>
<a id="more"></a><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: 新数据
op1=>operation: 计算新数据特征
与样本集特征之间的距离
op2=>operation: 提取前 k 个
最相似的数据的标签
count=>inputoutput: 统计标签
e1=>end: 返回出现次数最多的分类
e2=>end: 程序无法继续执行
c1=>condition: k 小于等于
样本集个数

st(right)->c1
c1(yes, right)->op1(right)->op2->count(right)->e1
c1(no)->e2(left)->st</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>2019 CCF会议总结</title>
    <url>/2019%20CCF%E4%BC%9A%E8%AE%AE%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<h1 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h1><h2 id="知识图谱问答系统概述"><a href="#知识图谱问答系统概述" class="headerlink" title="知识图谱问答系统概述"></a>知识图谱问答系统概述</h2><p>现在的<strong>搜索引擎</strong>工作流程是输入要搜索的内容，搜索引擎返回一大堆内容，供你自己选择。<br><strong>问答系统</strong>是下一代的搜索引擎的基本形态。</p>
<blockquote>
<p>以直接而准确的方式回答用户自然语言提问的自动问答系统将构成下一代搜索引擎的基本形态。</p>
</blockquote>
<p>下图展示问答系统在近几十年的发展历史。</p>
<ol>
<li>1960 年的问答系统属于专家系统（模版系统）</li>
<li>1990 - 2000 年的问答系统属于基于信息检索的 QA 系统</li>
<li>2000 - 2010 年的问答系统属于社区 QA 系统</li>
<li>2011 年之后的问答系统属于基于知识图谱的 QA 系统</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/问答系统的历史.jpg" alt="问答系统的历史"></p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>问答系统的分类（或者说三个阶段）：</p>
<ol>
<li>IR-based QA：基于<strong>关键词匹配 + 信息抽取</strong>，任然是基于<strong>浅层语义分析</strong></li>
<li>Community QA：依赖于网民贡献，问答过程任然依赖于<strong>关键词检索技术</strong></li>
<li>KB-based QA：Knowledge Base，例如：WolfframAlpha</li>
</ol>
<p>根据问答形式分类：</p>
<ol>
<li>一问一答：字面意思，也是演讲的主题</li>
<li>交互式问答：就是进行连续的复杂的问答</li>
<li>阅读理解</li>
</ol>
<div class="note warning">
            <p>KB-QA 现在只能解决事实性的问题，无法解决：</p><ol><li>怎么去天安门</li><li>西红柿炒鸡蛋怎么做等提问</li></ol><p>某公司（在会议上没听清，可能是一个公司）只有 5% 的问题能用 KB-QA 解决。</p>
          </div>
<h2 id="什么是知识图谱"><a href="#什么是知识图谱" class="headerlink" title="什么是知识图谱"></a>什么是知识图谱</h2><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/知识图谱示例.jpg" alt="知识图谱示例"></p>
<h3 id="知识图谱基本架构"><a href="#知识图谱基本架构" class="headerlink" title="知识图谱基本架构"></a>知识图谱基本架构</h3><p>图中三元组中的 Ent1、Ent2 等指的是 entity。entity 可以在架构中选取，比如将 concept 作为 entity 或者将 instance 作为 entity。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/知识图谱基本架构.jpg" alt="知识图谱基本架构"></p>
<h3 id="运用知识图谱问答"><a href="#运用知识图谱问答" class="headerlink" title="运用知识图谱问答"></a>运用知识图谱问答</h3><p>语义如何表示是其中的一个问题：</p>
<ol>
<li>使用符号表示的形式（传统方法）</li>
<li>使用分布式表示方法</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/运用知识图谱问答.jpg" alt="运用知识图谱问答"></p>
<h3 id="知识图谱问答的两类方法（根据技术路线分）"><a href="#知识图谱问答的两类方法（根据技术路线分）" class="headerlink" title="知识图谱问答的两类方法（根据技术路线分）"></a>知识图谱问答的两类方法（根据技术路线分）</h3><ol>
<li>语义解析(Semantic Parsing)：问句转换成形式化的查询语句，进行结构化查询得到答案</li>
<li>语义检索（Answer Retrieval &amp; Ranking）：简单的搜索得到候选答案，利用问句和候选答案的匹配程度(特征)抽取答案</li>
</ol>
<h2 id="公开的评测数据集"><a href="#公开的评测数据集" class="headerlink" title="公开的评测数据集"></a>公开的评测数据集</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/公开的评测数据集.jpg" alt="公开的评测数据集"><br>例如：</p>
<script type="math/tex; mode=display">
    \text{图数据结构}
    \begin{cases}
        QALD \\
        WebQuestions \\
        Simple Question\\
    \end{cases}\\
    \text{表数据结构}
    \begin{cases}
        WikiSQL & \text{一个表} \\
        Spider & \text{多个表} \\
    \end{cases}</script><h2 id="知识图谱问答基于的几种方法"><a href="#知识图谱问答基于的几种方法" class="headerlink" title="知识图谱问答基于的几种方法"></a>知识图谱问答基于的几种方法</h2><ol>
<li>基于符号语义解析的知识图谱问答<ul>
<li>语义表示（lambda 验算，DCS Tree）</li>
<li>语义解析方法（CCG）<ul>
<li>还有许多语义解析方法，略</li>
</ul>
</li>
</ul>
</li>
<li>基于语义检索的知识图谱问答<ul>
<li>基于显示特征的知识检索</li>
<li>基于端到端的知识图谱问答</li>
</ul>
</li>
<li>基于神经符号计算的知识图谱问答<ul>
<li>基于序列学习的解析方法</li>
<li>基于动作序列的解析方法</li>
<li>基于对战神经网络的端到端问答方法</li>
</ul>
</li>
</ol>
<h3 id="基于符号语义解析的知识图谱问答"><a href="#基于符号语义解析的知识图谱问答" class="headerlink" title="基于符号语义解析的知识图谱问答"></a>基于符号语义解析的知识图谱问答</h3><p>两种技术的具体实现过程略过，对比如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/Lambda演算vs.DCSTree.jpg" alt="Lambda演算vs.DCSTree"></p>
<h3 id="基于语义检索的知识图谱问答"><a href="#基于语义检索的知识图谱问答" class="headerlink" title="基于语义检索的知识图谱问答"></a>基于语义检索的知识图谱问答</h3><ul>
<li>基于显示特征的知识检索<ul>
<li>关键词检索</li>
<li>文本蕴含推理</li>
<li>逻辑表达式</li>
<li><div class="note primary">
            <p>给出了许多研究进展。</p>
          </div></li>
</ul>
</li>
<li>基于端到端的知识图谱问答<ul>
<li>LSTM</li>
<li>Attention Model</li>
<li>Memory Network</li>
<li><div class="note primary">
            <p>其中有部分问题：</p><ol><li>如何学习？<ul><li>RNN</li><li>CNN</li><li>Transformer</li></ul></li><li>问句如何表示？<ul><li>取所有词向量的平均值</li><li>关注答案不同的部分，问句的表示应该问句的不同部分</li><li>等</li></ul></li><li><strong>考虑多维度的相似度</strong><ul><li>从多个角度计算问句和知识的语义匹配（语义相似度）</li><li>问句如何表示？</li><li>依据问答特点，考虑答案不同维度的信息</li></ul></li></ol><p>PPT 中给出了许多研究进展，包括最基本的做法。</p>
          </div>
<img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/基于语义检索的知识图谱问答.jpg" alt="基于语义检索的知识图谱问答"></li>
</ul>
</li>
</ul>
<h3 id="基于神经符号计算的知识图谱问答"><a href="#基于神经符号计算的知识图谱问答" class="headerlink" title="基于神经符号计算的知识图谱问答"></a>基于神经符号计算的知识图谱问答</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/符号语义解析vs.深度学习.jpg" alt="符号语义解析vs.深度学习"></p>
<ul>
<li><p>基于序列学习的解析方法</p>
<ul>
<li>seq2seq<ul>
<li>RNN-based</li>
<li>with Attention</li>
</ul>
</li>
<li><p>基于序列学习的神经符号计算</p>
<div class="note primary">
            <p>就是运用<strong>基于符号语义解析的知识图谱问答</strong>的原理，让神经网络生成这些符号，而不是生成文字。</p>
          </div>
<blockquote>
<p>基于序列学习的方法将问句和答案的逻辑表达式看作为两个序列</p>
<ul>
<li>使用序列转换的神经网络模型（如 Seq2Seq）来建模</li>
<li>神经网络生成的逻辑表达式可能不合语法规范</li>
</ul>
</blockquote>
<ul>
<li>Seq2Tree</li>
</ul>
</li>
</ul>
</li>
<li>基于动作序列的解析方法<ul>
<li>Seq2Action</li>
</ul>
</li>
<li>基于对战神经网络的端到端问答方法</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>限定域的深度问答的准确度比较高，开放域的深度问答的准确度还是处于较低的水平。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/深度问答的性能.jpg" alt="深度问答的性能"></p>
<h1 id="对话系统"><a href="#对话系统" class="headerlink" title="对话系统"></a>对话系统</h1><p>对话系统也可以直白的称为聊天机器人。<br>目前 54% 的用户会使用闲聊（开放域对话）功能。26% 的用户会选择使用某些功能性功能，比如查出行路线、查天气等。其余小部分用户使用其他的功能。<br>目前大部分的聊天机器人都基于<strong>微软小冰</strong>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/各种聊天机器人.jpg" alt="各种聊天机器人"></p>
<p>聊天机器人一共分为两种：</p>
<ol>
<li>检索式</li>
<li>生成式</li>
</ol>
<h2 id="Response-Selection-for-Retrieval-based-Chatbots"><a href="#Response-Selection-for-Retrieval-based-Chatbots" class="headerlink" title="Response Selection for Retrieval-based Chatbots"></a>Response Selection for Retrieval-based Chatbots</h2><p>检索式又分为单轮和多轮。<br>单轮不考虑回复历史。下图展示了一个单轮回复的场景，用户提出一个问题，机器人需要在一堆回复中检索出一个最有可能的结果来对用户进行回复。多轮回复与单轮类似，只不过多轮需要考虑上下文的对话。最后也是选择一个最优可能的结果进行回复。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复.jpg" alt="检索式单轮回复"></p>
<div class="note info">
            <p>&emsp;&emsp;对于单轮：<br>&emsp;&emsp;回复不只回复 Top1 的候选回复，而是要训练一个 classifier，从而随机地返回一个回复。因为如果回复总是为同一个，用户可能会感觉很无聊。<br>&emsp;&emsp;对于多轮：<br>&emsp;&emsp;有一些挑战：</p><ul><li>A hierarchical data structure<ul><li>Words -&gt; utterances -&gt; session</li></ul></li><li>Information redundancy<ul><li>Not all words and utterances are useful for response selection</li></ul></li><li>Logics<ul><li>Order of utterances matters in response selection</li><li>Long-term dependencies among words and utterances</li><li>Constraints to proper responses</li></ul></li></ul>
          </div>
<p>下面是检索式单轮回复系统架构图和多轮回复系统架构图的对比。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复架构图.jpg" alt="检索式单轮回复架构图"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式多轮回复架构图.jpg" alt="检索式多轮回复架构图"></p>
<h3 id="单轮回复中使用的模型"><a href="#单轮回复中使用的模型" class="headerlink" title="单轮回复中使用的模型"></a>单轮回复中使用的模型</h3><p>一共有两种框架，分别为：Framework I 和 Framework II。<br><strong>Framework I 和 Framework II 的区别是</strong>：</p>
<ol>
<li>Framework I 是将句子表示为向量，Framework II 将字表示为向量。</li>
</ol>
<p><strong>Framework I 和 Framework II 的比较：</strong></p>
<ul>
<li>Efficacy（功效）：<ol>
<li>一般来讲，在外界公布出的数据集上，Framework II 模型比 Framework I 模型更好。因为在 Framework II 中的 interaction 充分保留了一个 message-response pair 中的匹配信息。</li>
</ol>
</li>
<li>Efficiency（效率）：<ol>
<li>由于过多的 interaction，Framework II 的模型普遍比 Framework I 的模型在计算上代价更大。</li>
<li>由于可以预先计算 messages and responses 的表示并将它们以索引形式存储。所以当对线上响应时间有严格要求时， Framework I 的模型更可取。</li>
</ol>
</li>
</ul>
<p>下图是 Framework I 的架构，其中最下层的 sentence embedding layer 大概就是词向量，然后需要经过一个 Representation function（这个 function 下面会给出架构）。最后将已经经过 Representation function 转换后的 q 和 r 送入 Matching layer，该层有一个 Matching function（这个 function 下面也会给出架构）。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I.jpg" alt="检索式单轮回复的 Framework I"></p>
<p>下图是 Representation funtion 的结构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I 的 Representation funtion.jpg" alt="检索式单轮回复的 Framework I 的 Representation funtion"></p>
<p>下图是 Matching funtion 的结构：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework I 的 Matching funtion.jpg" alt="检索式单轮回复的 Framework I 的 Matching funtion"></p>
<p><strong><em>有一些特殊的模型：Arc-I，Attentive LSTM 等</em></strong></p>
<p>Framework II 的架构与 Framework I 类似，只是多了一个 Interaction Function。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework II.jpg" alt="检索式单轮回复的 Framework II"></p>
<p>Interaction 由两种形式：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/2019 CCF会议总结/检索式单轮回复的 Framework II 中 Interaction 的两种类型.jpg" alt="检索式单轮回复的 Framework II 中 Interaction 的两种类型"></p>
<p><strong><em>有一些特殊的模型：Match Pyramid，Match LSTM 等</em></strong></p>
<p>PPT 中有数据集。以及很多 reference。</p>
<h3 id="多轮回复中使用的模型"><a href="#多轮回复中使用的模型" class="headerlink" title="多轮回复中使用的模型"></a>多轮回复中使用的模型</h3><p>&emsp;&emsp;对于多轮回复也有两种框架，分别为：Framework I 和 Framework II。<br>&emsp;&emsp;具体的架构略。PPT 里都有。</p>
<h1 id="技术总结"><a href="#技术总结" class="headerlink" title="技术总结"></a>技术总结</h1><h2 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h2><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>conference</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习算法（三）：RNN 各种机制</title>
    <url>/zcy/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ARNN%20%E5%90%84%E7%A7%8D%E6%9C%BA%E5%88%B6.html</url>
    <content><![CDATA[<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>&emsp;&emsp;该<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#长短期记忆——Long-Short-term-Memory-LSTM">博客</a>中描述了一个 LSTM 的例子，已经把大部分的东西概括了。但是今天看了别人的代码，这是第一次见到代码形式的 LSTM，感觉还是有些地方有问题。以下就记录这些问题。<br>&emsp;&emsp;下图是吴恩达深度学习第五周作业中的图片，是一个 LSTM 单元。与李宏毅老师做的图有略微不同，并且在下图中将 input gate 称为了 update gate。</p>
<ol>
<li>首先是<strong>输入的问题</strong>。一般来说一个 LSTM 的输入是前一<strong>个</strong> LSTM 的输出值 <script type="math/tex">a</script> 以及输入值 <script type="math/tex">x</script>（对于第 2 层的 LSTM 的 输入值就是前一<strong>层</strong>的输出值）。但是众所周知，<strong>LSTM 每个门的输入肯定只有一个向量，<script type="math/tex">a</script> 和 <script type="math/tex">x</script> 是两个向量，那么如何处理呢？</strong> <strong>1)</strong>在下图中使用了 <script type="math/tex">[a^{<t-1>},x^{<t>}]</script> 进行向量拼接。<strong>2)</strong>在我看的代码中直接使用了加法进行相加，代码<a href="https://github.com/Alex-Fabbri/lang2logic-PyTorch/blob/master/seq2seq/atis/lstm/main.py" target="_blank" rel="noopener">在这</a>，但是代码量太大了，随便看看就行了。二者的差别无非是向量的维度。</li>
<li>第一个 LSTM 的输入值怎么处理？因为它不存在前一个 LSTM。答：<strong>暂且使用随机初始化，具体还要补充</strong>（感觉 0 也可以，婴儿出生的时候不就是一张白纸吗。。。）。</li>
<li>记忆单元中的数据也可以随机初始化或者直接为 0。</li>
<li>每一层的 LSTM 都权重共享。意思是每一层都有多个 LSTM，里面的权重值其实是同一份。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/LSTM cell.jpg" alt="LSTM cell"></p>
<p>&emsp;&emsp;下图是多个 LSTM 运行的示意图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/多个 LSTM.jpg" alt="多个 LSTM"></p>
<p>&emsp;&emsp;下图是 LSTM 的反向传播，被称为 BPTT（backpropagation through time）。由于还没遇到过，并且 pytorch 都已经是自动求导，所以目前处于待补充状态。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习算法（三）：RNN 各种机制/LSTM反向传播.jpg" alt="LSTM反向传播"></p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><p>&emsp;&emsp;该<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#seq2seq">博文</a>也可供参考。以前想过 encoder-decoder 如何实现，以为很复杂。现在自己实现了一下，实际上就是很简单的代码，<a href="https://github.com/yan624/machine_learning_algorithms/blob/master/dl/seq2seq.py" target="_blank" rel="noopener">代码地址</a>。<br>&emsp;&emsp;由于 seq2seq 有两个输入。对于 encoder 输入包含隐藏状态 a 以及 输入值 x。对于 decoder 输入包含隐藏状态 a 和 encoder 的输出。这实际上跟 LSTM 差不多，如果 seq2seq 的神经元使用 LSTM 的话，实际上就一模一样了。我只使用了简单的加法，将两个输入合并。当然也可以使用其他方法，比如拼接。其他的方法可以自行发挥。<br>&emsp;&emsp;另外对于 seq2seq 学习，已经不需要每个输入的长度都相等了，可以在句子的最后加入一个结束符，如<code>&lt;EOS&gt;</code>，以此判断输入是否结束。但是这样做如何进行向量化呢？暂时未知，待补充。</p>
<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>&emsp;&emsp;<a href="https://www.zhihu.com/question/68482809/answer/264632289" target="_blank" rel="noopener">参考文章</a>；<a href="https://www.cnblogs.com/robert-dlut/p/5952032.html" target="_blank" rel="noopener">Attention历史</a><br>&emsp;&emsp;实际上九几年的时候在CV领域已经有这概念了。RNN 领域第一篇文章《Recurrent Models of Visual Attention》。<br>&emsp;&emsp;<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#Attention">博客</a>中有写到如何计算 Attention。</p>
<h2 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h2><p>&emsp;&emsp;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#Memory-Network">博客</a>有记一些基础的东西。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习算法（二）：simple RNN 推导与理解</title>
    <url>/zcy/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asimple%20RNN%20%E6%8E%A8%E5%AF%BC%E4%B8%8E%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_recurrent_neural_network" target="_blank" rel="noopener">github地址</a><br>simple RNN只实现了正向传播，反向传播没有实现。是因为simple RNN有梯度消失的问题，索性直接不写了。下一节直接写LSTM。</p>
<h1 id="simple-RNN和simple-NN的对比"><a href="#simple-RNN和simple-NN的对比" class="headerlink" title="simple RNN和simple NN的对比"></a>simple RNN和simple NN的对比</h1><p>本来觉得simple RNN挺简单的，只不过是simple NN的扩展，区别无非是将simple NN的神经元换成一个RNN cell。但是实际上没那么简单，特别是加上将数据向量化后计算。感觉简直和simple NN是两种架构。</p>
<h2 id="input"><a href="#input" class="headerlink" title="input"></a>input</h2><p>下图是一个simple NN的架构，其中的一层也叫做全连接层。所以也叫做前馈神经网络。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>网络中输入的x是一个纯数字，而不是向量。如果输入的是向量，就代表一次输入了多条样本。总的来说，一条样本向量化为：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1\\
    x_2\\
    x_3\\
\end{pmatrix}</script><p>多条样本向量化为：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 & x_4\\
    x_2 & x_5\\
    x_3 & x_6\\
\end{pmatrix}</script><p>但是在simple <strong>R</strong>NN中，因为RNN可以用做自然语言处理，在NLP领域一个数据就是一个单词或者一个词组，我们需要先将词组用数字表示，但是一个字只用一个数表示显然是不现实的。我们通常使用词向量（word vector）表示，所以问题就来了。如果我用simple NN来做自然语言处理，我该怎么处理这些数据。<br>我需要输入这句话——我 是 一名 学生。将其用变量一一对应：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 = \text{我}\\
    x_2 = \text{是}\\
    x_3 = \text{一名}\\
    x_4 = \text{学生}\\
\end{pmatrix}</script><p>如果再输入一句话——今天 天气 好像 不错。将其用变量一一对应：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    x_1 = \text{我} & x_5 = \text{今天}\\
    x_2 = \text{是} & x_6 = \text{天气}\\
    x_3 = \text{一名} & x_7 = \text{好像}\\
    x_4 = \text{学生} & x_8 = \text{不错}\\
\end{pmatrix}</script><p>这样会出现<strong>极大的问题</strong>，即文字无法进行数学运算。所以需要将文字转为词向量。但是问题是我如何在一个神经元输入一个向量？我如果输入多条样本，那么我整个输入值x就会变成<strong>3维的矩阵</strong>。simple NN显然是处理不了的。所以有个折中的方法，即将每个词的词向量加起来除以词的个数。即：</p>
<script type="math/tex; mode=display">
\frac{(v_{\text{我}} + v_{\text{是}} + v_{\text{一名}} + v_{\text{学生}})}{4} =  v_{\text{我是一名学生}}</script><p>其中v代表某个词对应的词向量。这样就将4个向量合并成一个向量了。然后simple NN就可以处理了。<br>但是这样处理肯定太勉强了，所以就出现了simple RNN，它可以处理上述这种问题。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型/吴恩达深度学习中的RNN示意图.jpg" alt="吴恩达深度学习中的RNN示意图"><br>上图就是一个simple <strong>R</strong>NN架构，看起来跟simple NN不一样。但是其实你只要将图片往右旋转90度，就一样了。还有一点不太一样，就是simple <strong>R</strong>NN中的一个神经元跟simple NN中的一层神经元一样。<br><div class="note info">
            <p>simple <strong>R</strong>NN其实就是simple NN的神经元被替换成了simple <strong>R</strong>NN的神经元。而simple <strong>R</strong>NN的一个神经元本身里面还有多个单元，图上就是一个神经元里有4个单元。<br>所以simple <strong>R</strong>NN的神经元可以被叫做记忆细胞。<br>如果重新描述一遍就是simple <strong>R</strong>NN其实就是simple NN的神经元被替换成了simple <strong>R</strong>NN的记忆细胞。<br>而simple <strong>R</strong>NN的记忆细胞里面有多个神经元用来处理进行向量计算。</p>
          </div></p>
<h2 id="simple-RNN的另一个输入值a"><a href="#simple-RNN的另一个输入值a" class="headerlink" title="simple RNN的另一个输入值a"></a>simple RNN的另一个输入值a</h2><p>下图是simple RNN的一个记忆细胞。由于其概念大都需要数值来演示，但是大量的数值难以书写，并且用语言实在难以描述。所以以下均使用一个矩阵的字母表示来演示。<br>此处会有几个问题：</p>
<ol>
<li>乍一看很简单，但是在实现代码的时候，脑子会转不过来。<strong>因为你碰到的是向量化后的运算</strong>。所以你对各个W，b，A以及X的形状难以确定。不信就自己写一下代码，如果你以前没写过simple RNN的代码，肯定要在确定形状这卡至少一个小时。</li>
<li>A的形状尤其难确定，会一时之间绕不过来。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/simple%20rnn%20cell.png" alt="simple rnn cell"></li>
</ol>
<h3 id="以一个记忆细胞为例"><a href="#以一个记忆细胞为例" class="headerlink" title="以一个记忆细胞为例"></a>以一个记忆细胞为例</h3><p>设词向量的维度为300，一个记忆细胞的units为32——keras代码表示：simpleRNN(32)，并且进行的是18元分类问题。</p>
<h4 id="以一个样本为例"><a href="#以一个样本为例" class="headerlink" title="以一个样本为例"></a>以一个样本为例</h4><p>，输入样本数为1。<br>则运算过程为：</p>
<script type="math/tex; mode=display">
\begin{align}
    A\_next & = tanh(Wx_{32, 300} * X_{300, 1} + Wa_{32, 32} * A\_prev_{32, 1} + ba_{32, 1})\\
    y & = softmax(Wy_{18, 32} * A\_next + by_{18, 1})
\end{align}</script><p>(数字, 数字) 的表示形式是在线性代数里表示形式，这样看起来方便点。第一个数字表示行，第二个数字表示列。tanh和softmax就不解释了。<br>我依次解释：</p>
<ol>
<li>权重值Wx比较好理解，由于输入值X是一个 (300, 1) 的矩阵（也可以叫向量）。Wx为 (32, 300) 是因为记忆细胞的一个单元为32，并且词的特征为300。如果将一个记忆细胞看作是一个隐藏层，那么神经元个数就是32，300就代表前一层的输入。</li>
<li>权重值Wa为什么是 (32, 32) ?由于a实际上就是激活值，激活值我们可以通过w * x计算得到。显然结果是 (32, 1) ，那么权重值Wa的第二个参数就可以确定大小了（矩阵相乘，第一个矩阵的第二维和第二个矩阵的第一维必须一样），权重值Wa的第一个参数实际上跟权重值Wx一样，都是units。</li>
<li>Wy为什么是 (18, 32) ?18是因为这是一个18元分类问题，32是因为A的第一维是32。</li>
</ol>
<p>其中最难确定的就是Wa的形状。经过上面推导就可以知道了，第一维代表units，第二维代表需要与A的第一维，即为A的第一维的大小。</p>
<h4 id="以128个样本为例"><a href="#以128个样本为例" class="headerlink" title="以128个样本为例"></a>以128个样本为例</h4><p>如果上面没懂可以再看一遍多个样本</p>
<script type="math/tex; mode=display">
\begin{align}
    A\_next & = tanh(Wx_{32, 300} * X_{300, 128} + Wa_{32, 32} * A\_prev_{32, 128} + ba_{32, 1})\\
    y & = softmax(Wy_{18, 32} * A\_next + by_{18, 1})
\end{align}</script><p>说白了A其实激活值，看起来比较晕是因为多了一个初始的A0，实际上A0就是激活值，只不过需要初始化一下，它的形状就是激活值的形状。</p>
<h3 id="以多个记忆细胞为例"><a href="#以多个记忆细胞为例" class="headerlink" title="以多个记忆细胞为例"></a>以多个记忆细胞为例</h3><p>过程就是上面的过程，唯一有区别的是输入值X。因为有了多个记忆细胞，所以X变成了3维，第3维就是timestep——时间步。<br>但是其实计算，跟上面一模一样，假设现在有6个时间步。伪代码如下：<br><figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">rnn_cell</span><span class="params">(A_prev, Xt, parameters)</span></span>:</span><br><span class="line">	<span class="keyword">do</span>上面的操作</span><br><span class="line"></span><br><span class="line">timesteps = <span class="number">6</span></span><br><span class="line">X = rand((<span class="number">300</span>, <span class="number">128</span>, timesteps))</span><br><span class="line">A0 = rand((<span class="number">32</span>, <span class="number">128</span>))</span><br><span class="line">parameters = 初始化所有参数</span><br><span class="line"><span class="keyword">for</span> ts <span class="keyword">in</span> range(timesteps):</span><br><span class="line">	rnn_cell(A0, X[:, :, ts], parameters)</span><br></pre></td></tr></table></figure></p>
<p>其实就是遍历每一个时间步而已。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/RNN.png" alt="RNN"></p>
<h1 id="simple-RNN-的缺陷"><a href="#simple-RNN-的缺陷" class="headerlink" title="simple-RNN 的缺陷"></a>simple-RNN 的缺陷</h1><p>RNN一个最大的缺陷就是梯度消失与梯度爆炸问题，由于这一缺陷，使得RNN在长文本中难以训练，这才诞生了LSTM及各种变体，来源于<a href="https://zhuanlan.zhihu.com/p/44163528" target="_blank" rel="noopener">专栏</a>。梯度消失的原因：参考<a href="https://zhuanlan.zhihu.com/p/28687529" target="_blank" rel="noopener">专栏</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>2017CS224n学习笔记</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2017CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<h1 id="开场白"><a href="#开场白" class="headerlink" title="开场白"></a>开场白</h1><p>&emsp;&emsp;略</p>
<h1 id="词向量表示：word2vec"><a href="#词向量表示：word2vec" class="headerlink" title="词向量表示：word2vec"></a>词向量表示：word2vec</h1><p>&emsp;&emsp;课程计划如下：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/课程计划.jpg" alt="课程计划"></p>
<p>&emsp;&emsp;<strong>神经网络词嵌入学习的通用做法</strong>：定义一个模型，根据中心词 <script type="math/tex">w_t</script> 去预测上下文单词。给定 <script type="math/tex">w_t</script> 的条件下 context 的概率。</p>
<script type="math/tex; mode=display">
p(context|w_t) = \dots</script><p>&emsp;&emsp;然后用损失函数判断预测的准确性，例如：</p>
<script type="math/tex; mode=display">
J = 1 - p(w_{-t}|w_t), \quad  \text{-t 代表 t 周围的单词}</script><p>&emsp;&emsp;如果可以精准地根据 t 预测到这些单词，那么概率就为 1，于是损失就没有了。但通常情况下，做不到这点。<strong>所以我们应该调整词汇表示，从而使损失最小化</strong>。<br>&emsp;&emsp;下图是以前的低维词向量表示方法，2003 年 Bengio 发表的这篇现在属于开创性的论文其实并没有太多人关注，因为那时候深度学习并没有很流行。但是当这篇论文开始流行的时候，就开始大行其道了。于是 2008 年 Collobert 和 Weston 开启了一个新方向，<strong>他们觉得如果我们只想要得到好的单词表示，我们甚至不需要构建一个具有预测功能的概率语言模型（probabilistic language model），我们只需要找到一种学习单词表示的方法即可</strong>。于是 2013 年有了 word2vec 模型。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/以前的低维词向量表示方法.jpg" alt="以前的低维词向量表示方法"></p>
<p>&emsp;&emsp;word2vec 是一个软件，实际上，它里面包含很多东西。有两个用于生成词汇向量的算法（Hierarchical softamx，negative sampling），还有两套效率中等的训练方法（Skip-grams，CBOW）。<em>这里的软件应该指的不是那种可以运行 exe 文件</em>。本节只讲 skip-grams 算法，并且不会讲那两个高效的词向量生成算法，而是将一个效率极低的算法（因为比较简单且包含了基本概念）。<br>&emsp;&emsp;skip-grams 模型的概念是：在每一个估算步中，都取一个词为中心词汇，然后尝试预测它<strong>一定范围内</strong>的上下文的词汇。这个模型将定义一个概率分布：<strong>给定一个中心词汇预测某个单词在它上下文中出现的概率</strong>。如下图所示：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/skip-grams模型.jpg" alt="skip-grams模型"></p>
<p>&emsp;&emsp;我们将会选取词汇的向量表示，以让概率分布值最大化。</p>
<h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>&emsp;&emsp;我们需要做的是定义一个半径 m，然后从中心词汇开始到距离为 m 的位置来预测周围的词汇。这句话比较抽象，因为到这为止，你还是构建不出一个模型（优化目标）。下面先给出模型的公式，注意一撇不是求导：</p>
<script type="math/tex; mode=display">
J'(\theta) = \prod^T_{t=1} \prod_{-m \leq m, j \neq 0} p(w_{t+j}|w_t;\theta)</script><p>&emsp;&emsp;其中定义一句话有 T 个单词，word t = 1 <script type="math/tex">\dots</script> T。上式中 m 为半径窗口，j 为整个窗口之中的索引。先不看第一个累乘符号，当 t = 1 时，也就是当中心词的索引为 1 时，以 m 为半径，预测该中心词汇的上下文单词出现的概率，并将所有的概率累乘。即公式： <script type="math/tex">\prod_{-m \leq m, j \neq 0} p(w_{t+j}|w_t;\theta)</script>。而第一个累乘符号指的是，将句子中每一个字都当做一次中心词汇，然后将概率再累乘起来。当然当中心词的索引比较靠前时，可能窗口会超出句子的前部，比如 中心词汇所以为 1，而 m = 5，则需要预测 -4，-3… 的位置，这显然不可能，所以需要自己做一下处理。<br>&emsp;&emsp;公式中的 <script type="math/tex">\theta</script> 是模型唯一的参数，让上下文所有词汇出现的概率都尽可能的高，其实 <script type="math/tex">\theta</script> 就是词向量，而模型的输入就是 one-hot 表示。但是，处理概率问题是一件很不爽的事，我们要做最大化操作，实际上就是解决对数分布的问题。这样求积就会变成求和，如下所示：</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{T} \sum^T_{t=1} \sum_{-m \leq m, j \neq 0} log \, p(w_{t+j}|w_t;\theta)</script><p>&emsp;&emsp;这样我们就得到了<strong>负的对数似然</strong>，上述公式就是最终版。但是这里还有一小点就是 m 其实也算是模型的参数，但是确是<strong>超参数</strong>，需要自己手动改的。所以上面说“<em>公式中的 <script type="math/tex">\theta</script> 是模型唯一的参数</em>”也没错。事实上这个模型还有很多其他的超参数，但是现在暂且视为常数。<br>&emsp;&emsp;公式前面有个负号，是因为我们要求最小化问题，而原式只能取最大值，所以取了个负号。</p>
<h3 id="确定相应的概率分布"><a href="#确定相应的概率分布" class="headerlink" title="确定相应的概率分布"></a>确定相应的概率分布</h3><p>&emsp;&emsp;那么我们具体应该怎么通过中心词汇来预测周围单词出现的概率呢？也就是说公式中的函数 p 应该是什么。其实 p 就是 softmax 函数。具体来说就是用由词向量构成的中心词汇去预测周围词汇的概率分布。下图就是 softmax 函数。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/softmax.jpg" alt="softmax"></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>&emsp;&emsp;最前面讲到需要有一个损失函数来判断预测的准确性。我们使用 cross-entropy loss。</p>
<h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><script type="math/tex; mode=display">
\begin{align}
     & \frac{\partial}{\partial v_c} log \frac{exp(u^T_o v_c)}{\sum^v_{w=1} exp(u^T_w v_c))} \\
    = & \frac{\partial}{\partial v_c} (\underbrace{log \, exp(u^T_o v_c)}_{1} - \underbrace{log \, \sum^v_{w=1} exp(u^T_w v_c)}_2) \\
     & \frac{\partial}{\partial v_c} log \, exp(u^T_o v_c) & \text{1} \\
    = & \frac{\partial}{\partial v_c} u^T_o v_c = u_o  \\
     & \frac{\partial}{\partial v_c} log \, \sum^v_{w=1} exp(u^T_w v_c) & \text{2} \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \frac{\partial}{\partial v_c} \sum^v_{x=1} exp(u^T_x v_c) \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \sum^v_{x=1} \frac{\partial}{\partial v_c} exp(u^T_x v_c) \\
    = & \frac{1}{\sum^v_{w=1} exp(u^T_w v_c)} \sum^v_{x=1} exp(u^T_x v_c) \, u_x \\
    = & \sum^v_{x=1} \frac{exp(u^T_x v_c)}{\sum^v_{w=1} exp(u^T_w v_c)} \, u_x \\
    = & \sum^v_{x=1} p(x|c) u_x \\
     & u_o - \sum^v_{x=1} p(x|c) u_x & \text{合并}\\
\end{align}</script><h1 id="高级词向量表示"><a href="#高级词向量表示" class="headerlink" title="高级词向量表示"></a>高级词向量表示</h1><p>&emsp;&emsp;略。</p>
<h1 id="Word-Window分类与神经网络"><a href="#Word-Window分类与神经网络" class="headerlink" title="Word Window分类与神经网络"></a>Word Window分类与神经网络</h1><p>&emsp;&emsp;略。</p>
<h1 id="反向传播和项目建议"><a href="#反向传播和项目建议" class="headerlink" title="反向传播和项目建议"></a>反向传播和项目建议</h1><p>&emsp;&emsp;讲反向传播，略。</p>
<h1 id="依存分析"><a href="#依存分析" class="headerlink" title="依存分析"></a>依存分析</h1><h2 id="语言结构分析历史回顾"><a href="#语言结构分析历史回顾" class="headerlink" title="语言结构分析历史回顾"></a>语言结构分析历史回顾</h2><h2 id="Dependency-Grammar-and-Dependency-Structure"><a href="#Dependency-Grammar-and-Dependency-Structure" class="headerlink" title="Dependency Grammar and Dependency Structure"></a>Dependency Grammar and Dependency Structure</h2><p>&emsp;&emsp;</p>
<h1 id="Tensorflow-入门"><a href="#Tensorflow-入门" class="headerlink" title="Tensorflow 入门"></a>Tensorflow 入门</h1><p>&emsp;&emsp;略，不学 tensorflow。</p>
<h1 id="RNN和语言模式"><a href="#RNN和语言模式" class="headerlink" title="RNN和语言模式"></a>RNN和语言模式</h1><p>&emsp;&emsp;详解 RNN 很多问题。</p>
<h1 id="机器翻译和高级循环神经网络"><a href="#机器翻译和高级循环神经网络" class="headerlink" title="机器翻译和高级循环神经网络"></a>机器翻译和高级循环神经网络</h1><p>&emsp;&emsp;花了二三十分钟讲机器翻译，然后讲解各类 RNN。</p>
<h1 id="神经机器翻译和注意力模型"><a href="#神经机器翻译和注意力模型" class="headerlink" title="神经机器翻译和注意力模型"></a>神经机器翻译和注意力模型</h1><p>&emsp;&emsp;先将机器翻译，后讲 attention。</p>
<h2 id="神经机器翻译"><a href="#神经机器翻译" class="headerlink" title="神经机器翻译"></a>神经机器翻译</h2><h2 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h2><p>&emsp;&emsp;下图是 attention 的工作原理。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/attention机制.jpg" alt="attention机制"></p>
<ol>
<li>图中的 a 代表 score，<script type="math/tex">\bar{h}_s</script> 代表 encoder 中每个 time step 生成的隐藏状态向量，<script type="math/tex">c_t</script> 代表 attention 之后的向量；</li>
<li>首先将开始标志输入到一个 decoder，代表开始进行翻译，输出一个单词后，将该 decoder 的<strong>隐藏状态</strong>（注意是隐藏状态而不是输出值，此节课视频中有明确指出）与 encoder 中的<strong>隐藏状态</strong>进行计算得到一个 score。打分的公式为 <script type="math/tex">score(h_{t - 1}, \bar{h}_s)</script>，score 具体是什么公式可以自己定义，最简单就是向量内积，下面会细说；</li>
<li>关于 score 函数，它有多种选择，<strong>注意一点</strong>下面的 score 函数只是对<strong>一个</strong>时间步上的隐藏状态打分，<script type="math/tex">\bar{h}_s</script> 也可以是个矩阵，即一步计算所有时间步的 attention score（这做法是最好的）。以下罗列几种做法，被广泛采用（2017 年的说法，现不知）的是第二个表达式，第三个表达式的 <script type="math/tex">v_a</script> 也是一个向量参数。另外对于第三个表达式 <script type="math/tex">v_a tanh(W_a [h_t;\bar{h}_s])</script>，它不是 score function，而是 Bahdanau，不知道为什么把它放到 score function 这。<script type="math/tex; mode=display">
score(h_t, \bar{h}_s) = 
\begin{cases}
h^T_t \bar{h}_s \\
h^T_t W_a \bar{h}_s \\
v_a tanh(W_a [h_t;\bar{h}_s])
\end{cases}</script></li>
<li>将 score 送入 softmax 得到概率；</li>
<li>通过公式 <script type="math/tex">c_t = \sum_s a_t(s)\bar{h}_s</script>，将所有的向量乘上注意力分数加起来；</li>
<li>将此新向量当做下一个 decoder 的输入；</li>
</ol>
<p>&emsp;&emsp;下图是加 attention 机制和不加的区别。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/加入attention机制后的性能.jpg" alt="加入attention机制后的性能"></p>
<h2 id="coverage"><a href="#coverage" class="headerlink" title="coverage"></a>coverage</h2><p>&emsp;&emsp;coverage = more attention，想法源于计算机视觉，请看下图。神经网络读入一张图片，要求输出一段话。但是我们知道一段话不仅要描写图中的鸟，还要描写鸟旁边的事物，所以就引出了多次注意，即神经网络需要注意图中更多的地方。将这一想法引入 NLP 中，其实就是多做几次 attention，<em>注：这一想法貌似就是后来 transformer 的 multi-head attention</em>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/more attention（coverage）.jpg" alt="more attention(coverage)"></p>
<h2 id="search"><a href="#search" class="headerlink" title="search"></a>search</h2><p>&emsp;&emsp;</p>
<h1 id="GRU及NMT的其他议题"><a href="#GRU及NMT的其他议题" class="headerlink" title="GRU及NMT的其他议题"></a>GRU及NMT的其他议题</h1><h2 id="GRUs-LSTMs"><a href="#GRUs-LSTMs" class="headerlink" title="GRUs/LSTMs"></a>GRUs/LSTMs</h2><h2 id="NMT-evaluation"><a href="#NMT-evaluation" class="headerlink" title="NMT evaluation"></a>NMT evaluation</h2><h1 id="语音处理的端对端模型"><a href="#语音处理的端对端模型" class="headerlink" title="语音处理的端对端模型"></a>语音处理的端对端模型</h1><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h1 id="树RNN和短语句分析"><a href="#树RNN和短语句分析" class="headerlink" title="树RNN和短语句分析"></a>树RNN和短语句分析</h1><p>&emsp;&emsp;人类语言具有嵌套结构（训练结构、树结构），如：[The man from [the company that you spoke with about [the project] yesterday]]。<br>&emsp;&emsp;那么如何使用向量来表示这些句子的语义呢？可以使用 tree RNN，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/Recursive vs. recurrent neural network.jpg" alt="Recursive vs. recurrent neural network"></p>
<blockquote>
<p>&emsp;&emsp;<strong>Tree recursive neural network 的问题在于你需要得到一个树形结构</strong>，这是一个比较大的问题。树形网络并没有火遍全球，在语言方面确实有原因喜欢这类的模型（原因后面会有讲到），但是如果你在 arxiv 里面找，人们在语言神经网络研究中所使用的的方法时，你会发现人们并不多使用树形结构模型。LSTMs 的比例几乎是其十倍之多。<br>&emsp;&emsp;这里面比较大的原因是树形递归神经网络的使用者必须构建一个树形结构。<strong>在你构建完成后，使用反向传播学习模型会是一个问题</strong>。<br>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a> 25分开始。</p>
</blockquote>
<h2 id="simple-tree-RNN"><a href="#simple-tree-RNN" class="headerlink" title="simple tree RNN"></a>simple tree RNN</h2><h3 id="树RNN的计算"><a href="#树RNN的计算" class="headerlink" title="树RNN的计算"></a>树RNN的计算</h3><p>&emsp;&emsp;那么具体如何使用树形递归神经网络计算呢？比如下图中使用向量 [3 3] 和 [8 5] 计算，输入进神经网络之后，就会输出一个向量 [8 3] 和一个分数 1.3，这个分数代表输出的向量 [8 3] 是否合理（即结构是否合理。如果不太理解什么是结构是否合理，请看下两张图以及博客内容即可理解）。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive neural network for training.jpg" alt="recursive neural network for training"></p>
<p>&emsp;&emsp;具体的做法如下图所示。应该很好理解，就不详细说明了，其中对于计算 score 的 U，我猜测可能是一个 trainable 的参数，视频中并没有详细的说明。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/recursive neural network details.jpg" alt="recursive neural network details"></p>
<p>&emsp;&emsp;那么到了真正的实战阶段应该怎么做呢？训练一个贪心的解析器，对于单词两两组合，然后发现最前的两个单词 “The cat” 组成的短语训练之后的分数最高，然后我们将 “The cat” 看作一个成分并且尤其对应的语义 [5 2]。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 1.jpg" alt="parsing a sentence with rnn"></p>
<p>&emsp;&emsp;接下来继续重复做，请注意现在的 “The cat” 是一个成分（可看作单词），而不是两个单词。又做一遍解析之后发现 “the mat” 的分数最高。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 2.jpg" alt="parsing a sentence with rnn——2"></p>
<p>&emsp;&emsp;再将 “the mat” 看作一个成分，并拥有对应的语义。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 3.jpg" alt="parsing a sentence with rnn 3"></p>
<p>&emsp;&emsp;以此类推，我们发现 “on the mat” 的分数最高，然后发现 “sat on the mat” 的分数最高，最后就得到 “The cat sat on the mat” 的分数最高。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/parsing a sentence with rnn 4.jpg" alt="parsing a sentence with rnn 4"></p>
<p>&emsp;&emsp;这是一棵解析树（parse tree），我们会得到这棵解析树的分数，它的分数由每个节点的分数加和得到。<strong>我们要做的就是找到由这堆节点所能组成的分数最高的解析树</strong>。<br>&emsp;&emsp;还需要一个优化目标，similar to max-margin parsing(Taskar et al. 2004), a supervised max-margin objective:</p>
<script type="math/tex; mode=display">J = \sum_i s(x_i, y_i) - \max_{y \in A(x_i)} (s(x_i, y) - \Delta(y, y_i))</script><p>&emsp;&emsp;最后我们还需要反向传播算法进行计算，这一工作早在 20 世纪 90 年代就由几个德国人做过了。Goller 和 Kuchler 提出了这个算法，并命名为 <strong>back propagation through structure</strong>.</p>
<h2 id="Syntactically-Untied-RNN"><a href="#Syntactically-Untied-RNN" class="headerlink" title="Syntactically-Untied RNN"></a>Syntactically-Untied RNN</h2><p>&emsp;&emsp;语义解绑树形递归神经网络，这被证明是构建高质量解析器的一个成功的方法。<br>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a>，50 分开始。</p>
<h2 id="Compositionality-Through-Recursive-Matrix-Vector-Spaces"><a href="#Compositionality-Through-Recursive-Matrix-Vector-Spaces" class="headerlink" title="Compositionality Through Recursive Matrix-Vector Spaces"></a>Compositionality Through Recursive Matrix-Vector Spaces</h2><p>&emsp;&emsp;<a href="https://www.bilibili.com/video/av41393758/?p=14" target="_blank" rel="noopener">第十四讲 - 树 RNN 和短语句法分析</a>，65 分开始。</p>
<h2 id="related-work-for-parsing"><a href="#related-work-for-parsing" class="headerlink" title="related work for parsing"></a>related work for parsing</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/CS224n学习笔记/related work for parsing.jpg" alt="related work for parsing"></p>
<h1 id="共指解析"><a href="#共指解析" class="headerlink" title="共指解析"></a>共指解析</h1><p>&emsp;&emsp;</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习算法（一）：simple NN（前馈神经网络的正反向推导）</title>
    <url>/zcy/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9Asimple%20NN%EF%BC%88%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%8F%8D%E5%90%91%E6%8E%A8%E5%AF%BC%EF%BC%89.html</url>
    <content><![CDATA[<div class="note info">
            <p>本文的公式不存在次方的说法，所以看见上标，不要想成是次方。<br>对于权重的表示问题，请看<a href="https://yan624.github.io/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">博客</a>，但是由于是以前的学习笔记，不保证完全正确。<br>如果想了解为什么梯度下降要对w和b求导，可以看<a href="https://yan624.github.io/学习笔记/梯度下降算法的推导.html">这篇</a>。<br><strong>建议边看边写，否则思维跟不上。</strong></p>
          </div>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>参考<a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">文章</a><br>以如下神经网络架构为例。参考<a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">文章</a>中使用了一个2 2 2的神经网络架构，但是现实中神经网络架构不会这么整整齐齐。所以还是使用了略复杂的架构，此外原文中未对bias（偏差）更新。另外原文也没有实现向量化后的计算。虽然在后面的代码写了，但是由于代码太长了，有一种代码我给出来了，你们自己去看的感觉。说实话没多少注释，都没看的欲望(╬￣皿￣)。然后她所使用的符号让我不太习惯，因为看吴恩达以及李宏毅老师使用的符号都是<script type="math/tex">w^l_{ji}\ a^l_i</script>等等，所以自己重新推导一遍，并且使用了数学公式，而不是截图，更好看一点。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>解释一下最下面的神经元，这个神经元初始化为1，也就是意味着1 * b = b。输入值为1，一个偏差乘1还是偏差本身。</p>
<h2 id="关于函数选用"><a href="#关于函数选用" class="headerlink" title="关于函数选用"></a>关于函数选用</h2><p>本文所有激活函数选择sigmoid函数，代价函数选择binary_crossentropy。</p>
<h2 id="一些约定"><a href="#一些约定" class="headerlink" title="一些约定"></a>一些约定</h2><div class="note info">
            <p>本文所有的输入值，激活值，输出值都是<strong>列向量</strong>。</p>
          </div>
<h1 id="初始化数据以及正向传播"><a href="#初始化数据以及正向传播" class="headerlink" title="初始化数据以及正向传播"></a>初始化数据以及正向传播</h1><h2 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h2><p>此处初始化各层的权重值，偏差。由于是演示，所以顺便把输入层也初始化了。<br>设</p>
<script type="math/tex; mode=display">
\begin{cases}
    x_1 = a^0_1 = 0.55, x_2 = a^0_2 = 0.72\\
    y_1 = 0.60, y_2 = 0.54\\
\end{cases}\\
\begin{cases}
    w^1_{11}=0.4236548, w^1_{12}=0.64589411\quad|\quad w^1_{21}=0.43758721, w^1_{22}=0.891773\quad|\quad w^1_{31}=0.96366276, w^1_{32}=0.38344152\\
    b^1_1=0.79172504, b^1_2=0.52889492, b^1_3=0.56804456\\
    w^2_{11}=0.92559664, w^2_{12}=0.07103606, w^2_{13}=0.0871293\quad|\quad w^2_{21}=0.0202184, w^2_{22}=0.83261985, w^2_{23}=0.77815675\\
    b^2_1=0.87001215, b^2_2=0.97861834\\
\end{cases}</script><p>不用多看，反正也用不到几次。。。</p>
<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>对于正向传播，应该是很熟悉了，所以我直接一次写完，不做过多解释。</p>
<h3 id="输入层到隐藏层"><a href="#输入层到隐藏层" class="headerlink" title="输入层到隐藏层"></a>输入层到隐藏层</h3><script type="math/tex; mode=display">
z^1_1 = w^1_{11} * a^0_1 + w^1_{12} * a^0_2 + 1 * b^1_1\\
z^1_2 = w^1_{21} * a^0_1 + w^1_{22} * a^0_2 + 1 * b^1_2\\
z^1_3 = w^1_{31} * a^0_1 + w^1_{32} * a^0_2 + 1 * b^1_3\\</script><p>带入sigmoid函数中，以下开始省略bias乘的1：</p>
<script type="math/tex; mode=display">
a^1_1 = \sigma{(z^1_1)}\\
a^1_2 = \sigma{(z^1_2)}\\
a^1_3 = \sigma{(z^1_3)}\\</script><h3 id="隐藏层到输出层"><a href="#隐藏层到输出层" class="headerlink" title="隐藏层到输出层"></a>隐藏层到输出层</h3><script type="math/tex; mode=display">
z^2_1 = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1\\
z^2_2 = w^2_{21} * a^1_1 + w^2_{22} * a^1_2 + w^2_{13} * a^1_3 + b^2_2\\</script><p>带入sigmoid函数中：</p>
<script type="math/tex; mode=display">
a^2_1 = \sigma{(z^2_1)}\\
a^2_2 = \sigma{(z^2_2)}\\</script><h3 id="计算代价"><a href="#计算代价" class="headerlink" title="计算代价"></a>计算代价</h3><p>以字母J记为代价函数的名称，最后一个表达式为最简版：</p>
<script type="math/tex; mode=display">
\begin{align}
    J & = -[(y_1 * \log(a^2_1) + (1 - y_1) * \log(1 - a^2_1) + (y_2 * \log(a^2_2) + (1 - y_2) * \log(1 - a^2_2)]\\
    J & = -\Sigma^2_{i = 1}{[(y_i * \log(a^2_i) + (1 - y_i) * \log(1 - a^2_i)]}\\
    J & = -\Sigma{[(y * \log(a^2) + (1 - y) * \log(1 - a^2)]}\\
\end{align}</script><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>上述的表达式全部是一个一个列出来的，如果使用向量来表示乘积那就方便很多。可以看到下面只用了五行就写完了上面<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#正向传播">正向传播</a>的所有步骤。<br><div class="note warning">
            <p>如果无法理解这一步那就是不会线性代数的问题，线性代数不在此文的介绍范围之内。</p>
          </div></p>
<script type="math/tex; mode=display">
\begin{align}
    z^1 & = w^1 * a^0 + b^1 & \text{输入层到隐藏层}\\
    a^1 & = \sigma{(z^1)} & \text{带入隐藏层的激活函数}\\
    z^2 & = w^2 * a^1 + b^2 & \text{隐藏层到输出层}\\
    a^2 & = \sigma{(z^2)} & \text{带入输出层的激活函数}\\
    J & = -\Sigma{[(y * \log(a^2) + (1 - y) * \log(1 - a^2)]} & \text{计算代价}\\
\end{align}</script><h2 id="上述表达式代码实现"><a href="#上述表达式代码实现" class="headerlink" title="上述表达式代码实现"></a>上述表达式代码实现</h2><p>最后几节有神经网络numpy实现的全部代码，可以直接跳过本节看下一节，这里的代码只是给出一个直观的理解，可以自己运行看看。<br>受到keras以及万物皆对象的启发，首先建立一个神经元对象<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">'sigmoid'</span>)</span>:</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line">        self.hyperparameters = dict()</span><br><span class="line">        <span class="comment"># W, b, A_prev的导数</span></span><br><span class="line">        self.grads = dict()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hyperparameters</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化超参数，在神经网络中权重值不能初始化为0，偏差可以</span></span><br><span class="line"><span class="string">        :param shape: 神经元的形状，(units, input_shape)</span></span><br><span class="line"><span class="string">        :return: hyperparameters，该神经元的超参数。可以通过hyperparameters['W']、hyperparameters['b']取值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 由于是演示，所以使用了随机初始化</span></span><br><span class="line">        W = np.random.rand(*shape)</span><br><span class="line">        b = np.random.rand(shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> W, b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构造神经元，目前只执行初始化超参数的步骤</span></span><br><span class="line"><span class="string">        :param input_shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        W, b = self._init_hyperparameters(shape=(self.units, input_shape))</span><br><span class="line">        self.hyperparameters[<span class="string">'W'</span>] = W</span><br><span class="line">        self.hyperparameters[<span class="string">'b'</span>] = b</span><br></pre></td></tr></table></figure></p>
<p>为方便起见，将大部分的函数都放入Model中，下面给出所有的代码<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> simple_neural_network.activation_function <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> simple_neural_network.cost_function <span class="keyword">import</span> *</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">'sigmoid'</span>)</span>:</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hyperparameters</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化超参数，在神经网络中权重值不能初始化为0，偏差可以</span></span><br><span class="line"><span class="string">        :param shape: 神经元的形状，(units, input_shape)</span></span><br><span class="line"><span class="string">        :return: hyperparameters，该神经元的超参数。可以通过hyperparameters['W']、hyperparameters['b']取值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 用于是演示，所以使用了随机初始化</span></span><br><span class="line">        hyperparameters = &#123;</span><br><span class="line">                <span class="string">'W'</span>: np.random.rand(*shape),</span><br><span class="line">                <span class="string">'b'</span>: np.random.rand(shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hyperparameters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        构造神经元，目前只执行初始化超参数的步骤</span></span><br><span class="line"><span class="string">        :param input_shape:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.hyperparameters = self._init_hyperparameters(shape=(self.units, input_shape))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 全部的神经元，并且根据神经元数量计算神经网络架构的层数，在计算时需要减1因为输入层不算入神经网络层数</span></span><br><span class="line">        self.neurons = list()</span><br><span class="line">        <span class="comment"># 按顺序缓存A, (Z, W, b)，由于输入层不需要任何缓存，所以放入None填充此位置。方便根据索引取值</span></span><br><span class="line">        self.value_caches = [<span class="keyword">None</span>]</span><br><span class="line">        <span class="comment"># 代价函数</span></span><br><span class="line">        self.cost = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, neuron)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        在模型中添加神经元</span></span><br><span class="line"><span class="string">        :param neuron:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.neurons.append(neuron)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(self, A_prev, W, b)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        正向传播，线性运算：Z = W * A + b</span></span><br><span class="line"><span class="string">        :param A_prev: 前一层的激活值</span></span><br><span class="line"><span class="string">        :param W: 权重值</span></span><br><span class="line"><span class="string">        :param b: 偏差</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        Z: 运算结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        cache = A_prev, (Z, W, b)</span><br><span class="line">        self.value_caches.append(cache)</span><br><span class="line">        <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nonlinear_forward</span><span class="params">(self, A_prev, W, b, activation)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        进入激活函数进行非线性计算</span></span><br><span class="line"><span class="string">        :param A_prev:</span></span><br><span class="line"><span class="string">        :param W:</span></span><br><span class="line"><span class="string">        :param b:</span></span><br><span class="line"><span class="string">        :param activation:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Z = self.linear_forward(A_prev, W, b)</span><br><span class="line">        <span class="keyword">if</span> activation == <span class="string">'sigmoid'</span>:</span><br><span class="line">            <span class="keyword">return</span> sigmoid(Z)</span><br><span class="line">        <span class="keyword">elif</span> activation == <span class="string">'relu'</span>:</span><br><span class="line">            <span class="keyword">return</span> relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deep_forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param X: 输入值</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        A: 最后一层的运算结果，也就是输出层的激活值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 计算神经网络层数，减1是为了去掉输入层，众所周知输入层不需要进行计算</span></span><br><span class="line">        L = len(self.neurons) - <span class="number">1</span></span><br><span class="line">        A = X</span><br><span class="line">        <span class="comment"># 循环整个神经网络，进行正向传播，从1开始，因为索引0是输入层</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 根据索引获取神经元实例</span></span><br><span class="line">            neuron = self.neurons[l]</span><br><span class="line">            A_prev = A</span><br><span class="line">            W = neuron.hyperparameters[<span class="string">'W'</span>]</span><br><span class="line">            b = neuron.hyperparameters[<span class="string">'b'</span>]</span><br><span class="line">            A = self.nonlinear_forward(A_prev, W, b, neuron.activation)</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compile</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y, epochs=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入层的神经元添加进去</span></span><br><span class="line">        self.neurons.insert(<span class="number">0</span>, SimpleNN(len(X)))</span><br><span class="line">        <span class="comment"># 初始化神经元的超参数</span></span><br><span class="line">        <span class="keyword">for</span> i, n <span class="keyword">in</span> enumerate(self.neurons[<span class="number">1</span>:]):</span><br><span class="line">            input_shape = self.neurons[i].units</span><br><span class="line">            n.build(input_shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始反向传播</span></span><br><span class="line">        AL = self.deep_forward(X)</span><br></pre></td></tr></table></figure></p>
<p>激活函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> z * (z &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    dZ = np.array(dA, copy=<span class="keyword">True</span>)  <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line"></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    s = sigmoid(Z)</span><br><span class="line">    <span class="keyword">return</span> dA * s * (<span class="number">1</span> - s)</span><br></pre></td></tr></table></figure></p>
<p>测试一下<br><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入值的大小</span></span><br><span class="line">input_size = 2</span><br><span class="line"><span class="comment"># 输出值的大小</span></span><br><span class="line">output_size = 2</span><br><span class="line"><span class="comment"># 方便书写，截断小数</span></span><br><span class="line">X0 = np.round(np.random.rand(input_size, 1), 2)</span><br><span class="line">Y0 = np.round(np.random.rand(output_size, 1),2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该模型为2 3 2架构</span></span><br><span class="line">model = Model()</span><br><span class="line">model.add(SimpleNN(3))</span><br><span class="line">model.add(SimpleNN(output_size))</span><br><span class="line">model.compile()</span><br><span class="line">model.fit(X0, Y0)</span><br><span class="line">print(model.value_caches[0])</span><br></pre></td></tr></table></figure></p>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><div class="note primary">
            <p>说是说反向传播，实际上整个流程就是在<strong><a href="https://baike.baidu.com/item/链式法则/3314017?fr=aladdin" target="_blank" rel="noopener">链式求导</a></strong>。如果把这点想通了，整个神经网络的难点就只在向量化上了。一定要理解为什么整个流程只是在做链式求导的问题，在这里我并不是随便一提。</p>
          </div>
<p>为了便于查找，把之前的图再放这。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"></p>
<h2 id="首先更新输出层的权重值（W）以及偏差值（b）"><a href="#首先更新输出层的权重值（W）以及偏差值（b）" class="headerlink" title="首先更新输出层的权重值（W）以及偏差值（b）"></a>首先更新输出层的权重值（W）以及偏差值（b）</h2><p>梯度下降公式大家应该都知道：<script type="math/tex">W = W - \alpha * grad</script>。其中的grad实际上就是W的导数，<a href="https://yan624.github.io/学习笔记/梯度下降算法的推导.html">参考</a>。<br>可以对照上图观察，<strong>输出层</strong>的权重值分别为:</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    w^2_{11}&w^2_{12}&w^2_{13}\\
    w^2_{21}&w^2_{22}&w^2_{23}\\
\end{pmatrix}</script><p>所以我们需要分别求：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} \tag{3.1.1}\label{3.1.1}</script><h3 id="求矩阵中第一个w的导数并更新w"><a href="#求矩阵中第一个w的导数并更新w" class="headerlink" title="求矩阵中第一个w的导数并更新w"></a>求矩阵中第一个w的导数并更新w</h3><p>先求<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>，我们知道这个神经网络的代价的表达式是</p>
<script type="math/tex; mode=display">
J = -[(y_1 * \log(a^2_1) + (1 - y_1) * \log(1 - a^2_1) + (y_2 * \log(a^2_2) + (1 - y_2) * \log(1 - a^2_2)]</script><p><strong>为了方便对照我将隐藏层到输出层的正向传播的步骤</strong>也写在下面：</p>
<script type="math/tex; mode=display">
\begin{align}
    z^2_1 & = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1\\
    z^2_2 & = w^2_{21} * a^1_1 + w^2_{22} * a^1_2 + w^2_{13} * a^1_3 + b^2_2\\
    a^2_1 & = \sigma{(z^2_1)} = \frac{1}{1 - e^{-z^2_1}}\\
    a^2_2 & = \sigma{(z^2_2)} = \frac{1}{1 - e^{-z^2_2}}\\
\end{align}</script><p>根据链式求导法则得：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}}</script><p>我们将其拆解，一步一步地求：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] & \text{首先对a求导，此步如果你不会微积分会有疑惑} \tag{3.1.2}\label{3.1.2}\\
    \frac{\partial a^2_1}{\partial z^2_1} & = (a^2_1) * (1 - a^2_1) & \text{这是对sigmoid函数的求导，百度一下求导过程}\\
    \frac{\partial J}{\partial z^2_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1} & \text{其次对z求导}\\
                                      & = \frac{\partial J}{\partial a^2_1} * (a^2_1) * (1 - a^2_1) \tag{3.1.3}\label{3.1.3}\\
    \frac{\partial z^2_1}{\partial w^2_{11}} & = a^1_1\\
    \frac{\partial J}{\partial w^2_{11}} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}} & \text{最后对w求导} \tag{3.1.4}\label{3.1.4}\\
                                         & = \frac{\partial J}{\partial z^2_1} * a^1_1\\
    \frac{\partial J}{\partial w^2_{11}} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1) * a^1_1 & \text{整合在一起}\\
\end{align}</script><p>其中<script type="math/tex">[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1)</script>实际上是可以化简的，化简为<script type="math/tex">a^2_1 - y_1</script>，同时去掉了负号，所以</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = (a^2_1 - y_1) * a^1_1</script><p>我们将数值带入其中，之前的正向传播已经得到了所有激活值。</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial w^2_{11}} = (0.85220348 - 0.60) * 0.81604509 = 0.20580941153491322</script><p>对<script type="math/tex">w^2_{11}</script>更新，</p>
<script type="math/tex; mode=display">
w^2_{11} = w^2_{11} - \alpha * \frac{\partial J}{\partial w^2_{11}}</script><p>学习速率<script type="math/tex">\alpha</script>选1，经过简单的运算，<script type="math/tex">w^2_{11} = 0.92559664 - 1 * 0.20580941153491322 = 0.7197872284650868</script><br><div class="note info">
            <p>如果细心点就会发现，<script type="math/tex">\frac{\partial J}{\partial w^2_{11}}</script>其实就等于这层的z的导数乘上前一层的激活值a。如果没发现也没关系，下面<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#向量化-1">向量化</a>这节会做一个总结。</p>
          </div></p>
<h3 id="求所有w的导数"><a href="#求所有w的导数" class="headerlink" title="求所有w的导数"></a>求所有w的导数</h3><p>同理可以求出所有的导数</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} \tag{\ref{3.1.1}}</script><h3 id="向量化-1"><a href="#向量化-1" class="headerlink" title="向量化"></a>向量化</h3><div class="note danger">
            <p>上面只求了一个w的导数，虽然其他的w的求导都是类似操作，但是真要算起来，对于自己没去算过的人，可能花一天都没有办法将其用<strong>向量化表示</strong>。<br>求导是十分简单的，但是向量化可能会有点问题。问题的主要来源是<strong>想偷懒</strong>。对于这种问题，最好得到解决办法是暴力破解，即求出所有的w的导数，然后再将其向量化。</p>
          </div>
<p>首先观察上述公式<script type="math/tex">\ref{3.1.4}</script>：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial w^2_{11}}</script><p>它由两部分组成，一个是<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>，第二部分是<script type="math/tex">\frac{\partial z^2_1}{\partial w^2_{11}}</script>，如果你自己求过导就会发现其实<script type="math/tex">\frac{\partial z^2_1}{\partial w^2_{11}} = a^1_1</script>。为了方便你们观察，我列出所有式子：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial w^2_{11}} = \frac{\partial J}{\partial z^2_1} * a^1_1 \quad \frac{\partial J}{\partial w^2_{12}} = \frac{\partial J}{\partial z^2_1} * a^1_2 \quad \frac{\partial J}{\partial w^2_{13}} = \frac{\partial J}{\partial z^2_1} * a^1_3\\
    \frac{\partial J}{\partial w^2_{21}} = \frac{\partial J}{\partial z^2_2} * a^1_1 \quad \frac{\partial J}{\partial w^2_{22}} = \frac{\partial J}{\partial z^2_2} * a^1_2 \quad \frac{\partial J}{\partial w^2_{23}} = \frac{\partial J}{\partial z^2_2} * a^1_3\\
\end{align}</script><p><strong>可能到这你有点烦躁了，因为表达式实在太多了。没关系，下方蓝色的note会给出总结，直接一步求解完毕。</strong><br>有没有发现，里面有一半是重复的元素？我们可以将它们组成向量得到：</p>
<script type="math/tex; mode=display">
\eqref{3.1.1}
\begin{pmatrix}
    \frac{\partial J}{\partial w^2_{11}} & \frac{\partial J}{\partial w^2_{12}} & \frac{\partial J}{\partial w^2_{13}}\\
    \frac{\partial J}{\partial w^2_{21}} & \frac{\partial J}{\partial w^2_{22}} & \frac{\partial J}{\partial w^2_{23}}\\
\end{pmatrix} = 
\begin{pmatrix}
\frac{\partial J}{\partial z^2_1}\\
\frac{\partial J}{\partial z^2_2}\\
\end{pmatrix} * 
\begin{pmatrix}
a^1_1 & a^1_2 & a^1_3
\end{pmatrix} \tag{3.1.5}</script><p>公式3.1.5和上面那六个表达式实际上计算的东西是一样的。进一步缩写为</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^2} = \frac{\partial J}{\partial z^2} * (a^1)^T \tag{3.1.6}</script><p>这里加了一个T代表转置，实际上我们所有的输入值，激活值，输出值都是列向量。<br><div class="note info">
            <p>总结一下，在这里<strong>求<script type="math/tex">\frac{\partial J}{\partial w}</script>的步骤为：求权重值所在层的z的导数<script type="math/tex">\frac{\partial J}{\partial z}</script>再乘上前一层的激活值</strong>。这是对一个w求导所做的运算，而对一整个W矩阵求导那就是公式3.1.6的那个向量化操作。但是观察公式3.1.6发现，其实求一个w和求一个W矩阵并无区别，无非是将数字相乘改为向量（矩阵）相乘。<br>另外，其实这对神经网络中每一层的操作都是一样。如果不信可以自己算一下。所以以后理解的时候，可以用这种方式理解，加快理解速度。</p>
          </div></p>
<h3 id="更新偏差"><a href="#更新偏差" class="headerlink" title="更新偏差"></a>更新偏差</h3><p>偏差比权重简单很多。</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] \tag{\ref{3.1.2}}\\
    \frac{\partial J}{\partial z^2_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial z^2_1}\\
                                      & = \frac{\partial J}{\partial a^2_1} * (a^2_1) * (1 - a^2_1) \tag{\ref{3.1.3}}\\
    \frac{\partial J}{\partial b^2_1} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial b^2_1} \\
                                      & = \frac{\partial J}{\partial z^2_1}\\
    \frac{\partial J}{\partial b^2_1} & = -[(\frac{y_1}{a^2_1} + \frac{1 - y_1}{a^2_1 - 1}) + 0] * (a^2_1) * (1 - a^2_1)\\
\end{align}</script><p>可以看到</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial b^2_1} = \frac{\partial J}{\partial z^2_1}</script><p>所以偏差的向量化比较简单：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial b^2_1}\\
    \frac{\partial J}{\partial b^2_2}\\
\end{pmatrix} = 
\begin{pmatrix}
    \frac{\partial J}{\partial z^2_1}\\
    \frac{\partial J}{\partial z^2_2}\\
\end{pmatrix}</script><h3 id="整理"><a href="#整理" class="headerlink" title="整理"></a>整理</h3><p>整理一下上一波的求导过程。目标是求得<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>，但是上面我并没有一步求导到底，相反我将每一步都写出来了，这是有原因的。因为<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>会在前一层对w求导时使用，所以在代码上当然需要保存副本。而<script type="math/tex">\frac{\partial J}{w^2_{11}}</script>已经在这次的反向传播中使用过了，它的价值也算是用完了。<br>在<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#首先更新输出层的权重值（W）以及偏差值（b）">首先更新输出层的权重值（W）以及偏差值（b）</a>中<strong>略有瑕疵</strong>的步骤（也就是上述所有步骤）是：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script></li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script></li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script></li>
</ol>
<div class="note info">
            <p>根据上面三步，我们可以观察出，如果需要求出一层的<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script>（<strong>步骤3</strong>），需要<strong>先</strong>求出<strong>同一层</strong>的<script type="math/tex">\frac{\partial J}{\partial a^2}\ \frac{\partial J}{\partial z^2}</script>（<strong>步骤1和2</strong>）。<strong><em>所以</em></strong>如果我们需要求出<strong>前一层</strong>的<script type="math/tex">\frac{\partial J}{\partial w^1}\ \frac{\partial J}{\partial b^1}</script>，必须先求出<strong>前一层</strong>的a的导数<script type="math/tex">\frac{\partial J}{\partial a^1}\ \frac{\partial J}{\partial z^1}</script>，而由于公式<script type="math/tex">z^2_1 = w^2_{11} * a^1_1 + w^2_{12} * a^1_2 + w^2_{13} * a^1_3 + b^2_1</script>，可以观察到上述<strong>步骤2</strong>对z求导之后其实拥有三个选项：</p><ol><li>求w的导数（<strong>步骤3</strong>）</li><li>求b的导数（<strong>步骤3</strong>）</li><li>求上一层a的导数</li></ol>
          </div>
<p>所以正确的步骤是：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script>（<strong>不变</strong>）</li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script>（<strong>不变</strong>）</li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script>（<strong>不变</strong>）</li>
<li>最后求出前一层的<script type="math/tex">\frac{\partial J}{\partial a^1}</script>，准备下一步的计算。</li>
</ol>
<p><strong>也就是说，我们在一层中进行求导，需要分别求4个参数的导数，即当前层的a，w，b以及前一层的a的导数。</strong></p>
<h2 id="更新隐藏层的权重值以及偏差值"><a href="#更新隐藏层的权重值以及偏差值" class="headerlink" title="更新隐藏层的权重值以及偏差值"></a>更新隐藏层的权重值以及偏差值</h2><p>由于上述步骤太多，来回滑动网页略繁琐，我再次把图放出来，以供参考。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/带参数的前馈神经网络模版.svg" alt="带参数的前馈神经网络模版"><br>隐藏层的更新与输出层略微不同，由于看公式不太形象，可以看上面的图。观察发现，隐藏层的某一个神经元链接着输出层的<strong>所有</strong>神经元。所以隐藏层的神经元的误差其实来源于与它相连接的输出层的神经元。<br>根据链式求导法则，我们知道：一个函数对一个变量求导，如果有多条路径可以到达该变量，那么就需要对每条路径都求导，最后将结果相加。转换成数学公式就跟下面公式3.2.1的求导过程一样。</p>
<h3 id="对第一个w求导"><a href="#对第一个w求导" class="headerlink" title="对第一个w求导"></a>对第一个w求导</h3><p>我们按照上一节<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#整理">《整理》</a>的四个步骤来做，先求出a的导数：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial a^1_1} & = \frac{\partial J}{\partial a^2_1} * \frac{\partial a^2_1}{\partial a^1_1} + \frac{\partial J}{\partial a^2_2} * \frac{\partial a^2_2}{\partial a^1_1} & \text{输出层两个神经元均要求导再相加}\\
                                      & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial a^1_1} + \frac{\partial J}{\partial z^2_2} * \frac{\partial z^2_1}{\partial a^1_1} & \text{之前求过z的导数，为了方便书写用它替换}\\
                                      & = \frac{\partial J}{\partial z^2_1} * w^2_{11} + \frac{\partial J}{\partial z^2_2} * w^2_{21} \tag{3.2.1}\label{3.2.1}\\
\end{align}</script><div class="note info">
            <p>我们可以观察到隐藏层的a的导数<script type="math/tex">\frac{\partial J}{\partial a^1_1}</script>实际上就是<strong>输出层</strong>的z的导数<script type="math/tex">\frac{\partial J}{\partial z^2_1}</script>乘上与之相连的<strong>输出层</strong>的神经元的w。<br>一般化之后就是：<strong>除了输出层</strong>，其他所有层的<strong>a的导数</strong>都是<strong>后一层</strong>的<strong>z的导数</strong>乘上<strong>后一层</strong>的w。因为输出层的<strong>a的导数</strong>是通过代价函数求的。</p>
          </div>
<p>所以下一步就是求z的导数：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial z^1_1} & = \frac{\partial J}{\partial z^2_1} * \frac{\partial z^2_1}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} + \frac{\partial J}{\partial z^2_2} * \frac{\partial z^2_1}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1}\\
                                      & = \frac{\partial J}{\partial z^2_1} * w^2_{11} * \frac{\partial a^1_1}{\partial z^1_1}  + \frac{\partial J}{\partial z^2_2} * w^2_{21} * \frac{\partial a^1_1}{\partial z^1_1}\\
                                      & = \frac{\partial J}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} & \text{这里a的导数参考}\ref{3.2.1}\\
    \frac{\partial a^1_1}{\partial z^1_1} & = a^1_1 * (1 - a^1_1) & \text{对sigmoid函数求导，前面已经说过了}\\
\end{align}</script><p>最后求出w的导数</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{\partial J}{\partial w^1_{11}} & = \frac{\partial J}{\partial a^1_1} * \frac{\partial a^1_1}{\partial z^1_1} * \frac{\partial z^1_1}{\partial w^1_{11}}\\
                                         & = \frac{\partial J}{\partial z^1_1} * \frac{\partial z^1_1}{\partial w^1_{11}}\\
                                         & = \frac{\partial J}{\partial z^1_1} * a^0_1
\end{align}</script><h3 id="向量化-2"><a href="#向量化-2" class="headerlink" title="向量化"></a>向量化</h3><p>你肯定已经想把它向量化了。先列出所有的表达式。</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w^1_{11}} = \frac{\partial J}{\partial z^1_1} * a^0_1\\
\frac{\partial J}{\partial w^1_{12}} = \frac{\partial J}{\partial z^1_1} * a^0_2\\
\frac{\partial J}{\partial w^1_{21}} = \frac{\partial J}{\partial z^1_2} * a^0_1\\
\frac{\partial J}{\partial w^1_{22}} = \frac{\partial J}{\partial z^1_2} * a^0_2\\
\frac{\partial J}{\partial w^1_{31}} = \frac{\partial J}{\partial z^1_3} * a^0_1\\
\frac{\partial J}{\partial w^1_{32}} = \frac{\partial J}{\partial z^1_3} * a^0_2\\</script><p>可以发现这其实跟上面的向量化步骤一模一样：</p>
<script type="math/tex; mode=display">
\begin{align}
    \begin{pmatrix}
        \frac{\partial J}{\partial w^1_{11}} & \frac{\partial J}{\partial w^1_{12}}\\
        \frac{\partial J}{\partial w^1_{21}} & \frac{\partial J}{\partial w^1_{22}}\\
        \frac{\partial J}{\partial w^1_{31}} & \frac{\partial J}{\partial w^1_{32}}\\
    \end{pmatrix} & = 
    \begin{pmatrix}
        \frac{\partial J}{\partial z^1_1}\\
        \frac{\partial J}{\partial z^1_2}\\
        \frac{\partial J}{\partial z^1_3}\\
    \end{pmatrix} * 
    \begin{pmatrix}
        a^0_1 & a^0_2
    \end{pmatrix} \\
    \frac{\partial J}{\partial w^1} & = \frac{\partial J}{\partial z^1} * (a^0)^T
\end{align}</script><h3 id="对偏差求导"><a href="#对偏差求导" class="headerlink" title="对偏差求导"></a>对偏差求导</h3><p>这一步更是简单，直接给结果了。</p>
<script type="math/tex; mode=display">
\begin{align}
    \begin{pmatrix}
        \frac{\partial J}{\partial b^1_1}\\
        \frac{\partial J}{\partial b^1_2}\\
        \frac{\partial J}{\partial b^1_3}\\
    \end{pmatrix} & = 
    \begin{pmatrix}
        \frac{\partial J}{\partial z^1_1}\\
        \frac{\partial J}{\partial z^1_2}\\
        \frac{\partial J}{\partial z^1_3}\\
    \end{pmatrix}\\
    \frac{\partial J}{\partial b^1} & = \frac{\partial J}{\partial z^1}
\end{align}</script><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><p>上述步骤看起来没什么问题，但是在实际编程中会有很大问题。在向量化的时候，我直接使用了<script type="math/tex">\frac{\partial J}{\partial z^1}</script>，但是问题就是<script type="math/tex">\frac{\partial J}{\partial z^1}</script>的向量化我直接跳过了。要向量化<script type="math/tex">\frac{\partial J}{\partial z^1}</script>，实际上得先向量化<script type="math/tex">\frac{\partial J}{\partial a^1}</script>。观察表达式<script type="math/tex">\ref{3.2.1}</script>，先给出所有的式子：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial a^1_1} = \frac{\partial J}{\partial z^2_1} * w^2_{11} + \frac{\partial J}{\partial z^2_2} * w^2_{21}\\
\frac{\partial J}{\partial a^1_2} = \frac{\partial J}{\partial z^2_1} * w^2_{12} + \frac{\partial J}{\partial z^2_2} * w^2_{22}\\
\frac{\partial J}{\partial a^1_3} = \frac{\partial J}{\partial z^2_1} * w^2_{13} + \frac{\partial J}{\partial z^2_2} * w^2_{23}\\</script><h4 id="向量化-3"><a href="#向量化-3" class="headerlink" title="向量化"></a>向量化</h4><script type="math/tex; mode=display">
\begin{pmatrix}
    \frac{\partial J}{\partial a^1_1}\\
    \frac{\partial J}{\partial a^1_2}\\
    \frac{\partial J}{\partial a^1_3}\\
\end{pmatrix} = 
\begin{pmatrix}
w^2_{11} & w^2_{12} & w^2_{13}\\
w^2_{21} & w^2_{22} & w^2_{23}\\
\end{pmatrix}^T * 
\begin{pmatrix}
\frac{\partial J}{\partial z^2_1} & \frac{\partial J}{\partial z^2_2}
\end{pmatrix}</script><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在反向传播中，每一层都只需要重复如下几步：</p>
<ol>
<li>求出<script type="math/tex">\frac{\partial J}{\partial a^2}</script></li>
<li>进一步求出<script type="math/tex">\frac{\partial J}{\partial z^2}</script></li>
<li>分别求出<script type="math/tex">\frac{\partial J}{\partial w^2}\ \frac{\partial J}{\partial b^2}</script></li>
<li>最后求出前一层的<script type="math/tex">\frac{\partial J}{\partial a^1}</script>，准备下一步的计算。此步骤的向量化操作在<a href="https://yan624.github.io/前馈神经网络的正反向推导.html#注意点">注意点</a>。</li>
</ol>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">使用numpy实现一个简单的神经网络</a></p>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>在做反向传播代码时，验算了很多遍，发现公式推导没有问题，但是梯度却一直在上升，心态都炸了。<br>最后发现，用了大半年的crossentropy在最前面居然要加上一个“-”号。以前由于是偷懒，在求导的时候一般不加负号，在求完导之后再补上。然后由于写习惯了，导致我忘记crossentropy居然是有负号的。</p>
<h1 id="撒花"><a href="#撒花" class="headerlink" title="撒花"></a>撒花</h1><p>在第二节有一步是初始化数据，但是全篇都没用几个地方用到。是因为数据测试起来太麻烦了，我需要在代码里一步一步分析神经网络的计算过程，从而获得数据。以后再补充吧。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
      </tags>
  </entry>
  <entry>
    <title>linux非root用户配置环境变量</title>
    <url>/linux%E9%9D%9Eroot%E7%94%A8%E6%88%B7%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F.html</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/gstblog/p/10160976.html" target="_blank" rel="noopener">参考文章</a><br>本文以配置anaconda的环境变量为例。</p>
<p>切换到用户目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br></pre></td></tr></table></figure></p>
<p>输入，发现有一个名为<code>.bashrc</code>的文件<br><figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">ll</span></span><br></pre></td></tr></table></figure></p>
<p>编辑它<br><figure class="highlight jboss-cli"><table><tr><td class="code"><pre><span class="line">vim ~<span class="string">/.bashrc</span></span><br></pre></td></tr></table></figure></p>
<p>在最后一行加上如下代码，保存并退出。<br><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=~/anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>
<div class="note warning">
            <p>PATH和=之间不能有空格。由于写java代码习惯了，加上了空格，导致报错。</p>
          </div>
<p>更新配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>梯度下降算法的推导</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E5%AF%BC.html</url>
    <content><![CDATA[<p>梯度下降算法大家都知道，公式是<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，其中J是代价函数。但是这个算法具体是怎么来的，可能不太清楚。<br>本文参考<br><a href="https://mp.weixin.qq.com/s/k26Fm0GL3fdVA9VbQIVAuQ" target="_blank" rel="noopener">微信公众号</a><br><a href="https://baike.baidu.com/item/梯度/13014729" target="_blank" rel="noopener">百度百科</a><br>由于没有专业的制图工具，所以只能手画了。。。</p>
<h1 id="梯度下降问题"><a href="#梯度下降问题" class="headerlink" title="梯度下降问题"></a>梯度下降问题</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/梯度下降草图.jpg" alt="梯度下降草图"><br>由图中可以观察到，我们将参数初始化到A点，我们的目标是将点移动到最小值点（或者极小值点）。那么问题就是如何移动了。<br>先给出梯度下降公式：<script type="math/tex">\theta = \theta - \alpha * J'(\theta)</script>，J是代价函数，这个公式应该不陌生。</p>
<h1 id="一阶泰勒展开式"><a href="#一阶泰勒展开式" class="headerlink" title="一阶泰勒展开式"></a>一阶泰勒展开式</h1><p>如果学过高数，应该知道<strong>一阶泰勒展开式</strong>的公式是：<script type="math/tex">f(x) = f(x_0) + (x - x_0) * f'(x_0) + R_n(x)</script>，其中<script type="math/tex">R_n(x)</script>是泰勒公式的余项，可以理解为一个无穷小量。既然是无穷小量那么便可以省略不写，但是即使是无穷小，其实等式的左右边还是有点差距的，所以将等式修改为<strong>约等于号</strong>。</p>
<script type="math/tex; mode=display">
f(x) \approx f(x_0) + (x - x_0) * f'(x_0)</script><p>但是由于我们最小化的代价函数的参数是<script type="math/tex">\theta</script>，所以我们可以将x替换为<script type="math/tex">\theta</script>，即</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script><p>如果不知道泰勒公式，可以看下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/泰勒公式线性近似.webp" alt="泰勒公式线性近似"><br>在点<script type="math/tex">\theta_0</script>处，找一条极短的直线来表示曲线，则直线的斜率为<script type="math/tex">f'(\theta_0)</script>，并且已知<script type="math/tex">\theta_0</script>，那么根据初中数学，可以获得直线公式<script type="math/tex">f(\theta) = f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>（还不懂看这个：<script type="math/tex">y-y_0=k(x-x_0)</script>===&gt;<script type="math/tex">y = y_0 + k(x-x_0)</script>）。<br><div class="note warning">
            <p>如果仔细看到了上一行的推导，你也许要问：为什么直线斜率是<script type="math/tex">f'(\theta_0)</script>。百度。</p>
          </div><br><div class="note warning">
            <p>如果对上式没有问题，可能要问为什么这个红线的箭头要向下，不能向上？我有强迫症，我就要让它向上，并且我还要让<script type="math/tex">\theta</script>在<script type="math/tex">\theta_0</script>右边。这个下面会讲，但是现在假定以下的步骤均围绕上图展开。</p>
          </div><br>至此准备工作完成。</p>
<h1 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h1><p>我们将<script type="math/tex">f(\theta) \approx f(\theta_0) + (\theta - \theta_0) * f'(\theta_0)</script>的<script type="math/tex">\theta - \theta_0</script>用字母<script type="math/tex">\alpha v</script>代替，实际上只用<script type="math/tex">v</script>代替也可以。但是还是使用<script type="math/tex">\alpha v</script>吧。</p>
<script type="math/tex; mode=display">
\theta - \theta_0 = \alpha v</script><p>所以公式被简化为如下形式，并且将导数的表示做一下改变，用<strong>倒三角</strong>表示</p>
<script type="math/tex; mode=display">
f(\theta) \approx f(\theta_0) + \alpha v * \nabla f(\theta_0)</script><p>由于我们的目标是使得<script type="math/tex">f(\theta)</script>比<script type="math/tex">f(\theta_0)</script>小，也就是使得<script type="math/tex">f(\theta) - f(\theta_0) < 0</script>。那么将公式转变为</p>
<script type="math/tex; mode=display">
f(\theta) -  f(\theta_0) \approx \alpha v * \nabla f(\theta_0) < 0</script><p>省略一部分</p>
<script type="math/tex; mode=display">
\alpha v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">\alpha</script>一般为正值，所以</p>
<script type="math/tex; mode=display">
v * \nabla f(\theta_0) < 0</script><p>由于<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>实际上都是向量。所以上式就转换为<strong>两个向量相乘在什么时候是小于0的</strong>，并且我们希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好，也就是<script type="math/tex">v * \nabla f(\theta_0)</script>越小越好。那么问题又转化为<strong>两个向量相乘在什么时候是最小的</strong>。<br><div class="note warning">
            <p>问题1：为什么<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>是向量。<br>由于以上都是使用二维的图来描述，所以无法体现是向量。但是实际上<script type="math/tex">\theta</script>不只有一个。<br>问题2：为什么希望<script type="math/tex">f(\theta) - f(\theta_0)</script>越小越好。<br>因为希望<script type="math/tex">f(\theta)</script>这一步迈远一点。</p>
          </div><br>以下为向量乘积的三种形式，由初中的知识可以得知，当向量相反时<script type="math/tex">cos(\alpha)</script>为-1，即cos函数的最小值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/梯度下降算法的推导/向量的乘积.webp" alt="向量的乘积"><br>由于公式可以转为如下，其中<script type="math/tex">\beta</script>是向量夹角</p>
<script type="math/tex; mode=display">
|v| * |\nabla f(\theta_0)| * cos(\beta) < 0</script><p>所以当<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>正好相反时，<script type="math/tex">cos(\beta) = -1</script>。也就是说当<script type="math/tex">v</script>和<script type="math/tex">\nabla f(\theta_0)</script>反向，<script type="math/tex">v * \nabla f(\theta_0)</script>最小。<br>众所周知，<script type="math/tex">\nabla f(\theta_0)</script>就是梯度，也就是梯度方向。所以只需要<script type="math/tex">v</script>为<script type="math/tex">\nabla f(\theta_0)</script>的反方向即可。所以</p>
<script type="math/tex; mode=display">
v  = -\frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}\\</script><p>之所以要除以<script type="math/tex">\nabla f(\theta_0)</script>的模，是因为<script type="math/tex">v</script>是单位向量。<br><div class="note warning">
            <p><script type="math/tex">v</script>为什么是单位向量。不太清楚，原博主没说明。</p>
          </div><br>将<script type="math/tex">v</script>带入到<script type="math/tex">\theta - \theta_0 = \alpha * v</script>中</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha * \frac{\nabla f(\theta_0)}{|\nabla f(\theta_0)|}</script><p>一般地，因为<script type="math/tex">|\nabla f(\theta_0)|</script>是标量，可以并入到中，即简化为：</p>
<script type="math/tex; mode=display">
\theta = \theta_0 - \alpha *\nabla f(\theta_0)</script><a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习中的一些疑问总结</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%96%91%E9%97%AE%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<h1 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h1><div class="note primary">
            <p>为什么<script type="math/tex">w*x+b=z</script>之后，需要使用一个激活函数<script type="math/tex">a = \sigma(z)</script>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
          </div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。<br>那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。</p>
<script type="math/tex; mode=display">
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w' * a^0 + b' \qquad由于w和b只是一堆常数，所以将w'代替w^2 * w^1，b'同理
    \end{array}
\right.</script><p>通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<script type="math/tex">a^2 = w' * a^0 + b'</script>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。<br>只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降的意义"><a href="#梯度下降的意义" class="headerlink" title="梯度下降的意义"></a>梯度下降的意义</h1><div class="note primary">
            <p>首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？</p>
          </div>
<h1 id="dropout背后的原理"><a href="#dropout背后的原理" class="headerlink" title="dropout背后的原理"></a>dropout背后的原理</h1><div class="note primary">
            <p>dropout背后的原理是什么？</p>
          </div>
<p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a></p>
<h1 id="为什么Mini-batch比普通的梯度下降快？"><a href="#为什么Mini-batch比普通的梯度下降快？" class="headerlink" title="为什么Mini-batch比普通的梯度下降快？"></a>为什么Mini-batch比普通的梯度下降快？</h1><div class="note primary">
            <p>为什么Mini-batch比普通的梯度下降快？</p>
          </div>
<h1 id="指数加权平均的作用"><a href="#指数加权平均的作用" class="headerlink" title="指数加权平均的作用"></a>指数加权平均的作用</h1><div class="note primary">
            <p>指数加权平均的作用</p>
          </div>
<h1 id="为什么要deep"><a href="#为什么要deep" class="headerlink" title="为什么要deep"></a>为什么要deep</h1><div class="note primary">
            <p>学了有一段时间的深度学习，但是有个问题一直没想明白。那就是将hidden layer叠多层的意义是什么？</p>
          </div>
<p>可以观察下图发现，确实越deep代价越小。左边的两列显示了，层数越多代价越小。右边两列显示即使整个神经网络参数类似，但是明显越deep代价越小。同一行代表hidden layer的参数接近。那个size指的是神经网络中的参数。再看最后一行，它显示即使参数暴增到16k，代价也不是很低。图中红框还显示了2层2k的model比1层16k的model好多了。<br>下图是由底下的论文的作者做的实验得出的结论。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/隐藏层层数对cost的影响.jpg" alt="隐藏层层数对cost的影响"><br><strong>那么为什么神经网络越深效果越好呢？</strong><br>这其实归功于modularization——模块化。如下图所示，如果直接写一个model用于将4种人分类，那么可能会出现某类人的数据并不多的情况，比如长发的男生的数据可能并不多。那么分类的准确率可能会不是很高。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/解释why deep的例子.jpg" alt="解释why deep的例子"><br>下图中先将其分类为男女以及长短发，然后再进一步分类。虽然说长发男生的数据比较少，但是男女和长短发的数据有很多，我们可以得到一个很好的模型。之后我们再叠一层用于进一步分类，此时，由于我们已经做了上一步的分类，所以新的一层可以使用上一层的特征。<br>上一层的分类已经把难的事情——辨别男女，辨别长短发等解决了，所以后一层只要使用少量的数据就能进行分类。<br><strong>没有使用模块化</strong>的那个模型，它是用少量的数据硬生生地去识别长发男生。<strong>使用模组化</strong>的模型是先识别男女以及长短发，再通过调用前一步的特征判断。下图4个分类器区别可能只是辨别的方式不同而已，比如权重不同。输入一张图片之后第一层已经可以辨别是男还是女，长发还是短发，然后后一层经过简单的运算就可以确定了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/模块化后.jpg" alt="模块化后"><br>经过上面的解释，可能已经大致理解是什么意思了。但是真要讲清还有点问题，尤其是模块化怎么做。<br>但是李宏毅老师说模块化其实是神经网络从数据中<strong>自动</strong>学到的。</p>
<h2 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h2><p>用数电的逻辑门来举例，但是我没怎么学过数电，所以没有理解。<br>另一个比较贴近生活的例子，就是剪窗花。没有人会一瓣花一瓣花的去剪窗花，都是将纸先折好，然后一步剪完。这就是模块化了。</p>
<p>还有其他领域的人也有过解读，个人理解why deep这个问题可能到现在没有一个官方的回应，可能前辈也是误打误撞才发现deep learning很牛。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/其他领域对为什么要deep的解读.jpg" alt="其他领域对为什么要deep的解读"></p>
<h2 id="吴恩达老师的解释"><a href="#吴恩达老师的解释" class="headerlink" title="吴恩达老师的解释"></a>吴恩达老师的解释</h2><p><a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>也做了解释。</p>
<h1 id="词向量乘上权重以及做梯度下降有什么意义"><a href="#词向量乘上权重以及做梯度下降有什么意义" class="headerlink" title="词向量乘上权重以及做梯度下降有什么意义"></a>词向量乘上权重以及做梯度下降有什么意义</h1><p><a href="https://mooc.study.163.com/learn/2001280005?tid=2001391038&amp;_trace_c_p_k2_=023fecd41c524f0d9485b18d2d773f53#/learn/content?type=detail&amp;id=2001770038" target="_blank" rel="noopener">本文灵感</a><br>本文疑问：</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<h2 id="准备词向量"><a href="#准备词向量" class="headerlink" title="准备词向量"></a>准备词向量</h2><p>假设有这么一句话：I want a glass of orange ___.<br>要做的是估计划线处应该填入什么词。答案是juice。<br>首先我们需要一个词典——vocabulary，每个单词对应一个索引，这是通用步骤。词表大小为10000。<br>然后将上述的句子，从单词转成索引形式。即：<br>I want a glass of orange —-&gt; 4343 9665 1 3852 6163 6257<br>此外每一个单词都会对应一个词向量，而词表中所有单词的词向量就组合一个词嵌入矩阵。词表以及词向量都是可以找一些预训练的，比如<strong>GloVe</strong>。<br>梳理一遍就是：<br>单词:索引<br>索引:词向量<br>所以可以通过单词间接地获取到词向量。关于索引对应词向量，实际上是里面没有索引的因为一个矩阵它本身就有一个属性表示索引，如第0行就是代表第0个单词，第1行就是代表第一个单词。<br>总而言之，我们通过单词获取索引后，就能通过该索引直接获取词向量。伪代码可以表示为：<br><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">index = vocabulary.get_index('want') <span class="comment"># 索引为9665</span></span><br><span class="line">word_vector = embedding_matrix[index, :] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p><strong>对于词嵌入矩阵的行代表词向量，还是列代表词向量不必纠结。</strong>你要乐意可以改成<br><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">word_vector = embedding_matrix[:, index] <span class="comment"># 获得词向量</span></span><br></pre></td></tr></table></figure></p>
<p>如果使用one hot编码来执行上述代码就是将9665转为one hot编码，即除了9665位置为1，其余位置全为0。然后<script type="math/tex">word\_vector = embedding\_matrix^T * word\_one\_hot</script>。这样也能得到词向量，但是由于one hot编码全是0，算起来速度太慢了。<br>现在有了句子“I want a glass of orange”的所有词向量，接下来要做的是将这些词向量从头到尾拼在一起，接成一个更长的向量，也就是6倍长的向量。原词向量是300维，拼接完成后是1800维。然后将这个向量输入一个神经网络中，最后经过softmax函数进行预测，预测范围是在10000个单词中，看谁的概率大。</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>将词向量送入神经网络中当然还需要梯度下降进行迭代。这里会有很多疑问，</p>
<ol>
<li>词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？</li>
<li>神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？</li>
<li>梯度下降到底在算什么东西，在算谁的最小值？</li>
<li>为什么它可以预测出应该填入juice？</li>
<li>它怎么预测其他句子？</li>
</ol>
<p>我进行逐一思考，本文仅为自己的理解。<br>首先其实有一件事很多视频没讲，可能他们认为这是一件很平常的事，所以没讲。<br>上述的这个步骤并不是预测步骤，而是在进行迭代，所以是一个训练步骤。人家之所以说<em>我们可以通过这个神经网络预测出单词为juice</em>，是因为逻辑上是这样的。<br>由于是训练步骤，所以我们有一个很重要的数据，最终结果。最终结果我们是知道的，然而我们初学者在考虑整个流程时，没把最终结果算进去，因为老师说<em>我们可以通过这个神经网络预测出单词为juice</em>，由于是<em>预测</em>，那么结果肯定没有啊。这很合乎逻辑。所以就陷入了一个思维的怪圈，<strong>正确的逻辑是：</strong></p>
<ol>
<li>首先我们知道最终结果，所以当第一次迭代时，所有的权重都是随机初始化的，1、2两个问题也就没有意义了。第一次迭代完毕后，结果肯定稀巴烂，所以进行梯度下降。</li>
<li>这里面我们又会碰到一个问题，就是梯度下降到底在算什么？其实这里的疑问来自我们的潜意识始终将句子当做文字在看，自然而然就意识不到梯度下降在干什么。而其实我们在几步之前就已经将文字转为词向量了。<br>词向量说白了就是一堆浮点型数字，而最终结果juice也是一个词向量，所以实际上就是将一个权重矩阵乘上一个1800维的向量，得到一个输出值（may be 激活值），然后将这个输出值和juice的向量放入代价函数中进行计算，接下来的梯度下降其实就是跟正常的步骤一样。</li>
<li><strong>梯度下降就是在寻找一个合适的权重矩阵使得权重矩阵乘1800维向量得到的值接近juice的向量。</strong><br>这里在解释第3个问题时，顺便也解释了第1、2个问题。<strong>权重值实际上就是用来使得预测值和实际结果越接近越好</strong></li>
<li>由于开头就说了我们实际上是知道划线处应该填juice，所以第4个问题压根不需要解答，因为我们本来就知道应该填juice，也没必要预测。之前之所以有这个问题，是因为我们潜意识觉得老师说<em>我们可以通过这个神经网络预测出单词为juice</em>。</li>
<li>至于第5个问题如何预测其他句子。打个比方，现在预测一个新的句子：I want a glass of apple ___.<br>由于我们知道词嵌入矩阵是由很多单词的词向量组成的。而一个单词词向量其实就是一堆特征组成的，对比两个句子，apple和orange的词向量肯定很接近，因为它们都是水果，它们的特征（水多不多，好不好吃，是不是水果，有没有性别特征，是不是动词等）都类似。<br>而我们之前已经训练了一个神经网络，我们得到了所需要的权重值，我们直接把这个权重值乘上新句子的词向量，那么结果肯定和权重值乘上之前句子的词向量的结果很接近。因为它们之间的区别仅仅是apple和orange的词向量不同，并且apple和orange的词向量其实也并不是完全不同，只是略有不同。所以二者的结果自然也差不多。<br>最后经过softmax函数在10000个单词之中预测，输出的结果必然都是juice。</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>这里还会有一个问题，那么如果需要预测的句子单词数不一致怎么办？<br>之前两个例子的句子都是6个单词，预测第7个单词。那要是前面有10个单词，我要预测第11个单词怎么办？<br>其实可以只考虑划线处前几个单词，比如只考虑划线处前4个单词，这样输入的维度就相同了。<br>在本文开头的参考视频里，吴恩达老师讲得很清楚了。由古圣先贤总结，一般来说取前4个单词即可，当然你自己也可以用其他办法。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习学习记录大纲</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E5%A4%A7%E7%BA%B2.html</url>
    <content><![CDATA[<h1 id="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题"><a href="#《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题" class="headerlink" title="《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题"></a>《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题</h1><p><a href="https://yan624.github.io/学习笔记/《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题.html">文章地址</a><br>由于神经网络中参数太多，而有些参数的表现形式太过复杂， 比如文中权重——<script type="math/tex">w^l_{ji}</script>有太多上标下标，所以写了一篇文章记录一下。</p>
<h1 id="对神经网络整体的理解"><a href="#对神经网络整体的理解" class="headerlink" title="对神经网络整体的理解"></a>对神经网络整体的理解</h1><p><a href="https://yan624.github.io/学习笔记/对神经网络整体的理解.html">文章地址</a><br>通常学习深度学习从一个最简单的神经网络开始，但是由于对深度学习时0基础，所以需要同时学习大量算法以及其原理，比如梯度下降，Momentum，Adam，RMSprop，adagrad等等算法。所以写了一篇文章记录一下大部分的算法以及原理。</p>
<h1 id="吴恩达李宏毅综合学习笔记：RNN入门"><a href="#吴恩达李宏毅综合学习笔记：RNN入门" class="headerlink" title="吴恩达李宏毅综合学习笔记：RNN入门"></a>吴恩达李宏毅综合学习笔记：RNN入门</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">文章地址</a><br>学习完神经网络之后，可以学习其他的神经网络模型。由于本人初步决定学习nlp，所以基本没有看CNN，直接学了RNN。本文就是学习RNN的记录，包括了许多算法。</p>
<h1 id="吴恩达深度学习学习笔记：自然语言处理与词嵌入"><a href="#吴恩达深度学习学习笔记：自然语言处理与词嵌入" class="headerlink" title="吴恩达深度学习学习笔记：自然语言处理与词嵌入"></a>吴恩达深度学习学习笔记：自然语言处理与词嵌入</h1><p><a href="https://yan624.github.io/学习笔记/吴恩达深度学习学习笔记：自然语言处理与词嵌入.html">文章地址</a><br>学习完RNN之后，就可以学习 NLP 的概念了，这里面讲得虽然还是神经网络，但是其实都是 NLP 领域的知识。</p>
<h1 id="练习Keras-RNN的代码"><a href="#练习Keras-RNN的代码" class="headerlink" title="练习Keras RNN的代码"></a>练习Keras RNN的代码</h1><p><a href="https://yan624.github.io/学习笔记/练习Keras RNN的代码.html">文章地址</a><br>学习完Simple NN，RNN 和 NLP 之后，就可以练习一下了。文中使用 Keras 框架，写了几个例子练习。</p>
<h1 id="疑问总结"><a href="#疑问总结" class="headerlink" title="疑问总结"></a>疑问总结</h1><p><a href="https://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">文章地址</a><br>深度学习入门后必然有很多疑问待解答，此篇解决疑问。</p>
<h1 id="开始继续学习机器学习"><a href="#开始继续学习机器学习" class="headerlink" title="开始继续学习机器学习"></a>开始继续学习机器学习</h1><p>待续。。。</p>
<h1 id="开始CNN"><a href="#开始CNN" class="headerlink" title="开始CNN"></a>开始CNN</h1><p>待续。。。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>outline</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>练习Keras RNN的代码</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BB%83%E4%B9%A0Keras%20RNN%E7%9A%84%E4%BB%A3%E7%A0%81.html</url>
    <content><![CDATA[<h1 id="《Python深度学习》第6章预测imdb的影评"><a href="#《Python深度学习》第6章预测imdb的影评" class="headerlink" title="《Python深度学习》第6章预测imdb的影评"></a>《Python深度学习》第6章预测imdb的影评</h1><p><a href="https://github.com/yan624/deep-learning-notes/tree/master/RNN_learning/imdb_predication" target="_blank" rel="noopener">imdb影评代码</a></p>
<h1 id="第五课第二周作业：Emojify"><a href="#第五课第二周作业：Emojify" class="headerlink" title="第五课第二周作业：Emojify"></a>第五课第二周作业：Emojify</h1><p>本文实现吴恩达深度学习第五课第二周的Emojify作业。目的是了解一个小型nlp系统的构建流程。<br>首先需要导入以下模块。由于在Jupyter中打开的ipynb文件全是英文，所以本文的大部分标题也用英文，方便ctrl F。<br>Emojifier-V1略。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>加了很多注释，但是代码的顺序我做了很大的改动。下面博客里面的代码，是作业里面的代码，基本没改几个字。<br><a href="https://github.com/yan624/deep-learning-notes/tree/master/RNN_learning/emojify" target="_blank" rel="noopener">emojify_V2代码</a></p>
<h2 id="Emojifier-V2-Using-LSTMs-in-Keras"><a href="#Emojifier-V2-Using-LSTMs-in-Keras" class="headerlink" title="Emojifier-V2: Using LSTMs in Keras"></a>Emojifier-V2: Using LSTMs in Keras</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.seed</span>(<span class="number">0</span>)</span><br><span class="line">from keras<span class="selector-class">.models</span> import Model</span><br><span class="line">from keras<span class="selector-class">.layers</span> import Dense, Input, Dropout, LSTM, Activation</span><br><span class="line">from keras<span class="selector-class">.layers</span><span class="selector-class">.embeddings</span> import Embedding</span><br><span class="line">from keras<span class="selector-class">.preprocessing</span> import sequence</span><br><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.seed</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight capnproto"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br></pre></td></tr></table></figure>
<h3 id="Overview-of-the-model"><a href="#Overview-of-the-model" class="headerlink" title="Overview of the model"></a>Overview of the model</h3><h3 id="Keras-and-mini-batching"><a href="#Keras-and-mini-batching" class="headerlink" title="Keras and mini-batching"></a>Keras and mini-batching</h3><p>本练习中，我们使用mini-batch算法训练Kears。大部分深度学习框架要求在相同的mini-batch中所有序列都要等长。这使得可以执行向量化，如果你有一个3个单词的句子和一个4个单词的句子，它们之间的计算会不同（一个需要3个timestep，一个需要4个timestep，也就是说需要的LSTM个数不同），所有同时计算它们是不可能的，即无法向量化。<br>通用的解决办法是使用padding。具体来说，设置一个序列的最大长度，然后使其他的序列都与该长度等长。比如序列的最大长度是20，那么将其他的序列在后面补充0，知道长度等于20。所以句子“I love you”会在“you”后面被补充17个0。即<script type="math/tex">\begin{pmatrix}e_i & e_{love} & e_{you} & \overrightarrow{0} & \overrightarrow{0} & \cdots & \overrightarrow{0}\end{pmatrix}</script>，e代表词向量。如果长度大于20的话会被裁剪。<br>以下代码实现将句子转换为句子中的每个单词转为索引，这个索引是GloVe词嵌入的，每一个单词对应一个id。id就是索引。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sentences_to_indices</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将句子转为索引形式，短于max_len的句子后面补充0，长于max_len的句子直接截断</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape</span></span><br><span class="line">    X_indices = np.zeros((m, max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] =word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure></p>
<p>以下设计Embedding层，使用keras的Embedding类。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pretrained_embedding_layer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此方法创建了一个Embedding层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># GloVe的总单词数量</span></span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                  <span class="comment"># adding 1 to fit Keras embedding (requirement)</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    <span class="comment"># 词嵌入矩阵，之前V1压根没用词嵌入矩阵，将这个矩阵放入Embedding层中供keras使用</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable.</span></span><br><span class="line">    <span class="comment"># Use Embedding(...). Make sure to set trainable=False.</span></span><br><span class="line">    embedding_layer = Embedding(vocab_len, emb_dim, trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure></p>
<h3 id="Building-the-Emojifier-V2"><a href="#Building-the-Emojifier-V2" class="headerlink" title="Building the Emojifier-V2"></a>Building the Emojifier-V2</h3><p>以下代码完成Emojify。主要是keras代码。<br><figure class="highlight nix"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: Emojify_V2</span></span><br><span class="line"></span><br><span class="line">def Emojify_V2(input_shape, word_to_vec_map, word_to_index):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    <span class="attr">sentence_indices</span> = Input(input_shape, <span class="attr">dtype='int32')</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    <span class="attr">embedding_layer</span> = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    <span class="attr">embeddings</span> = embedding_layer(sentence_indices)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    <span class="comment"># 这个128是神经元的个数</span></span><br><span class="line">    <span class="attr">X</span> = LSTM(<span class="number">128</span>, <span class="attr">return_sequences=True)(embeddings)</span></span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    <span class="attr">X</span> = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    <span class="attr">X</span> = LSTM(<span class="number">128</span>, <span class="attr">return_sequences=False)(X)</span></span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    <span class="attr">X</span> = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    <span class="attr">X</span> = Dense(<span class="number">5</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    <span class="attr">X</span> = Activation('softmax')(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    <span class="attr">model</span> = Model(<span class="attr">inputs=sentence_indices,</span> <span class="attr">outputs=X)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    return model</span><br></pre></td></tr></table></figure></p>
<h2 id="系统整体流程"><a href="#系统整体流程" class="headerlink" title="系统整体流程"></a>系统整体流程</h2><p>RNN</p>
<ol>
<li>读取GloVe文件和训练数据</li>
<li>将训练数据的label转为one hot表示</li>
<li>求出所有训练数据中最长句子的长度，该长度就是LSTM的个数。由于向量化的要求，LSTM的个数需要相同，以最长长度作为LSTM的个数，当然并不需要每个项目都这么设置，完全可以自己选，随便举几个例子比如20,50，100等。</li>
<li>设计Embedding层</li>
<li>建立神经网络模型</li>
<li>将每句话转换为索引表示，如果长度不够就填0，够了就截断。《Python深度学习》中使用了pad_sequences类</li>
<li>使用模型预测，第6条就是输入的训练数据，第2条就是输入的标签</li>
</ol>
<p>Embedding层需要输入一个词嵌入矩阵，就是一个二维数组。每行代表一个单词的特征向量，行数就是单词的索引。而输入的训练数据被处理成单词的索引形式。如一组训练样本，每个单词被替换成唯一的索引。<br>Embedding层太过复杂，故不作详解。第一章的代码中全部有注释。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Jupyter出现gbk codec cant decode byte 0x93 in position 3136：illegal multibyte sequence</title>
    <url>/bug/Jupyter%E5%87%BA%E7%8E%B0gbk-codec-cant-decode-byte-0x93-in-position-3136%EF%BC%9Aillegal-multibyte-sequence.html</url>
    <content><![CDATA[<p>一般来说是open()方法没有加encoding=’utf-8’，但是没用，试了其他办法没一个能用。<br>解决办法：重启Jupyter。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达深度学习学习笔记：自然语言处理与词嵌入</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5.html</url>
    <content><![CDATA[<h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>我们一直使用<a href="https://yan624.github.io/学习笔记/吴恩达深度学习与李宏毅深度学习学习笔记：RNN序列模型.html#one hot编码">one hot编码</a>，这在之前已经记过笔记。这种表示方法的最大缺点是将每个词孤立起来，并且泛化能力不强。由于每个向量的内积都是0，所以它们之间的距离都是一样的。比如</p>
<ol>
<li>I want a glass of orange juice.</li>
<li>I want a glass of apple <em>_</em>.<br>这两个句子是很常见的句子，所以自然而然的想到划线处应该是juice。但是由于one hot编码，程序并不知道orange和apple之间的关系，也就猜不出来。</li>
</ol>
<h2 id="Featurized-representation：-word-embedding"><a href="#Featurized-representation：-word-embedding" class="headerlink" title="Featurized representation： word embedding"></a>Featurized representation： word embedding</h2><p>既然one hot有问题，那么自然就有人发明了新的算法。<br>使用特征来表示每个词。如果适应特征化来表示，那么最后发现orange和apple的特征差不多，就可以推测出划线处应该填写什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Featurized representation： word embedding.jpg" alt="Featurized representation： word embedding"></p>
<h2 id="Visualizing-word-embedding"><a href="#Visualizing-word-embedding" class="headerlink" title="Visualizing word embedding"></a>Visualizing word embedding</h2><p>可以使用t-SNE算法将数据可视化为二维的图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达深度学习学习笔记：自然语言处理与词嵌入/Visualizing word embedding.jpg" alt="Visualizing word embedding"></p>
<h1 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h1><h2 id="类比"><a href="#类比" class="headerlink" title="类比"></a>类比</h2><p>看下图中的表格，现在已知对应关系man-&gt;woman，能否推出king对应于queen？也就是说king-&gt;<em>_</em>，填空题。<br>解法是：<br>求出man和woman之间的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-1\\
0.01\\
0.03\\
0.09\\
\end{pmatrix} - 
\begin{pmatrix}
1\\
0.02\\
0.02\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>假设计算king和queen的差</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-0.95\\
0.93\\
0.70\\
0.02\\
\end{pmatrix} - 
\begin{pmatrix}
0.97\\
0.95\\
0.69\\
0.01\\
\end{pmatrix} \approx
\begin{pmatrix}
-2\\
0\\
0\\
0\\
\end{pmatrix}</script><p>算法的原理就是找到一个词使得man和woman的差与king和新词的差接近。翻译为代码就是<script type="math/tex">find\ word\ w: argmax\ sim(e_w, e_{king} - e_{man} + e _{woman})</script>。但是算法的准确度只有30%-75%。</p>
<h3 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h3><p>余弦相似度也可以计算相似度。公式为<script type="math/tex">sim(u,v) = \frac{u^Tv}{\parallel u\parallel_2\parallel v\parallel_2}</script></p>
<h1 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h1><p>略。大致意思是一个嵌入矩阵E乘上one hot编码可以得到一个单词的特征向量。E就是全部单词的特征矩阵。</p>
<h1 id="如何train一个词嵌入矩阵"><a href="#如何train一个词嵌入矩阵" class="headerlink" title="如何train一个词嵌入矩阵"></a>如何train一个词嵌入矩阵</h1><p>在早期深度学习的研究人员都是使用比较复杂的算法，但是随着时间的推移，这些复杂的算法被慢慢的简化。以至于现在的新手看到这些简化版的算法时，会疑惑这样简单的算法时怎么工作的。所以现在先介绍一个比较复杂的算法，再慢慢介绍简化版的。<br><div class="note info">
            <p>这节好像是用来讲如何建立神经语言模型的，以后再看。之前讲了嵌入矩阵E，但是E中全部的特征向量是已经假定存在的，那么这些特征从何而来呢？就是这节讲的，去训练得来的。但是其实有已经训练好的，我们可以直接拿来用，网上有很多。</p>
          </div></p>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><h1 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h1><h1 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h1><p>就是词嵌入中可能带有一些偏见，比如男女偏见、种族偏见等。现在的目的就是除去这种偏见。<br>暂且不看，其他的算法都还没学。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达李宏毅综合学习笔记：RNN入门</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%BB%BC%E5%90%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ARNN%E5%85%A5%E9%97%A8.html</url>
    <content><![CDATA[<h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>课程</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>2~8</td>
<td>吴恩达深度学习</td>
<td>one hot编码、RNN包括双向和深层、GRU、LSTM</td>
</tr>
<tr>
<td>9~14</td>
<td>李宏毅机器学习</td>
<td>RNN包括双向和深层、LSTM、RNN反向传播、seq2seq</td>
</tr>
<tr>
<td>15~20</td>
<td>李宏毅深度学习</td>
<td>计算图、语言模型中的深度学习、几个有用的网络架构。到原视频的 p12 结束，由于后续部分涉及到了 GAN 等其他模型，所以不在此处做笔记，详见<a href="https://yan624.github.io/zcy/对神经网络整体的理解.html">对神经网络整体的理解</a>博文中靠后的几节</td>
</tr>
</tbody>
</table>
</div>
<h1 id="字母表示"><a href="#字母表示" class="headerlink" title="字母表示"></a>字母表示</h1><p>假设：<br>x: Harry Potter and Hermione Granger invented a new spell.<br>y: 1 1 0 1 1 0 0 0 0<br>其中1代表人名地名之类的单词。这句话一共有九个单词，则x可以表示为：<script type="math/tex">x^{<1>} x^{<2>} \cdots x^{<t>} \cdots x^{<9>}</script>。<br>则y可以表示为：<script type="math/tex">y^{<1>} y^{<2>} \cdots y^{<t>} \cdots y^{<9>}</script><br>输入的长度表示为<script type="math/tex">T_x</script>，则<script type="math/tex">T_x = 9</script>。<br>输出的长度表示为<script type="math/tex">T_y</script>，则<script type="math/tex">T_y = 9</script>。<br>之前在神经网络中<script type="math/tex">X^i</script>或<script type="math/tex">X^(i)</script>代表第i个训练样本。现在在序列模型中，<script type="math/tex">X^{(i)<t>}</script>代表代表第i个训练样本的第t个元素。对应地，<script type="math/tex">T^i_x</script>就代表第i个样本的输入长度。</p>
<h1 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one hot编码"></a>one hot编码</h1><p>在我们做自然语言处理时，一件需要事先决定的是，怎么表示一个序列里的单词。<br>第一件事就是做一张词表（Vocabulary）有时也叫字典（Dictionary），然后将表示方法中要使用的单词列出一列。最后将一个单词用一个稀疏向量表示，如Harry表示为<script type="math/tex">\begin{pmatrix}0&0&0&\cdots&1&0&\cdots&0\end{pmatrix}</script>。1所在位置就是Harry这个单词在词表中的所在位置。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/ont%20hot%E4%BE%8B%E5%AD%90.jpg" alt="ont hot例子"></p>
<h1 id="循环神经网络——RNN"><a href="#循环神经网络——RNN" class="headerlink" title="循环神经网络——RNN"></a>循环神经网络——RNN</h1><div class="note primary">
            <p>RNN解决了什么问题。</p>
          </div>
<p>与Simple Neural Network不同的是，循环神经网络的每一层都要有输入x和输出y。<br>第一步与Simple Neural Network类似，<script type="math/tex">a_1 = w_{aa} * x^{<1>} + b_a</script>，这样就获得了激活值a，但是这时需要使用sigmoid函数或者其他函数直接算出y，另外与Simple Neural Network不同的是，它在计算激活值时需要附带加上前一层的激活值乘上一个权重，此权重与其他的权重类似，也是NN自己训练的。所以第二个序列的计算公式是<script type="math/tex">a_2 = w_{aa} * a_1 + w_{ax} * x^{<2>} + b_a</script>。后面的序列就跟第二个序列一样。<strong>注意一点，RNN中平行方向是时间序列，并不是隐藏层，并且此例中为了方便起见，垂直方向只有一个隐藏层。那几个圆圈是神经元</strong>。看下图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84RNN%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="吴恩达深度学习中的RNN示意图"></p>
<ol>
<li><p>由于为了一般化，第一层需要修改成跟后面的计算类似，所以引入一个零向量<script type="math/tex">a_0</script>来计算<script type="math/tex">a_1</script>。<br>所以RNN的计算公式为：</p>
<script type="math/tex; mode=display">
\left\{ 
 \begin{array}{c}
     a^{<1>} = g_1(w_{aa} * a^{<0>} + w_{ax} * x^{<1>} + b_a)\\
     \hat{y}^{<1>} = g_2(w_{ya} * a^{<1>} + b_y)\\
     a^{<2>} = g_1(w_{aa} * a^{<1>} + w_{ax} * x^{<2>} + b_a)\\
     \hat{y}^{<2>} = g_2(w_{ya} * a^{<2>} + b_y)\\
     \vdots\\
     a^{<t>} = g_1(w_{aa} * a^{<t-1>} + w_{ax} * x^{<t>} + b_a)\\
     \hat{y}^{<t>} = g_2(w_{ya} * a^{<t>} + b_y)\\
 \end{array}
\right.</script><p>注意上式中的<script type="math/tex">w_{aa}</script>、<script type="math/tex">w_{ax}</script>、<script type="math/tex">w_{ya}</script>、<script type="math/tex">b_{a}</script>和<script type="math/tex">b_{y}</script>并没有上标或者下标，所以意味着每一层同一个符号的权重值和偏差值都是一样的。另外对于激活函数也是用户自行选择，在<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html">对神经网络整体的理解</a>一文中已经解释的很清楚了，为了区分输入与输出的激活函数不同，我特意使用了不同的下标，这个下标仅代表这个意思。</p>
</li>
<li><p>为了进一步地一般化，我们将<script type="math/tex">w_{aa}</script>、<script type="math/tex">w_{ax}</script>合并成为<script type="math/tex">w_{a}</script>，如果表示为矩阵形式就是<script type="math/tex">w_{a} = \begin{pmatrix}w_{aa} | w_{ax}\end{pmatrix}</script>，然后将1中的最后两行表达式一般化为：</p>
<script type="math/tex; mode=display">
a^{<t>} = g_1(w_{a} * [a^{<t-1>}, x^{<t>}] + b_a)\\
\hat{y}^{<t>} = g_2(w_{y} * a^{<t>} + b_y)\\</script><p>表达式<script type="math/tex">[a^{<t-1>}, x^{<t>}]</script>的意思是将两个向量堆起来，如果表示为矩阵形式就是<script type="math/tex">\begin{pmatrix} a^{<t-1>}\\ x^{<t>}\\ \end{pmatrix}</script>，上式为了排版问题就不写成矩阵形式了。</p>
</li>
</ol>
<h2 id="RNN的反向传播"><a href="#RNN的反向传播" class="headerlink" title="RNN的反向传播"></a>RNN的反向传播</h2><p>跟Simple Neural Network类似，也要先定义一个cost function，可以选择crossentropy。由于RNN每一层都有输出值y，所以需要对每一层都求出代价，最后将这些代价值加起来</p>
<div class="note primary">
    <p>吴恩达老师在讲反向传播的实现时并没有讲计算过程，所以有点糊里糊涂的。从代价函数到激活值反向传播还可以理解，但是从后一层到前一层的反向传播理解不了。另外由于权重值一样，那么权重值到底该怎么更新？</p>
</div>

<h2 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h2><p>上面讲到的都是<script type="math/tex">T_x = T_y</script>，但是有时候输入和输出的长度并不相同。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84RNN%E5%AE%9E%E4%BE%8B.jpg" alt="不同类型的RNN实例"><br>多对多（many to many）、多对一（many to one）、一对一（one to one）、一对多（one to many）架构<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg" alt="不同类型的RNN结构"></p>
<h2 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h2><h2 id="对新序列采样"><a href="#对新序列采样" class="headerlink" title="对新序列采样"></a>对新序列采样</h2><h2 id="长期依赖，梯度消失"><a href="#长期依赖，梯度消失" class="headerlink" title="长期依赖，梯度消失"></a>长期依赖，梯度消失</h2><p>观察两个句子：</p>
<ul>
<li>The cat, which already ate…, was full.</li>
<li>The cats, which already ate…, were full.</li>
</ul>
<p>这两个句子只有复数形式上的不同，但是开头的名词影响到了最后面的be动词。但是我们目前见到的最基本的RNN不擅长捕获这种长期依赖效应。<br>用梯度消失解释一下为什么，其实原理相同的，这里引用之前的文章<br><a href="https://yan624.github.io//%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8">梯度消失和梯度爆炸</a></p>
<h1 id="GRU单元——Gate-Recurrent-Unit"><a href="#GRU单元——Gate-Recurrent-Unit" class="headerlink" title="GRU单元——Gate Recurrent Unit"></a>GRU单元——Gate Recurrent Unit</h1><p>中文名为门控循环单元。它解决了梯度消失的问题。</p>
<h2 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h2><p>c = memonry cell，使用<script type="math/tex">c^{<t>}</script>符号表示输出，其中<script type="math/tex">c^{<t>} = a^{<t>}</script>，由于后面的LSTM的c和a代表意思不同，所以这里直接使用c来表示输出值。所以本小章下的c你都看作是a即可。</p>
<h2 id="GRU工作流程"><a href="#GRU工作流程" class="headerlink" title="GRU工作流程"></a>GRU工作流程</h2><p>由于通过<script type="math/tex">c^{<t-1>}</script>来更新<script type="math/tex">c^{<t>}</script>的值，但是现在我们使用GRU，GRU就是来控制是否更新<script type="math/tex">c^{<t>}</script>的值的，这里使用“更新”的名词可能有点怪，因为<script type="math/tex">c^{<t>}</script>实际上是通过<script type="math/tex">c^{<t-1>}</script><strong>计算</strong>出来的。那么公式<script type="math/tex">a^{<t>} = g_1(w_{a} * [a^{<t-1>}, x^{<t>}] + b_a)</script>变为<script type="math/tex">\tilde{c}^{<t>} = tanh(w_{c} * [c^{<t-1>}, x^{<t>}] + b_c)</script>，这里的<script type="math/tex">\tilde{c}^{<t>}</script>是一个候选值——candidate value，类似于中间变量，而激活函数我们选择tanh。<br>GRU的核心是有一个Gate，就是上面说的是否更新值的功能，它的公式为<script type="math/tex">\Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)</script>，<script type="math/tex">\Gamma_u</script>的u的意思是update，sigmoid函数的输出范围在0-1之间，所以就完成了类似更新的功能。如果是0就代表不让你更新，如果是1就代表让你更新，这里听起来还有点绕，没关系看下面的表达式。<br>这时开始执行更新步骤：<script type="math/tex">c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}</script>，这一步可以看出如果<script type="math/tex">\Gamma_u</script>等于1就将<script type="math/tex">c^{<t>}</script>更新为<script type="math/tex">\tilde{c}^{<t>}</script>，如果等于0就相当于不让你更新，结果还是上一个的c，即<script type="math/tex">c^{<t-1>}</script>。<br>将公式写在一起，GRU的工作流程就是：</p>
<script type="math/tex; mode=display">
\begin{cases}
    \tilde{c}^{<t>} = tanh(w_{c} * [c^{<t-1>}, x^{<t>}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)\\
    c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}\\
\end{cases}</script><h2 id="GRU完整版"><a href="#GRU完整版" class="headerlink" title="GRU完整版"></a>GRU完整版</h2><p>可以看到下式中就多了一个<script type="math/tex">\Gamma_r</script>，但是为什么不用上面的简化版呢？那是因为经研究者多年的尝试，发现下面的版本是很实用的，也算是一个标准版，你可以自己开发不同的版本。</p>
<script type="math/tex; mode=display">
\begin{cases}
    \tilde{c}^{<t>} = tanh(w_{c} * [\Gamma_r * c^{<t-1>}, x^{<t>}] + b_c)\\
    \Gamma_u = \sigma(w_{u} * [c^{<t-1>}, x^{<t>}] + b_u)\\
    \Gamma_r = \sigma(w_{r} * [c^{<t-1>}, x^{<t>}] + b_r)\\
    c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}\\
\end{cases}</script><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>吴恩达老师讲得感觉理解起来有点费劲，因为他觉得图片比文字更难理解，所以写了一大堆公式，只是再后面补充了图片。所以我建议看李宏毅老师的深度学习视频来理解LSTM。李宏毅老师的视频用了一张图片很好的解释了LSTM，并且他还举了一个例子，更加生动形象。<br>可能是东西方的差异，我感觉是图片好理解点，所以我选择看李宏毅老师的视频。这里就不写了，因为我在<strong>下面写了</strong>李宏毅老师课程的<strong>笔记</strong>。</p>
<h1 id="双向神经网络"><a href="#双向神经网络" class="headerlink" title="双向神经网络"></a>双向神经网络</h1><h1 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h1><hr>
<p>李宏毅机器学习</p>
<hr>
<h1 id="字母表示-1"><a href="#字母表示-1" class="headerlink" title="字母表示"></a>字母表示</h1><p>跟吴恩达老师讲的类似，李宏毅老师也讲了文字如何表示，与吴恩达老师不同的是，李宏毅老师多讲了几个。<br>最简单的方法利用向量来表示文字，就是上面说过的one-hot：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/1%20of%20N%20encoding.jpg" alt="1 of N encoding"><br>因为会出现某些单词没见到过，所以需要使用other这一维来表示。并且在右边的图中还可以使用字母来表示。然后理想上只要将词向量放入神经网络就会出现结果。但是Feedforward Network其实没办法解决这问题。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/beyond%201-of-N%20encoding.jpg" alt="beyond 1-of-N encoding"><br>可以看到下图，由于Feedforward Network没有记忆，所以两个句子对它来说是一个意思，但是对人来说可以很明显判断出第一句话台北是目的地，第二句话台北是出发地。Feedforward Network它只能训练当前的词，前一个词是什么它并不知道。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward%20Network%E6%97%A0%E6%B3%95%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="Feedforward Network无法解决的问题"></p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>上面讲到Feedforward Network由于没有记忆，无法记住前一个或者前几个词，所以就诞生了RNN。RNN其实也没那么神秘，就是每次输入并交给激活函数计算完毕后，将计算结果存入缓存中，并且在下一次计算时，将缓存取出来一起计算。就是下图的蓝色方框，由于是第一次计算，其中初始化为0。下图第一遍已经在计算了，实际上已经准备更新蓝色方框中的值了。RNN在上面的章节中其实已经写过了，都是类似的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E4%B8%80%E4%B8%AARNN%E7%9A%84%E5%B0%8F%E5%9E%8B%E4%BE%8B%E5%AD%90.jpg" alt="一个RNN的小型例子"><br>经过上面的例子发现，当前的输入已经在依赖前一个的缓存了，所以当顺序有所变化，或者前一个数据有所变化时，RNN可以察觉到，输出的结果也自然不同。</p>
<h2 id="deep-RNN"><a href="#deep-RNN" class="headerlink" title="deep RNN"></a>deep RNN</h2><p>我一共写了两个RNN的笔记，无论是吴恩达老师的还是李宏毅老师的到目前为止，RNN其实都不是deep的，之前也在疑惑，RNN横轴有很多层，但是实际上那些层只是不同时间的输入，根本不算deep。今天继续看下去，发现这个问题终于有解了，RNN也可以是deep的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/deep%20RNN.jpg" alt="deep RNN"></p>
<h2 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h2><p>上面讲的RNN都被称为Elman Network。还有另一种辩题叫做Jordan Network，它将输出值缓存起来。传说之中Jordan Network可以有更好的性能。<br><div class="note primary">
            <p>为什么有更好的性能</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Elman%20Network%E5%92%8Cordan%20Network.jpg" alt="Elman Network和ordan Network"></p>
<h2 id="双向RNN——Bidirectional-RNN"><a href="#双向RNN——Bidirectional-RNN" class="headerlink" title="双向RNN——Bidirectional RNN"></a>双向RNN——Bidirectional RNN</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Bidirectional%20RNN.jpg" alt="RNN——Bidirectional RNN"></p>
<h1 id="长短期记忆——Long-Short-term-Memory-LSTM"><a href="#长短期记忆——Long-Short-term-Memory-LSTM" class="headerlink" title="长短期记忆——Long Short-term Memory(LSTM)"></a>长短期记忆——Long Short-term Memory(LSTM)</h1><div class="note primary">
            <p>LSTM的神经元个数不同有什么区别？其他的NN架构也有同样的疑问</p>
          </div>
<p>上面讲的memory实际上是最简单的，LSTM才是现在最常用的Memory。Menory在RNN中实际只是一个神经元而已，它负责输入和输出。它们之间的关联是：RNN依旧是RNN，只不过把RNN中的神经元换成了LSTM。我们知道神经元的逻辑其实很简单，只有输入——计算——输入到激活函数——输出激活值，而LSTM只不过麻烦一点罢了。<br>下图就是一个LSTM。Input Gate中如果f(z)是1就代表Gate打开，也就是f(z)*g(z) = 1 * g(z) = g(z)，就相当于可以让外界输入。如果f(z)=0，Gate被关闭，那么 f(z)*g(z)=0，是不是就像不允许外界输入一样？因为你输入多少都被置为0。而Forget Gate也类似，当f(z)=1时，即Forget Gate被打开，这里与直觉有点相反，因为Gate打开，有点感觉像遗忘。但是其实c*f(z) = 1，所以Forget Gate为1其实是记住原本的c的意思。<br>另外图中也写到了，Gate的激活函数一般选sigmoid，里面的值就代表Gate的打开程度。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E7%A4%BA%E4%BE%8B.jpg" alt="LSTM示例"></p>
<h2 id="LSTM的例子"><a href="#LSTM的例子" class="headerlink" title="LSTM的例子"></a>LSTM的例子</h2><p>例子介绍：只有一个LSTM，输入有3维，输出有1维。<script type="math/tex">x_2 = 1</script>则<script type="math/tex">x_1</script>的值就会被存到Memory中，<script type="math/tex">x_2 = -1</script>则重置Memory，<script type="math/tex">x_3 = 1</script>则输出。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E4%BE%8B%E5%AD%90%E4%BB%8B%E7%BB%8D.jpg" alt="LSTM例子介绍"><br>注：下图中的蓝色数字和灰色数字是权重值。<br><div class="note primary">
            <p>权重值是初始化的？还是固定的？还是初始化后自己可以训练的？其实就是LSTM的反向传播算法要弄懂。</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E4%BE%8B%E5%AD%90%E8%AE%A1%E7%AE%97.jpg" alt="LSTM例子计算"></p>
<ol>
<li>Input Gate：<br>将偏差设为-10是因为我们通过x2来对Input Gate控制。平常x2=0，计算x*w+b=-10，那么通过sigmoid function就会得到一个接近于0的值，所以就实现了将Input Gate关闭的功能。而如果x2=1，那么x2*100=100，通过sigmoid function就会得到一个接近于1的值，Input Gate就实现了打开的功能。</li>
<li>Forget Gate: 这里的功能跟Input Gate类似。</li>
<li>Output Gate: 如果Output Gate被关闭，那么输出0.</li>
</ol>
<h2 id="多个LSTM工作场景"><a href="#多个LSTM工作场景" class="headerlink" title="多个LSTM工作场景"></a>多个LSTM工作场景</h2><p>里面的<script type="math/tex">x^t</script>就是对应于NN中的一个向量，它分别乘上4个参数矩阵得到4个不同的向量，以此操控LSTM，而LSTM实际上就等于神经元，说白了就是一个类似激活函数的功能。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF.jpg" alt="LSTM实际工作场景"><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%99%AF2.jpg" alt="LSTM实际工作场景2"><br>多个LSTM连起来工作就是像下面一样，红线和红线旁边的那个黑色曲线链接的值之前没有讲过，但是下图的这样才是LSTM实际的长相，所以之前讲的那么复杂实际上还是LSTM的简化版。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/LSTM%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="LSTM实际工作流程"></p>
<h1 id="RNN反向传播"><a href="#RNN反向传播" class="headerlink" title="RNN反向传播"></a>RNN反向传播</h1><p>BPTT——backpropagation through time，与NN的backpropagation类似，李宏毅老师也没讲原理直接跳过了。<br>然而不幸的是，RNN的training是很困难的。下面蓝色的线是希望的结果，但是实际上是绿色的线，会出现剧烈地抖动，最后在某个点出现NAN。这就是类似梯度消失问题。可以使用一些办法解决，但是现在用得最多的方法是LSTM。<br><div class="note primary">
            <p>为什么LSTM能解决RNN的难题</p>
          </div><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/RNN%20training%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98.jpg" alt="RNN trWaining碰到的问题"></p>
<h1 id="其他解决梯度消失的办法"><a href="#其他解决梯度消失的办法" class="headerlink" title="其他解决梯度消失的办法"></a>其他解决梯度消失的办法</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%85%B6%E4%BB%96%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8A%9E%E6%B3%95.jpg" alt="其他解决梯度消失的办法"></p>
<h1 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h1><h2 id="Many-to-one"><a href="#Many-to-one" class="headerlink" title="Many to one"></a>Many to one</h2><p>输入一个向量sequence，只输出一个向量。</p>
<ol>
<li>语义分析。比如分析电影评论是好是坏。</li>
<li>key term extraction。对文档提取关键词。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many to One.jpg" alt="Many to One"></p>
<h2 id="Many-to-many-Output-is-shorter"><a href="#Many-to-many-Output-is-shorter" class="headerlink" title="Many to many(Output is shorter)"></a>Many to many(Output is shorter)</h2><p>输入和输出都是向量sequence，但是输出要短。</p>
<ol>
<li>Speech Recognition 。语音辨识。</li>
</ol>
<h2 id="Many-to-many-No-limitation"><a href="#Many-to-many-No-limitation" class="headerlink" title="Many to many(No limitation)"></a>Many to many(No limitation)</h2><p>输入和输出都是序列且长短不一。被称为 <strong>Sequence to sequence learning</strong> 。</p>
<ol>
<li>Machine Translation. 机器翻译。</li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Many to Many（No Limitation）.jpg" alt="Many to Many(No Limitation)"></p>
<h2 id="Beyond-Sequence"><a href="#Beyond-Sequence" class="headerlink" title="Beyond Sequence"></a>Beyond Sequence</h2><ol>
<li>Syntactic parsing</li>
</ol>
<hr>
<p>李宏毅深度学习</p>
<hr>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>此系列视频还有两个Review视频，分别为第一个视频：Basic Structures for Deep Learning Models(Part 1)， 第二个视频：Basic Structures for Deep Learning Models(Part 2)。<br>个人认为Review视频不需要看，而且这两个视频时间贼长，加起来得有两个多小时。没必要浪费时间，即使你根本没学过Review中的知识点也不用去看。他的Review里不会讲很深，基本上就过过场，就算有很深的东西也完全不影响继续往下学。1P时长80分钟，说实话如果自己属于小白阶段，去看那么长的视频是挺打击人的兴趣的，如果是大佬或者已经入门的人当然看得津津有味了。<br><a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html#字母表示-1">此文</a>记录了李宏毅机器学习视频中讲解的RNN的笔记。</p>
<h1 id="Computational-Graph-amp-Backpropagation"><a href="#Computational-Graph-amp-Backpropagation" class="headerlink" title="Computational Graph &amp; Backpropagation"></a>Computational Graph &amp; Backpropagation</h1><div class="note danger">
            <p>2019年6月7号更新：关于计算图这章，现在才发现原来很重要，因为这是完成<strong>自动求导</strong>的关键。学了 pytorch 之后才发现的。</p>
          </div>
<h2 id="什么是Computational-Graph"><a href="#什么是Computational-Graph" class="headerlink" title="什么是Computational Graph"></a>什么是Computational Graph</h2><p>这实际上跟要学的深度学习没什么关系，只是名字好听点，无视就好，如下图就是一个Computational Graph。主要用来在计算神经网络一些输出时，便于理解。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E4%BE%8B%E5%AD%90.jpg" alt="Computational Graph例子"><br>在看一个比较贴近实际的例子，顺便复习一下链式求导法则。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Computational%20Graph%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99%E7%A4%BA%E4%BE%8B.jpg" alt="Computational Graph链式求导法则示例"></p>
<h2 id="通过链式求导的例子理解反向传播（Backpropagation）算法"><a href="#通过链式求导的例子理解反向传播（Backpropagation）算法" class="headerlink" title="通过链式求导的例子理解反向传播（Backpropagation）算法"></a>通过链式求导的例子理解反向传播（Backpropagation）算法</h2><p>首先进行正向链式求导，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E6%AD%A3%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="正向链式求导"><br>图中要求计算e对a求偏导，首先给出a=3, b=2。其中c=a+b, d=b+1。<br>按照李宏毅老师使用链式求导法则，先要计算c对a求导得到1。e再对c求导得到b+1，带入b=2，得到3。所以3对a求偏导等于1*3=3。<br>上面这种链式求导法则有点乱，如果没仔细学过<em>微积分</em>可能难以理解。其实对于方程e = (a+b) * (b+1)，e对a求偏导，直接看出来都可以。利用考研时的口诀“左导右不导，左不导右导”（也就是<a href="https://baike.baidu.com/item/%E8%8E%B1%E5%B8%83%E5%B0%BC%E8%8C%A8%E5%85%AC%E5%BC%8F/8779293?fr=aladdin" target="_blank" rel="noopener">莱布尼茨公式</a>），直接得到结果<script type="math/tex">\frac{\partial e}{\partial a} = b+1</script>。<br>然后将b=2带入b+1得到结果还是3。</p>
<p>接着进行反向模式，如下图:<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E5%8F%8D%E5%90%91%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC.jpg" alt="反向链式求导"><br>现在图中要求计算<script type="math/tex">\frac{\partial e}{\partial a}</script>以及<script type="math/tex">\frac{\partial e}{\partial b}</script>，当然你可以分别进行两次链式求导，得到结果。但是如果从e出发，也就是反向，那么就可以同时得到<script type="math/tex">\frac{\partial e}{\partial a}</script>以及<script type="math/tex">\frac{\partial e}{\partial b}</script>的结果。<br>不要在意e为什么等于1，只不过一个输入而已。<br>此外，如果阅读过《deep learning and neural network》一书，看过吴恩达机器学习视频或者其它资料的应该已经能反应出来。连接线上的求偏导实际上就跟神经网络上的权重一个意思，然后也是一层一层地反向传播。<br>这个输入e实际上就是神经网络中的反向传播算法中的输入。就是最后一层神经元的误差<script type="math/tex">\delta^l = h-y</script>。这里吴恩达老师和《deep learning and neural network》作者的最后一层误差公式不一样，<strong>目前不明</strong>，暂时不做解释，这里的公式是吴恩达老师的。<br>然后就是误差*权重+偏差得到前一层的误差，具体不展开。</p>
<h2 id="反向传播的好处"><a href="#反向传播的好处" class="headerlink" title="反向传播的好处"></a>反向传播的好处</h2><p>如果你的root只有一个，那么这个Computational Graph中的所有偏微分就都可以一次性算出。对应于神经网络，我们就是要这样的效果。</p>
<h2 id="参数共享（Parameter-sharing）"><a href="#参数共享（Parameter-sharing）" class="headerlink" title="参数共享（Parameter sharing）"></a>参数共享（Parameter sharing）</h2><p>略，看了一眼貌似挺简单。16:20</p>
<h2 id="Computational-Graph-for-Feedforword-Net"><a href="#Computational-Graph-for-Feedforword-Net" class="headerlink" title="Computational Graph for Feedforword Net"></a>Computational Graph for Feedforword Net</h2><p>李宏毅深度学习p3从21:16到52:48讲解梯度下降算法、前馈神经网络以及反向传播算法的具体数学原理<br>一直没看懂原理，以后再看。</p>
<h2 id="Computational-Graph-for-Recurrent-Network"><a href="#Computational-Graph-for-Recurrent-Network" class="headerlink" title="Computational Graph for Recurrent Network"></a>Computational Graph for Recurrent Network</h2><h1 id="Deep-Learning-for-Language-Modeling"><a href="#Deep-Learning-for-Language-Modeling" class="headerlink" title="Deep Learning for Language Modeling"></a>Deep Learning for Language Modeling</h1><p>语言模型就是预测一个word sequence出现的几率有多大。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Language%20Modeling.jpg" alt="Language Modeling"></p>
<h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>N-gram是自然语言处理中的算法。2-gram读作bi-gram。</p>
<h3 id="传统做法"><a href="#传统做法" class="headerlink" title="传统做法"></a>传统做法</h3><ul>
<li>怎么预测一句话出现的几率</li>
<li>收集大量文本作为训练数据<ul>
<li>然后计算<script type="math/tex">w_1\cdots w_n</script>这句话在训练数据中出现的概率</li>
</ul>
</li>
<li>N-gram语言模型：<ul>
<li>如何计算一小部分的概率？例如下图的p(beach|nice)出现的概率。就是将nice beach出现的次数除以nice出现的次数。</li>
</ul>
</li>
</ul>
<p>前两条是理想的处理办法，但是麻烦的是要预测的句子在语料库——corpus中八成一次都没出现过。于是就需要使用N-gram模型。它的处理办法就是将句子拆成比较小的部分——component，再把每个小部分的概率乘起来就是句子出现的几率。像下图这种只考虑前一个单词的模型叫做2-gram model。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/N-gram.jpg" alt="N-gram"></p>
<h3 id="NN-based-LM"><a href="#NN-based-LM" class="headerlink" title="NN-based LM"></a>NN-based LM</h3><p>怎么做基于NN的N-gram？<br>做法：</p>
<ol>
<li>搜集training数据</li>
<li>learn一个Neural Network，通过两个词predict下一个词，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/NN-based%20LM.jpg" alt="NN-based LM"></li>
<li>使用cross entropy minimize</li>
<li>有了Neural Network后算一个句子的几率，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/%E8%AE%A1%E7%AE%97%E5%8F%A5%E5%AD%90%E7%9A%84%E5%87%A0%E7%8E%87.jpg" alt="计算句子的几率"><br>其中STRAT是一个token，代表句子的起始。</li>
</ol>
<h3 id="RNN-based-LM"><a href="#RNN-based-LM" class="headerlink" title="RNN-based LM"></a>RNN-based LM</h3><p>往上翻<strong>循环神经网络——RNN</strong>，原理就是这个。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/RNN-based LM.jpg" alt="RNN-based LM"></p>
<h3 id="Challenge-of-N-gram"><a href="#Challenge-of-N-gram" class="headerlink" title="Challenge of N-gram"></a>Challenge of N-gram</h3><h4 id="NN-based-model"><a href="#NN-based-model" class="headerlink" title="NN-based model"></a>NN-based model</h4><p>为什么要使用NN-based model。相较于传统方法有什么好处。<br>就是概率估不准，因为永远没有足够的数据。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Challenge%20of%20N-gram.jpg" alt="Challenge of N-gram"><br><div class="note info">
            <p>视频13:20~27:01仔细讲解了为什么要使用NN，而且把我困惑了快一个月的问题解决了，就是将文字转为数字之后进行训练的意义。</p>
          </div></p>
<h4 id="RNN-based-model"><a href="#RNN-based-model" class="headerlink" title="RNN-based model"></a>RNN-based model</h4><p>为什么要使用RNN-based model。相较于传统方法有什么好处。</p>
<h1 id="几个有用的network架构"><a href="#几个有用的network架构" class="headerlink" title="几个有用的network架构"></a>几个有用的network架构</h1><h2 id="Spatial-Transformer-Layer"><a href="#Spatial-Transformer-Layer" class="headerlink" title="Spatial Transformer Layer"></a>Spatial Transformer Layer</h2><p>&emsp;&emsp;<a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" target="_blank" rel="noopener">论文地址</a>，中文可以叫<strong>空间变换层</strong>。<br>&emsp;&emsp;此神经网络架构的出现的原因：CNN 对图片的缩放以及旋转无所谓（CNN is invariant to scaling androtation）。比如说在图片的局部地区中，一个人移动一点点距离，对 CNN 来说其实没什么多大区别。不过距离有点远的话，还是有点影响的。</p>
<h2 id="Highway-Network"><a href="#Highway-Network" class="headerlink" title="Highway Network"></a>Highway Network</h2><p>&emsp;&emsp;先对前馈神经网络和 RNN 进行一下对比。</p>
<ol>
<li>Feedforward NN 不是每一步都有输入。</li>
<li>Feedforward NN 每一层都有不同的参数。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Feedforward NN和RNN的对比.jpg" alt="Feedforward NN和RNN的对比"></li>
</ol>
<p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Network 论文地址</a>；<a href="https://arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="noopener">Highway Network 实战论文地址</a><br>&emsp;&emsp;Highway Network 的想法就是把 RNN <strong>立</strong>起来，把它当做前馈神经网络来用。<br>&emsp;&emsp;<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">Highway Network 的改进版论文地址</a>，这个就是<strong>残差神经网络</strong>。</p>
<h2 id="Grid-LSTM"><a href="#Grid-LSTM" class="headerlink" title="Grid LSTM"></a>Grid LSTM</h2><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1507.01526.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;太复杂了，估计以后也很难用到。。。</p>
<h2 id="Recusive-Network"><a href="#Recusive-Network" class="headerlink" title="Recusive Network"></a>Recusive Network</h2><p>&emsp;&emsp;Recursive Network 是 Recurrent Network 更 Generalize 的版本。Recurrent Network 是 Recursive Network 的一个特殊的例子，如果翻译成中文的话，实际上名字都一样。所以可以称之为递归式网络。以下是 RNN 和 Recursive Network 的对比图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Recursive Network示意图.jpg" alt="Recursive Network示意图"></p>
<p>&emsp;&emsp;在做  Recursive Network 之前，需要考虑输入的序列的结构。图中将 <script type="math/tex">x_1</script> 和 <script type="math/tex">x_2</script> 一同输入进一个 function，但是其实可以不这么做，具体要怎么输入，取决于输入数据的结构。<strong>而由于 f 与 f 前后相接，所以在写代码时需要预先做好设计</strong>。<br>&emsp;&emsp;举个具体的例子，要判断“not very good”包含什么情绪，可以先使用语法解析，将句子结构化，然后根据句子的语法结构来使用 Recursive Network 进行训练，如下图：<br>&emsp;&emsp;“very”的词向量和“good”的词向量一同放入 f 中训练，我们可以将得到的向量看做是“very good”的意思。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/根据句子语法结构训练1.jpg" alt="根据句子语法结构训练1" title="根据句子语法结构训练1"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/根据句子语法结构训练2.jpg" alt="根据句子语法结构训练2" title="根据句子语法结构训练2"></p>
<p>&emsp;&emsp;当然两个词向量不能是简单的相加，具体做法可以自行选择。最简单的做法可以参考下图的上半部分，而下图的下半部分被称为 <strong>Recursive Neural Tensor Network</strong>，总而言之就是一个很复杂的做法来解决两个词向量不仅仅是进行简单的拼接或者相加。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Recursive Neural Tensor Network.jpg" alt="Recursive Neural Tensor Network"></p>
<p>&emsp;&emsp;对于 f 还有其他的做法，如 Matrix-Vector Recursive Network，<a href="https://arxiv.org/pdf/1503.00075.pdf" target="_blank" rel="noopener">Tree LSTM 2015</a> 等。具体就不记了，以后可以查 Recursive Network 相关论文。</p>
<h1 id="Conditional-Generation-by-RNN-amp-Attention"><a href="#Conditional-Generation-by-RNN-amp-Attention" class="headerlink" title="Conditional Generation by RNN &amp; Attention"></a>Conditional Generation by RNN &amp; Attention</h1><p>&emsp;&emsp;注意本文讲的是 RNN <strong>入门</strong>，而下面的部分也只是讲普通的 RNN Generation，甚至连 decoder 部分都没用。下图是生成文字，其实也可以生成图片、音频等，我就不一一截图了，第二张图将这些<strong>想法</strong>及其<strong>论文</strong>都汇总了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/一个简单的Generation.jpg" alt="一个简单的Generation"></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Generation汇总.jpg" alt="Generation汇总"></p>
<h2 id="Conditional-Generation"><a href="#Conditional-Generation" class="headerlink" title="Conditional Generation"></a>Conditional Generation</h2><p>&emsp;&emsp;但是在真实的场景中，我们不仅仅是希望只生成随机的句子，我们更偏向于生成一些基于某些条件的句子，比如：当看见一张一个人正在跳舞的图片，我们希望电脑生成“A young girl is dancing”；当给予一个条件“Hello”时，我们希望电脑生成“Hello, nice to see you.”。<br>&emsp;&emsp;一个实际的例子，我们可以将一张图片输入进 CNN，从而产生一个向量，再把该向量输入进 decoder 部分，最后生成句子。如下图所示，其他类型的<strong>条件生成</strong>也类似。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Image Caption Generation.jpg" alt="Image Caption Generation" title="Image Caption Generation"></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>&emsp;&emsp;将 <script type="math/tex">z_0</script> 与 <script type="math/tex">h_1 h_2 h_3 h_4</script> 分别做一次 match，至于 match 怎么计算可以看下图右边，总而言之，就是可以由你自己设计。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制.jpg" alt="Attention机制"></p>
<p>&emsp;&emsp;然后获得 <script type="math/tex">a^1_0 a^2_0 a^3_0 a^4_0</script>，之后将它们输入 softmax 层（有实验发现其实不经过 softmax 层也可以，甚至效果更好），最后将所有 a 分别乘上它们对应的向量并且相加，得到一个向量 <script type="math/tex">c^0</script>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制计算score.jpg" alt="Attention机制计算score"></p>
<p>&emsp;&emsp;使用 Attention 机制计算完毕后，将向量 <script type="math/tex">c^0</script> 输入进 decoder 即可，接下来的计算都是以此类推。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention机制计算完毕后输入到decoder.jpg" alt="Attention机制计算完毕后输入到decoder"></p>
<h3 id="Attention应用到Speach-Recognition"><a href="#Attention应用到Speach-Recognition" class="headerlink" title="Attention应用到Speach Recognition"></a>Attention应用到Speach Recognition</h3><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Attention for Speach Recognition.jpg" alt="Attention for Speach Recognition"></p>
<h2 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h2><p>&emsp;&emsp;<a href="https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" target="_blank" rel="noopener">论文地址</a>，<a href="https://www.bilibili.com/video/av9770302/?p=8" target="_blank" rel="noopener">视频地址</a>43:00开始。<br>&emsp;&emsp;Memory Network 最先被用在 Reading Comprehension，说白了就是一个 Attention 机制。下图就是一个简易的 <strong>Memory Network</strong>。</p>
<ol>
<li>首先将 document 由多个句子组成，句子由 vector x 表示。具体如何表示的问题，可以由自定义解决，如 bag of word 或者由词向量表示；</li>
<li>query 就是问题，也由 vector q 表示；</li>
<li>使用 q 对每个句子做 attention 得到 match score <script type="math/tex">\alpha</script>，然后使用 <script type="math/tex">\alpha</script> 和 x 做 weighted sum；</li>
<li>最后将 weighted sum 后的 vector 和 vector q 都丢到 DNN 中，得到答案。</li>
</ol>
<p>&emsp;&emsp;注：这是在做阅读理解。document -&gt; vector 等于 input(I) 和 generalization(G)，attention 等于 output(O)，生成答案等于 response(R)。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/一个简易的Memory Network.jpg" alt="一个简易的Memory Network" title="一个简易的Memory Network"></p>
<p>&emsp;&emsp;Memory Network 还有更复杂的版本，即 attention 的 vector 和抽取信息的 vector 并不需要是同一个，如下图所示。</p>
<ol>
<li>将 document  表示为句子时，使用两组向量。一组用于计算 match score，一组用于 weighted sum。</li>
<li>其他的步骤都差不多，但是有一个地方不一样。在 weighted sum 得到一个 vector 之后，可以和 q 加在一起，得到一个新的 q，再重复 步骤 1。而且这个步骤可以做很多次，做完之后再输入进 DNN 获取答案。这个步骤被称之为 Hopping，注意从 document 抽取的两组 vector 在 hopping 的时候，可以是不一样的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/更复杂的memory network.jpg" alt="更复杂的memory network"></li>
</ol>
<h2 id="Neural-Turing-Machine"><a href="#Neural-Turing-Machine" class="headerlink" title="Neural Turing Machine"></a>Neural Turing Machine</h2><h2 id="Tips-for-Generation"><a href="#Tips-for-Generation" class="headerlink" title="Tips for Generation"></a>Tips for Generation</h2><p>&emsp;&emsp;这里听不太懂，跳过了。有 Beam Search 之类的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/对Generation的建议1.jpg" alt="对Generation的建议1"></p>
<h1 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h1><p>&emsp;&emsp;<a href="https://pdfs.semanticscholar.org/eb5c/1ce6818333560d0d3247c0c74985ef295d9d.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;举一个简单的例子助于理解 Pointer Network。在二维坐标系中任意给出 4 个点，我们的目标是找到几个点，将它们连起来形成一个封闭圈，剩下的那几个点要正好在这个封闭圈之中，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/助于理解Pointer Network的一个例子.jpg" alt="助于理解Pointer Network的一个例子"></p>
<p>&emsp;&emsp;当然，这肯定已经有一些算法可以解了，比如在坐标系中计算距离。但是今天我们使用硬 train 一发的方法，即不管三七二十一将它输入到神经网络里面训练。首先制造一些训练数据，然后给 encoder-deocder 训练。<strong>具体的训练步骤为</strong>：输入点的坐标，输出 one-hot 表示 <code>{1,2,3,4,END}</code> 的五维向量，碰到 END 则代表解码完毕。<br>&emsp;&emsp;但结果是网络训练不起来，因为在上述的例子中我们只输入了 4 个点，我们的目的是得到 1-4 个点。但是如果我们的测试数据是输入 400 个点呢？那么我们也只会得到 1-4 个点，因为 <code>{1,2,3,4,END}</code> 是预先定义好的。你可能会想那就多定义一点啊，但是下次我要是输入 4000 个点呢？要是 40000 个点呢？总有你无法预先定义的时候。<br>&emsp;&emsp;所以我们需要 <strong>Pointer Network</strong> 来<strong>动态的改变类别</strong>（具体做法详见下一小节），注意我这里直接说成类别了，我们可以把 decoder 部分看作是多元分类的工作，如输出 4000 个点，就是 4000 元分类。<br>&emsp;&emsp;<strong>上面的例子其实是 Pointer Netwoek 论文中的一个例子，但是对于这个例子来说，使用 Pointer Network 其实没多大意义，因为问题本身有更简单的解法，下面说一下有意义的用途。</strong><br>&emsp;&emsp;Pointer Network 应用于 <strong>Summarization</strong>，<a href="https://www.aclweb.org/anthology/P17-1099" target="_blank" rel="noopener">论文地址</a>。给定一篇文档，让机器做出总结。对于此类问题，我们会碰到很多<strong>生僻的地名、人名</strong>等等字词。我们可以使用 Pointer Network 来解决这个问题。<br>&emsp;&emsp;下图就是做法，整张图的意思就是在做文本摘要的工作，输入一个句子，输出摘要。先不看中间的黄色圆圈 <script type="math/tex">p_{gen}</script>，看看其他部分（红黄两部分）就是很普通的 encoder-decoder。但是对于这个 encoder-decoder 来说，词表中并没有 <em>Aregentina</em> 这个单词。那么我们就可以使用 Pointer Network，这个 <script type="math/tex">p_{gen}</script> 就是概率（具体描述见下一节）。最后结果就是我们将注意力关注到 <em>Aregentina</em> 这个单词。当然对于 encoder-decoder 这部分的工作也是要做的，我们可以将两个结果加起来，从而判断出最终要产生哪个单词，做法详见原论文。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Pointer Network for Summarization.jpg" alt="Pointer Network for Summarization"></p>
<p>&emsp;&emsp;还可用于 <strong>machine learning</strong>、<strong>chatbot</strong> 等。</p>
<h2 id="具体做法"><a href="#具体做法" class="headerlink" title="具体做法"></a>具体做法</h2><p>&emsp;&emsp;具体的实现就是像下图一样，首先在输入的序列之前加入一个 END 序列，然后将 decoder 删掉。我们还是使用 Attention 机制计算每个序列的 attention score，但是这次的 score 不再乘上它对应的向量，而是直接当做向量输出，意思就是把所以的 score 做一次 max，最大的就输出 1。而<strong>停止条件就是 END 这个序列的 score 是最大的，即为 1 就停止训练。</strong><br>&emsp;&emsp;这样的做法乍一看好像无法理解，我解释一下。由于 encoder 是对序列的长度不敏感的，也就是说如果预先定义的类别是 40 维，而我输入 400 个点，那么对于 encoder 来说，它可以增加神经元的数量从而使得 400 个点<strong>正好</strong>全部输入进 encoder。但是对于 decoder 来说，它输出只能是 40 维。<strong>那么 Pointer Network 的做法是将 decoder 删除，把输出的工作也交给 encoder 去做。所以我输入 400 个点，自然也就可以输出 400 维的类别</strong>（这里应该是 401 维，因为还有一个 END 序列）。看下图的 encoder，<script type="math/tex">h^4</script> 的分数是 0.7，所以我们的输出就是 4，当然对于向量来说就是 <script type="math/tex">(0, 0, 0, 0, 1)</script>。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/吴恩达李宏毅综合学习笔记：RNN入门/Pointer Network的做法.jpg" alt="Pointer Network的做法"></p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>git学习记录</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/git%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html</url>
    <content><![CDATA[<p>本文是在学习<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">该教程</a>时/后做的笔记。<br>我现在用git基本都是用<a href="https://desktop.github.com/" target="_blank" rel="noopener">Github Desktop</a>，前面的是下载地址。用起来方便又快捷。事实上我也不会用git的命令o(<em>￣︶￣</em>)o所以今天稍微学一下。</p>
<h1 id="git-init"><a href="#git-init" class="headerlink" title="git init"></a>git init</h1><p>切换到想要创建仓库的文件夹，执行命令<code>git init</code>就会在该文件夹下创建一个.git的文件夹，这个文件夹是隐藏的。</p>
<h1 id="git-add-git-commit"><a href="#git-add-git-commit" class="headerlink" title="git add/git commit"></a>git add/git commit</h1><p>使用命令<code>git add whatever.txt</code>将文件添加到仓库。使用命令<code>git commit -m &quot;wrote a file&quot;</code>将文件提交到仓库，-m后面的是描述这份文件你改了什么。其实就是相当于desktop的一个按钮，按一下就把全部有改动文件都提交了。<br>这样就完成了提交一份文件。这里就会有疑问了，为什么设计成先add再commit？直接commit不就行了？因为commit可以提交多份文件，你可以使用add命令一份一份地添加文件，再使用commit一次性提交到仓库。<br>该命令指示推送到本地仓库，并非远程仓库。</p>
<h1 id="git-status"><a href="#git-status" class="headerlink" title="git status"></a>git status</h1><p>查看仓库当前的修改状态</p>
<h1 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h1><p>发现某份文件被修改了，但是忘记改了什么怎么办？使用命令<code>git diff modified_file.txt</code>查看，它会显示文件哪里被修改了。diff就是difference的意思。</p>
<h1 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h1><p>git可以记住你的历史提交版本，如果有一天电脑损坏，自己干了什么完全忘记，可以使用<code>git log</code>命令。它会显示以前所有的历史记录。这条命令会显示很详细的信息，但是就是因为信息太详细了，人可能看不过来，可以加上<code>--pretty=oneline</code>来限制。类似<code>c3fca95239a4bbe21ee2991e0a914fb522060e74</code>这种是版本号（commit id）。</p>
<h1 id="git-rest"><a href="#git-rest" class="headerlink" title="git rest"></a>git rest</h1><p>该命令可以回退版本。<code>git reset --hard HEAD^</code>，命令里的HEAD代表当前版本，^代表上一个版本，如果想要回退至前100个版本，可以使用HEAD~100。<br>也可以直接指定commit id，如<code>git reset --hard c3fca95239a4bbe21ee2991e0a914fb522060e74</code>，commit id可以不写全，写个开头就行了<code>git reset --hard c3fca</code><br>注意回退版本后，如果关闭git bash那么就无法查询到该版本之后的所有版本。</p>
<h1 id="git-reflog"><a href="#git-reflog" class="headerlink" title="git reflog"></a>git reflog</h1><p>该命令记住了你每一步操作，如果回退版本后后悔了，可以使用该命令查询以前的commit id。</p>
<h1 id="git-checkout-—filename"><a href="#git-checkout-—filename" class="headerlink" title="git checkout —filename"></a>git checkout —filename</h1><p>把文件夹在工作区的修改全部撤销。总之，就是让这个文件回到最近一次git commit或git add时的状态。<br>参考<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374831943254ee90db11b13d4ba9a73b9047f4fb968d000" target="_blank" rel="noopener">文章</a></p>
<h1 id="git-rm"><a href="#git-rm" class="headerlink" title="git rm"></a>git rm</h1><p>删除文件，与linux命令类似。</p>
<h1 id="git-remote"><a href="#git-remote" class="headerlink" title="git remote"></a>git remote</h1><p>将本地的仓库和远程的仓库关联，使用命令<code>git remote add origin git@github.com:github_account_name/repository_name.git</code><br>注意将github_account_name和repository_name分别替换成github账号名和仓库名。添加关联后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的。下一步，就可以把本地库的所有内容推送到远程库上。</p>
<h1 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h1><p><code>git push -u origin master</code></p>
<blockquote>
<p>把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。<br>由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。<br>从现在起，只要本地作了提交，就可以通过命令：<br><code>git push origin master</code><br>把本地master分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！</p>
</blockquote>
<h1 id="git-clone"><a href="#git-clone" class="headerlink" title="git clone"></a>git clone</h1><p>上面说了将本地仓库和远程仓库关联，并将本地仓库的文件推送到远程仓库，那么自然也可以从远程仓库clone文件到本地仓库。<br>使用命令：<code>git clone git@github.com:github_account_name/repository_name.git</code><br>还可以从<a href="https://github.com/yan624/yan624.github.io.git这样的地址克隆" target="_blank" rel="noopener">https://github.com/yan624/yan624.github.io.git这样的地址克隆</a></p>
<h1 id="对分支的管理"><a href="#对分支的管理" class="headerlink" title="对分支的管理"></a>对分支的管理</h1><p>字太多，不想打了。看下面教程。<br><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840038939c291467cc7c747b1810aab2fb8863508000" target="_blank" rel="noopener">教程</a></p>
<h2 id="git-checkout-b-branch-name"><a href="#git-checkout-b-branch-name" class="headerlink" title="git checkout -b branch_name"></a>git checkout -b branch_name</h2><p>创建名为branch_name的分支并切换到该分支，-b参数代表切换。</p>
<h2 id="git-branch"><a href="#git-branch" class="headerlink" title="git branch"></a>git branch</h2><p>查看当前分支，如果分支之前有*就代表这个分支是主分支。</p>
<h2 id="git-merge-branch-name"><a href="#git-merge-branch-name" class="headerlink" title="git merge branch_name"></a>git merge branch_name</h2><p>合并分支</p>
<h2 id="git-branch-d-branch-name"><a href="#git-branch-d-branch-name" class="headerlink" title="git branch -d branch_name"></a>git branch -d branch_name</h2><p>删除名为branch_name分支</p>
<h2 id="合并分支发生冲突"><a href="#合并分支发生冲突" class="headerlink" title="合并分支发生冲突"></a>合并分支发生冲突</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840202368c74be33fbd884e71b570f2cc3c0d1dcf000" target="_blank" rel="noopener">解决办法</a></p>
<h2 id="强大的分支功能"><a href="#强大的分支功能" class="headerlink" title="强大的分支功能"></a>强大的分支功能</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013758410364457b9e3d821f4244beb0fd69c61a185ae0000" target="_blank" rel="noopener">创建分支的策略</a></p>
<h2 id="bug分支。将当前工作暂存，先修改出现的bug"><a href="#bug分支。将当前工作暂存，先修改出现的bug" class="headerlink" title="bug分支。将当前工作暂存，先修改出现的bug"></a>bug分支。将当前工作暂存，先修改出现的bug</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137602359178794d966923e5c4134bc8bf98dfb03aea3000" target="_blank" rel="noopener">暂存命令</a></p>
<h2 id="feature分支"><a href="#feature分支" class="headerlink" title="feature分支"></a>feature分支</h2><p>与上面类似，无非概念不同</p>
<h2 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013760174128707b935b0be6fc4fc6ace66c4f15618f8d000" target="_blank" rel="noopener">教程</a></p>
<h2 id="git-rebase"><a href="#git-rebase" class="headerlink" title="git rebase"></a>git rebase</h2><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="使用github"><a href="#使用github" class="headerlink" title="使用github"></a>使用github</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000" target="_blank" rel="noopener">教程</a></p>
<h1 id="使用码云"><a href="#使用码云" class="headerlink" title="使用码云"></a>使用码云</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00150154460073692d151e784de4d718c67ce836f72c7c4000" target="_blank" rel="noopener">教程</a></p>
<h1 id="配置文件的更多配置"><a href="#配置文件的更多配置" class="headerlink" title="配置文件的更多配置"></a>配置文件的更多配置</h1><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00150154460073692d151e784de4d718c67ce836f72c7c4000" target="_blank" rel="noopener">教程</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>更改hexo next主题的fontawesome版本至最新</title>
    <url>/%E6%9B%B4%E6%94%B9hexo-next%E4%B8%BB%E9%A2%98%E7%9A%84fontawesome%E7%89%88%E6%9C%AC%E8%87%B3%E6%9C%80%E6%96%B0.html</url>
    <content><![CDATA[<p>由于在后续又发现在其他地方也用到了icon，所以需要大量更改配置。<strong>如果不会写程序的还是别改了。</strong></p>
<p>next的fontawesome默认版本是4.6.2，在写本文时，fontawesome的最新版本是5.8.1。貌似fontawesome在5.0.0版本之后改版了。总之一直出现方框乱码。<br>后来发现，现在的fontawesome链接已经跟以前不一样了，它现在分为3大类别。<br>现在的使用方法是：在next主题的_config.xml中搜索fontawesome，并更改属性<code>fontawesome: //cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.1/css/all.min.css</code>，注意里面的文件是all.min.css，而不是font-awesom.min.css。<br>但是这样更改之后还是会出现方框乱码，原因是next默认使用的fa的类，而有时候我们需要使用fab或其他的类。所以需要修改一下源代码。<br>找到layout/_macro/menu/menu-item.swig，定位class=”menu-item-icon，将后面的“fa fa-fw fa-”删去。以后再修改icon不能只加一个名字了。可以像我这样修改：<code>assorted: /assorted || fa fa-fw fa-layer-group</code>。<br>可以看到我将icon的名称补全了。如果想用fab的类，可以像这样修改：<code>python: /python || fab fa-fw fa-python</code>。以此类推。</p>
<p>这样修改以后，如果不想用fontawesome了，想用其他的icon库，改起来也很方便。</p>
<p>layout/_macro/menu/menu-item.swig被layout/_partials/header/sub-menu.swig引用。</p>
<p>另外由于fontawesome版本改动，社交软件的icon也需要更改，在_config.xml中搜索github，将icon改为<code>fab fa-fw fa-github</code>。找到ayout/_macro/siderbar.swig，搜索fa fa-fw fa-，看看定位的地点上面是不是<code>{百分号  if theme.social_icons.enable 百分号}</code>。是的话将fa fa-fw fa-删除。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>ViewPager无法刷新数据</title>
    <url>/ViewPager%E6%97%A0%E6%B3%95%E5%88%B7%E6%96%B0%E6%95%B0%E6%8D%AE.html</url>
    <content><![CDATA[<p>实现ViewPager刷新数据功能，在网上找了很多资料都已经过时了。<br>由于本人并不是android开发出身，完全是做app玩的。所以很多术语都不知道，如果看不懂就算了。。。</p>
<p>实现PagerAdapter类，我命名为HomePagerAdapter<br><figure class="highlight aspectj"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HomePagerAdapter</span> <span class="keyword">extends</span> <span class="title">PagerAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;View&gt; pageView;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HomePagerAdapter</span><span class="params">(ArrayList&lt;View&gt; pageView)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pageView = pageView;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> mChildCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">notifyDataSetChanged</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mChildCount = getCount();</span><br><span class="line">        <span class="keyword">super</span>.notifyDataSetChanged();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getItemPosition</span><span class="params">(Object object)</span>   </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> ( mChildCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            mChildCount --;</span><br><span class="line">            <span class="keyword">return</span> POSITION_NONE;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">return</span> <span class="keyword">super</span>.<span class="title">getItemPosition</span><span class="params">(object)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//获取当前窗体界面数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">int</span> <span class="title">getCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="function"><span class="keyword">return</span> pageView.<span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">//判断是否由对象生成界面</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isViewFromObject</span><span class="params">(View arg0, Object arg1)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="keyword">return</span> arg0==arg1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">destroyItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position, Object object)</span> </span>&#123;</span><br><span class="line">        container.removeView(pageView.get(position));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Object <span class="title">instantiateItem</span><span class="params">(ViewGroup container, <span class="keyword">int</span> position)</span> </span>&#123;</span><br><span class="line">        View view = pageView.get(position);</span><br><span class="line">        container.addView(view);</span><br><span class="line">        <span class="keyword">return</span> view;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">finishUpdate</span><span class="params">(ViewGroup container)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(container.getChildCount() == <span class="number">0</span>)&#123;</span><br><span class="line">            pageView.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意destroyItem、instantiateItem方法，PagerAdapter中还有两个同名的方法，但是已被废弃，注意参数不同。<br>实现刷新数据主要在destroyItem、instantiateItem、finishUpdate三个方法。其他的方法其实别人的教程也写了，但是我这三个方法是我自己研究的，我没见别人写过。</p>
<p>解释流程。<br>第一步，改变数据，我是将数据保存在了<code>private ArrayList&lt;View&gt; pageView;</code>中。<br>第二步，调用<code>adapter.notifyDataSetChanged();</code>方法，它首先会销毁item，即调用<code>destroyItem(ViewGroup container, int position, Object object)</code>方法。随即调用<code>instantiateItem(ViewGroup container, int position)</code>方法。<br>一般来说大家都是这么干的，因为将数据改变后，调用<code>adapter.notifyDataSetChanged();</code>方法。直觉认为这么做合乎常理。<br>但是这里注意一点，假设<code>private ArrayList&lt;View&gt; pageView;</code>中原先保存两个View，改变数据将这个View删除，从新添加三个新的View，那么在<code>destroyItem(ViewGroup container, int position, Object object)</code>方法中，它无法删除，仔细看里面的代码<code>container.removeView(pageView.get(position));</code>，发现它是通过position这个索引获取对象，再在container容器中通过对象查找删除。那么问题来了，你之前已经将两份View删除了，它还怎么通过position获取到呢？所以在这一步出了问题。<br>当然这一步出了问题后，后面的创建页面步骤更是稀巴烂。</p>
<p>正确步骤如下：<br><figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将页面刷新，渲染新的数据集</span></span><br><span class="line"><span class="selector-tag">homePagerAdapter</span><span class="selector-class">.notifyDataSetChanged</span>();</span><br><span class="line"><span class="selector-tag">changeData</span>(inflater, data);</span><br><span class="line"><span class="selector-tag">homePagerAdapter</span><span class="selector-class">.notifyDataSetChanged</span>();</span><br></pre></td></tr></table></figure></p>
<p>第一步，不要更改数据，直接调用<code>homePagerAdapter.notifyDataSetChanged();</code>，目的是让其删除原先的view。<br>第二步，更改数据。<br>第三步，再次调用<code>homePagerAdapter.notifyDataSetChanged();</code>，完成页面的创建。由于container中已经没有view了，所以删除那个步骤做了也等于没做，但是由于数据已经更新页面还是会被创建出来。</p>
<p>最后强调用一点。在HomePagerAdapter类中一个<code>finishUpdate(ViewGroup container)</code>方法，注意看里面的代码。<strong>以上的所有步骤，全部依赖于这几句代码。</strong><br>上面第一步说到直接调用notifyDataSetChanged()方法目的是删除原先的view，但是view删除后，你必须将<code>private ArrayList&lt;View&gt; pageView;</code>中的数据也删除。<strong>这里补充一点，pageView内是我创建的View，而container中是android自己维护的界面</strong>，我也不知道怎么称呼，就将其称为界面吧。<br>在finishUpdate()方法中判断，如果container中已经没有界面了，那就直接移除pageView中所有的数据，也就是算更新数据了。值得注意的是，这里面逻辑及其复杂，这行清空数据的代码，只有放在<code>finishUpdate(ViewGroup container)</code>中执行，并且必须加上那个if条件判断，app才能正常运行。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>android</category>
      </categories>
  </entry>
  <entry>
    <title>在写了大量内容以及大量数学表达式后，mathJax无法渲染数学表达式</title>
    <url>/bug/%E5%9C%A8%E5%86%99%E4%BA%86%E5%A4%A7%E9%87%8F%E5%86%85%E5%AE%B9%E4%BB%A5%E5%8F%8A%E5%A4%A7%E9%87%8F%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%90%8E%EF%BC%8CmathJax%E6%97%A0%E6%B3%95%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BE%BE%E5%BC%8F.html</url>
    <content><![CDATA[<p>之前写的数学表达式明明可以渲染，但是接下去隔了n行的数学表达式无法渲染。推测是因为单行数学表达式在文字前面换行。<br>比如说：</p>
<blockquote>
<p>文字文字文字文字：·￥￥·<br>该表达式渲染正常。</p>
</blockquote>
<p>如果，</p>
<blockquote>
<p>文字文字文字文字：<br>·￥￥·<br>那么下面的数学表达式将全部无法渲染。<br><a id="more"></a></p>
</blockquote>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>运用其他的插件，在hexo中添加提示弹窗</title>
    <url>/%E8%BF%90%E7%94%A8%E5%85%B6%E4%BB%96%E7%9A%84%E6%8F%92%E4%BB%B6%EF%BC%8C%E5%9C%A8hexo%E4%B8%AD%E6%B7%BB%E5%8A%A0%E6%8F%90%E7%A4%BA%E5%BC%B9%E7%AA%97.html</url>
    <content><![CDATA[<p>由于我写了很多学习记录，但是这些都是自己看书或者看视频学来的。万一有错误的地方正好被人看见，他又是新手，误以为我的是对的，这样就不好了。所以准备做一个提示弹窗，在所有的带有“学习笔记”的标签的文章中自动弹出提示。</p>
<ol>
<li>下载一个自己喜欢的弹窗插件，可以去<a href="http://www.jq22.com/" target="_blank" rel="noopener">jQuery插件库</a>找。</li>
<li>将css和js放在next主题下的source文件夹中，如下图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/next%E4%B8%BB%E9%A2%98%E7%9B%AE%E5%BD%95.jpg" alt="next主题目录"><br>css文件放入css文件，js文件放入js文件夹。我自己创了一个spop的文件夹，用于单独放置我的弹窗插件。</li>
<li>打开layout文件夹，进入_macro文件夹，找到post.swig文件。搜索class=”post-block”，这个标签的位置在下图箭头所指的地方：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/%E5%BC%B9%E7%AA%97%E6%8F%92%E4%BB%B6%E4%BB%A3%E7%A0%81%E5%86%99%E5%85%A5%E7%9A%84%E4%BD%8D%E7%BD%AE.png" alt="弹窗插件代码写入的位置"><br>如果打开了这个文件，找了post-block标签，可以看到标签内部第一行代码为<code>&lt;link itemprop=&quot;mainEntityOfPage&quot; href=&quot;{ config.url }{ url_for(post.path) }&quot;/&gt;</code>。由于hexo渲染问题我将href属性里的值去掉了一对{}。<br>将下面的代码放在上述代码上面或者下面即可。<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="name">for</span></span> tag <span class="keyword">in</span> post.tags %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml">	<span class="comment">&lt;!--判断该文章是否为学习笔记--&gt;</span></span></span><br><span class="line"><span class="xml">	</span><span class="template-tag">&#123;% <span class="name"><span class="name">if</span></span> tag.name == '学习笔记' and !is_home() %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml">  		<span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"/css/spop/spop.min.css"</span>&gt;</span></span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"/js/spop/spop.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span></span><br><span class="line"><span class="xml">			spop(&#123;</span></span><br><span class="line"><span class="xml">				template: '<span class="tag">&lt;<span class="name">h4</span> <span class="attr">class</span>=<span class="string">"spop-title"</span>&gt;</span>注意<span class="tag">&lt;/<span class="name">h4</span>&gt;</span>此文章仅为博主的学习笔记，其中可能含有极大的理论错误。',</span></span><br><span class="line"><span class="xml">				group: 'tips',</span></span><br><span class="line"><span class="xml">				position  : 'bottom-center',</span></span><br><span class="line"><span class="xml">				style: 'success',</span></span><br><span class="line"><span class="xml">				autoclose: 5500,</span></span><br><span class="line"><span class="xml">				onOpen: function () &#123;</span></span><br><span class="line"><span class="xml">					//这里设置灰色背景色</span></span><br><span class="line"><span class="xml">				&#125;,</span></span><br><span class="line"><span class="xml">				onClose: function() &#123;</span></span><br><span class="line"><span class="xml">					//这里可以取消背景色</span></span><br><span class="line"><span class="xml">					spop(&#123;</span></span><br><span class="line"><span class="xml">						template: 'ε = = (づ′▽`)づ',</span></span><br><span class="line"><span class="xml">						group: 'tips',</span></span><br><span class="line"><span class="xml">						position  : 'bottom-center',</span></span><br><span class="line"><span class="xml">						style: 'success',</span></span><br><span class="line"><span class="xml">						autoclose: 1500</span></span><br><span class="line"><span class="xml">					&#125;);</span></span><br><span class="line"><span class="xml">				&#125;</span></span><br><span class="line"><span class="xml">			&#125;);</span></span><br><span class="line"><span class="xml">		<span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="xml">	</span><span class="template-tag">&#123;% <span class="name"><span class="name">endif</span></span> %&#125;</span><span class="xml"></span></span><br><span class="line"><span class="xml"></span><span class="template-tag">&#123;% <span class="name"><span class="name">endfor</span></span> %&#125;</span><span class="xml"></span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>is_home()是next主题的方法，用于判断当前页面是否在主页。因为主页一次性加载了所有的文章，如果不加这个方法，会在主页弹出无数个弹窗。效果如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/hexo/%E6%8F%90%E7%A4%BA%E5%BC%B9%E7%AA%97%E6%95%88%E6%9E%9C%E5%9B%BE.jpg" alt="提示弹窗效果图"><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>Android开发，使用腾讯云的API请求对象存储中的资源始终失败</title>
    <url>/bug/Android%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BD%BF%E7%94%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E7%9A%84API%E8%AF%B7%E6%B1%82%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E5%A7%8B%E7%BB%88%E5%A4%B1%E8%B4%A5.html</url>
    <content><![CDATA[<ol>
<li>腾讯云api内部在调用时，把url转义了。我的链接是<a href="http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt?abcdefg，它内部给我转义成http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt%3Fabcdefg，就是把&quot;?&quot;转义成了&quot;%3F&quot;。总而言之，我使用api一直获取不到资源，然后我在浏览器上试验了一下。发现把%3F改回?就可以访问了，实际上应该不是这样，反正就给我产生了误导。我想尽办法都不能将其转义回来，最后只好放弃。腾讯云api内部肯定自己转义了一下，真的坑爹。" target="_blank" rel="noopener">http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt?abcdefg，它内部给我转义成http://###.cos.ap-shanghai.myqcloud.com/11111/app_list.txt%3Fabcdefg，就是把&quot;?&quot;转义成了&quot;%3F&quot;。总而言之，我使用api一直获取不到资源，然后我在浏览器上试验了一下。发现把%3F改回?就可以访问了，实际上应该不是这样，反正就给我产生了误导。我想尽办法都不能将其转义回来，最后只好放弃。腾讯云api内部肯定自己转义了一下，真的坑爹。</a></li>
<li>尝试自己写代码请求资源，结果发现如果url的协议是https就可以访问到资源了。这可能是android的问题，于是我又使用腾讯的api，配置更改如下：<figure class="highlight pony"><table><tr><td class="code"><pre><span class="line"><span class="type">CosXmlServiceConfig</span> serviceConfig = <span class="function"><span class="keyword">new</span> <span class="title">Builder</span>()</span></span><br><span class="line"><span class="function">				.<span class="title">isHttps</span>(true)</span></span><br><span class="line"><span class="function">                .<span class="title">setRegion</span>(region)</span></span><br><span class="line"><span class="function">                .<span class="title">setDebuggable</span>(true)</span></span><br><span class="line"><span class="function">                .<span class="title">builder</span>();</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>将isHttps设为ture，协议就改为了https。可是又报了另一个错：The specified key does not exist.它说我密钥不存在。</p>
<ol>
<li>如果使用http协议访问就会说无法解析域名，总之用腾讯云的api无法访问到资源就对了。</li>
<li>放弃腾讯云的api，自己手写代码去请求资源！</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>android</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>对神经网络整体的理解</title>
    <url>/zcy/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<div class="note info">
            <p>本文疑问的总结地址<a href="https://yan624.github.io/学习笔记/深度学习中的一些疑问总结.html">在这</a></p>
          </div>
<h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th>描述的内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2~4</td>
<td>神经网络和深度学习的发展史。</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>从二元分类开始。</td>
</tr>
<tr>
<td style="text-align:center">6~10</td>
<td>浅层神经网络的介绍。如神经网络中一些参数代表的意思、激活函数、梯度下降、随机初始化。</td>
</tr>
<tr>
<td style="text-align:center">11~15</td>
<td>深层神经网络的介绍。正向传播和反向传播中向量化后的计算、参数和超参数、神经网络和大脑的关系。</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td>一个Simple NN的例子。</td>
</tr>
<tr>
<td style="text-align:center">17~22</td>
<td>深度学习的实用性层面。数据切分、偏差与方差、正则化、dropout、其他正则化方法、均值归一化、梯度消失和梯度爆炸、梯度检验。</td>
</tr>
<tr>
<td style="text-align:center">23~25</td>
<td>一些优化算法。Mini-batch、指数加权平均、Momentum、RMSprop、Adam、Adagrad。</td>
</tr>
<tr>
<td style="text-align:center">26~29</td>
<td>超参数调试、Batch正则化、激活函数以及一些深度学习框架。</td>
</tr>
<tr>
<td style="text-align:center">30~end</td>
<td>本文略长，后序的文章请看对应章节的链接。</td>
</tr>
</tbody>
</table>
</div>
<h1 id="神经网络和深度学习的发展"><a href="#神经网络和深度学习的发展" class="headerlink" title="神经网络和深度学习的发展"></a>神经网络和深度学习的发展</h1><p>TODO</p>
<h1 id="神经网络和深度学习的关系"><a href="#神经网络和深度学习的关系" class="headerlink" title="神经网络和深度学习的关系"></a>神经网络和深度学习的关系</h1><p>TODO</p>
<h1 id="为什么要深度学习"><a href="#为什么要深度学习" class="headerlink" title="为什么要深度学习"></a>为什么要深度学习</h1><p>TODO</p>
<h1 id="从二元分类开始"><a href="#从二元分类开始" class="headerlink" title="从二元分类开始"></a>从二元分类开始</h1><p>暂时省略，因为这里已经会了。</p>
<h1 id="神经网络的表示"><a href="#神经网络的表示" class="headerlink" title="神经网络的表示"></a>神经网络的表示</h1><p>规定如下，l：第几层；w：权重值；b：偏差；z：输出值；a：激活值；i，j：都代表第几个神经元，如<script type="math/tex">w^l_i</script>代表第l层的第i个权重值；W：向量化后的权重值；Z：向量化后的输出值；A：向量化后的激活值；<script type="math/tex">\alpha</script>：学习速率；<script type="math/tex">\lambda</script>：正则化项；</p>
<p>如果输出值z和激活值a无法理解或者区分，没关系，继续往下看就知道了。<br>如下图所示，一般规定input layer为第0层，不算入神经网络的层数中，所以下图是一个三层神经网络架构。</p>
<ol>
<li>input layer的输入值被称为x，下图一共有三个输入所以分别被称为<script type="math/tex">x_1\ x_2\ x_3</script>。为了方便起见，可以将input layer的值x以<script type="math/tex">a^0</script>来代替，下面解释a代表什么。</li>
<li>hidden layer中的值被称为a——<strong>激活值</strong>（activations），图中有四个神经元，所以分别被称为<script type="math/tex">a^1_1\ a^1_2\ a^1_3</script>，上标代表着所在神经网络中的第几层，下标代表着所在层中的第几个神经元。如果表示成向量形式就是<script type="math/tex; mode=display">
\begin{pmatrix}
 x_1\\
 x_2\\
 x_3\\
\end{pmatrix} = 
\begin{pmatrix}
 a^0_1\\
 a^0_2\\
 a^0_3\\
\end{pmatrix} 和
\begin{pmatrix}
 a^1_1\\
 a^1_2\\
 a^1_3\\
 a^1_4\\
\end{pmatrix} 和
\begin{pmatrix}
 a^2_1\\
 a^2_2\\
 a^2_3\\
 a^2_4\\
\end{pmatrix} 和
\begin{pmatrix}
 a^3_1\\
\end{pmatrix}</script></li>
</ol>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%8E%9F%E5%9B%BE.jpg" alt="神经网络架构原图"></p>
<h2 id="神经网络中神经元的一些参数的含义，特别解释w的含义"><a href="#神经网络中神经元的一些参数的含义，特别解释w的含义" class="headerlink" title="神经网络中神经元的一些参数的含义，特别解释w的含义"></a>神经网络中神经元的一些参数的含义，特别解释w的含义</h2><p>hidden layer和output layer的每个神经元都有几个参数。分别为<script type="math/tex">w^l\ b^l</script>，对照上图，这里的<script type="math/tex">w^l</script>是一个(4,3)的矩阵，<script type="math/tex">b^l</script>是一个(4,1)的向量。解释如下：</p>
<script type="math/tex; mode=display">
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4\\
\end{cases}</script><p>可以看到一个公式中有三个w和一个b，一共有四个公式。<script type="math/tex">w^l_{ij}</script>代表第l-1层的第j个神经元到第l层的第i个神经元上的w。如<script type="math/tex">w^1_{12}</script>代表第0层的第2个神经元到第1层的第1个神经元上的w。注意这里的i和j实际上是与直觉相反的，也就是说按直觉来看应该是<script type="math/tex">w^l_{ji}</script>才正常。如果对w的表示有疑惑的，可以看<a href="https://yan624.github.io/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%ADweight%E7%9A%84%E8%A1%A8%E7%A4%BA%E9%97%AE%E9%A2%98.html">这篇</a>。<br>注意下这里的z是<strong>输出值</strong>，之前一直在说hidden layer中的值是a——激活值，其实a就是将z放到一个<strong>激活函数</strong>（activation function）中得到的一个值，这个激活函数是随用户挑选的，如果不能理解激活函数是什么，就暂时理解为激活函数自己想设成什么就设成什么。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg" alt="神经网络架构图"></p>
<h1 id="神经网络中的输出是怎么计算的"><a href="#神经网络中的输出是怎么计算的" class="headerlink" title="神经网络中的输出是怎么计算的"></a>神经网络中的输出是怎么计算的</h1><h2 id="以一个样本为例"><a href="#以一个样本为例" class="headerlink" title="以一个样本为例"></a>以一个样本为例</h2><p>第0层是输入层，所以是不需要计算的，x我本来就有，我还计算什么？对吧。从hidden layer1开始到output layer每一层都需要计算一连串的值，下面给出第一层的计算公式：</p>
<script type="math/tex; mode=display">
\begin{cases}
    z^1_1 = a^0_1 * w^1_{11} + a^0_2 * w^1_{12} + a^0_3 * w^1_{13} + b^1_1，a^1_1 = \sigma(z^1_1)\\
    z^1_2 = a^0_1 * w^1_{21} + a^0_2 * w^1_{22} + a^0_3 * w^1_{23} + b^1_2，a^1_2 = \sigma(z^1_2)\\
    z^1_3 = a^0_1 * w^1_{31} + a^0_2 * w^1_{32} + a^0_3 * w^1_{33} + b^1_3，a^1_3 = \sigma(z^1_3)\\
    z^1_4 = a^0_1 * w^1_{41} + a^0_2 * w^1_{42} + a^0_3 * w^1_{43} + b^1_4，a^1_3 = \sigma(z^1_4)\\
\end{cases}</script><p>这里的<script type="math/tex">\sigma(z)</script>函数其实就是上面说的<strong>激活函数</strong>，一般来讲<script type="math/tex">\sigma</script>这个符号特指sigmoid function: <script type="math/tex">\frac{1}{1+e^{-z}}</script>。<br>这4行公式其实在上面已经给出部分，每一行包含两个公式，也就是说一个神经元中实际上先得到了z，然后再通过激活函数将z转为a。这里可能会有疑惑，已经得到z了为什么还要用一个函数将z转为a呢？这样不是毫无意义？下面有一部分会具体解释，也可以看下面几篇的解释：<br><a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">神经网络激励函数的作用是什么？有没有形象的解释？</a><br><a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a><br>现在回到本文，正如我上面所说，我一共写了四个公式（激活函数现在暂时不看），所以我要分别计算四个公式，也就是说要计算四次。那么有没有办法只计算一次就得到所有结果呢？答案是<strong>向量化</strong>（vectorization），现在开始用向量化来解决这个问题。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    z^1_1\\
    z^1_2\\
    z^1_3\\
    z^1_4\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&w^1_{12}&w^1_{13}\\
    w^1_{21}&w^1_{22}&w^1_{23}\\
    w^1_{31}&w^1_{32}&w^1_{33}\\
    w^1_{41}&w^1_{42}&w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^0_1\\
    a^0_2\\
    a^0_3\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^1 = w^1 * a^0 + b^1</script><p>以下是整个神经网络的计算过程，也就是说只需要下面6行就可以代替上文占据几个屏幕的内容。</p>
<script type="math/tex; mode=display">
\begin{array}{c|}
    z^1 = w^1 * a^0 + b^1\\
    a^1 = \sigma(z^1)\\
    z^2 = w^2 * a^1 + b^2\\ 
    a^2 = \sigma(z^2)\\
    z^3 = w^3 * a^2 + b^3\\
    a^3 = \sigma(z^3)\\ 
\end{array} =>记为P</script><p>最后一个a就是整个神经网络的输出值，也就是预测值（prediction），也可以用<script type="math/tex">\hat{y}</script>表示，自然<script type="math/tex">\hat{y} = a^3</script>。</p>
<h2 id="向量化计算多个样本"><a href="#向量化计算多个样本" class="headerlink" title="向量化计算多个样本"></a>向量化计算多个样本</h2><p>上面我没有特意地说明其实我们只使用了一个样本，我们一直在使用<script type="math/tex">a^0_1\ a^0_2\ a^0_3</script>，但是<script type="math/tex">a^0_1\ a^0_2\ a^0_3</script>实际上只是<strong>一个</strong>样本。<script type="math/tex">a^0</script>代表的是一个样本，<script type="math/tex">a^0_1</script>代表的是样本中的第一个特征，如果不明白我可以举个例子：<script type="math/tex">a^0_1</script>代表天气样本中的第一个特征——温度，<script type="math/tex">a^0_2</script>代表湿度，<script type="math/tex">a^0_3</script>代表PM2.5，<script type="math/tex">a^0</script>代表整一个天气样本。<br>那么如果有成千上万个样本，总不能使用P计算成千上万次吧。这里再次使用向量化进行计算。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    z^{11}_1&z^{12}_1&\cdots\\
    z^{11}_2&z^{12}_2&\cdots\\
    z^{11}_3&z^{12}_3&\cdots\\
    z^{11}_4&z^{12}_4&\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^1_{11}&w^1_{12}&w^1_{13}\\
    w^1_{21}&w^1_{22}&w^1_{23}\\
    w^1_{31}&w^1_{32}&w^1_{33}\\
    w^1_{41}&w^1_{42}&w^1_{43}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{01}_1&a^{01}_1&\cdots\\
    a^{01}_2&a^{02}_1&\cdots\\
    a^{01}_3&a^{03}_1&\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^1_1\\
    b^1_2\\
    b^1_3\\
    b^1_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^1 = w^1 * a^0 + b^1</script><script type="math/tex; mode=display">
\begin{pmatrix}
    z^{21}_1&z^{22}_1&\cdots\\
    z^{21}_2&z^{22}_2&\cdots\\
    z^{21}_3&z^{22}_3&\cdots\\
    z^{21}_4&z^{22}_4&\cdots\\
\end{pmatrix} = 
\begin{pmatrix}
    w^2_{11}&w^2_{12}&w^2_{13}&w^2_{14}\\
    w^2_{21}&w^2_{22}&w^2_{23}&w^2_{24}\\
    w^2_{31}&w^2_{32}&w^2_{33}&w^2_{34}\\
    w^2_{41}&w^2_{42}&w^2_{43}&w^2_{44}\\
\end{pmatrix} *
\begin{pmatrix}
    a^{11}_1&a^{11}_1&\cdots\\
    a^{11}_2&a^{12}_1&\cdots\\
    a^{11}_3&a^{13}_1&\cdots\\
    a^{11}_3&a^{14}_1&\cdots\\
\end{pmatrix} + 
\begin{pmatrix}
    b^2_1\\
    b^2_2\\
    b^2_3\\
    b^2_4\\
\end{pmatrix}</script><script type="math/tex; mode=display">===>\ z^2 = w^2 * a^1 + b^2</script><p>省略号代表后面有无数个样本，同理矩阵相乘也可以只用一个字母表示。上标的第二个数字代表是第几个样本，第一个数字依旧是代表所属第几层。</p>
<h1 id="※-激活函数"><a href="#※-激活函数" class="headerlink" title="※ 激活函数"></a>※ 激活函数</h1><p>&emsp;&emsp;<a href="https://blog.csdn.net/program_developer/article/details/78704224" target="_blank" rel="noopener">神经网络激活函数的作用是什么？</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>激活函数名称</th>
<th>如何选择</th>
</tr>
</thead>
<tbody>
<tr>
<td>sigmoid</td>
<td><strong>输出层</strong>为<strong>二元分类</strong>时选用。对于隐藏层来说，基本不会用 sigmoid 函数，因为现在已经有更好的激活函数<br> <strong>缺点</strong>：1）会产生梯度消失/弥散（<strong>注：不会 sigmoid 导致梯度爆炸</strong>），详见下面的 Sigmoid 章节；2）不是原点对称；3）计算 exp 较耗时。</td>
</tr>
<tr>
<td>tanh</td>
<td><strong>优点</strong>：1）原点对称；2）比 sigmoid 快。<br> <strong>缺点</strong>：1）还是有梯度消失</td>
</tr>
<tr>
<td><strong>ReLU</strong></td>
<td>首选 ReLU，如果 ReLU 不行，再换其他形式的 ReLU。<a href="https://github.com/llSourcell/Which-Activation-Function-Should-I-Use" target="_blank" rel="noopener">观点来源</a><br> <strong>优点</strong>：1）解决了部分梯度消失问题；2）收敛速度更快。<br> <strong>缺点</strong>：1）梯度消失的问题没有完全解决，在激活函数（-）部分相当于让神经元死亡，且无法复活。</td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td></td>
</tr>
<tr>
<td>Parametric ReLU</td>
<td></td>
</tr>
<tr>
<td>Randomized ReLU</td>
<td></td>
</tr>
<tr>
<td>ELU</td>
<td></td>
</tr>
<tr>
<td>SELU</td>
<td></td>
</tr>
<tr>
<td>GELU</td>
<td></td>
</tr>
<tr>
<td>Swish</td>
<td></td>
</tr>
<tr>
<td>softmax</td>
<td><strong>输出层</strong>为<strong>多元分类</strong>时选用。只适合于输出层</td>
</tr>
<tr>
<td>log_softmax</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p>&emsp;&emsp;上文中我们一直假设使用 sigmoid function 作为激活函数。但是事实上还有很多其他选择，甚至其他的激活函数比sigmoid funtion效果要更好。<br>&emsp;&emsp;上面讲过<script type="math/tex">\sigma(z)</script>特指 sigmoid function，现在我们将表达式改为：<script type="math/tex">a = g(z)</script>，用g来表示激活函数，它可以是线性的，也可以是非线性的。<br>&emsp;&emsp;引用吴恩达在深度学习视频中的话：</p>
<blockquote>
<p>有一个函数总是比 sigmoid function 表现得更好，就是tanh函数或者叫双曲正切函数，公式为：<script type="math/tex">\frac{e^z-e^{-z}}{e^z+e^{-z}}, \, x\in(-1,1)</script>，在数学上实际是<script type="math/tex">\sigma</script>函数平移后的版本。<br><strong>事实证明，如果将<script type="math/tex">g(z)</script>选为 tanh 函数，效果几乎总比 <script type="math/tex">\sigma(z)</script> 函数要好。</strong></p>
</blockquote>
<p>&emsp;&emsp;有一个例外是 output layer，它还是使用 sigmoid funtion，因为 output layer 跟普通的分类问题没什么区别，它要得到0~1之间的一个概率。<br>&emsp;&emsp;sigmoid function 的值总是位于 0~1 之间，tanh function 的值总是位于 -1~1 之间。</p>
<h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><p>&emsp;&emsp;Sigmoid 函数将导致梯度消失。梯度的问题，在下面的章节（21 章左右）中会有讲到，但是此部分只讲 sigmoid 函数的梯度消失问题。<br>&emsp;&emsp;首先我们脑中大概有个 sigmoid 函数的图像，这应该很简单。注意函数的两边，我们发现函数曲线在 <script type="math/tex">-\infty</script> 方向越来越接近 0，在 <script type="math/tex">\infty</script> 方向越来越接近 1。以正方向为例，我们可以得知输入 sigmoid 的值越大，sigmoid 的输出值越接近 1。<br>&emsp;&emsp;做一个小小的测试，当输入值为 3 时，输出值为 0.9526，当输入值为 9 时，输出值为 0.9999。可以观察发现，输入值相差巨大的情况下，输出值居然相差无几。当然如果举一个更极端的例子，比如输入 20 和 2000，就会发现输出值都非常接近 1，详见下图。也就是说，输入值相差巨大，但是经过 sigmoid 之后，输出值居然相差无几。<strong>换句话说就是一个值在经过 sigmoid 之后被衰减了</strong>。通俗来讲，我管你是 2000 还是 20000000，只要经过我 sigmoid，你输出就只能是一个接近 1 的数。这样就是被衰减了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/sigmoid梯度消失.jpg" alt="sigmoid梯度消失"></p>
<p>&emsp;&emsp;其次我们又知道对于一个神经网络而言，它一般会叠的很深，四五层都很常见。有了以上两个基础，下面举个具体的例子。<br>&emsp;&emsp;我们知道梯度下降算法的公式是 <script type="math/tex">W = W - \alpha \Delta W</script>，在进行一次梯度下降后，对于 W 来说，<strong>变化</strong>就是 <script type="math/tex">\alpha \Delta W</script>，不严格的说其实只有 <script type="math/tex">\Delta W</script>。然后将新的 W 传入 sigmoid 函数，我们就会发现 <script type="math/tex">\Delta W</script> 被衰减了（为什么会衰减上面已经说过了）。而 <script type="math/tex">\Delta W</script> 其实是<strong>梯度</strong>，也就是说梯度被衰减了，然后再经过多层神经网络之后，梯度被一减再减。<br>&emsp;&emsp;综上所述，梯度在第一层可能很大，在经过几层 sigmoid 函数之后，可能就<strong>减</strong>没了。<br>&emsp;&emsp;<strong>不过，由于 sigmoid 导数的取值范围是 (0, 0.25)，所以梯度也不会很大，但是这仍然架不住多层的神经网络</strong>。</p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>但是不管是<script type="math/tex">\sigma</script>或者tanh函数都一个缺点，那就是当z非常大或者非常小时，函数的斜率（导数的梯度）很小。这样会拖慢梯度下降。在机器学习中还有一个函数，即ReLU函数——Rectified Linear Unit，表达式为<script type="math/tex">max(0, z)</script>。<br>所以在选择激活函数时有一些经验法则：</p>
<ol>
<li>如果你的输出值是0或1，那么<script type="math/tex">\sigma</script>函数很适合做output layer的激活函数，非二元分类的情况下使用tanh函数几乎都比<script type="math/tex">\sigma</script>优越。藏层单元全用ReLU函数，现在ReLU函数已经是隐藏层的默认激活函数了，大多数人都这么做。</li>
<li>还有个叫Leaky ReLU的函数比ReLU稍微好点，但是目前暂时不是很多人用。</li>
</ol>
<h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>Sogmoid函数<script type="math/tex">\sigma = \frac{1}{1 + e^{-z}}</script>适用于二元分类，那么碰到多元分类怎么么办呢？Softmax函数就可以解决这个问题。<br>Softmax函数计算步骤如下，假设是n元分类：</p>
<script type="math/tex; mode=display">
Z^L = W^L * A^{L-1} + b^L\\
t = e^{Z^L}\\
A^L = \frac{e^{Z^L}}{\sum^n_{i=1}t_i},\quad A^L_i = \frac{t_i}{\sum^n_{i=1}t_i}\\</script><p>多元分类中每一个神经元代表对应标签的概率是多少，并且将概率相加等于1。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/softmax%E4%BE%8B%E5%AD%90.jpg" alt="softmax例子"></p>
<h2 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h2><div class="note primary">
            <p>为什么<script type="math/tex">w*x+b=z</script>之后，需要使用一个激活函数<script type="math/tex">a = \sigma(z)</script>？不能不加这个激活函数吗？权重值乘上输入值加上偏差之后直接输出就行了，何必再带入激活函数中？</p>
          </div>
<p>由于y=bx+c是线性函数，只不过将输入值乘了一个常量并进行位移而已，所以使用非y=bx+c型的激活函数就算是使用了非线性函数。而在神经元中不加激活函数，实际上就是把激活函数设置成y=x这个函数。<br>那么现在从头再做一次神经网络计算，与上面的区别是这次不加激活函数。</p>
<script type="math/tex; mode=display">
\left \{ 
    \begin{array}{ll}
        a^1 = w^1 * a^0 + b^1 \qquad这是计算第一层的激活值，现在没有使用激活函数。\\
        a^2 = w^2 * a^1 + b^2 \qquad这是计算第二次的激活值，也没有使用激活函数。\\
        a^2 = w^2 * (w^1 * a^0 + b^1) + b^2 \qquad消掉a^1，两式合并之后\\
        a^2 = w^2 * w^1 * a^0 + w^2 * b^1 + b^2 \qquad合并同类项后\\
        a^2 = w' * a^0 + b' \qquad由于w和b只是一堆常数，所以将w'代替w^2 * w^1，b'同理
    \end{array}
\right.</script><p>通过上面几个式子发现，我们计算了第一层和第二层之后，结果居然还是一个线性的式子，和最初的输入没有什么差别，也就是说你无论叠了几层hidden layer最后输出还是类似<script type="math/tex">a^2 = w' * a^0 + b'</script>的式子，那么深度学习的意义在哪呢？还不如直接把hidden layer去掉。所以重点就是线性的hidden layer没有任何用处，因为两个线性方程的组合结果还是线性方程。<br>只有在一个地方可以使用线性方程，那就是回归问题——线性回归。</p>
<h1 id="梯度下降，反向传播算法——backpropagation解析"><a href="#梯度下降，反向传播算法——backpropagation解析" class="headerlink" title="梯度下降，反向传播算法——backpropagation解析"></a>梯度下降，反向传播算法——backpropagation解析</h1><div class="note primary">
            <p>首先虽然全部的过程已经理清，但是还有几个问题：为什么要对w求导？梯度下降的意义是什么？为什么使用梯度下降就可以解决误差问题？</p>
          </div>
<p>本节的示例均建立在一个样本的情况下，如果是多个样本经过神经网络，可能略微不同。我看了吴恩达老师的深度学习课程，发现多个样本与一个样本的区别，可能只在偏差b那里会有点不同。<br>下图以一个三层神经网络为例，说明正向与反向传播过程。由于神经元之间的链接太多会导致混乱，所以下图只链接了第一个神经元。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="使用导数解释反向传播算法"><br>下图略微简化一下反向传播算法中的导数项，并且完成了最后的权重值优化。值得注意的是：如果cost function不同，下面求导结果会略微不同，本文统一使用<script type="math/tex">cost = \frac{1}{m} * \sum{(\hat{y} - y)^2}</script>，但是神经网络一般是使用<strong>交叉熵</strong>——crossentropy，其公式为：<script type="math/tex">cost = -\frac{1}{m} * (y * log(\hat{y}) + (1 - y) * log(1 - \hat{y}))</script>。使用前者，<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}} = (a^{(3)} - y) * g'(z^{(3)})</script>；如果使用后者，<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}} = a^{(3)} - y</script>。可以看到使用两个不同的代价函数，会有不同的结果，这是因为两个函数求导的结果不一样。而两者对表达式<script type="math/tex">\frac{\partial{J}}{\partial{z^{(3)}}}</script>的结果只差了一个<script type="math/tex">g'(z^{(3)})</script>，这完全是巧合罢了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E7%AE%80%E5%8C%96%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.jpg" alt="简化反向传播算法"><br><strong>另外再提醒一下自己，这里全是以一个样本为例。但是仅仅这样权重已经是一个二维矩阵了，要是如果传入多个样本，权重岂不是是一个三维矩阵？然而不管传入几个样本权重实际上对于不同的样本是没有变化的，所以还是二维矩阵。</strong></p>
<h1 id="※-随机初始化"><a href="#※-随机初始化" class="headerlink" title="※ 随机初始化"></a>※ 随机初始化</h1><p>对于逻辑回归可以将<strong>权重</strong>（weight）全部初始化为0，但是对于神经网络来说，将个权重初始化为0，再使用梯度下降会完全无效。实际上将偏差b初始化为0是可以的，但是权重不行。<br>解释起来太麻烦，详情看吴恩达深度学习——01神经网络和深度学习第三周浅层神经网络，3.11随机初始化。吴恩达老师解释地还是很清楚的。<br>可以像以下这样设置weight：<script type="math/tex">w^l = np.random.randn((2, 2)) * 0.01</script>//这可以产生参数为(2, 2)的高斯分布随机变量，后面再成一个很小的数，比如0.01。而对于b，之前说了初始化为0也可以。<br>对于上式的0.01可能会感到很疑惑，为什么要乘这么一个值。因为我们一般将weight初始化为很小的值，如果weight值很大，最终导致z也很大，那么会落在sigmoid function或者tanh function的平缓部分，会使梯度的写了很小，意味着梯度下降算法会非常慢，所以学习得很慢。</p>
<h2 id="初始化补充"><a href="#初始化补充" class="headerlink" title="初始化补充"></a>初始化补充</h2><p>经在作业中做的测试得出如下结论：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Train accuracy</strong></th>
<th><strong>Problem/Comment</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>3-layer NN with <strong>zeros initialization</strong></td>
<td>50%</td>
<td>fails to break symmetry</td>
</tr>
<tr>
<td>3-layer NN with large <strong>random initialization</strong></td>
<td>83%</td>
<td>too large weights</td>
</tr>
<tr>
<td>3-layer NN with <strong>He initialization</strong></td>
<td>99%</td>
<td><strong>recommended method</strong></td>
</tr>
</tbody>
</table>
</div>
<p>其中”He initialization”最近（论文是2015年的）新搞出来得初始化算法，现在推荐使用此算法进行初始化。</p>
<h1 id="核对矩阵维数"><a href="#核对矩阵维数" class="headerlink" title="核对矩阵维数"></a>核对矩阵维数</h1><p>w的维数应该与dw的维数相同。b和db的维数相同</p>
<h1 id="为什么使用深度表示——Why-deep-representations"><a href="#为什么使用深度表示——Why-deep-representations" class="headerlink" title="为什么使用深度表示——Why deep representations"></a>为什么使用深度表示——Why deep representations</h1><p>引用在2017course深度学习课程上吴恩达老师的话</p>
<blockquote>
<p>深度神经网络能解决很多问题，其实并不需要很大的神经网络，但是得有深度。得有比较多的隐藏层。</p>
</blockquote>
<p>为什么深度神经网络会很好用？</p>
<ol>
<li>深度神经网络到底在计算什么？假设现在在做一个人脸识别系统。那么神经网络的第一层会去找照片里的边缘部分；第二层会去识别人类的特征，比如耳朵，鼻子，嘴巴；第三层会去识别不同的人脸。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%9B%B4%E8%A7%82%E6%98%A0%E5%83%8F.jpg" alt="深度表示的直观映像"><br>这种识别模式可能难以理解，但是会在卷积神经网络——Convolutional Neural Network中详细解释。<br>这视频的这一章节有点难以总结，可以看看<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701022" target="_blank" rel="noopener">该视频</a>，总共也就10分钟。</li>
</ol>
<h1 id="深层神经网络块"><a href="#深层神经网络块" class="headerlink" title="深层神经网络块"></a>深层神经网络块</h1><p>此视频中画出了深度神经网络的<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&amp;id=2001701023" target="_blank" rel="noopener">代码流程</a>。</p>
<h1 id="参数VS超参数"><a href="#参数VS超参数" class="headerlink" title="参数VS超参数"></a>参数VS超参数</h1><p>有如下超参数（hyperparameters）：W, b, lerning rate <script type="math/tex">\alpha</script>, iterations, hidden layer L, hidden units, choice of activatation function.这些超参数都需要自己设置。<br>上面这些都是基础的，实际上还有其他的超参数，稍后会涉及到。 </p>
<h1 id="神经网络和大脑有什么关系？"><a href="#神经网络和大脑有什么关系？" class="headerlink" title="神经网络和大脑有什么关系？"></a>神经网络和大脑有什么关系？</h1><p>计算机视觉、其他深度学习领域或者其他学科在早期可能都受过人类大脑的启发，但是近年来人类将神经网络类比为大脑的次数越来越少，也就是说近年来大家都不怎么认为这二者有关联。</p>
<h1 id="一个Simple-NN的例子"><a href="#一个Simple-NN的例子" class="headerlink" title="一个Simple NN的例子"></a>一个Simple NN的例子</h1><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/simple_neural_network" target="_blank" rel="noopener">Simple Neural Network</a>例子</p>
<hr>
<p>本节以下开始利用算法改善深层神经网络</p>
<hr>
<h1 id="训练-开发-测试集"><a href="#训练-开发-测试集" class="headerlink" title="训练/开发/测试集"></a>训练/开发/测试集</h1><p>训练集——training set<br>开发集/交叉验证集/验证集——dev set/cross validation set/validation set<br>测试集——test set</p>
<p>以前数据量小的时候，比如100个样本、10000个样本。一般将数据按三七分，七份训练集，三份测试集。验证集（以下均称验证集）在训练集中再细分，比如二八分，八份训练集。<br>但是现在进入大数据时代，验证集和测试集已经没有必要占大量比例了。比如现在有100万的样本，那么验证集和测试集只需要各抽取大约10000的样本即可。也就是98/1/1的比例，甚至验证集和测试集可以再降低占比。</p>
<h2 id="训练集和验证集-测试集分布不匹配"><a href="#训练集和验证集-测试集分布不匹配" class="headerlink" title="训练集和验证集/测试集分布不匹配"></a>训练集和验证集/测试集分布不匹配</h2><p>如下图，吴恩达老师建议最好让<strong>验证集</strong>和<strong>测试集</strong>匹配，即来自同一源，要都来自网络高清图，要么都来自手机低像素拍摄。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E5%BC%80%E5%8F%91%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%86%E5%B8%83%E4%B8%8D%E5%8C%B9%E9%85%8D.jpg" alt="训练集和验证集测试集分布不匹配"><br>如果直接不设置测试集也是可以的。</p>
<h1 id="偏差-方差"><a href="#偏差-方差" class="headerlink" title="偏差/方差"></a>偏差/方差</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%86%B3%E7%AD%96%E7%95%8C%E9%99%90.jpg" alt="欠拟合和过拟合的决策界限"></p>
<p>下图讲述了什么是<strong>过拟合</strong>，什么是<strong>欠拟合</strong>，如图所示，该神经网络用于判断一张图片是猫还是狗。<br>左边。训练样本中的误差为1%，这个值已经很小了，但是在验证集上的误差有11%。这就代表了过拟合，试想一下，在训练集上误差很小是因为你的决策界限划分的很好，在上图中的最后一个例子，整条决策界限画的十分完美，但是我们要知道在验证集中，这样一条完美的线肯定不能再拟合的很好。因为训练集和验证集即使来源于同一份数据，他们之间的分布也是不一样的，你训练出一条完美的曲线，在另一份数据集上肯定是过于完美了。所以导致了下图中验证集上的误差有11%。我们称这种情况为<strong>高方差</strong>——high variance。<br>中间。训练样本中的误差为15%，这已经不需要再看验证集上的误差了。因为训练集上的误差那么大，肯定是没有拟合好，所以这就是欠拟合，我们称为<strong>高偏差</strong>——high bais。<br>右边。如果训练集中的误差很高，验证集上的误差更高，那么可以判断为同时具有高方差和高偏差。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88.jpg" alt="欠拟合和过拟合"><br>如果训练集上的误差为0.5%，验证集上的误差为1%。那这就是低方差和低偏差，这是很好结果。<br>最后一点，以上均建立在人眼判断的误差为0%上以及训练集和验证集来自相同分布。如果人眼判断的误差也高达15%，那么中间的例子也算是可以的结果一般来说<strong>最优误差</strong>也被称为<strong>贝叶斯误差</strong>。<br>关于上图同时高方差和高方差，就如同下图紫色线条的决策界限一般。过渡拟合了数据，但是拟合的数据其实狗屁不通。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E5%90%8C%E6%97%B6%E9%AB%98%E6%96%B9%E5%B7%AE%E5%92%8C%E9%AB%98%E5%81%8F%E5%B7%AE%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84.jpg" alt="同时高方差和高偏差是怎么样的"></p>
<h2 id="机器学习遇到偏差或方差的解决办法"><a href="#机器学习遇到偏差或方差的解决办法" class="headerlink" title="机器学习遇到偏差或方差的解决办法"></a>机器学习遇到偏差或方差的解决办法</h2><div class="note info">
    <p>笔记中都记了，懒得再写一遍了。补充一点，遇到偏差或方差都可以更换神经网络架构，比如换成CNN或者RNN，如果是高偏差还可以使用更大的神经网络。</p>
</div>

<p>可以看这个6分半中的小视频，<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702115" target="_blank" rel="noopener">机器学习基础</a>。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>如果出现了过拟合，即高方差的情况，第一件想到的事是<strong>正则化</strong>——regularization。当然也可以增加数据，不过有时候数据不是那么容易获取的。可以对W使用<a href="https://www.baidu.com/s?wd=L2%E8%8C%83%E6%95%B0" target="_blank" rel="noopener">L2范数</a>进行正则化，当然对b也可以进行L2范数正则化，不过一般不加。L2范数的公式为<script type="math/tex">||w||^2_2 = \sum_{j=1}^n w^2_j = W^T * W</script><br>因此代价函数修改为<script type="math/tex">cost = \frac{1}{m} \sum^m_{i=1} g(\hat{y}^i, y^i) + \frac{\lambda}{2m}||w||^2_2</script>，<script type="math/tex">\lambda</script>是正则化的超参数。这里的w实际上是一个二维矩阵，所以L2范数需要把里面的每一个值的平方都加起来。<br>如果加入了正则化项，那么在计算dW时有点变化。将会变为：<script type="math/tex">dW = dZ * A\_prev + \frac{\lambda}{m} w ^ l</script></p>
<h2 id="为什么正则化可以防止过拟合"><a href="#为什么正则化可以防止过拟合" class="headerlink" title="为什么正则化可以防止过拟合"></a>为什么正则化可以防止过拟合</h2><p>略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=9c4c25c37dd148a5b85b2edc8dca540e&amp;from=study#/learn/content?type=detail&amp;id=2001702116" target="_blank" rel="noopener">1.5 为什么正则化可以减少过拟合？</a></p>
<h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><div class="note primary">
            <p>那么问题又来了，dropout背后的原理是什么？</p>
          </div>
<p>dropout，中文翻译为<strong>随机失活</strong>。<br>先将神经网络复制一遍，然后dropout会遍历神经网络的每一层，并设置消除神经网络中结点的概率，比如设置0.5。下图的带X的结点就是准备消除的。另外每一层的概率都可以是不同的，如果在某一层不担心会过拟合可以将概率设为1.0，比如输出层。如果觉得某些层比其他层更容易过拟合，可以把那些层的keep-prob设置的更低。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/dropout%E5%BE%85%E5%88%A0%E9%99%A4%E7%BB%93%E7%82%B9.jpg" alt="dropout待删除结点"><br>下图则是消除后的神经网络。将结点的进出的链接全部删除。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BD%BF%E7%94%A8dropout%E5%90%8E%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg" alt="使用dropout后的神经网络"><br>dropout使用之后，就让一个样本进入神经网络进行训练。而对于其他样本也如法炮制，需要再进行复制一遍神经网络，并进行dropout。<br>以上均是逻辑上的做法，接下来讲实际编码该怎么做。</p>
<ol>
<li>设置一个结点保留的概率——keep-prob，假设为0.8。<script type="math/tex">d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) < keep-prob</script>，这样会得到一个True和False的数组，但是python中Ture等于1，False等于0。</li>
<li>让<script type="math/tex">a^3</script>乘上这个向量。<script type="math/tex">a^3 = np.multiply(a^3, d^3)</script>。由于False等于0，所以变相地将<script type="math/tex">a^3</script>中的值失活了。</li>
<li>最后一步看起来有点奇怪，<script type="math/tex">a^3 /= keep-prob</script>。<br>完整代码如下：<script type="math/tex; mode=display">
\begin{cases}
 d^3 = np.random.randn(a^3.shape[0], a^3.shape[1]) < keep-prob\\
 a^3 = np.multiply(a^3, d^3)\\
 a^3 /= keep-prob\\
\end{cases}</script></li>
</ol>
<p>对于最后一步，由于<script type="math/tex">Z^4 = W^4 * A^3 + b^4</script>，由于<script type="math/tex">A^3</script>被dropout减少0.2，为了使得<script type="math/tex">Z^4</script>不受影响，所以对<script type="math/tex">A^3</script>除0.8，来保证<script type="math/tex">A^3</script>的值不变。由于早期的版本没有除于keep-prob，使得测试阶段，平均值越来越复杂。<br>最后，从技术上来讲，输入值也可以使用dropout，但是基本不这么做，直接把keep-prob设为1.0即可，当然0.9也可以。不过太低的值一般不会去设置。<br>以上的步骤被称为<strong>Inverted dropout</strong>——<strong>反向随机失活</strong>。<br>dropout在计算机视觉中用的非常多，甚至成了标配。但要记住一点，dropout是一种正则化方法，为了预防过拟合。所以除非算法过拟合，不然不会使用dropout。由于计算机视觉的特殊性，他们才经常用dropout。<br>dropout的缺点是使我们失去了代价函数这一调试功能。我们经常使用代价函数得到误差，从而画出曲线图。但是使用dropout之后，这样的曲线图就不再准确了。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在测试阶段不再使用dropout，因为我们不希望输出结果是随机的，如果使用dropout预测会受到干扰。</p>
<h3 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h3><div class="note primary">
    <p>略。有点晦涩。</p>
</div>

<p>看<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701044" target="_blank" rel="noopener">视频</a>。。</p>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ol>
<li>Data augment——数据增强。如果拟合猫咪图片分类器，可以对原图片做一些处理，来增加数据，比如翻转、旋转、随机裁剪等。</li>
<li>Early stopping。在训练时画出代价的曲线图，x轴为迭代次数，再绘制验证时的误差。然后选择验证误差曲线图中最低点的迭代次数，下次训练时就改用这个迭代次数，或者也可以在程序中写一个条件判断。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/early%20stopping.jpg" alt="early stopping"></li>
</ol>
<h1 id="均值归一化输入"><a href="#均值归一化输入" class="headerlink" title="均值归一化输入"></a>均值归一化输入</h1><div class="note info">
    <p>略。其实很简单。</p>
</div>

<h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702118&amp;cid=2001699114" target="_blank" rel="noopener">视频</a><br><a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另外一个参考视频</a>，08:37开始。<br><a href="https://www.bilibili.com/video/av10590361/?p=37" target="_blank" rel="noopener">另一个</a>13:50~18左右</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701047" target="_blank" rel="noopener">视频</a></p>
<h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>Gradient checking(Grad check).<br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001701048" target="_blank" rel="noopener">原理视频</a><br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=1f1aedd0dd9f431da0ce64963f010916#/learn/content?type=detail&amp;id=2001702119" target="_blank" rel="noopener">实战视频</a><br>梯度检验可以帮助我们发现神经网络中的一些bug。具体原理是，通过数学上导数的定义来确认反向传播算法是否正确。如果学过高数就会知道，使用导数的定义求解和直接使用公式求解，两者结果十分接近或者一模一样。如果二者不一样说明肯定是求错了。<br>对应于神经网络，那就肯定是代码写错了。具体操作可在视频中看见，每个视频都不超过10分钟。</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果梯度检验确实发现问题，要检查每一项，看看是哪个i的w和b有问题。</li>
<li>记得正则化项，它也被包含在w的梯度中。</li>
<li>梯度检验不能和dropout一起用。</li>
<li><del>在随机初始化时就运行一遍梯度检验；或许在训练一会后可以再运行一遍梯度检验。当W和b接近于0时，梯度下降正确执行在现实中几乎不太可能。</del>吴恩达老师说这条他在现实中几乎不会这么做，并且第五条的翻译，个人感觉翻得有问题，然后看了英文原文后，感觉原文表达得也不是很好，我看不太懂，所以这条就不算进注意事项了。</li>
</ol>
<h1 id="Mini-batch梯度下降"><a href="#Mini-batch梯度下降" class="headerlink" title="Mini-batch梯度下降"></a>Mini-batch梯度下降</h1><div class="note primary">
            <p>为什么Mini-batch比普通的梯度下降快？</p>
          </div>
<p>普通的梯度下降——vanilla gradient descent，是将整个数据集同时做运算，而Mini-batch梯度下降算法是以一组为单位，分别进行梯度下降，所有组执行完毕后再进行下一次迭代。<br>假设现在有m个样本。</p>
<script type="math/tex; mode=display">
X = \begin{pmatrix}x^1&x^2&x^3&\cdots&x^m\end{pmatrix}\\
Y = \begin{pmatrix}y^1&y^2&x^3&\cdots&y^m\end{pmatrix}\\</script><p>使用Mini-batch，假设每1000个样本为一组：</p>
<script type="math/tex; mode=display">
X = \begin{pmatrix}\underbrace{x^1\cdots x^{1000}}_{X^{\{1\}}} & \underbrace{x^{1001}\cdots x^{2000}}_{X^{\{2\}}} & \cdots&\underbrace{\cdots x^m}_{X^{\{t\}}}\end{pmatrix}\\
Y = \begin{pmatrix}\underbrace{y^1\cdots y^{1000}}_{Y^{\{1\}}} & \underbrace{y^{1001}\cdots y^{2000}}_{Y^{\{2\}}} & \cdots&\underbrace{\cdots y^m}_{Y^{\{t\}}}\end{pmatrix}\\</script><p>如果使用代码实现就是类似下面这样的伪代码：<br><figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>, ..., t</span><br><span class="line">	forwardprop <span class="keyword">on</span> X^&#123;t&#125;</span><br><span class="line">	compute cost</span><br><span class="line">	backprop <span class="keyword">to</span> compute grads</span><br><span class="line">	update weights <span class="keyword">and</span> bais</span><br></pre></td></tr></table></figure></p>
<p>for循环完成之后就完成了神经网络的第一次迭代。</p>
<h2 id="理解mini-batch"><a href="#理解mini-batch" class="headerlink" title="理解mini-batch"></a>理解mini-batch</h2><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/Mini-batch%E5%92%8C%E6%99%AE%E9%80%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="Mini-batch和普通梯度下降的区别"></p>
<ol>
<li>如果将batch设为m，那它就是普通的梯度下降算法。</li>
<li>如果将batch设为1，就叫做随机梯度下降——SGD</li>
<li>batch在1到m之间就是mini-batch</li>
</ol>
<p>SGD和普通梯度下降的区别，“+”代表代价最小点。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/SGD%E5%92%8Cvanilla%20gradient%20descent%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="SGD和vanilla gradient descent的区别"><br>SGD和mini-batch的区别。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/SGD%E5%92%8Cmini-batch%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="SGD和mini-batch的区别"></p>
<p><strong>应该记住的有：</strong></p>
<ol>
<li>普通梯度下降、mini-batch和SGD之间的区别就是执行一次参数更新所需的样本数量不同。</li>
<li>你需要自己调整学习速率<script type="math/tex">\alpha</script>。</li>
<li>当mini-batch的量调整良好时，它通常优于普通梯度下降和SGD（尤其是在训练集特别大时）。</li>
</ol>
<h2 id="mini-bacth实现步骤"><a href="#mini-bacth实现步骤" class="headerlink" title="mini-bacth实现步骤"></a>mini-bacth实现步骤</h2><ol>
<li>打乱数据。创建一个打乱数据之后的副本，其中X和Y的每一列都代表一个训练样本。注意X和Y是同步地随机打乱样本，即X中第<script type="math/tex">i^{th}</script>个样本和Y中第<script type="math/tex">i^{th}</script>标签在打乱之后还是是对应的。此步骤确保样本被随机地分割到不同的mini-batches中。下图是步骤示意图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E7%AC%AC%E4%B8%80%E6%AD%A5%E6%89%93%E4%B9%B1%E6%95%B0%E6%8D%AE.jpg" alt="mini-batch第一步打乱数据"></li>
<li>切分。将打乱数据后的XY切分进<code>mini_batch_size</code>大小（下图是64）的mini-batches中。不过注意训练样本的数量并不总能被<code>mini_batch_size</code>整除。最后的mini-batch可能要小点，但是不需要担心这点。使用<code>math.floor()</code>向上取整即可。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E7%AC%AC%E4%BA%8C%E6%AD%A5%E5%88%87%E5%88%86.png" alt="mini-batch第二步切分"></li>
</ol>
<h2 id="一些经验"><a href="#一些经验" class="headerlink" title="一些经验"></a>一些经验</h2><p>如果小数据量（大约小于2000）的话，<strong>只</strong>执行步骤1即可；如果样本数目较大，执行步骤1和步骤2，一般将batch设置在64~512之间，考虑到电脑的内存设置和使用方式，batch的大小设置为2的次方，代码的运行速度会比较快；</p>
<h1 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h1><p>为了更好地理解其他优化算法，需要使用到指数加权平均。这章介绍一下它。<br>Exponentially weighted averages，在统计学中被称为指数加权移动平均——Exponentially weighted moving averages。<br>指数加权平均有一个公式：<script type="math/tex">V_t = \beta * V_{t-1} + (1- \beta) * \theta_t</script>，<script type="math/tex">V_0 = 0</script>，其目的是使用<script type="math/tex">V_t</script>代替<script type="math/tex">\theta_t</script>。<script type="math/tex">V_t</script>可视为<strong>约等于</strong><script type="math/tex">\frac{1}{1 - \beta}</script>个样本的平均值。这里可能会有疑问，为什么<script type="math/tex">V_t</script>可视为约等于<script type="math/tex">\frac{1}{1 - \beta}</script>个样本的平均值？其实我也不知道，也不想知道，我又不是学统计学或者数学的。<br>下图中的数据为伦敦一年之间的温度，来源于吴恩达的深度学习视频，可以看到其中的数据十分杂乱，也就是常在网络上看到别人所说的“噪点”多。我们可以使用<strong>指数加权平均</strong>来画出一条线，就是下图的红线，来代表温度变化的趋势，这样会使得更容易让人类理解和观察。<br>下图中的<script type="math/tex">\beta</script>为0.9。而<script type="math/tex">\frac{1}{1 - 0.9} = 10</script>，所以<script type="math/tex">V_t</script>代表过去<em>十天</em>内的平均温度。如果<script type="math/tex">\beta</script>为0.98，那么<script type="math/tex">V_t</script>代表过去<em>五十</em>天内的平均温度<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E4%BE%8B%E5%AD%90.jpg" alt="指数加权平均例子"><br>下图是不同<script type="math/tex">\beta</script>值的对比。注意到一点，绿色（<script type="math/tex">\beta</script>=0.98）的线比红色的线要平坦一点，这是因为你多平均了几天的温度，所以这根线波动更新、更平坦。但是缺点是曲线进一步向右移，拟合的不是很好。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%B8%8D%E5%90%8Cbeta%E5%80%BC%E7%9A%84%E5%AF%B9%E6%AF%94.jpg" alt="不同beta值的对比"><br>现在看到了平均了10天和50天温度的曲线，现在试试<script type="math/tex">\beta=0.5</script>，也就是只平均两天的温度。由于只平均了两天的温度，数据太少，所以曲线有更多的噪声，更有可能出现异常值。但是这个曲线能更快适应温度变化。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/beta%E5%80%BC%E7%AD%89%E4%BA%8E0.5%E7%9A%84%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87.jpg" alt="beta值等于0.5的指数加权平均"></p>
<h2 id="理解其作用"><a href="#理解其作用" class="headerlink" title="理解其作用"></a>理解其作用</h2><div class="note primary">
            <p>略。至今看不懂。</p>
          </div>
<h2 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h2><p>之前的曲线其实都是理想状态下的，回想绿色的曲线是50天内的温度平均值。但是其实绿色曲线会是紫色曲线那样的轨迹。初始化<script type="math/tex">V_0=0</script>，原数据中<script type="math/tex">\theta_0 = 40</script>，所以其实<script type="math/tex">V_1 = 0.02 * 40 = 8</script>，从而绿色曲线的起点实际上很低。因为起点并没有计算50天内的温度平均，我们默认将<script type="math/tex">V_0</script>初始化为0。<br>我们可以用下图右边的公式将其修正。算出<script type="math/tex">V_t</script>后再做如下计算：<script type="math/tex">\frac{V_t}{1 - \beta^t}</script>，其中<script type="math/tex">\beta</script>的上标t是指<strong>t次方</strong>。<br>另外由于t越大，<script type="math/tex">\beta^t</script>的值越接近0，所以对后面的值几乎没影响。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/指数加权平均偏差修正.jpg" alt="指数加权平均偏差修正"></p>
<h1 id="※-Learning-rate-decay"><a href="#※-Learning-rate-decay" class="headerlink" title="※ Learning rate decay"></a>※ Learning rate decay</h1><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>介绍 </th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td><strong>优点</strong>：<br> 1）在损失函数是凸函数的情况下能够保证收敛到一个较好的全局最优解；2）数据量过大时，batch 方法可以减少机器压力，从而更快地收敛；3）当训练集有很多冗余时（类似的样本出现多次），batch方法收敛更快。<br> <strong>缺点</strong>：<br>1）<script type="math/tex">\alpha</script> 是一个定值，它的选取直接决定了解的好坏，过小会导致收敛太慢，过大会导致震荡而无法收敛到最优解；2）对于非凸问题，只能收敛到局部最优，并且没有任何摆脱局部最优的能力（一旦梯度为0就不会再有任何变化）；3）更新方向完全依赖当前的 batch</td>
</tr>
<tr>
<td>Momentum</td>
<td>累积梯度，充当动量，注：虽然 Momentum 和 RMSprop 类似，但是实际上在计算上并不一样， RMSprop 要多一个除以 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 的步骤，且没有偏差修正这一步骤<br> <strong>优点</strong>：1）一定程度上缓解了 SGD 收敛不稳定的问题，并且有一定的摆脱局部最优的能力（即如同一个滚轮下坡一样，它拥有惯性，在到达鞍点时不会立即停止，会因为惯性再向前一点距离，从而可能离开此鞍点）。<br> <strong>缺点</strong>：1）多了一个超参数需要调整，它的选取同样会影响到结果。</td>
</tr>
<tr>
<td>RMSprop</td>
<td>通过除以 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 减小波动幅度从而收敛更快<br> <strong>优点</strong>：1）不需要手动调整学习率，可以自动调整。<br></td>
</tr>
<tr>
<td>Adam</td>
<td><strong>优点</strong>：1）结合 Momentum 和 RMSProp，稳定性好，同时相比于 Adagrad 不用存储全局所有的梯度，适合处理大规模数据。<br> <strong>缺点</strong>：1）有三个超参数需要调整</td>
</tr>
<tr>
<td>Adagrad</td>
<td>RMSProp 的简化版<br> <strong>适用场景</strong>：Adagrad非常适合处理稀疏数据（如 one-hot）<br> <strong>优点</strong>：1）不需要手动调节 <script type="math/tex">\alpha</script>，它会发生自适应的变化。<br> <strong>缺点</strong>：1）学习率单调递减，在迭代后期可能导致学习率变得特别小而导致收敛及其缓慢。</td>
</tr>
<tr>
<td>Adadelta</td>
<td>Adadelta 是 Adagrad 的一种扩展算法，以处理Adagrad学习速率单调递减的问题。RMSprop 可以算作 Adadelta 的一个特例<br> <strong>优点</strong>：1）不需要手动调整学习率，可以自动调整；2）不需要手动设置<strong>初始</strong> <script type="math/tex">\alpha</script>。<br> <strong>缺点</strong>：1）后期容易在小范围内产生震荡</td>
</tr>
<tr>
<td>—————-</td>
<td><a href="https://blog.csdn.net/gangyin5071/article/details/81810358#11-sgd" target="_blank" rel="noopener">机器学习各优化算法的简单总结</a><br> <a href="http://ruder.io/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a> <br> <a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms ARXIV</a> <br> <a href="https://blog.csdn.net/google19890102/article/details/69942970" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms 中文翻译</a></td>
</tr>
</tbody>
</table>
</div>
<p>凸优化与非凸优化：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/凸优化与非凸优化.jpg" alt="凸优化与非凸优化"></p>
<h2 id="动量梯度下降——Momentum"><a href="#动量梯度下降——Momentum" class="headerlink" title="动量梯度下降——Momentum"></a>动量梯度下降——Momentum</h2><p>&emsp;&emsp;Momentum改进自SGD，让每一次的参数更新方向不仅取决于当前位置的梯度，还受到上一次参数更新方向的影响。<br>&emsp;&emsp;不管是普通的梯度下降、mini-batch、SGD 还是其他的什么，都是通过 <script type="math/tex">W -= \alpha * dW</script> 来更新权重。但是在动量梯度下降中，使用到了<strong>指数加权平均</strong>。尤其是针对mini-batch算法，因为mini-batch算法抖动过大，上面的章节介绍了mini-batch的梯度下降误差曲线，指数加权平均正好可以解决。<br>&emsp;&emsp;可以观察下图发现，梯度下降的波动比较大，也就是噪点较多，我们可以使用指数加权平均来减少噪点。下面的公式就是用其减少了梯度dW和db。下式中还对指数加权平均进行了优化，使用了<strong>偏差修正</strong>。</p>
<script type="math/tex; mode=display">
    \begin{cases}
        compute\ dW, db\\
        V_{dW} = \beta * (V_{dW})_{prev} + (1 - \beta) * dW,\quad V_{db} = \beta * (V_{db})_{prev} + (1 - \beta) * db\\
        V^{corrected}_{dW} = \frac{V_{dW}}{1 - \beta^t},\quad V^{corrected}_{db} = \frac{V_{db}}{1 - \beta^t} \\
        W -= \alpha * V^{corrected}_{dW}\\
        b -= \alpha * V^{corrected}_{db}\\
    \end{cases}</script><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/对神经网络整体的理解/梯度下降示意图.jpg" alt="梯度下降示意图"></p>
<p>在梯度下降时的这种波动减慢了下降的速度，无法使用更大的学习速率。因为梯度已经很大了，如果使用更大的学习速率，可能梯度直接爆炸了，直接无法收敛。为了避免摆动过大需要使用较小的学习速率。<br>还可以从另一种角度看待。我们希望在纵轴上学习的慢点，我们希望摆动小点，不就是希望纵轴小点吗。而在横轴上我们又希望学习的快点，因为我们希望越快接近中心越好。<br>这个<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">视频</a>讲的直观一点，可以参考一下，从36:00开始看，虽然讲的是RMSprop但是讲的原理跟Momentum的原理一样。</p>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>在课后练习中有更详细的说明，在此补充一下。</p>
<blockquote>
<p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.</p>
</blockquote>
<p>大致意思就是使用Momentum可以使得mini-batch的振荡更小，观察下图。。。说实话我并没有观察出什么，不知道Coursera是怎么想的。我把此图的提示贴出来：</p>
<blockquote>
<p>Figure 3: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence  v  and then take a step in the direction of  v .</p>
</blockquote>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/mini-batch%E4%BD%BF%E7%94%A8Momentum%E5%90%8E.png" alt="mini-batch使用Momentum后"></p>
<blockquote>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable  v . Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of  v  as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.</p>
</blockquote>
<p>Momentum考虑到了之前的梯度，从而用其来缓和参数更新。我们将前一次梯度的“方向”存进变量v。后续不翻译了。</p>
<h2 id="均方根传播——RMSprop"><a href="#均方根传播——RMSprop" class="headerlink" title="均方根传播——RMSprop"></a>均方根传播——RMSprop</h2><div class="note primary">
            <p>&emsp;&emsp;问题：对梯度做平方，且 <script type="math/tex">S_{dW}</script> 的计算公式是加法（即修改过的指数加权平均），那么 <script type="math/tex">S_{dW}</script> 岂不是会越来越大？不就意味着动量越来越大，到最后停不下来了？<br>&emsp;&emsp;<strong>当然不是，可以减小</strong>。设 <script type="math/tex">dW_1 = 15 \quad S_{dW1} = 20 \quad \beta = 0.9 \quad dW_2 = 2</script>，则 <script type="math/tex">S_{dW2} = 0.9 * 20 + (1 - 0.9) * 15 = 19.5</script>，而 <script type="math/tex">S_{dW3} = 0.9 * 19.5 + (1 - 0.9) * 2 = 17.75</script>。比较 <script type="math/tex">S_{dW1} \, S_{dW2} \, S_{dW3}</script> = 20 19.5 17.75，明显在下降，比较 <script type="math/tex">dW_1 \, dW_2</script> 发现梯度减少引起得 <script type="math/tex">dS</script> 减少。<br>&emsp;&emsp;<strong>RMSProp 的本质</strong>是：在梯度大的地方，减小 <script type="math/tex">\alpha</script>（即陡坡则减小步伐）；在梯度小的地方，增大 <script type="math/tex">\alpha</script>（即缓坡则增大步伐）。解释如下：<br>&emsp;&emsp;RMSProp的更新公式为：</p><script type="math/tex; mode=display">    \begin{cases}        S_{dW} = \beta_2 * (S_{dW})_{prev} + (1 - \beta_2) * (dW)^2 \\        W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon} \\    \end{cases}</script><p>&emsp;&emsp;对于 <script type="math/tex">W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}</script> 来说，其实就是普通的权重更新公式多除以一个 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script>。而由于 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 是梯度的累加，故为简便起见，将 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 看作梯度（Q：为什么可以将这个视为梯度？A：<script type="math/tex">\sqrt{S_{dW}}</script> 是对梯度 dW 的类似累加操作，开个根号后差不多就是梯度）。<br>&emsp;&emsp;我们知道在<strong>陡坡</strong>处梯度大，我们需要<strong>减小步伐</strong>，要不然容易一步迈长了，而减小步伐的意思就是减小 <script type="math/tex">\alpha</script>。<br>&emsp;&emsp;假设现在陡坡处梯度为 100，那么就是将原本的梯度下降公式多除以了 100（因为我们已将 <script type="math/tex">\sqrt{S_{dW}} + \epsilon</script> 视为梯度）。是不是梯度越大，则分母越大？分母越大则意味着 <script type="math/tex">\alpha</script> 越小。<br>&emsp;&emsp;反之，<strong>缓坡</strong>梯度小，我们就需要<strong>增大步伐</strong>，假设梯度为 0.1，那么就代表将原本的权重更新公式除以 0.1。分母越小，则 <script type="math/tex">\alpha</script> 越大。</p>
          </div>
<p>Root mean square prop.<br>一个类似Momentum的算法，没必要死记公式，略。<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d#/learn/content?type=detail&amp;id=2001702124" target="_blank" rel="noopener">视频地址</a>。<br>或者<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">另一个参考视频</a>，36:00开始。<br><strong>RMSprop 没有使用偏差修正。</strong>但是在 Adam 中的 RMSprop 使用了偏差修正。</p>
<script type="math/tex; mode=display">
    \begin{cases}
        compute\ dW, db\\
        S_{dW} = \beta_2 * (S_{dW})_{prev} + (1 - \beta_2) * (dW)^2,\quad S_{db} = \beta_2 * (S_{db})_{prev} + (1 - \beta_2) * (db)^2\\
        W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}\\
        b -= \alpha * \frac{db}{\sqrt{S_{db}} + \epsilon}\\
    \end{cases}</script><p>在更新 W 和 b 时的算法与之前的 Momentum 算法略微不同。另外为了防止 dW 和 db 等于 0，导致分母为 0，所以在分母加了一个极小值<script type="math/tex">\epsilon</script>，在 Keras 中取了 1e-7， 吴恩达老师说 1e-8 是个不错的选择。<br><strong>RMSprop 算法也是使用了指数加权平均算法。</strong>并且还结合了 Adagrad。<br><div class="note info">
            <p>&emsp;&emsp;对于理解 RMSprop。<strong>可以观察出 RMSprop 和 Momentum 长得有点像，但是这两个算法的具体关系暂时不清楚</strong>。并且 RMSprop 其实还有简化版的算法，叫做 Adagrad。之前对这些优化算法（Momentum, RMSprop, Adam 等）的理解都是<em>改变 W 和 b 的大小从而使得梯度下降更快</em>。但是又今天看了一遍<a href="https://www.bilibili.com/video/av10590361/?p=18" target="_blank" rel="noopener">李宏毅老师的视频</a>，发现还有其他的理解。其实这些算法都在<strong>改变学习速率的大小</strong>。<br>&emsp;&emsp;比如 RMSprop 算法，观察<script type="math/tex">W -= \alpha * \frac{dW}{\sqrt{S_{dW}} + \epsilon}</script>，我们可以改写成<script type="math/tex">W = W - \frac{\alpha}{\sqrt{S_{dW}} + \epsilon} * dW</script>。看dW之前的那项<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>实际上就是对学习速率<script type="math/tex">\alpha</script>乘上了<script type="math/tex">\frac{1}{\sqrt{S_{dW}} + \epsilon}</script>。<br>&emsp;&emsp;所以对RMSprop的理解是：<strong>如果梯度过大<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>就会相对减小，如果梯度过小<script type="math/tex">\frac{\alpha}{\sqrt{S_{dW}} + \epsilon}</script>就会相对增大</strong>。因为其实<script type="math/tex">S_{dW}</script>就是 dW 算出来的，而梯度过大就是 dW 过大，dW过大就是<script type="math/tex">S_{dW}</script>过大。一个很大的数取倒数，这个数就变很小了。梯度过小同理。<br>&emsp;&emsp;而为什么梯度过大就要是学习速率<script type="math/tex">\alpha</script>变小呢？因为梯度过大就是说梯度较为陡峭，可以想象一座陡峭的山，如果跨一大步是不是直接掉下去了？而掉在哪是未知的，很有可能掉到最低点的前面，这样大概率是回不到最低点的（或者是极小值点）。而如果<script type="math/tex">\alpha</script>小点就很好了，因为可以一小步一小步的走，最终可能会走到极小值点（或者最小值点）。梯度过小同理。平原地方肯定要大跨步走，你小步伐走要走到什么时候才能走到极小值点？<br>&emsp;&emsp;<strong>另外 RMSprop 可以算是 Adagrad 算法的改进版，但是这二者的具体关系未知。</strong></p>
          </div></p>
<h2 id="优化算法历史介绍"><a href="#优化算法历史介绍" class="headerlink" title="优化算法历史介绍"></a>优化算法历史介绍</h2><blockquote>
<p>在深度学习的历史中，有不少学者，包括许多知名学者，提出了优化算法并解决了一些问题。但之后这些算法被指出并不能一般化，并不能适用于多种神经网络。<br>时间久了，深度学习圈子里的人开始多少有点质疑全新的优化算法。<br>但是RMSprop和Adam是少有的经受住人们考验的两种算法。已被证明适用于不同的深度学习结构。</p>
</blockquote>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>全称：Adaptive Moment Estimation<br>这里的 RMSprop 使用了偏差修正。<br><strong>Adam 算法是 Momentum 和 RMSprop 结合起来的算法。</strong>Momentum算法解决算法在纵轴上波动过大的问题，它可以使用类似于物理中的动量来累积梯度。而RMSprop可以在横轴上收敛速度更快同时使得波动的幅度更小。所以将两种算法结合起来表现可能会更好。<br><div class="note primary">
            <p>我的理解是 RMSprop 算法也算是在累计梯度。所以我感觉只使用 RMSprop 和使用 Adam 差不多。</p>
          </div></p>
<script type="math/tex; mode=display">
\begin{array}{l}
    compute\ dW, db\\
    V_{dW} = \beta_1 * V_{dW} + (1 - \beta_1) * dW,\quad V_{db} = \beta_1 * V_{db} + (1 - \beta_1) * db\\
    S_{dW} = \beta_2 * S_{dW} + (1 - \beta_2) * (dW)^2,\quad S_{db} = \beta_2 * S_{db} + (1 - \beta_2) * (db)^2\\
    V^{corrected}_{dW} = \frac{V_{dW}}{1 - \beta_1},\quad V^{corrected}_{db} = \frac{V_{db}}{1 - \beta_1}\\
    S^{corrected}_{dW} = \frac{S_{dW}}{1 - \beta_2},\quad S^{corrected}_{db} = \frac{S_{db}}{1 - \beta_2}\\
    W -= \alpha * \frac{V^{corrected}_{dW}}{\sqrt{S^{corrected}_{dW}} + \epsilon}\\
    b -= \alpha * \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}} + \epsilon}\\
\end{array}</script><p><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">adam paper</a>在这。</p>
<h3 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h3><ol>
<li><script type="math/tex">\alpha</script>需要自行调整。</li>
<li><script type="math/tex">\beta_1</script>一般设置为0.9，计算<script type="math/tex">dW</script>。</li>
<li><script type="math/tex">\beta_2</script>Adam的作者推荐0.999，计算<script type="math/tex">(dW)^2</script>。</li>
<li><script type="math/tex">\epsilon</script>其实不是很重要，但是Adam作者推荐设置为<script type="math/tex">10^{-8}</script>。其实不设置也可以，并不会影响算法的性能。</li>
</ol>
<p>所以在该算法中其实只要调整<script type="math/tex">\alpha</script>就够了，其他的参数也可以调整，但是一般不调整。</p>
<h2 id="其他的学习速率衰减算法"><a href="#其他的学习速率衰减算法" class="headerlink" title="其他的学习速率衰减算法"></a>其他的学习速率衰减算法</h2><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>&emsp;&emsp;让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。<br><a href="https://www.bilibili.com/video/av10590361/?p=6" target="_blank" rel="noopener">李宏毅 Adagrad 参考视频</a>，从06:30开始。<br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d#/learn/content?type=detail&amp;id=2001702125" target="_blank" rel="noopener">吴恩达深度学习——学习速率衰减</a></p>
<h2 id="优化算法总结"><a href="#优化算法总结" class="headerlink" title="优化算法总结"></a>优化算法总结</h2><div class="table-container">
<table>
<thead>
<tr>
<th><strong>optimization method</strong></th>
<th><strong>accuracy</strong></th>
<th><strong>cost shape</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient descent</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Momentum</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Adam</td>
<td>94%</td>
<td>smoother</td>
</tr>
</tbody>
</table>
</div>
<h1 id="如何为超参数选择范围"><a href="#如何为超参数选择范围" class="headerlink" title="如何为超参数选择范围"></a>如何为超参数选择范围</h1><p>上面说了那么多算法，其中包括了许多超参数，那么应该怎么为超参数选择值呢？</p>
<h2 id="超参数的重要程度"><a href="#超参数的重要程度" class="headerlink" title="超参数的重要程度"></a>超参数的重要程度</h2><p>按照吴恩达老师的排序，超参数的重要程度如下：</p>
<ol>
<li>learning rate<script type="math/tex">\alpha</script></li>
<li>Momentum的<script type="math/tex">\beta</script>, hidden layer units, mini-batch size</li>
<li>layer的数量，learning rate decay</li>
<li>Adam中的<script type="math/tex">\beta_1\quad \beta_2\quad \epsilon</script>不是很重要，一般按<script type="math/tex">0.9\quad 0.99\quad 10^{-8}</script>设置</li>
</ol>
<h2 id="超参数的取值"><a href="#超参数的取值" class="headerlink" title="超参数的取值"></a>超参数的取值</h2><ol>
<li>随机取值</li>
<li>从粗糙到精细的策略。首先进行随机取值，发现某个点的效果很好，并且附近的点也很好，然后放大这块区域，进行更密集地取值。下图被圈出来的蓝点就是效果不错的，然后被方框画出一大块区域进行密集地取值或者也可以在这块区域随机取值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E4%BB%8E%E7%B2%97%E7%B3%99%E5%88%B0%E7%B2%BE%E7%BB%86%E7%9A%84%E5%8F%96%E5%80%BC%E7%AD%96%E7%95%A5.jpg" alt="从粗糙到精细的取值策略"></li>
</ol>
<h2 id="选择合适的范围"><a href="#选择合适的范围" class="headerlink" title="选择合适的范围"></a>选择合适的范围</h2><p><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701053" target="_blank" rel="noopener">视频1</a><br><a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036&amp;_trace_c_p_k2_=48e14d97b8284ad0b4e8d32be2605c3d&amp;from=study#/learn/content?type=detail&amp;id=2001701054" target="_blank" rel="noopener">视频2</a></p>
<h2 id="补充——用神经网络训练另一个神经网络的超参数"><a href="#补充——用神经网络训练另一个神经网络的超参数" class="headerlink" title="补充——用神经网络训练另一个神经网络的超参数"></a>补充——用神经网络训练另一个神经网络的超参数</h2><p>&emsp;&emsp;看完李宏毅深度学习后的补充，他在教学视频中也讲述了如何调整超参数，有个说的挺有创意的，就是<strong>用神经网络来训练超参数如何取值</strong>。典型的例子就是 <strong>Swish</strong>，它可以用神经网络训练出最好的几个激活函数。</p>
<h1 id="batch-normalization——对激活值均值归一化"><a href="#batch-normalization——对激活值均值归一化" class="headerlink" title="batch normalization——对激活值均值归一化"></a>batch normalization——对激活值均值归一化</h1><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">论文地址</a><br>&emsp;&emsp;这个原来不是一个算法，它就是让我们对神经网络的每一层都做一次normalization，从而提供性能，而算法是在batch中做的，所以叫这名。<br>&emsp;&emsp;<a href="https://mooc.study.163.com/learn/2001281003?tid=2001391036#/learn/content?type=detail&amp;id=2001701055&amp;cid=2001693088" target="_blank" rel="noopener">视频地址</a>，第25章写了均值归一化，它对输入值进行了均值归一，更易于算法优化。而batch normalization对激活值进行了均值归一化，说白了是一个东西。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>&emsp;&emsp;代价函数为<script type="math/tex">cost(\hat{y}, y) = - \sum^n_{j=1} y_j * log(\hat{y_j})</script>。<br><div class="note primary">
            <p>但是这里可能会有点奇怪。因为二元分类的代价函数是<script type="math/tex">cost(\hat{y}, y) = - \sum^n_{j=1} (y_j * log(\hat{y_j}) + (1 - y_j) * log(\hat{1-y_j}))</script>。怎么多元分类的表达式那么短？</p>
          </div></p>
<h1 id="选择深度学习框架"><a href="#选择深度学习框架" class="headerlink" title="选择深度学习框架"></a>选择深度学习框架</h1><p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%9A%84%E7%90%86%E8%A7%A3/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E9%80%89%E6%8B%A9.jpg" alt="深度学习框架选择"></p>
<h1 id="序列模型——RNN"><a href="#序列模型——RNN" class="headerlink" title="序列模型——RNN"></a>序列模型——RNN</h1><p>&emsp;&emsp;<a href="https://yan624.github.io/学习笔记/吴恩达李宏毅综合学习笔记：RNN入门.html">传送门</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>next主题中使用Mathjax</title>
    <url>/next%E4%B8%BB%E9%A2%98%E4%B8%AD%E4%BD%BF%E7%94%A8Mathjax.html</url>
    <content><![CDATA[<p>百度了一圈最后还是没有解决问题，因为其他的博客都说要去<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code>文件修改源代码，但是我压根没有这个文件，hexo-renderer-kramed这个文件夹不存在。</p>
<p>最后还是自己试出来了，步骤如下：</p>
<ol>
<li><p>安装kramed。hexo 默认的渲染引擎是 marked，但是 marked 不支持 mathjax。所以要卸载它。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">npm <span class="keyword">uninstall</span> hexo-renderer-marked <span class="comment">--save</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-renderer-kramed <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>停用hexo-math，然后安装 hexo-renderer-mathjax 包。其实我也不知道怎么判断自己有没有在使用hexo-math。总而言之卸载就行了，管它存不存在。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">npm <span class="keyword">uninstall</span> hexo-math <span class="comment">--save</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-renderer-mathjax <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>开启mathjax，在next主题文件夹里，具体路径：/themes/next/_config.yml<br>你只需要把enable属性改为true即可</p>
<figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"># Math Equations Render Support</span><br><span class="line">math:</span><br><span class="line">  enable: <span class="keyword">true</span></span><br><span class="line"></span><br><span class="line">  # <span class="keyword">Default</span>(<span class="keyword">true</span>) will load mathjax/katex script <span class="keyword">on</span> demand</span><br><span class="line">  # That <span class="keyword">is</span> it only render those page who <span class="keyword">has</span> `mathjax: <span class="keyword">true</span>` <span class="keyword">in</span> Front Matter.</span><br><span class="line">  # <span class="keyword">If</span> you <span class="keyword">set</span> it <span class="keyword">to</span> <span class="keyword">false</span>, it will load mathjax/katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: <span class="keyword">true</span></span><br><span class="line"></span><br><span class="line">  engine: mathjax</span><br><span class="line">  #engine: katex</span><br><span class="line"></span><br><span class="line">  # hexo-rendering-pandoc (<span class="keyword">or</span> hexo-renderer-kramed) needed <span class="keyword">to</span> full MathJax support.</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: <span class="keyword">true</span></span><br><span class="line">    # Use <span class="number">2.7</span>.<span class="number">1</span> <span class="keyword">as</span> <span class="keyword">default</span>, jsdelivr <span class="keyword">as</span> <span class="keyword">default</span> CDN, works everywhere even <span class="keyword">in</span> China</span><br><span class="line">    cdn: <span class="comment">//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span></span><br><span class="line">    # <span class="keyword">For</span> direct link <span class="keyword">to</span> MathJax.js <span class="keyword">with</span> CloudFlare CDN (cdnjs.cloudflare.com)</span><br><span class="line">    #cdn: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br><span class="line"></span><br><span class="line">    # See: https:<span class="comment">//mhchem.github.io/MathJax-mhchem/</span></span><br><span class="line">    #mhchem: <span class="comment">//cdn.jsdelivr.net/npm/mathjax-mhchem@3</span></span><br><span class="line">    #mhchem: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0</span></span><br><span class="line"></span><br><span class="line">  # hexo-renderer-markdown-it-plus (<span class="keyword">or</span> hexo-renderer-markdown-it <span class="keyword">with</span> markdown-it-katex plugin) needed <span class="keyword">to</span> full Katex support.</span><br><span class="line">  katex:</span><br><span class="line">    # Use <span class="number">0.7</span>.<span class="number">1</span> <span class="keyword">as</span> <span class="keyword">default</span>, jsdelivr <span class="keyword">as</span> <span class="keyword">default</span> CDN, works everywhere even <span class="keyword">in</span> China</span><br><span class="line">    cdn: <span class="comment">//cdn.jsdelivr.net/npm/katex@0.7.1/dist/katex.min.css</span></span><br><span class="line">    # CDNJS, provided <span class="keyword">by</span> cloudflare, maybe the best CDN, but <span class="keyword">not</span> works <span class="keyword">in</span> China</span><br><span class="line">    #cdn: <span class="comment">//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css</span></span><br><span class="line"></span><br><span class="line">    copy_tex:</span><br><span class="line">      # See: https:<span class="comment">//github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex</span></span><br><span class="line">      enable: <span class="keyword">false</span></span><br><span class="line">      copy_tex_js: <span class="comment">//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js</span></span><br><span class="line">      copy_tex_css: <span class="comment">//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>最后一步，修改/themes/next/layout/_layou.swig文件<br>将如下代码添加到该文件&lt;/body&gt;标签的上面一行</p>
<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="params">&lt;script type="text/x-mathjax-config"&gt;</span></span><br><span class="line">MathJax.Hub.Config(&#123;</span><br><span class="line">	<span class="comment">//下面的HTML-CSS和SVG用于在小屏幕上，数学公式可以自动换行，我测试过后发现无效，但是貌似有人可以。</span></span><br><span class="line">	<span class="string">"HTML-CSS"</span>: &#123; </span><br><span class="line"><span class="symbol">		linebreaks:</span> &#123; </span><br><span class="line"><span class="symbol">			automatic:</span> true </span><br><span class="line">		&#125; </span><br><span class="line">	&#125;,SVG: &#123;</span><br><span class="line"><span class="symbol">		linebreaks:</span> &#123; </span><br><span class="line"><span class="symbol">			automatic:</span> true </span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,menuSettings: &#123;</span><br><span class="line"><span class="symbol">		zoom:</span> <span class="string">"None"</span></span><br><span class="line">	&#125;,</span><br><span class="line"><span class="symbol">	showMathMenu:</span> false,</span><br><span class="line"><span class="symbol">	jax:</span> [<span class="string">"input/TeX"</span>,<span class="string">"output/CommonHTML"</span>],</span><br><span class="line"><span class="symbol">	extensions:</span> [<span class="string">"tex2jax.js"</span>],</span><br><span class="line"><span class="symbol">	TeX:</span> &#123;</span><br><span class="line"><span class="symbol">		extensions:</span> [<span class="string">"AMSmath.js"</span>,<span class="string">"AMSsymbols.js"</span>],</span><br><span class="line"><span class="symbol">		equationNumbers:</span> &#123;</span><br><span class="line"><span class="symbol">			autoNumber:</span> <span class="string">"AMS"</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,tex2jax: &#123;</span><br><span class="line"><span class="symbol">		inlineMath:</span> [[<span class="string">"\\("</span>, <span class="string">"\\)"</span>]],</span><br><span class="line"><span class="symbol">		displayMath:</span> [[<span class="string">"\\["</span>, <span class="string">"\\]"</span>]]</span><br><span class="line">	&#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="params">&lt;/script&gt;</span></span><br><span class="line"><span class="params">&lt;script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/<span class="number">2.7</span><span class="number">.1</span>/MathJax.js?config=TeX-MML-AM_CHTML"&gt;</span><span class="params">&lt;/script&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>1-3步参考<a href="https://www.jianshu.com/p/523e806d6681" target="_blank" rel="noopener">如何在hexo中支持Mathjax</a><br>4步参考<a href="http://npm.taobao.org/package/hexo-renderer-kramed" target="_blank" rel="noopener">hexo-renderer-kramed</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>《神经网络与深度学习》学习笔记：反向传播算法中weight的表示问题</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%ADweight%E7%9A%84%E8%A1%A8%E7%A4%BA%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p><a href="https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html" target="_blank" rel="noopener">本文中文章节地址</a><br><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">本文英文章节地址</a></p>
<h3 id="神经网络中一些符号的定义"><a href="#神经网络中一些符号的定义" class="headerlink" title="神经网络中一些符号的定义"></a>神经网络中一些符号的定义</h3><p>引用自<a href="https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html#%E7%83%AD%E8%BA%AB%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E8%AE%A1%E7%AE%97%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%82%E7%82%B9" target="_blank" rel="noopener">原文中文翻译</a></p>
<blockquote>
<p>我们首先给出网络中权重的清晰定义。我们使用<script type="math/tex">w^l_{jk}</script>表示从 <script type="math/tex">(l−1)^{th}</script> 层的 <script type="math/tex">k^{th}</script> 个神经元到 <script type="math/tex">(l)^{th}</script> 层的 <script type="math/tex">j^{th}</script> （<font style="color:red">注意：这个地方中文文章中写错了</font>，他写成了<script type="math/tex">l^{th}</script>）个神经元的链接上的权重。例如，下图给出了第二隐藏层的第四个神经元到第三隐藏层的第二个神经元的链接上的权重：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E9%93%BE%E6%8E%A5%E4%B8%8A%E7%9A%84%E6%9D%83%E9%87%8D%E8%A1%A8%E7%A4%BA%E5%9B%BE%E8%A7%A3.png" alt="神经网络中的神经元链接上的权重表示图解"><br>我们对网络偏差和激活值也会使用类似的表示。显式地，我们使用 <script type="math/tex">b^l_J</script> 表示在 <script type="math/tex">l^{th}</script> 层 <script type="math/tex">j^{th}</script> 个神经元的偏差，使用 <script type="math/tex">a^l_j</script> 表示 <script type="math/tex">l^{th}</script> 层 <script type="math/tex">j^{th}</script> 个神经元的激活值。下面的图清楚地解释了这样表示的含义：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE%E5%92%8C%E6%BF%80%E6%B4%BB%E5%80%BC%E5%9B%BE%E8%A7%A3.png" alt="神经网络中的偏差和激活值图解"></p>
</blockquote>
<p>说白了就是<strong>前一层的</strong>的神经元到后一层的神经元上的链接的权重。</p>
<p>这里解释一下，我在<a href="https://www.bilibili.com/video/av9770302" target="_blank" rel="noopener">李宏毅深度学习的视频</a>中，也听到他讲过。<script type="math/tex">w^l_{jk}</script>中的j和k在这个公式中是写反的，可以想想：上文说到<script type="math/tex">w^l_{jk}</script>是代表第k个神经元到第j个神经元链接上的权重，按逻辑来说，似乎写成<script type="math/tex">w^l_{kj}</script>更合理。因为我们说话写字都是按顺序的，没有说是反着来的。<br>之所以这么反着写，是因为后面在对权重（weight）、激活值（activations）进行向量化时，不需要加一个转置。接下来解释一下为什么不用加：<br>以上图为例，有如下权重：<script type="math/tex">w^3_{21}</script> <script type="math/tex">w^3_{22}</script> <script type="math/tex">w^3_{23}</script> <script type="math/tex">w^3_{24}</script> 现在将上标3移除组成一个向量——<script type="math/tex">\begin{pmatrix}w_{21}&w_{22}&w_{23}&w_{24}\end{pmatrix}</script>，大家已经发现了吧，我将这个向量横着写的。众所周知，矩阵的下标第一个数字代表矩阵的行，第二个数字代表矩阵的列。所以如果将w这个矩阵补全就是下面这样。</p>
<script type="math/tex; mode=display">\begin{pmatrix}
w_{11}&w_{12}&w_{13}&w_{14}\\
w_{21}&w_{22}&w_{23}&w_{24}\\
\end{pmatrix}</script><p>这是一个2行4列的矩阵，因为第三层layer只有两个神经元，自然只有两个权重向量。先给出公式：<script type="math/tex">z^3_2</script> = <script type="math/tex">w^3_2</script> * <script type="math/tex">a^2</script>，这里原本应该还要加上偏差b，但是由于改起来太麻烦以下都不加上b了，该公式就是<script type="math/tex">\begin{pmatrix}w_{21}&w_{22}&w_{23}&w_{24}\end{pmatrix}</script>乘<script type="math/tex">a^2</script>，<script type="math/tex">a^2</script>就是第二层的激活值的<strong>列</strong>向量表现形式，这很好理解就不解释了。运用考研线性代数的知识可以知道，行向量乘以列向量结果是一个<strong>数值</strong>，这样就得到了<script type="math/tex">z^3_2</script>的值。<br>我们再将第二层到第三层的其他向量补上就是如下的矩阵运算公式，为了方便查看，我将上标又加上了。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
z^3_1\\
z^3_2\\
\end{pmatrix} = 
\begin{pmatrix}
w^3_{11}&w^3_{12}&w^3_{13}&w^3_{14}\\
w^3_{21}&w^3_{22}&w^3_{23}&w^3_{24}\\
\end{pmatrix} * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix}</script><p>上式可以简化为<script type="math/tex">a^l = \sigma(z^l)</script>，其中<script type="math/tex">z^l</script> = <script type="math/tex">w * a^{l-1}</script>。合并之后写为<script type="math/tex">a^l = \sigma(w * a^{l-1})</script>，<script type="math/tex">\sigma</script>只是代表某个函数而已。</p>
<h4 id="如果不交换jk的位置"><a href="#如果不交换jk的位置" class="headerlink" title="如果不交换jk的位置"></a>如果不交换jk的位置</h4><p>绕了这么一大圈，终于说完了为什么jk交换位置要好，因为w被向量化后被表示成行向量了。行乘列直接就可以乘，不需要再对w转置。如果我们不交换jk的位置，那么w表示成如下形式：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix}</script><p>下面这样是乘不了的。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix} * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix} = 
个屁</script><p>如果下面这样就可以。T代表将矩阵转置。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{11}&w^3_{12}\\
w^3_{21}&w^3_{22}\\
w^3_{31}&w^3_{32}\\
w^3_{41}&w^3_{42}\\
\end{pmatrix}^T * 
\begin{pmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
a^2_4\\
\end{pmatrix} = 
\begin{pmatrix}
z^3_1\\
z^3_2\\
\end{pmatrix}</script><p>简化之后就是<script type="math/tex">\sigma(w^T</script> * <script type="math/tex">a^{l-1})</script> = <script type="math/tex">a^l</script>。所以绕了那么大一圈，其实只是为了少加一个T。。。</p>
<h4 id="w的下标总结"><a href="#w的下标总结" class="headerlink" title="w的下标总结"></a>w的下标总结</h4><p>说实话我还是希望加上T的，因为上面说了那么一大堆，说实话是很绕的，还不如直接不交换jk的位置。从各个方面来说都是极为通顺的，不妥的仅仅是多加了T。<br>最后还是无法理解别人为什么将jk交换位置的，可以试着不去想神经网络，完全去思考数学中的矩阵。我给点提示，对于元素<script type="math/tex">w^3_{24}</script>首先3和2是定死的，同理得到<script type="math/tex">w^3_{21}</script> <script type="math/tex">w^3_{22}</script> <script type="math/tex">w^3_{23}</script>，现在开始不要想关于神经网络的事，想想向量<script type="math/tex">\begin{pmatrix}w^3_{21}&w^3_{22}&w^3_{23}&w^3_{24}\end{pmatrix}</script>是不是行向量？<br>再想想下面的向量是不是列向量</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
w^3_{12}\\
w^3_{22}\\
w^3_{32}\\
w^3_{42}\\
\end{pmatrix}</script><p>我仅仅是交换了jk的位置对吧，虽然说数值没有任何变化，但是从数学角度讲，从行向量转为了列向量。</p>
<p>然而我觉得这样徒增了学习成本。我佛他们。</p>
<h3 id="计算输出的公式"><a href="#计算输出的公式" class="headerlink" title="计算输出的公式"></a>计算输出的公式</h3><p>公式比较简单，就是<script type="math/tex">a^l = \sigma(z^l)</script>，<script type="math/tex">z^l</script> = <script type="math/tex">w^T</script> * <script type="math/tex">a^{l-1}</script> + <script type="math/tex">b^l</script></p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.</title>
    <url>/bug/FutureWarning-Conversion-of-the-second-argument-of-issubdtype-from-float-to-np-floating-is-deprecated-In-future-it-will-be-treated-as-np-float64-np-dtype-float-type.html</url>
    <content><![CDATA[<p>网上很多人说要修改h5py的版本，但是我压根没装这个库。<br><a href="https://blog.csdn.net/u013092293/article/details/80447201" target="_blank" rel="noopener">参考文章</a><br>将numpy版本降低即可，我原先好像是1.16，记不清了，我没留意到。<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">conda install numpy==<span class="number">1.13</span><span class="number">.0</span></span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（六）：SVM</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9ASVM.html</url>
    <content><![CDATA[<p>看《机器学习实战》这本书的 SVM 部分，感觉始终无法理解，因为书里把数学公式的推导直接省略了。所以在b站找了视频学习 SVM。<br>本文<a href="https://zhuanlan.zhihu.com/p/24638007" target="_blank" rel="noopener">参考文章</a><br>本文<a href="https://www.bilibili.com/video/av37947862/?p=75" target="_blank" rel="noopener">参考视频</a></p>
<p>首先分割超平面（separating hyperplane）的函数表达式是<script type="math/tex">w^T * x + b = 0</script>，而它上下两条间隔最远的超平面的表达式分别为</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 1 \\
    w^T * x + b = & -1 \\
\end{align}</script><p>至于为什么正好等于 1 和 -1，其实是为了方便计算。实际上可以等于任何值。看以下推导，先让其等于连个随机的值，比如 2 和 -3：</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 2 \\
    w^T * x + b = & -3 \\
\end{align}</script><p>解一个不等式</p>
<script type="math/tex; mode=display">
\begin{align}
    2u + v = & 1 \\
    -3u + v = & -1 \\
\end{align}</script><p>解得 u = <script type="math/tex">\frac{2}{5}</script>， v = <script type="math/tex">\frac{1}{5}</script><br>于是使</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 2 \\
    w^T * x + b = & -3 \\
\end{align}</script><p>左右分别乘上<script type="math/tex">\frac{2}{5}</script>再加上<script type="math/tex">\frac{1}{5}</script>，即可得到</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x + b = & 1 \\
    w^T * x + b = & -1 \\
\end{align}</script><p>而 W 和 b 只不过是一个表示的符号而已，虽然经过运算，两个 W 和两个 b 已经不是同一个了，但是这么表示没太大问题。<br>将连个表达式更进一步表示为</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * x_1 + b = & 1 \\
    w^T * x_2 + b = & -1 \\
\end{align}</script><p>两者相减得</p>
<script type="math/tex; mode=display">
\begin{align}
    w^T * (x_1 - x_2) = 2 \\
\end{align}</script><p>也即</p>
<script type="math/tex; mode=display">
\begin{align}
    ||w^T|| * ||(x_1 - x_2)|| cos\theta = 2 \\
\end{align}</script><p>两条竖线代表，向量的模长，<script type="math/tex">\theta</script>代表 <script type="math/tex">W^T</script>和<script type="math/tex">x_1 - x_2</script>之间的夹角。<br>由于初中知识，我们知道：<script type="math/tex">cos\alpha</script>=邻边比斜边，即邻边=斜边*<script type="math/tex">cos\alpha</script>。所以</p>
<script type="math/tex; mode=display">
\begin{align}
    ||w^T|| * d = & 2 \\
    d = \frac{2}{||w^T||}
\end{align}</script><a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（五）：PCA</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9APCA.html</url>
    <content><![CDATA[<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（四）：K均值（K-means）</title>
    <url>/zcy/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9AK%E5%9D%87%E5%80%BC%EF%BC%88K-means%EF%BC%89.html</url>
    <content><![CDATA[<h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><p>K-均值(K-means)算法是无监督算法，也是聚类(clustering)算法。</p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li>随机初始化几个点作为簇的质心，初始化方式有多种，可以自行选择。已知两种：<ul>
<li>随机选择K个样本作为簇的质心。来源于吴恩达机器学习视频</li>
<li>min + (max - min) * (0到1之间的小数)，其中最大值最小值均代表。来源于《机器学习实战》</li>
</ul>
</li>
<li>计算某个样本到每个簇的质心之间的代价（距离），代价函数有多种可自行选择。比如：<ul>
<li>均方误差(mse)<br>选出其中最小的代价，并求出簇的索引，然后将该样本划分给该质心所属的簇。每个样本都执行这步直到样本遍历完毕。</li>
</ul>
</li>
<li>经过步骤2，每个样本都归属于一个簇。然后遍历每一个簇，将簇中的数据求均值，将该均值作为簇的新质心。</li>
<li>重复以上步骤，直到发现每个样本都归属于某个簇，并且样本归属不会再发生变化。<br>简化如下：<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line">随机初始化质心</span><br><span class="line">	<span class="keyword">while</span> <span class="literal">true</span></span><br><span class="line">		<span class="keyword">for</span> 每个样本</span><br><span class="line">			<span class="keyword">for</span> 每个质心</span><br><span class="line">				计算每个样本到质心的代价</span><br><span class="line">			选出最小的代价</span><br><span class="line">			将样本分配给这个簇</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span> 每个质心</span><br><span class="line">			求出该簇所属的样本的均值</span><br><span class="line">			将该均值设置为簇的新质点</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">if</span> 样本归属不再发生变化</span><br><span class="line">			<span class="keyword">break</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="举一个简单的例子"><a href="#举一个简单的例子" class="headerlink" title="举一个简单的例子"></a>举一个简单的例子</h3><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/kmeans/simple_demo" target="_blank" rel="noopener">这里有一个简单例子</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>selenium下打开Chrome闪退</title>
    <url>/bug/selenium%E4%B8%8B%E6%89%93%E5%BC%80Chrome%E9%97%AA%E9%80%80.html</url>
    <content><![CDATA[<p>网上的其他解决办法都不对，后来发现了是因为一个很逗的错。<br>以下代码闪退<br><figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="symbol">from</span> <span class="keyword">selenium </span><span class="meta">import</span> webdriver</span><br><span class="line"><span class="symbol">webdriver.Chrome</span>()</span><br></pre></td></tr></table></figure></p>
<p>以下代码正常运行<br><figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="symbol">from</span> <span class="keyword">selenium </span><span class="meta">import</span> webdriver</span><br><span class="line"><span class="symbol">chrome</span> = webdriver.Chrome()</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>使用WikiExtractor处理维基百科上的数据步骤-2019年3月</title>
    <url>/%E4%BD%BF%E7%94%A8WikiExtractor%E5%A4%84%E7%90%86%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE%E6%AD%A5%E9%AA%A4-2019%E5%B9%B43%E6%9C%88.html</url>
    <content><![CDATA[<p>网上有很多博客介绍如何使用开源工具wiki extractor解压提取维基百科上的数据，但是我试了一下他们的命令发现没一个能用的，而且他们对于该命令基本都是一笔带过，没有做深入的解释，对于我这种小白在第一步就卡死了。另外他们大部分人都是用linux系统，还有些使用Mac系统，用Windows的只有少数，而且他们提供的命令还不好用。所以我在此提供<strong>Windows系统</strong>的使用办法。</p>
<ol>
<li>数据来源：<a href="https://dumps.wikimedia.org/zhwiki/20190320/" target="_blank" rel="noopener">维基百科数据</a></li>
<li>首先进入<a href="https://github.com/attardi/wikiextractor" target="_blank" rel="noopener">wiki extractor的官网</a>。发现里面有很多py文件，与其他人写的博客上的教程完全不一样。别人的教程只有一个<em>WikiExtractor.py</em>文件。</li>
<li>将该项目clone下来，放在你的项目中。如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/wiki%20extractor%E6%96%87%E4%BB%B6%E5%AD%98%E6%94%BE%E8%B7%AF%E5%BE%84.png" alt="wiki extractor文件存放路径"><br>我下的是压缩包，解压开后就存放在wikiextractor-master文件夹。至于test.py只是用来测试gensim的word2vec算法好不好用的，无视就好。</li>
<li>进入wikiextractor-master文件夹执行<code>python setup.py install</code>，该步骤用于安装wiki extractor。其实<a href="https://github.com/attardi/wikiextractor" target="_blank" rel="noopener">wiki extractor的官网</a>也写了这一步。但是不知道为什么其他人的博客没人介绍。</li>
<li>退出该文件夹到nlp_learning文件夹，执行<code>python wikiextractor-master/WikiExtractor.py -b 1200M -o extracted zhwiki-20190320-pages-articles-multistream1.xml-p1p162886.bz2</code>，这个命令应该很好理解，说一下里面的extracted，它是目标文件夹，就是提取出来的文本存放的那个文件夹。可以看到上面的图片里有这个文件夹。-b代表每多大字节输出一份文件，参数的具体使用方法可以到<a href="https://github.com/attardi/wikiextractor#usage" target="_blank" rel="noopener">这里</a>查询。</li>
<li>接下慢慢等就行了，我的文件157MB，大概洗脸刷牙后就提取完了。</li>
</ol>
<p>参考了<a href="https://blog.csdn.net/grafx/article/details/78575850" target="_blank" rel="noopener">博客</a>，但是他这篇博客提供命令我也执行不了。如果你也成功不了，可以试试我的步骤。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title>运用文本相似度实现（证券）智能客服的记录</title>
    <url>/%E8%BF%90%E7%94%A8%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%AE%9E%E7%8E%B0%EF%BC%88%E8%AF%81%E5%88%B8%EF%BC%89%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E7%9A%84%E8%AE%B0%E5%BD%95.html</url>
    <content><![CDATA[<p>数据来源于维基百科。<br>数据来源：<a href="https://dumps.wikimedia.org/zhwiki/20190320/" target="_blank" rel="noopener">维基百科数据</a></p>
<h3 id="从维基百科提取数据"><a href="#从维基百科提取数据" class="headerlink" title="从维基百科提取数据"></a>从维基百科提取数据</h3><p>不会操作的，具体参考我这篇<a href="https://yan624.github.io/%E4%BD%BF%E7%94%A8WikiExtractor%E5%A4%84%E7%90%86%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE%E6%AD%A5%E9%AA%A4-2019%E5%B9%B43%E6%9C%88.html">博客:使用WikiExtractor处理维基百科上的数据步骤-2019年3月</a></p>
<h3 id="将数据从繁体字转为简体字"><a href="#将数据从繁体字转为简体字" class="headerlink" title="将数据从繁体字转为简体字"></a>将数据从繁体字转为简体字</h3><p>具体参考该博客：<a href="https://blog.csdn.net/sinat_29957455/article/details/81290356" target="_blank" rel="noopener">windows使用opencc中文简体和繁体互转</a>，我试过里面的教程，没有任何问题。直接将博客往下拉，看<em>3、OpenCC的使用</em>即可。执行命令后，命令行会没有任何反应，这时只是在跑代码而已，我的文件157兆，大概只用了15秒。我把我的命令贴下来：<br><figure class="highlight taggerscript"><table><tr><td class="code"><pre><span class="line">E:<span class="symbol">\p</span>ython_workspace<span class="symbol">\n</span>lp_learning<span class="symbol">\o</span>pencc-1.0.4<span class="symbol">\b</span>in<span class="symbol">\o</span>pencc -i E:<span class="symbol">\p</span>ython_workspace<span class="symbol">\n</span>lp_learning<span class="symbol">\z</span>hwiki-20190320-pages-articles<span class="symbol">\A</span>A<span class="symbol">\w</span>iki_00 -o E:<span class="symbol">\p</span>ython_workspace<span class="symbol">\n</span>lp_learning<span class="symbol">\z</span>hwiki-20190320-pages-articles<span class="symbol">\o</span>pencc<span class="symbol">\w</span>iki_00 -c E:<span class="symbol">\p</span>ython_workspace<span class="symbol">\n</span>lp_learning<span class="symbol">\o</span>pencc-1.0.4<span class="symbol">\s</span>hare<span class="symbol">\o</span>pencc<span class="symbol">\t</span>2s.json</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title>linux Ubuntu安装NVIDIA驱动</title>
    <url>/linux-Ubuntu%E5%AE%89%E8%A3%85NVIDIA%E9%A9%B1%E5%8A%A8.html</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/ghw15221836342/article/details/79571559" target="_blank" rel="noopener">参考文章</a><br>以下步骤基于ubuntu16，下载的文件在Downloads文件夹。</p>
<ol>
<li>使用命令<code>lspci |grep -i nvidia</code>，查看显卡型号</li>
<li>然后去<a href="http://www.nvidia.com/Download/index.aspx?lang=en-us" target="_blank" rel="noopener">官网</a>查找对应显卡版本。</li>
<li>下载NVIDIA驱动文件，名称类似为NVIDIA-Linux-x86_64-375.20.run。</li>
<li>卸载已有的驱动。由于我的是新机器，所以卸载步骤未执行。</li>
<li>然后使用命令禁用nouveau，使用以下命令编辑配置文件。<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">sudo <span class="keyword">vim</span> /etc/modprobe.d/blacklist.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>在最后一行添加： <code>blacklist nouveau</code></p>
<ol>
<li>之后执行<code>sudo update-initramfs -u</code>，完成之后需要重启电脑，重启电脑命令：<code>reboot</code>。电脑重启之后执行<code>lsmod |grep nouveau  #没有输出，即说明安装成功</code>.</li>
<li>接下来开始安装驱动。。。</li>
<li>使用<code>ctrl+alt+f3</code>命令进入命令行界面。</li>
<li><p>给驱动run文件赋予执行权限（若出现[sudo] 计算机名 ◆ ◆ ◆ ◆，这是因为安装了中文的ubuntu，输入登录密码即可）。使用如下命令：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">cd</span> <span class="selector-tag">Downloads</span></span><br><span class="line"><span class="selector-tag">sudo</span> <span class="selector-tag">chmod</span> <span class="selector-tag">a</span>+<span class="selector-tag">x</span> <span class="selector-tag">NVIDIA-Linux-x86_64-375</span><span class="selector-class">.20</span><span class="selector-class">.run</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>安装：<code>sudo ./NVIDIA-Linux-x86_64-375.20.run –no-opengl-files</code></p>
</li>
<li>一直选择OK、yes、continue按回车键</li>
<li>可能会出现没有gcc的错误，如果出现此问题，安装gcc即可。使用<code>sudo apt-get install gcc</code>安装。</li>
<li>可能出现没有make工具的错误。使用<code>sudo apt-get install make</code>安装</li>
<li><code>nvidia-smi</code>查看是否安装成功。<a id="more"></a></li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习算法（三）：决策树</title>
    <url>/zcy/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91.html</url>
    <content><![CDATA[<h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>由于新手不太懂术语，规定行为一条数据，列为一组特征。<br><strong>特征</strong>代表一列数据。如性别、年龄、身高等。<br><strong>分类</strong>代表一个特征中不同的分类。如性别中分类为男女，天气中分类为晴、阴、雨、雪等，收入中分类为贫困、低收入、小康、中高收入者、富人等。</p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>引用自《机器学习实战》</p>
<blockquote>
<p>划分数据集的大原则：将无需的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种办法就是使用信息论度量信息，信息论是量化处理信息的分支学科。我们可以在划分数据之前或之后使用信息论量化度量信息的内容。<br>在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。</p>
</blockquote>
<p>节选</p>
<blockquote>
<p>集合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父克劳德·香农。</p>
</blockquote>
<p><strong>下面这一段待修改，在进一步学习后发现有问题</strong></p>
<p>如果不懂什么是信息增益（information）和熵（entropy）也无所谓，只需要知道熵越大则不确定性越大即可。因为我们需要的是不确定性小的<strong>特征</strong>，所以熵越小越好。试想决策树就是将信息一分为二，是不是最好某一个特征只有两种<strong>分类</strong>，如男、女。这样不确定性是极小的，因为它只能分为男和女。如果某一个特征有多个分类，如鸟类下有多种详细的分类，那么该特征的熵值就会很大，不确定性也很大。想想决策树就是将信息一分为二，我该选哪条线将鸟类一分为二？这是很难想的事情，所以这个特征并不好。以上只是举例，在具体项目中不一定对，并且决策树也不一定是二叉树（此处存疑？）。<br>熵的计算公式如下，其中pi是选择该分类的概率，即有20条数据，其中13条是男性，7条是女性。则男性的pi=13/20，女性的pi=7/20。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E5%85%AC%E5%BC%8F.jpg" alt="信息熵的公式"></p>
<h3 id="决策树构建步骤"><a href="#决策树构建步骤" class="headerlink" title="决策树构建步骤"></a>决策树构建步骤</h3><p>决策树构建首先需要一个根节点，这是毋庸置疑的，但是根节点也不是一拍脑门就突然有了的。需要使用一些算法，现在一些常用的算法如：ID3(信息增益)、C4.5(信息增益率，解决ID3的问题，考虑自身熵)、CART(使用GINI系数来当做衡量标准，GINI系数和熵的衡量标准类似，但是计算方式不同)等。<br>以下使用<a href="https://baike.baidu.com/item/ID3%E7%AE%97%E6%B3%95/5522381" target="_blank" rel="noopener">ID3算法</a>，<strong>给出如下训练数据</strong>，来源于唐宇迪机器学习视频中的截图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4%E2%80%94%E2%80%94%E4%B8%BE%E7%9A%84%E4%BE%8B%E5%AD%90%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE.jpg" alt="例子中的数据"></p>
<ol>
<li>首先计算全部数据的熵值，其实就是计算一下给出的数据中的最后一列（最后一列可以当做y）的熵值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E7%9A%84%E7%86%B5%E5%80%BC.jpg" alt="计算原始数据的熵值"></li>
<li>计算其他特征的熵值。解释一下什么叫其他特征的熵值：比如 outlook 这个特征值，如果要计算它，首先需要知道 outlook 这个特征值分为 3 类，sunny、overcast 和 rainy，然后计算依次计算这三种类别的熵值。具体步骤是，比如计算 sunny 的熵值，那就将原数据中 outlook 不是 sunny 类别的数据全部删除，然后使用信息熵的公式计算剩余数据的熵值。其他的类别计算方式以此类推。计算完毕之后就得到了三个熵值，然后按照类别在特征中的比例相加即可。<br>&emsp;&emsp;如下图右侧，当天气为 sunny 时去打球的概率为<script type="math/tex">\frac{2}{5}</script>，不去打球的概率为<script type="math/tex">\frac{3}{5}</script>，利用上面的熵值公式计算结果为 0.971。再分别计算 outlook 其他<strong>分类</strong>：overcast 和 rainy 的熵值，此时得到了三个不同分类的熵值，即 sunny为<script type="math/tex">\frac{5}{14}</script>， overcast为<script type="math/tex">\frac{4}{14}</script>。最终 outlook 的熵值即为<script type="math/tex">\frac{5}{14} * 0.971 + \frac{4}{14} * 0 + \frac{5}{14} * 0.971 = 0.693</script>。最后其他特征值也需要分别计算它们的分类的熵值。<br>&emsp;&emsp;<strong>这里可能会出现困惑</strong>，如果每个特征值都有多个分类，那计算量不是特别大？而且这里的分类是文字，计算量较小，如果分类是数字那么分类几乎不会重复，计算量不是突破天际？我在学决策树时出现了这种疑惑，后来发现算法确实是这样的，并没有错。<br>&emsp;&emsp;分类为文字称为<strong>离散值</strong>，分类为数值称为<strong>连续值</strong>。上面说到决策树碰到连续值计算量会非常大，解决办法：<a href="https://www.baidu.com/s?wd=%E5%86%B3%E7%AD%96%E6%A0%91%E8%BF%9E%E7%BB%AD%E5%80%BC%E5%A4%84%E7%90%86" target="_blank" rel="noopener">决策树连续值处理</a><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E5%85%B6%E4%BB%96%E7%89%B9%E5%BE%81%E7%9A%84%E7%86%B5%E5%80%BC.jpg" alt="计算其他特征的熵值"><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4%E2%80%94%E2%80%94%E5%85%B6%E4%BB%96%E7%89%B9%E5%BE%81%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%88%86%E7%B1%BB.jpg" alt="其他特征以及其分类"></li>
<li>通过1和2对比，选择出熵值最低的特征，并且熵值需要小于原始数据的基本熵值。<br>&emsp;&emsp;上述123步，均可以直接在choose_best_feature_index(dataset)函数中计算获得，顶多加几个封装过的函数，比如calculate_entropy(dataset)，split_dataset(dataset, axis, value)等。</li>
<li>删除已作出决策的数据，并将数据再次执行123步操作。<br>&emsp;&emsp;比如经过一轮筛选发现 outlook 为熵值最低的特征，则按照 outlook 的 3 个类别，决策树将会有 3 条分支，节点与节点连接的边上的值分别是 sunny、overcast 和 rainy。<br>&emsp;&emsp;首先按类别 sunny 继续往下决策，比如所有数据中，类别为 sunny 的数据全部为 yes，即想要出去打球，那么决策树递归结束。因为这个算法的目的就是要判断在这些特征下，这一天去打球是否合适，如果 sunny 下的每条数据都是 yes，就代表我们的目的已经达到了，也就没必要继续决策了。接下来假设类别 overcast 与 sunny 相反，那么就将类别是 overcast 的所有数据从原数据集中删去，再重复 123 步。</li>
<li>构造决策树，决策树可以使用递归算法构建。<br>&emsp;&emsp;新手入门系列，完全可以仅使用python的dict来存储决策树。如：{‘tearRate’: {‘reduced’: ‘no lenses’, ‘normal’: {‘astigmatic’: {‘no’: {‘age’: {‘young’: ‘soft’, ‘pre’: ‘soft’, ‘presbyopic’: {‘prescript’: {‘hyper’: ‘soft’, ‘myope’: ‘no lenses’}}}}, ‘yes’: {‘prescript’: {‘hyper’: {‘age’: {‘young’: ‘hard’, ‘pre’: ‘no lenses’, ‘presbyopic’: ‘no lenses’}}, ‘myope’: ‘hard’}}}}}}</li>
</ol>
<h3 id="举一个简单的例子"><a href="#举一个简单的例子" class="headerlink" title="举一个简单的例子"></a>举一个简单的例子</h3><p><a href="https://github.com/yan624/machine_learning_algorithms/tree/master/decision_tree/simple_demo" target="_blank" rel="noopener">这里有一个简单例子</a><br>运行结果：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%80%E5%8D%95%E4%B8%BE%E4%BE%8B%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C%E5%9B%BE.png" alt="决策树简单举例最终结果图"></p>
<h3 id="其他还未解决的问题"><a href="#其他还未解决的问题" class="headerlink" title="其他还未解决的问题"></a>其他还未解决的问题</h3><ul>
<li>决策树处理缺失值</li>
<li>决策树处理连续值</li>
<li>剪枝的问题<a id="more"></a></li>
</ul>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（二）：逻辑回归</title>
    <url>/zcy/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.html</url>
    <content><![CDATA[<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法（一）：线性回归</title>
    <url>/zcy/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><ul>
<li>优点：结果易于理解，计算上不复杂。</li>
<li>缺点：对非线性的数据拟合不好。</li>
<li>适用数据类型：数值型和标称型数据。</li>
</ul>
<p>&emsp;&emsp;比如预测汽车的功率，可能会这么计算：<code>horse_power = 0.0015 * annual_salary - 0.99 * hours_listening_to_public_radio</code>。<br>&emsp;&emsp;这就是所谓的<strong>回归方程（regression equation）</strong>，其中 0.0015 和 -0.99 称作<strong>回归系数（regression weights）</strong>，求这些回归系数的过程就是回归。</p>
<blockquote>
<p>回归一词的由来：是由达尔文（Charles Darwin）的表兄弟 Francis Galton 发明的。</p>
</blockquote>
<p>&emsp;&emsp;线性回归的代价函数是：</p>
<script type="math/tex; mode=display">
cost = \sum^m_{i = 1}(y_i - x^T_i W)^2</script><p>&emsp;&emsp;用矩阵可以表示为 <script type="math/tex">(y - XW)^T(y - XW)</script>，如果对 W 求导就可以得到 <script type="math/tex">X^T(Y - XW)</script>。令其等于 0，解得 W:</p>
<script type="math/tex; mode=display">
\hat{W} = (X^TX)^{-1} X^Ty</script><p>&emsp;&emsp;这公式也可以被称为<strong>正规方程</strong>。需要注意的点是 <script type="math/tex">(X^TX)^{-1}</script> 是一个求逆的过程，但是矩阵求逆在有些情况下是无解的，所以 <script type="math/tex">(X^TX)^{-1}</script> 必须有解才可以使用正规方程，如果无解就不能直接使用正规方程。<br>&emsp;&emsp;上述求解过程被称为最小二乘法（ordinary least squares），简称 OLS。</p>
<h1 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h1><p>&emsp;&emsp;线性回归的一个问题是有可能出现欠拟合现象。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方差。其中一个方法是局部加权线性回归（Locally Weighted Linear Regression, LWLR），<strong>该算法给待预测点附近的每个点赋予一定的权重</strong>。所以正规方程的公式变为：</p>
<script type="math/tex; mode=display">
\theta = (X^TWX)^{-1}X^TWy</script><p>&emsp;&emsp;需要说明一点省得搞混了，刚才我自己都乱了，<em>这里引入的偏差并不是线性方程 y = WX + b 中的偏差</em>。公式中的 W 是一个矩阵，用来给每个数据点赋予权重。LWLR 使用“核”（与支持向量机中的核类似）来<u>对附近的点赋予更高的权重</u>。核的类型可以自由选择，最常用的核就是高斯核，对应权重如下（<strong>那两竖不是求绝对值，而是求模长</strong>）：</p>
<script type="math/tex; mode=display">
w(i, i) = exp(\frac{|x^{(i)} - x|}{-2k^2})</script><p>&emsp;&emsp;这样就构建了一个只含对角的权重矩阵 W，并且点 x 与 <script type="math/tex">x^{(i)}</script> 越近，w(i, i)将会越大。上述公式<strong>只</strong>包含一个需要用户指定的参数 k，它决定了对附近的点赋予多大的权重。对高斯核公式的解释：</p>
<ol>
<li>W 是一个对角矩阵，因为高斯核每次赋值都是在 (i, i)点上赋值。主要说明 <script type="math/tex">|x^{(i)} - x|</script> 部分，其他部分都很好理解。首先说明 x 代表待预测的点，其次开始遍历每一项数据，数据记为t，计算每一个 |t - x| ，这样就得到了一个对角矩阵。意思就是通过待预测点经过一系列运算，为所有数据加上了一个权重。当然这里肯定也计算自己本身，不过自己减自己就是等于 0，e 的 0 次方等于1，所以就等于没有给自己加权重。最后使用公式 <script type="math/tex">(X^TWX)^{-1}X^TWy</script> 求出了解，也就是回归方程的回归系数 W。这里面有两个一模一样的 W，但是意思完全不同，应该可以理解，我就不改了。有了回归系数就可以做预测了，prediction = XW。结束。</li>
<li>这样一来就计算好了第一个数据，注意：假设我们有 200 条数据，上述的步骤仅仅才计算了第一条数据，接着取出第二条数据 x（待预测点），再经过上述的步骤，又得到了回归系数 W，做预测，结束。</li>
<li>如此反复，直到取出第 200 条数据 x（待预测点），再经过上述的步骤，又得到了回归系数 W，做预测，结束。</li>
<li>这部分理解起来比较麻烦，故下面放出代码。<strong>从for 循环开始到 return 前都是上述123步的代码版本</strong>。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LWLR</span><span class="params">(X, Y, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    局部加权线性回归，Locally Weighted Linear Regression</span></span><br><span class="line"><span class="string">    :param X: input</span></span><br><span class="line"><span class="string">    :param Y: label</span></span><br><span class="line"><span class="string">    :param k: 超参数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        prediction: 预测值</span></span><br><span class="line"><span class="string">        W: 回归系数，占个位置，可能以后会有用，现在没卵用</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    X, Y = np.matrix(X), np.matrix(Y)</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(np.dot(X.T, X)) == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"矩阵不可逆"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    weights = np.matrix(np.eye(m))</span><br><span class="line">    prediction = np.empty((m, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 使用第 i 个数据点对包括自己在内的每个数据点赋予权重，对自己就是赋予 1 的权重值，也就是说等于没赋予。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        predict_dot = X[i, :]</span><br><span class="line">        <span class="comment"># 计算权重</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 这是高斯核的公式</span></span><br><span class="line">            difference = X[j, :] - predict_dot</span><br><span class="line">            weights[j, j] = np.exp((difference * difference.T) / (<span class="number">-2</span> * (k ** <span class="number">2</span>)))</span><br><span class="line">        W = (X.T * weights * X).I * (X.T * weights * Y)</span><br><span class="line">        prediction[i] = predict_dot * W</span><br><span class="line">    <span class="keyword">return</span> prediction, np.empty((m, m, m))</span><br></pre></td></tr></table></figure>
<h1 id="岭回归（L2）"><a href="#岭回归（L2）" class="headerlink" title="岭回归（L2）"></a>岭回归（L2）</h1><p>&emsp;&emsp;岭回归中的岭是什么？</p>
<blockquote>
<p>岭回归使用了单位矩阵乘以常量 <script type="math/tex">\lambda</script>，观察单位矩阵 I，发现值 1 贯穿这个对角线，其余元素全是 0。形象地，在 0 构成的平面上有一条由 1 组成的“岭”，这就是岭的由来。</p>
</blockquote>
<p>&emsp;&emsp;简单来说，岭回归就是在矩阵 <script type="math/tex">X^TX</script> 上加上一个 <script type="math/tex">\lambda I</script>从而使得矩阵非奇异。进而能对 <script type="math/tex">X^X + \lambda I</script>求逆，其中 I 是一个单位矩阵，而 <script type="math/tex">\lambda</script> 是用户定义的数值。正规方程变为：</p>
<script type="math/tex; mode=display">
\theta = (X^TX + \lambda I)^{-1}X^Ty</script><p>&emsp;&emsp;这里发现这个公式与加入 L2 正则化一样。（经百度之后，发现还真一样，所以这部分就不细写了）<br>&emsp;&emsp;加入 L2 正则化的代价函数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    cost & = \sum^m_{i = 1}(y_i - x^T_i W)^2 + \lambda ||W||_2\\
    & = \sum^m_{i = 1}(y_i - x^T_i W)^2 + \lambda \sum^m_{j = 1}w^2_j
\end{aligned}</script><p>&emsp;&emsp;<a href="https://www.zhihu.com/question/28221429" target="_blank" rel="noopener">知乎问题</a></p>
<h1 id="lasso-L1"><a href="#lasso-L1" class="headerlink" title="lasso(L1)"></a>lasso(L1)</h1><p>略。<br>补充，参考<a href="https://www.cnblogs.com/mantch/p/10242077.html" target="_blank" rel="noopener">文章</a>，发现岭回归其实就是在<strong>代价函数</strong>（<u>注意是代价函数，不是正规方程</u>）中加入了 L2 正则化，lasso 其实就是在<strong>代价函数</strong>中加入了 L1 正则化。</p>
<h1 id="前向逐步回归"><a href="#前向逐步回归" class="headerlink" title="前向逐步回归"></a>前向逐步回归</h1><a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop学习记录（一）：Hadoop集群搭建</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AHadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html</url>
    <content><![CDATA[<h3 id="使用jps，发现没有namenode"><a href="#使用jps，发现没有namenode" class="headerlink" title="使用jps，发现没有namenode"></a>使用jps，发现没有namenode</h3><p>使用命令<code>/opt/hadoop/sbin/start-all.sh</code>启动hadoop，之后使用jsp，一般来讲会出现如下6个线程<br><figure class="highlight basic"><table><tr><td class="code"><pre><span class="line"><span class="symbol">55598 </span>Jps</span><br><span class="line"><span class="symbol">54490 </span>NameNode</span><br><span class="line"><span class="symbol">54684 </span>DataNode</span><br><span class="line"><span class="symbol">54931 </span>SecondaryNameNode</span><br><span class="line"><span class="symbol">55332 </span>NodeManager</span><br><span class="line"><span class="symbol">38251 </span>ResourceManager</span><br></pre></td></tr></table></figure></p>
<p>但是我发现我运行后没有出现namenode。仔细观察中断启动时的日志发现，namenode的日志记录在<strong>/opt/hadoop/logs/hadoop-zhangyu-namenode-a2d8c523dd0b.log</strong>中，打开它，发现在最后居然报了异常。由于Hadoop是java写的，异常很好认（我就是学java出身的）。<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">java<span class="selector-class">.io</span><span class="selector-class">.IOException</span>: There appears to be <span class="selector-tag">a</span> gap <span class="keyword">in</span> the edit log.  We expected txid <span class="number">1</span>, but got txid <span class="number">16</span>.</span><br></pre></td></tr></table></figure></p>
<p>百度之后发现namenode的元数据破坏，需要恢复，具体参考<a href="https://blog.csdn.net/u011478909/article/details/51864071" target="_blank" rel="noopener">某csdn博客</a>。<br>使用<code>hadoop namenode -recover</code>即可恢复。</p>
<h3 id="使用jps，发现没有datanode"><a href="#使用jps，发现没有datanode" class="headerlink" title="使用jps，发现没有datanode"></a>使用jps，发现没有datanode</h3><p>总结写在前：<br>由于意外hadoop没有创建data文件夹，执行<code>cp -r /data/tmp/hadoop/tmp/dfs/data /data/tmp/hadoop/hdfs</code>，并修改clusterId，clusterId在<strong>/data/tmp/hadoop/hdfs/data/current/VERSION</strong>中。</p>
<p>在执行上面的操作修复namenode之后，发现原来在的datanode居然不见了。还是使用老办法发现日志文件（日志文件名字太长就不写了），异常：<code>All specified directories are failed to load。</code>。经过百度之后发现是datanode和namenode配置文件的VERSION文件中的clusterId不一致。所以只需要将clusterId更改成一致即可。<br>但是我的出现的问题重点不是这个，我在修改clusterId时发现：我的namenode配置在/data/tmp/hadoop/hdfs/name中。但是检查我的datanode配置是否在/data/tmp/hadoop/hdfs/data中时，发现hadoop根本没有创建这个文件夹。于是又进行排查，发现在/data/tmp/hadoop/tmp/dfs中存在data文件夹。于是使用命令<code>cp /data/tmp/hadoop/tmp/dfs/data /data/tmp/hadoop/hdfs</code>，将这份data文件夹拷贝到/data/tmp/hadoop/hdfs，并更改clusterId。<strong>VERSION文件在/data/tmp/hadoop/hdfs/data/current</strong></p>
<a id="more"></a>]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda的锅：使用pyquery出现urlopen-error-unknown-url-type-https</title>
    <url>/bug/anaconda%E7%9A%84%E9%94%85%EF%BC%9A%E4%BD%BF%E7%94%A8pyquery%E5%87%BA%E7%8E%B0urlopen-error-unknown-url-type-https.html</url>
    <content><![CDATA[<p>使用pyquery出现urlopen-error-unknown-url-type-https的异常，上网查发现是openssl的问题。话不多说直接说结果。<br>参考文章如下：<br><a href="https://blog.csdn.net/gw85047034/article/details/88039705" target="_blank" rel="noopener">anaconda新建环境在PyCharm执行import ssl失败</a></p>
<p>由于anaconda的python版本是3.7.0，而我在创建虚拟环境时，使用命令<code>conda create -n spider python=3</code>，最后的python=3会选择python最新的版本安装，所以anaconda选择了python3.7.3。于是anaconda的python版本和虚拟环境的python版本冲突了，而我把虚拟环境给PyCharm用了，所以就是anaconda和PyCharm冲突了。只要把虚拟环境的python版本换成和anaconda的python版本一致就可以了。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>error</tag>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫学习记录（一）：正则表达式</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html</url>
    <content><![CDATA[<h3 id="正则表达式的使用"><a href="#正则表达式的使用" class="headerlink" title="正则表达式的使用"></a>正则表达式的使用</h3><p>正则表达式的匹配规则网上都有，这里不放出来了，毕竟只是记录学习的博文。<br>以下讲几个一直没搞明白的点。</p>
<h4 id="贪婪与非贪婪"><a href="#贪婪与非贪婪" class="headerlink" title="贪婪与非贪婪"></a>贪婪与非贪婪</h4><p>实现介绍通用匹配，以<code>.*</code>表示。<code>.</code>代表匹配任意字符，<code>*</code>代表匹配0个或多个表达式所以两个连用就代表匹配任意字符。比如字符串<code>&#39;Hello Python and Anaconda&#39;</code>，表达式为<code>&#39;^Hello.*Anaconda$&#39;</code>。代码如下：<br><figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">txt = <span class="string">'Hello Python and Anaconda'</span></span><br><span class="line">result = re.match(<span class="string">"^Hello.*Anaconda$"</span>, txt)</span><br><span class="line"><span class="keyword">print</span>(result.<span class="keyword">group</span>())</span><br></pre></td></tr></table></figure></p>
<p>可以得出我们想要的结果，因为<code>.*</code>代表匹配任意个字符，所以该正则表达式会在<strong><em>Hello</em></strong>之后一直匹配成功，直到遇到<strong><em>Anaconda</em></strong>停止。其中<code>.*</code>其实就是贪婪匹配。但是有时候贪婪匹配会出很大的问题。比如字符串<code>&#39;My tel number is 15012345678&#39;</code>，正则表达式为<code>&#39;^My.*(\d+)$&#39;</code>。具体代码如下：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">txt = <span class="string">'My tel number is 15012345678'</span></span><br><span class="line">result = re.match(<span class="string">"^My.*(\d+)$"</span>, txt)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group()</span></span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group(<span class="number">1</span>)</span></span>)</span><br></pre></td></tr></table></figure></p>
<p>首先这段代码意思是获取字符串中的手机号码，<code>(\d+)</code>代表一个组，这个组里面匹配的东西就是我们需要的结果，而我们的代码也看起来很正常<code>\d+</code>就意味着匹配多个数字。可以看到这段代码<code>print(result.group(1))</code>就是输出手机号。但是结果却只匹配到了8，因为<code>.*</code>代表了贪婪匹配，它会尽可能的匹配到多的字符。因为<code>.*</code>代表匹配任意多的字符串，所以它看到1501234567会一直往下匹配，知道看到8，发现8已经是最后一个数字了，如果这个8不与<code>\d+</code>匹配岂不是出现系统漏洞了，因为<code>\d+</code>就是让你匹配数字，你不匹配不是违反了规则？所以匹配成功。最后将8加入组中，通过result.group(1)获取到这一组。所以贪婪匹配在这种情况是有问题，这就引出了非贪婪匹配：<code>.*?</code>。代码如下：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">txt = <span class="string">'My tel number is 15012345678'</span></span><br><span class="line">result = re.match(<span class="string">"^My.*?(\d+)$"</span>, txt)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group()</span></span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group(<span class="number">1</span>)</span></span>)</span><br></pre></td></tr></table></figure></p>
<p>如果运行这段代码，就会发现匹配成功了。所以以后尽量使用非贪婪模式<code>.*?</code>，而不是贪婪模式<code>.*</code>。</p>
<h4 id="修饰符"><a href="#修饰符" class="headerlink" title="修饰符"></a>修饰符</h4><p>代码<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">txt = <span class="string">'My tel number is 15012345678'</span></span><br><span class="line">result = re.match(<span class="string">"^My.*?(\d+)$"</span>, txt, re.S)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group()</span></span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(result.group(<span class="number">1</span>)</span></span>)</span><br></pre></td></tr></table></figure></p>
<p>这里引用了上面的代码，注意多了个re.S，它代表使.匹配包括换行在内的所有字符。也就是说不加re.S，.其实是匹配不到换行符的。应用场景：网页代码中嵌套标签会经常出现这些换行符，要加入re.S。其他的方法re.search(),re.findall()用法类似。<br>待补充。。。</p>
<h4 id="re模块中的方法"><a href="#re模块中的方法" class="headerlink" title="re模块中的方法"></a>re模块中的方法</h4><ol>
<li>match()<br>这方法从头开始匹配，如果一开始即第一个字符就不匹配，直接就算匹配失败。可以用于校验用户输入的信息是否合乎规范。</li>
<li>search()<br>从头开始匹配，扫描整个字符串，如果碰到一段字符串匹配成功了就立即返回，它只会返回一个结果。</li>
<li>findall()<br>跟它名字一样，扫描整个字符串，获得所有匹配成功的结果。</li>
<li>待补充。。。</li>
</ol>
<h3 id="举一个栗子"><a href="#举一个栗子" class="headerlink" title="举一个栗子"></a>举一个栗子</h3><p>爬取牛客网的技术栈。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda安装问题以及之前安装的Python环境取舍问题</title>
    <url>/bug/anaconda%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%E4%BB%A5%E5%8F%8A%E4%B9%8B%E5%89%8D%E5%AE%89%E8%A3%85%E7%9A%84Python%E7%8E%AF%E5%A2%83%E5%8F%96%E8%88%8D%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>今天安装anaconda时发现了一些问题。</p>
<h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>我在安装anaconda之前，已经安装了Python3.6，这个Python环境是很久以前还在学Java时安装的。然后现在安装完anaconda之后，发现anaconda不会自动将原有的Python环境收为己用。所以就出现了原有的Python怎么处理的问题。</p>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>如果原先的Python环境不是很重要，可以直接删了，改用anaconda提供的虚拟环境。当然同时也要删除电脑环境变量中的Python路径。为了保险可以先删除电脑环境变量中的Python路径，然后打开命令行，输入python，发现已经移除后再删除Python。最后还需要将anaconda添加到环境变量中，不过anaconda在安装时有个选项可以直接将其添加进环境变量，如果之前选择了这个选项。可以不用再添加环境变量了。anaconda环境变量总共需要输入三个变量，如下：<br><figure class="highlight taggerscript"><table><tr><td class="code"><pre><span class="line">D:<span class="symbol">\a</span>naconda</span><br><span class="line">D:<span class="symbol">\a</span>naconda<span class="symbol">\S</span>cripts</span><br><span class="line">D:<span class="symbol">\a</span>naconda<span class="symbol">\L</span>ibrary<span class="symbol">\b</span>in</span><br></pre></td></tr></table></figure></p>
<p>注：我所有的编程软件、工具等等都放在D盘。<br>最后参考以下两篇，其中有anaconda安装教程：<br><a href="https://www.jianshu.com/p/eaee1fadc1e9" target="_blank" rel="noopener">Anaconda完全入门指南</a><br><a href="https://blog.csdn.net/qq_37025885/article/details/79158153" target="_blank" rel="noopener">如何在已安装Python条件下，安装Anaconda，并将原有Python添加到Anaconda中</a></p>
<p>其中，他们还创建了其他的虚拟环境，但是我觉得base环境用用算了，因为我刚学python，还不太懂。<br>至于anaconda可以创建多个虚拟环境，虽然我没用过anaconda，但是我估计理念跟github差不多。想要一个环境直接从别人那拷过来，就不需要自己安装了。<br>注意记得将Pycharm中的python环境改为anaconda的虚拟环境。其他的虚拟环境路径在D:\anaconda\envs。base环境在：D:\anaconda\python.exe<br>如下图，进入设置界面，左上角File进入。点OK就行了。图中有个叫Virtualenv Environment的选项，这个看起来是虚拟环境的意思，其实是python本身自带的。它跟anaconda功能一样也是创建一个虚拟环境，但是这是python提供的。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/f095a0ecb23b43dfa99e2a82b720c6ff.jpg" alt="创建Python环境" title="创建Python环境"></p>
<p>最后，在随便一个音乐平台、网站、软件搜索anaconda，歌手Nicki Minaj。不用谢我(ฅωฅ*)。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>spring boot中使用@ConfigurationProperties注解</title>
    <url>/spring%20boot%E4%B8%AD%E4%BD%BF%E7%94%A8%20ConfigurationProperties%E6%B3%A8%E8%A7%A3.html</url>
    <content><![CDATA[<p>在使用@ConfigurationProperties注解前必须引入这个依赖<br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 为了使用@ConfigurationProperties注解，必须加入这个--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-configuration-processor<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>使用方法：<br><figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="variable">@Component</span></span><br><span class="line"><span class="variable">@ConfigurationProperties</span>(prefix = <span class="string">"io.github.yan624.file"</span>)</span><br><span class="line"><span class="variable">@Data</span></span><br><span class="line">public class PropertiesConfig &#123;</span><br><span class="line">    <span class="selector-tag">private</span> <span class="selector-tag">String</span> <span class="selector-tag">root</span>;</span><br><span class="line">    <span class="selector-tag">private</span> <span class="selector-tag">String</span> <span class="selector-tag">qrPath</span>;</span><br><span class="line">    <span class="selector-tag">private</span> <span class="selector-tag">String</span> <span class="selector-tag">imgPath</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>yml文件：<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">io:</span></span><br><span class="line"><span class="attr">  github:</span></span><br><span class="line"><span class="attr">    yan624:</span></span><br><span class="line"><span class="attr">      file:</span></span><br><span class="line"><span class="attr">        root:</span> <span class="attr">E:/itchat4j/</span></span><br><span class="line"><span class="attr">        qr-path:</span> <span class="string">$&#123;io.github.yan624.file.root&#125;/login</span></span><br><span class="line"><span class="attr">        img-path:</span> <span class="string">$&#123;io.github.yan624.file.root&#125;/img</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure></p>
<p>其中@Data是lombok插件的注解，用于生成get/set方法<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring注解</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习2017春学习记录及作业：octave实现</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E6%98%A5%E4%BD%9C%E4%B8%9A%EF%BC%9Aoctave%E5%AE%9E%E7%8E%B0.html</url>
    <content><![CDATA[<p>所有代码在<a href="https://github.com/yan624/lihongyi-machine-learning-2017-homework" target="_blank" rel="noopener">lihongyi-machine-learning-2017-homework</a>上</p>
<h3 id="学习情况和做题背景"><a href="#学习情况和做题背景" class="headerlink" title="学习情况和做题背景"></a>学习情况和做题背景</h3><h4 id="学习情况"><a href="#学习情况" class="headerlink" title="学习情况"></a>学习情况</h4><p>先看了<strong>《深度学习》（花书）</strong>这本书，由于我是零基础学习该方向，感觉这本书除了前几章数学方面略看得懂，其他的一脸懵逼。由于本人在学习某一知识之前会预先搜集资料。于是我打开了在知乎收藏的文章，点进了<strong>吴恩达机器学习</strong>网易云课堂。我看了几个算法：线性回归，逻辑回归，神经网络。并且做了线性回归和逻辑回归的练习题。个人感觉吴恩达的视频很适合入门，但是想要仔细理解太难了。我根据吴恩达老师提供的题目，照葫芦画瓢做了一下，做还是做出来了，但是这都是很简单的例子。大都是只有三四个参数，所以感觉只是入门了。<br>这时候我再去看<strong>《深度学习》</strong>这本书，发现有些内容我看得懂了。但是有只是有些而已，往后看还是无法理解。至此，在寒假中我彻底放弃继续看<strong>《深度学习》</strong>。这个时候我又继续搜集资料，我发现了另一个教学视频就是<strong>李宏毅机器学习</strong>，经过自己的查询，最终决定看这个。李宏毅老师讲的略微会难一点，因为他面向的是研究生，这个从他的课程介绍上可以看得出。可以对比一下，</p>
<ul>
<li><strong>吴恩达机器学习</strong>不会涉及任何数学部分，高数、线代、概率论在吴恩达老师的课堂上只会出现名词和推导过程，他不会解释为什么，他假设看这视频的人都不懂数学。因为他的课程面向的是所有人。而且他的课程当中很多知识都是讲解基础知识，深入的东西他会只介绍名词，甚至不会介绍。所以<strong><em>很</em></strong>适合做入门视频，<strong>最重要的是</strong>他的视频都很短，最长的也只有十几分钟。</li>
<li><strong>李宏毅机器学习</strong>讲的比吴恩达的深入许多，他会讲解原理，而且讲的也很清楚，并且数学部分他也不会跳过，而且他都是假设你会这部分的数学知识。另外他讲课是讲例子的，主要的例子是预测神奇宝贝进化后的cp值（大概就是战斗力的意思），还会出现一些游戏如帝国时代、我的世界等。这些都会使得课堂很有意思。在b站有他的视频，不过都很长，一个视频打都有60来分钟，有少数极短。最后他的作业数据量大做起来很有意思。<br>所以我建议先看吴恩达老师的机器学习视频看到神经网络，先入个门要不然看其他人的教学视频可能跟不上节奏，然后看李宏毅老师视频，注意完成他的作业，会很好的帮助自己理解。</li>
</ul>
<h4 id="做题背景"><a href="#做题背景" class="headerlink" title="做题背景"></a>做题背景</h4><p>做完吴恩达老师布置的作业还以为已经懂了什么是线性回归，没想到做李宏毅老师的作业时发现连开个头都困难。在自己瞎做之后，经过五六天完成了作业，但是期间出现了大量问题，所以写这个博文用来纪录。</p>
<h3 id="hw1-PM2-5预测"><a href="#hw1-PM2-5预测" class="headerlink" title="hw1:PM2.5预测"></a>hw1:PM2.5预测</h3><h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><ol>
<li>李宏毅老师给的数据，没看懂是什么意思。可能是吴恩达老师和李宏毅老师的概念有一些处理，看了很久没理解这个作业的意思。这个没办法，我自己琢磨了一会才搞懂。</li>
<li>开头第一步，完全不知道干什么。<strong>解决</strong>：选函数。吴恩达称之为hypothesis函数，李宏毅称之为模型(Model)，都一个意思，以下称为hypothesis。</li>
<li>hypothesis函数需要一些特征值，即X。当然还有一个theta。但是特征那么多总不能全选吧，而且是几次方的式子也确定不了。<strong>解决</strong>：将每一个特征和需要预测的y，使用画图函数画出来，特征x作为横坐标，待预测y作为纵坐标。然后观察哪些图像关系比较正常，就用哪个特征（比如呈线性关系的就比较正常）。这也是看到其他人的博客才发现特征是这么选的，之前做了几天都是凭感觉选的。。。</li>
<li>做了半天才发现代码写错了。<strong>解决</strong>：做的时候不要太急，把代码好好检查一下。倒不是程序错误，也不是数据分割错误，我是数据划分的时候少划分一大半导致样本量一直很小。</li>
<li>学习速率(alpha)选择多少正常，第一次做心理没底，选了稍微大一点的如0.1，误差大得直接超出浮点型最大值。<strong>解决</strong>：使用adagrad，可以根据迭代次数的增加，自动减少alpha。</li>
<li>梯度下降的迭代次数选择多少正常，第一次做心理没底。<strong>解决</strong>：经过我反复尝试，虽然我还是没有得到肯定的答案，但是我一般将迭代次数设置为4000.</li>
<li>theta初始值设置为多少正常，第一次做心理没底。<strong>解决</strong>：全部为0，包括bais也设置为0。另外吴恩达老师讲过在神经网络中不能这么设置。</li>
<li>怎么选择hypothesis函数的表达式，到底是二次项，三次项，四次项还是五次项等等。<strong>解决</strong>：暂时没百度过，我看吴恩达老师的视频，个人理解应该是最后在1-10次中选择。</li>
<li>除了一个特征的几次方项，那么多个特征的乘积怎么选择，如有特征值x1,x2,x3，x1*x2*x3还是选x1*x3还是选x2*x3，甚至这些乘积里面的特征也可以加上次方。那项的可能就多了去了，到底该怎么选。<strong>解决</strong>：我刚开始是不加这些项的，也就是表达式的一个项中只会有一个特征。后面加也会自己慢慢尝试，基本上就是碰运气。</li>
<li>标准化的lambda怎么设置。<strong>解决</strong>：吴恩达老师有讲怎么调整lambda值以及lambda值调整会出现问题的情况也讲了，所以我都是感觉误差稳定了或者误差太大了才去调的。一般都设为0。</li>
<li>其他必要的变量怎么设置。<strong>解决</strong>：我真的是完全自己调，然后碰运气。。。</li>
<li>怎么看我训练出来的结果如何。<strong>解决</strong>：梯度下降本质是最后得到一组theta。你需要预测数据，首先肯定有X，然后X * theta，就会得到预测结果。接下来是使用代价函数看误差就行了。吴恩达称之为代价函数(Cost function)，李宏毅称之为损失函数(Loss function)，而且算法也有略微不同，不过问题不大。值得注意的是，之前训练的样本被称为train_data，现在预测的被称为cross_validation_data，是完全不同的数据，不要再用train_data去做预测，具体为什么，吴恩达老师讲的很明白了。</li>
<li>误差到多少才算可以。<strong>解决</strong>：吴恩达老师给出了几幅图，但是个人认为这几幅图有几个瑕疵的地方，那就是他没标刻度，可能是故意没标的，因为根本没刻度。所以压根不知道误差到什么程度才算可以。后来经过不断测试和百度，发现网上一个人认为40多的误差他说可以接受。我个人认为，训练误差0-20差不多，交叉验证误差0-40差不多，预测误差感觉40以下差不多。预测误差就是最终做出的预测产生的误差，所以我感觉要再低点，但是我的作业中训练的一直降不下去，最低都是20多。</li>
</ol>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol>
<li>通过分别画出特征值和待输出值的二维图，观察图像来选择特征。</li>
<li>尽可能地把数据处理部分的代码写的完全正确。</li>
<li>选择hypothesis函数，可以从1次方开始尝试。</li>
<li>初始化值。alpha=0.001,iters_num=4000,lambda=0.</li>
<li>开始梯度下降。</li>
<li>出现误差，通过吴恩达老师给出的建议以及李宏毅老师给的补充，可以自行调整。对于我的话，误差太巨大，我一般先调整学习速率alpha(learning rate)和迭代次数iters_num.误差大概到100以内左右才会动特征数量，hypothesis表达式等等这些变量。误差到很小了（这个很小还真不确定，大概0-40左右吧）才会调整lambda。</li>
<li>待补充。</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>octave添加io包读取一些特定的文件，如csv,xls等</title>
    <url>/octave%E6%B7%BB%E5%8A%A0io%E5%8C%85%E8%AF%BB%E5%8F%96%E4%B8%80%E4%BA%9B%E7%89%B9%E5%AE%9A%E7%9A%84%E6%96%87%E4%BB%B6%EF%BC%8C%E5%A6%82csv%E3%80%81xls%E7%AD%89.html</url>
    <content><![CDATA[<p>输入<code>pkg install -forge io</code>，octave会自动下载，资源在国外会有点卡，需要等一会。<br>过一会会显示</p>
<blockquote>
<p>For information about changes from previous versions of the io package, run ‘news io’.</p>
</blockquote>
<p>这不是说明安装失败了，而是安装成功了。意思是让你仔细了解一下io包个版本的区别。输入news io查询。<br>最后输入<code>pkg load io</code>加载io包。（这句代码不是输入一次永久有效，只要关闭了cli界面，就要重新输入一次，就像java中导包一样）<br>当读取csv文件时，我使用了csv2cell(“文件名”)函数，但是这个函数有问题，它返回的值不是矩阵貌似是字符串。建议使用csvread(“文件名”)函数<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>AI</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>octave</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world.html</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title>用idea创建gradle构建的kotlin项目</title>
    <url>/%E7%94%A8idea%E5%88%9B%E5%BB%BAgradle%E6%9E%84%E5%BB%BA%E7%9A%84kotlin%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<p>在网上找了找发现没几个人用maven创建kotlin项目，而我以前一直都用maven，没用过gradle，所以只能当学习gradle了。<br>第一步：选择gradle<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/f9fdef1607a94d4788848429af5af5a6.png" alt="选择gradle" title="选择gradle"><br>第二步填写GroupId和ArrtifactId，这个很简单。略。<br>第三步，默认是这样的窗口，但是我选择了User auto-import（自动导包功能），其他都是默认值。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/70cb9fa0d58142f094e14a8c8a9f0a79.png" alt="一些配置" title="一些配置"><br>第四步，看看项目的所在文件夹没问题就可以finish了。<br>第五步，发现项目的文件夹有问题，只有个.idea。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/fa8abf0e9fa54d658512abb05b9436ae.png" alt="项目目录只有.idea文件夹" title="项目目录只有.idea文件夹"><br>第六步，我是第一次用gradle，只要等gradle下完就行了。如果下完还是没有，那么重启一下idea就行了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/6acb9a4b777f4c08b60fcce2a8f425eb.png" alt="成功构建" title="成功构建"><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>android</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>（十）搭建springcloud：记录项目运行的日志</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E5%8D%81%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E8%AE%B0%E5%BD%95%E9%A1%B9%E7%9B%AE%E8%BF%90%E8%A1%8C%E7%9A%84%E6%97%A5%E5%BF%97.html</url>
    <content><![CDATA[<p>项目运行避免不了出现错误，这是日志就帮了大忙。<br>springboot提供了一个便捷的方法配置日志。在application.yml中加入以下配置，其中com.yan624代表包名，即这个包下的所有类输出的日志都是error以上级别。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">logging:</span></span><br><span class="line"><span class="attr">  file:</span> <span class="string">upms.log</span></span><br><span class="line"><span class="attr">  level:</span></span><br><span class="line">    <span class="string">com.yan624:</span> <span class="string">error</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure></p>
<p>如果如下配置，就代表项目的所有类输出的日志都是error以上级别</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">logging:</span></span><br><span class="line"><span class="attr">  file:</span> <span class="string">upms.log</span></span><br><span class="line"><span class="attr">  level:</span></span><br><span class="line"><span class="attr">    root:</span> <span class="string">error</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>
<p>但是这样配置无法做出更灵活的操作，比如将info的日志输出到info.log；将warn的日志输出到warn.log；将error的日志输出到error.log。<br>所以可以删除上面的配置，在classpath路径中新建logback.xml文件，里面填入：<br><figure class="highlight dust"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 修改一下自己想配置的路径，就一个点代表项目根路径--&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_PATH"</span> <span class="attr">value</span>=<span class="string">"."</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- start springboot default configuration of console log pattern --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 彩色日志 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 彩色日志依赖的渲染类 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">conversionRule</span> <span class="attr">conversionWord</span>=<span class="string">"clr"</span> <span class="attr">converterClass</span>=<span class="string">"org.springframework.boot.logging.logback.ColorConverter"</span>/&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">conversionRule</span> <span class="attr">conversionWord</span>=<span class="string">"wex"</span> <span class="attr">converterClass</span>=<span class="string">"org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter"</span>/&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">conversionRule</span> <span class="attr">conversionWord</span>=<span class="string">"wEx"</span> <span class="attr">converterClass</span>=<span class="string">"org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter"</span>/&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 彩色日志格式 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"CONSOLE_LOG_PATTERN"</span></span></span></span><br><span class="line"><span class="xml">              value="$</span><span class="template-variable">&#123;CONSOLE_LOG_PATTERN:-%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;</span><span class="xml">)</span><span class="template-variable">&#123;faint&#125;</span><span class="xml"> %clr($</span><span class="template-variable">&#123;LOG_LEVEL_PATTERN:-%5p&#125;</span><span class="xml">) %clr($</span><span class="template-variable">&#123;PID:- &#125;</span><span class="xml">)</span><span class="template-variable">&#123;magenta&#125;</span><span class="xml"> %clr(---)</span><span class="template-variable">&#123;faint&#125;</span><span class="xml"> %clr([%15.15t])</span><span class="template-variable">&#123;faint&#125;</span><span class="xml"> %clr(%-40.40logger</span><span class="template-variable">&#123;39&#125;</span><span class="xml">)</span><span class="template-variable">&#123;cyan&#125;</span><span class="xml"> %clr(:)</span><span class="template-variable">&#123;faint&#125;</span><span class="xml"> %m%n$</span><span class="template-variable">&#123;LOG_EXCEPTION_CONVERSION_WORD:-%wEx&#125;</span><span class="xml">&#125;"/&gt;</span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"FILE_LOG_PATTERN"</span> <span class="attr">value</span>=<span class="string">"%date %-5level [$</span></span></span><span class="template-variable">&#123;HOSTNAME&#125;</span><span class="xml"><span class="tag"><span class="string"> %thread] %caller</span></span></span><span class="template-variable">&#123;1&#125;</span><span class="xml"><span class="tag"><span class="string">%msg%n"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"CONSOLE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">Pattern</span>&gt;</span>$</span><span class="template-variable">&#123;CONSOLE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">Pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.ThresholdFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>info<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- end springboot default configuration of console log pattern --&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!-- 输出自己项目的日志，我比较喜欢将第三方的日志和自己的日志区分开存放，如果没我这样的强迫症可以不配置 --&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"MY_PROJECT_LOG_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$</span><span class="template-variable">&#123;LOG_PATH&#125;</span><span class="xml">/my-project.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--日志文件输出格式--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$</span><span class="template-variable">&#123;FILE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>my-project/my-project-%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.%i.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                除按日志记录之外，还配置了日志文件不能超过20M，若超过20M，日志文件会以索引0开始，</span></span><br><span class="line"><span class="xml">                命名日志文件，例如info/info-2019-01-03.0.log</span></span><br><span class="line"><span class="xml">            --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span></span><br><span class="line"><span class="xml">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--日志文件保留天数--&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 此日志文件只记录info级别的 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>info<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$</span><span class="template-variable">&#123;LOG_PATH&#125;</span><span class="xml">/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--日志文件输出格式--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$</span><span class="template-variable">&#123;FILE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>info/info-%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.%i.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                除按日志记录之外，还配置了日志文件不能超过20M，若超过20M，日志文件会以索引0开始，</span></span><br><span class="line"><span class="xml">                命名日志文件，例如info/info-2019-01-03.0.log</span></span><br><span class="line"><span class="xml">            --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span></span><br><span class="line"><span class="xml">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--日志文件保留天数--&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 此日志文件只记录info级别的 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>info<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                以下两个配置代表匹配到日志等级为info则输出，否则拒绝输出。</span></span><br><span class="line"><span class="xml">                如果删除这两行配置，那么这两个配置的默认值为NEUTRAL，意味着从日志等级info开始一层一层向上打印，即info-&gt;warn-&gt;error-&gt;fatal-&gt;off</span></span><br><span class="line"><span class="xml">                再比如上面<span class="tag">&lt;<span class="name">level</span>&gt;</span>配置的是error,那么输出error-&gt;fatal-&gt;off</span></span><br><span class="line"><span class="xml">                如果上面<span class="tag">&lt;<span class="name">level</span>&gt;</span>配置的是off，那么就只输出off等级的日志。</span></span><br><span class="line"><span class="xml">             --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"WARN_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$</span><span class="template-variable">&#123;LOG_PATH&#125;</span><span class="xml">/warn.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--日志文件输出格式--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$</span><span class="template-variable">&#123;FILE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>warn/warn-%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.%i.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                除按日志记录之外，还配置了日志文件不能超过20M，若超过20M，日志文件会以索引0开始，</span></span><br><span class="line"><span class="xml">                命名日志文件，例如warn/warn-2019-01-03.0.log</span></span><br><span class="line"><span class="xml">            --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span></span><br><span class="line"><span class="xml">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--日志文件保留天数--&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 此日志文件只记录warn级别的 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>warn<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$</span><span class="template-variable">&#123;LOG_PATH&#125;</span><span class="xml">/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--日志文件输出格式--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$</span><span class="template-variable">&#123;FILE_LOG_PATTERN&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span> <span class="comment">&lt;!-- 此处设置字符集 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>error/error-%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.%i.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--</span></span></span><br><span class="line"><span class="xml">                除按日志记录之外，还配置了日志文件不能超过20M，若超过20M，日志文件会以索引0开始，</span></span><br><span class="line"><span class="xml">                命名日志文件，例如error/error-2019-01-03.0.log</span></span><br><span class="line"><span class="xml">            --&gt;</span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span></span><br><span class="line"><span class="xml">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--日志文件保留天数--&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 此日志文件只记录error级别的 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>error<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">springProfile</span> <span class="attr">name</span>=<span class="string">"test"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 控制台输出info级别以上的日志 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"CONSOLE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">springProfile</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">springProfile</span> <span class="attr">name</span>=<span class="string">"prod"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--这里的name属性代表所属包的名称是以io.github.yan624.messageporter开头的类，会按照这个logger的配置打印日志--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"io.github.yan624.messageporter"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"MY_PROJECT_LOG_FILE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 打印warn，error级别的日志 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"warn"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"WARN_FILE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 控制台输出info级别以上的日志 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"CONSOLE"</span> /&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">springProfile</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure></p>
<p>并且在application.yml文件中自己配置<br><figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">spring:</span></span><br><span class="line"><span class="symbol">  profiles:</span></span><br><span class="line"><span class="symbol">    active:</span> prod</span><br></pre></td></tr></table></figure></p>
<p>这样代表了激活了prod的日志配置。对应于logback.xml文件中的<code>&lt;springProfile name=&quot;prod&quot;&gt;。。。&lt;/springProfile&gt;</code>。<br>如果希望把自己某个包中的类的日志输出，在appender-&gt;filter标签下不能写onMacth和onMisMatch具体原因不知，反正我是去掉了就可以输出了。参考springProfile-&gt;logger的name=”com.yan624”。其中MY_PROJECT_FILE的appender我并没有配置onMacth和onMisMatch。<br>名为CONSOLE的appender是springboot的默认输出格式，我在该xml中已经写明注释。<br>其他的应该都很好理解。<br>最后效果就是：在warn.log和error.log文件中分别输出warn和error级别的日志。在控制台输出info<strong>以上</strong>级别的日志。在my-project.log文件中输出我自己项目的info级别<strong>以上</strong>的日志。<br>上面我写了info级别的配置，但是我并没有使用，另外debug级别的配置我没有写，因为我用不上。</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（九）搭建springcloud：在linux上运行springboot项目</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E4%B9%9D%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E5%9C%A8linux%E4%B8%8A%E8%BF%90%E8%A1%8Cspringboot%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<p>具将本地的jar文件拖到服务器上时发现，jar文件只有3kb。果然使用“java -jar 项目名.jar”命令后，显示<strong>no main manifest attribute, in springboot项目名.jar</strong>。<br>解决办法：在项目中（不是聚合父工程中）添加如下代码</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 指定该Main Class为全局的唯一入口 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.yan624.eureka.Eureka_Server7001<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">layout</span>&gt;</span>JAR<span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>repackage<span class="tag">&lt;/<span class="name">goal</span>&gt;</span><span class="comment">&lt;!--可以把依赖的包都打包到生成的Jar包中--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>由于如果直接使用java -jar 项目名.jar启动springboot项目，会造成你启动了这个项目就不用想输入其他命令了，所以使用 “<strong>nohup java -jar springboot项目名.jar &gt;记录日志的文件名.log 2&gt;&amp;1 &amp;</strong>”命令，来让spirngboot项目后台运行，并且记录日志。<br>如果不想输出日志，可以使用“<strong>nohup java -jar springboot项目名.jar &gt;/dev/null 2&gt;&amp;1 &amp;</strong>”</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>springboot启动出现Unable to start EmbeddedWebApplicationContext due to missing EmbeddedServlet</title>
    <url>/bug/springboot%E5%90%AF%E5%8A%A8%E5%87%BA%E7%8E%B0Unable-to-start-EmbeddedWebApplicationContext-due-to-missing-EmbeddedServlet.html</url>
    <content><![CDATA[<p>网上的解决办法打都相同，但是我这个问题纯属操作失误。<br>因为我想要启动我的微服务，但是在idea里配置的时候配置错了。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/abaa4d89631040ac90ccc023a34ac1d3.png" alt="idea配置" title="idea配置"><br>我把主启动类配错了。可以看到下图主启动类是CMS_BLOG8082，但是手滑选择了CmsBlogApiApplication。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/a61afd34f52f46299eca32ab412f668b.png" alt="主启动类" title="主启动类"><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>idea启动的项目，打开网页出现部分文字乱码</title>
    <url>/bug/idea%E5%90%AF%E5%8A%A8%E7%9A%84%E9%A1%B9%E7%9B%AE%EF%BC%8C%E6%89%93%E5%BC%80%E7%BD%91%E9%A1%B5%E5%87%BA%E7%8E%B0%E9%83%A8%E5%88%86%E6%96%87%E5%AD%97%E4%B9%B1%E7%A0%81.html</url>
    <content><![CDATA[<p>起因：<br>在文件中右键点击file encoding，打算更改编码，但是出现一个窗口显示Reloead,Convert,Cancel。由于不懂这几个按键按后的效果是什么所以瞎按，导致文件完全乱码，恢复不回去。<br>之后在打开网页时，发现有部分中文乱码，而且乱码还是出现在前端部分，而且我的文件格式全是utf-8，所以按理说应该没有任何问题。最终也没有解决问题。<br>解决办法：<br>重新配置idea。重置方法详见：<a href="https://jingyan.baidu.com/article/fa4125ac1c613f28ac7092c1.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/fa4125ac1c613f28ac7092c1.html</a><br><em>更新：这个方法治标不治本，导致了部分页面恢复正常了，但是其他页面还是有问题。</em><br><strong>终极解决办法：</strong>找到错误的根源，可能是html，也可能是js。大部分都是js的错误，由于js动态地将中文添加到页面，导致乱码。所以需要将这份js文件，复制出来，用windows的文本文档打开，另存为的时候修改编码为urf-8。再复制回项目即可。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>error</tag>
        <tag>idea</tag>
      </tags>
  </entry>
  <entry>
    <title>（零）搭建springcloud：创建项目</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E9%9B%B6%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<h3 id="创建springboot项目"><a href="#创建springboot项目" class="headerlink" title="创建springboot项目"></a>创建springboot项目</h3><p>右键父工程选择new-&gt;module，创建一个子项目（packaging为jar）。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/33cb351041724b248d26b86121954e93.png" alt="创建module" title="创建module"><br>点击Module之后会出现如下界面，选择spring Initializr(你如果要创建maven项目的话也选这个)，什么都不要干，点击Next。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/41f65d0615394aefa53946aa6300d5d2.png" alt="创建springboot项目" title="创建springboot项目"><br>之后会出现这个界面，填写好信息，注意Packaging是jar，因为springboot是用netty启动的，不需要打成war包，点击Next。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/4ad4d8919f1341098719e99ceaee285b.png" alt="配置" title="配置"></p>
<h3 id="创建聚合工程"><a href="#创建聚合工程" class="headerlink" title="创建聚合工程"></a>创建聚合工程</h3><p>右键父工程选择new-&gt;module，创建一个子项目（jpackaging为pom）。<br>选择maven即可。</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>ip地址引起的无法单点登录</title>
    <url>/bug/ip%E5%9C%B0%E5%9D%80%E5%BC%95%E8%B5%B7%E7%9A%84%E6%97%A0%E6%B3%95%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95.html</url>
    <content><![CDATA[<p>今天碰到一个很奇怪的问题，单点登录明明自认为代码没有任何问题，但是始终登录不进去。<br>最后发现，测试配置的cas-service地址是手动输入的localhost，所以当使用单点登录时在sso系统进行验证完毕时会回调这个地址回到原系统。这一切都符合预期运行。但是当原系统调用backUrl时出问题了。因为我点的是eureka的Application表格的Status列中链接，而这个链接我正好配置显示更详细的ip地址，所以由本来的localhost替换成了我的局域网中的ip地址。<br>综上：我需要登录的地址是localhost，而我需要回调的ip地址是我局域网的地址，显然两个ip不同那么shiro的subject也会不同，自然造成了每次使用isAuthenticated方法都会出问题。当然这只是特例，因为我的上述的两个地址虽然说不同但是实际上都是我一个人的，而上线之后应该不会遇到这种情况了。<br>可以尝试将这几个ip地址映射为一个地址，或者在当用户多次跳转回登录页面时，让它的地址栏强制去除backUrl参数，并给出相应提示。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>（八）搭建springcloud：修改静态文件，如html，js后立即生效</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E5%85%AB%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E4%BF%AE%E6%94%B9%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6%EF%BC%8C%E5%A6%82html%EF%BC%8Cjs%E5%90%8E%E7%AB%8B%E5%8D%B3%E7%94%9F%E6%95%88.html</url>
    <content><![CDATA[<ol>
<li>setting—&gt;Build,Execution,Deployment—&gt;Compiler  找到 build project automatically<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/0e3e14335ea94258adf2616de29ed82d.PNG" alt="idea配置自动构建" title="idea配置自动构建"></li>
<li>按ctrl+shift+alt+/，选择 Registry<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/1ed4835b01e54c82b6d0023db60873ac.jpg" alt="更改注册" title="更改注册"></li>
<li>重启<br>如果还是不行，就用麻烦点的办法，每次修改在idea按ctrl+shift+f9即可</li>
</ol>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E4%B8%83%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9Aspringboot%E5%A6%82%E4%BD%95%E5%B0%86%E6%89%80%E4%BE%9D%E8%B5%96%E7%9A%84jar%E5%8C%85%E7%9A%84%E7%B1%BB%E6%94%BE%E5%85%A5spring%E5%AE%B9%E5%99%A8.html</url>
    <content><![CDATA[<figure class="highlight dart"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@EnableEurekaClient</span></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Upms8081</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	public <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="built_in">String</span>[] args) &#123;</span><br><span class="line">		<span class="built_in">Object</span>[] objects = <span class="keyword">new</span> <span class="built_in">Object</span>[<span class="number">3</span>];</span><br><span class="line">		objects[<span class="number">0</span>] = Upms8081.<span class="keyword">class</span>;</span><br><span class="line">		objects[<span class="number">1</span>] = UpmsApiAppliction.<span class="keyword">class</span>;</span><br><span class="line">		objects[<span class="number">2</span>] = CommonServiceStarter.<span class="keyword">class</span>;</span><br><span class="line">		SpringApplication.run(objects, args);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其他的class对象分别是所需要初始化类jar包中的springboot启动类</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>shiro的filter配置问题，自定义的filter交给spring容器管理会有问题</title>
    <url>/bug/shiro%E7%9A%84filter%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%EF%BC%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84filter%E4%BA%A4%E7%BB%99spring%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E4%BC%9A%E6%9C%89%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>此项目基于springboot，放在springmvc中，filter由spring管理不知道为什么就没问题。<br>我配置了/api/v1/<strong>——&gt;roles[]，/</strong>———&gt;authc<br>/api/v1/<strong>一直无法访问，一直调转到authc这个过滤器去验证，我就怀疑是不是/</strong>也匹配到了/api/v1/**，但是以前做的时候一直没问题，所以绕了一大圈。<br>最后发现！！！！！<br>重要的事说三遍，不要将自定义的filter交给spring容器管理！不要将自定义的filter交给spring容器管理！不要将自定义的filter交给spring容器管理！<br>因为在一个链接的所有配置好的filter执行完毕后，它会接着执行另外一个过滤链，该过滤链由servlet容器提供。也就是说shiro一共会执行两个过滤链，一个是你自己配置好的，第二个是servlet提供的（由于本人没有阅读过spring源码，故不清楚spring注册的bean是怎么进入servlet容器的）。<br>所以当你把自定义的过滤器交给spring管理时，这个过滤器无论如何都会至少执行一次。<br>具体的代码在org.apache.shiro.web.servlet.doFilter(ServletRequest request, ServletResponse response)，第一行就是判断自定义的过滤链是否执行完毕，如果执行完毕就执行servlet提供的过滤链。（注shiro版本为1.4.0）<br>如果出现这个问题只需要将filter的创建由自己完成即可。<br>tips:由于可能在filter中需要注入某些对象。这时只能用springContext获取bean了。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E5%85%AD%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E5%9C%A8idea%E5%90%AF%E5%8A%A8%E9%A1%B9%E7%9B%AE%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88hibernate-validator%E4%B8%8Ejavax-validation%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7%E9%97%AE%E9%A2%98%EF%BC%89.html</url>
    <content><![CDATA[<p>由于是从eclipse迁移过来，之前的不同ide开发的问题搞了好久终于搞完了，以为终于成功了，然而又出了一大堆问题。<br>项目无法启动，报了一个奇葩的错：</p>
<blockquote>
<p>Could not initialize class org.hibernate.validator.internal.engine.Configuration</p>
</blockquote>
<p>大致意思貌似就是javax.validation和hibernate-validator版本冲突了（详见<a href="https://stackoverflow.com/questions/14730329/jpa-2-0-exception-to-use-javax-validation-package-in-jpa-2-0）" target="_blank" rel="noopener">stackoverflow JPA 2.0 : Exception to use javax.validation.* package in JPA 2.0</a><br>在无数次更换版本中，终于找到了一个没有问题的版本。hibernate-validator（5.2.4.Final版），我试过很多版本都有问题，包括4.1.3，5.1.6，5.3.6，5.4.1，6.0.1都是没有用的而其中个别版本居然还不会传递依赖，就是hibernate-validator本身是依赖javax.validation（1.1.0.Final版）的，所幸的是5.2.4.Fianl依赖了javax.validation，也省的我再去试javax.validation的版本。<br>所以最后应该将hibernate-validator添加到你需要的工程的pom文件中.spring-boot-starter-web这个jar包它本身是依赖hibernate-validator的，所以我把它直接排除了（exclusions标签）。而springboot默认依赖的版本是5.3.6是有问题的，不知道是不是我电脑的原因，反正我是运行不起来。</p>
<p>总结：以后使用hibernate-validator要用5.2.4.Final版本。其次，我在eclipse里运行我的程序一点问题都没，移植到idea就各种问题，也不知道是不是idea太严格了。<br><strong><em>补充：由于我的是聚合工程，我搞了两三个小时，<font color="red">发现我在pom文件中改动jar包的版本，虽然idea旁边的maven dependencies中相应的改变了jar包的版本，但是在实际运行过程中，它还是使用的之前版本的jar包，跟没改一样。</font>由于我是聚合工程，我将几乎所有的jar包版本都统一放在parent工程，所以后来发现只需要clean install一下parent工程即可。</em></strong></p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（五）搭建springcloud：在idea启动springboot项目</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E4%BA%94%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E5%9C%A8idea%E5%90%AF%E5%8A%A8springboot%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<p>点击idea右上角 edit configuration<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/4803d077b7534b4fbf2f6f5a915db876.png" alt="idea配置启动类" title="idea配置启动类"><br>点击左上角的<font color="green">绿色“+”</font>然后点击maven就会出现右边窗口，注意要起一个name，然后下面的参数这么配置点击ok<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/e3a99aa12a0b4ad7a5544eaf1510cf4d.png" alt="idea配置maven命令" title="idea配置maven命令"><br>然后再到相同位置就会多了一个你起的名称的一个运行配置。点击即可。<br>最后还有最<strong>重要</strong>的一点，需要在你启动的项目的pom.xml文件中加上配置如下（主要是spring-boot-maven-plugin）：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-surefire-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">skip</span>&gt;</span>true<span class="tag">&lt;/<span class="name">skip</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>更新：此方式启动，无法进行调试，貌似可以通过配置解决。后来我都不用这个方式启动项目，在图二中不选择maven，选择Spring Boot即可运行正常。</strong></p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（三）搭建springcloud：整合mybatis-plus</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E4%B8%89%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E6%95%B4%E5%90%88mybatis-plus.html</url>
    <content><![CDATA[<p>由于一直使用mybatis的example，无法忍受着过长的代码，所以转向了mybatis-plus，配置过于简单，就不演示了。<br>参考官网<a href="http://mp.baomidou.com/#/install" target="_blank" rel="noopener">http://mp.baomidou.com/#/install</a><br>如果看不懂，可以看mybatis-plus提供的整合springboot例子<br>网址：<a href="https://gitee.com/baomidou/mybatisplus-spring-boot" target="_blank" rel="noopener">https://gitee.com/baomidou/mybatisplus-spring-boot</a><br>如果还看不懂，尼玛别整合了。（手动狗头）<br>提示：<br>Service可以继承mybatis-plus提供的ServiceImpl类<br>mapper层可以继承BaseMapper，这样如果有些mapper比较简单完全可以省去mapper.xml<br>IService可以继承IService</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E5%9B%9B%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9Aeclipse%E8%BF%81%E7%A7%BB%E5%88%B0idea%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88maven-install%E5%91%BD%E4%BB%A4%E5%87%BA%E9%94%99%EF%BC%89.html</url>
    <content><![CDATA[<p>在使用maven的install命令时出现java.lang.ArrayIndexOutOfBoundsException: 10640<br>试了三个小时，一直以为idea有问题，最后没想到是maven版本的问题，我的是3.5.0，只要改成3.5.2以上就可以了（我改成3.5.4）<br>真的奇怪之前在eclipse上用一直好好地</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（一）搭建springcloud：springboot整合thymeleaf</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E4%B8%80%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9Aspringboot%E6%95%B4%E5%90%88thymeleaf.html</url>
    <content><![CDATA[<p>thymeleaf的配置类如下，要注意的是它的prefix不是以前的”/WEB-INF/“，而是”classpath:/templates/“。templates文件夹位于resources源文件夹下，该文件夹的名称不是我随便写的，是springboot规定的，用于放置动态文件。类似的还有static用于放置静态文件。<br>其中我分别写了linux（即生产环境）及windows（即开发环境）下的配置，可以根据需要自己删留。（由于一般开发环境为windows，所以我自己偷懒写的加载bean条件，如果开发环境也是linux那么可以直接删掉）其中还可能包含我自己写的工具类，可以直接删去。<br>注：差点忘了，我的thymeleaf是3.0以上的版本，3.0以上的版本变动有些大，如果不是该版本以上，可能无法使用该配置类。springboot默认依赖的版本是2.1.6（貌似），所以需要手动更改，不会的看<a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@ComponentScan</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThymeleafConfig</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(ThymeleafConfig.class);</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ThymeleafViewResolver <span class="title">viewResolver</span><span class="params">(SpringTemplateEngine templateEngine)</span></span>&#123;</span><br><span class="line">        ThymeleafViewResolver viewResolver = <span class="keyword">new</span> ThymeleafViewResolver();</span><br><span class="line">        viewResolver.setTemplateEngine(templateEngine);</span><br><span class="line">        viewResolver.setOrder(<span class="number">1</span>);</span><br><span class="line">        viewResolver.setCharacterEncoding(<span class="string">"utf-8"</span>);</span><br><span class="line">        <span class="comment">//不知道这个缓存和下面的模板解析器的缓存是什么关系，但是应该不是同一个意思</span></span><br><span class="line">        <span class="comment">//我在模板解析器设置false，在这行代码之上打印了一下还是true</span></span><br><span class="line">        viewResolver.setCache(<span class="keyword">false</span>);</span><br><span class="line">        viewResolver.setViewNames(<span class="keyword">new</span> String[] &#123;<span class="string">"thymeleaf/*"</span>, <span class="string">".html"</span>, <span class="string">".xhtml"</span>&#125;);</span><br><span class="line">        <span class="keyword">return</span> viewResolver;</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 模板解析器</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Conditional</span>(WindowsCondition.class)</span><br><span class="line">    <span class="meta">@Primary</span><span class="comment">//springboot貌似内置一个模板解析器，所以将这个bean定义为首选</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SpringResourceTemplateResolver <span class="title">windowsTemplateResolver</span><span class="params">()</span></span>&#123;</span><br><span class="line">        SpringResourceTemplateResolver templateResolver = <span class="keyword">new</span> SpringResourceTemplateResolver();</span><br><span class="line">        templateResolver.setPrefix(<span class="string">"classpath:/templates/"</span>);</span><br><span class="line">        templateResolver.setSuffix(<span class="string">".html"</span>);</span><br><span class="line">        templateResolver.setOrder(<span class="number">1</span>);</span><br><span class="line">        templateResolver.setTemplateMode(TemplateMode.HTML);</span><br><span class="line">        <span class="comment">//不缓存页面，即服务器开启时修改thymeleaf页面会有变化。</span></span><br><span class="line">        templateResolver.setCacheable(<span class="keyword">false</span>);</span><br><span class="line">        LOGGER.info(<span class="keyword">new</span> LogPrintingTemplate()</span><br><span class="line">                .type(<span class="string">"thymeleaf页面缓存禁用(￣▽￣)~*"</span>)</span><br><span class="line">                .print());</span><br><span class="line">        templateResolver.setCharacterEncoding(<span class="string">"utf-8"</span>);</span><br><span class="line">        <span class="keyword">return</span> templateResolver;</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="meta">@Conditional</span>(LinuxCondition.class)</span><br><span class="line">    <span class="meta">@Primary</span><span class="comment">//springboot貌似内置一个模板解析器，所以将这个bean定义为首选</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SpringResourceTemplateResolver <span class="title">linuxTemplateResolver</span><span class="params">()</span></span>&#123;</span><br><span class="line">        SpringResourceTemplateResolver templateResolver = <span class="keyword">new</span> SpringResourceTemplateResolver();</span><br><span class="line">        templateResolver.setPrefix(<span class="string">"classpath:/WEB-INF/"</span>);</span><br><span class="line">        templateResolver.setSuffix(<span class="string">".html"</span>);</span><br><span class="line">        templateResolver.setOrder(<span class="number">1</span>);</span><br><span class="line">        templateResolver.setTemplateMode(TemplateMode.HTML);</span><br><span class="line">        <span class="comment">//缓存页面</span></span><br><span class="line">        templateResolver.setCacheable(<span class="keyword">true</span>);</span><br><span class="line">        LOGGER.info(<span class="keyword">new</span> LogPrintingTemplate()</span><br><span class="line">                .type(<span class="string">"thymeleaf页面缓存开启(￣▽￣)~*"</span>)</span><br><span class="line">                .print());</span><br><span class="line">        templateResolver.setCharacterEncoding(<span class="string">"utf-8"</span>);</span><br><span class="line">        <span class="keyword">return</span> templateResolver;</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 模版引擎</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SpringTemplateEngine <span class="title">templateEngine</span><span class="params">(ITemplateResolver templateResolver)</span></span>&#123;</span><br><span class="line">        SpringTemplateEngine templateEngine = <span class="keyword">new</span> SpringTemplateEngine();</span><br><span class="line">        templateEngine.setTemplateResolver(templateResolver);</span><br><span class="line">        templateEngine.setEnableSpringELCompiler(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> templateEngine;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Conditional里的类如下，linux的类似</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowsCondition</span> <span class="keyword">implements</span> <span class="title">Condition</span></span>&#123;</span><br><span class="line">       </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">matches</span><span class="params">(ConditionContext context, AnnotatedTypeMetadata metadata)</span> </span>&#123;</span><br><span class="line">        Environment environment = context.getEnvironment();</span><br><span class="line">        String osName = environment.getProperty(<span class="string">"os.name"</span>);</span><br><span class="line">        <span class="keyword">boolean</span> isWindows = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">if</span>(osName.toLowerCase().contains(<span class="string">"windows"</span>))   isWindows = <span class="keyword">true</span>;</span><br><span class="line">         </span><br><span class="line">        <span class="keyword">return</span> isWindows;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结：</p>
<ol>
<li>resources源文件夹下，templates文件夹放置动态文件，static文件夹下放置静态文件，其下的子文件夹可以随意创建。</li>
<li>thymeleaf的html文件建议放置在templates/thymeleaf文件夹下（我的习惯）</li>
<li>static文件夹下的资源是可以通过浏览器直接访问的，类似于传统web工程的webapp下的文件；而templates文件夹下的文件必须写上映射，即创建controller层，然后跟以前一样写mapping，这样就可以通过浏览器访问，然后springmvc视图解析，将视图返回给浏览器。类似于以前的WEB-INF文件夹。<br>4.<em> 更新：由于今天在配置thymeleaf时给我产生了疑惑，所以在此特地重申：templates和static文件夹的名字不是我起的，是springboot的规定。</em></li>
</ol>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a></p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>（二）搭建springcloud：修改thymeleaf版本</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%EF%BC%88%E4%BA%8C%EF%BC%89%E6%90%AD%E5%BB%BAspringcloud%EF%BC%9A%E4%BF%AE%E6%94%B9thymeleaf%E7%89%88%E6%9C%AC.html</url>
    <content><![CDATA[<p>起因：springboot整合thymeleaf默认依赖的版本是3.0以下的版本，而我一直是用的3.0以上的版本，而且貌似3.0以上的版本性能更好，所以果断自己做了版本的更改。但是在更改是有一点问题。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- thymeleaf --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-thymeleaf<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>由于我想排除依赖，所以我在其中排除了thymeleaf-spring4的依赖，但是我发现thymeleaf核心包的版本还是3.0以下，具体解决过程不写了，解决如下：<br>上面的依赖不要删，在下面再添加依赖，父工程中添加版本，可以选择3.0以上的版本（我的是3.0.9）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>这样就可以了，但是最好再多做一步，将第一个依赖改为这样：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- thymeleaf --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-thymeleaf<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>nz.net.ultraq.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf-layout-dialect<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>nz.net.ultraq.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf-layout-dialect<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>由于thymeleaf版本变迁，所以上面的这个dialect版本必须也要跟着改变（我使用2.0.0，建议使用2.0.0以上版本）。<br>最后附上父工程配置</p>
<figure class="highlight dust"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="comment">&lt;!-- 视图层，在springboot工程中二者缺一不可，因为springboot默认依赖thymeleaf2.0 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">thymeleaf.version</span>&gt;</span>3.0.9.RELEASE<span class="tag">&lt;/<span class="name">thymeleaf.version</span>&gt;</span><span class="comment">&lt;!-- thymeleaf版本 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">thymeleaf-layout-dialect.version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">thymeleaf-layout-dialect.version</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.thymeleaf/thymeleaf-spring4 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf-spring4<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;thymeleaf.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>nz.net.ultraq.thymeleaf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>thymeleaf-layout-dialect<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;thymeleaf-layout-dialect.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p><a href="https://yan624.github.io/（零）搭建springcloud：创建项目.html">（零）搭建springcloud：创建项目</a><br><a href="https://yan624.github.io/（一）搭建springcloud：springboot整合thymeleaf.html">（一）搭建springcloud：springboot整合thymeleaf</a><br><a href="https://yan624.github.io/（二）搭建springcloud：修改thymeleaf版本.html">（二）搭建springcloud：修改thymeleaf版本</a><br><a href="https://yan624.github.io/（三）搭建springcloud：整合mybatis-plus.html">（三）搭建springcloud：整合mybatis-plus</a><br><a href="https://yan624.github.io/（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）.html">（四）搭建springcloud：eclipse迁移到idea的问题（maven install命令出错）</a><br><a href="https://yan624.github.io/（五）搭建springcloud：在idea启动springboot项目.html">（五）搭建springcloud：在idea启动springboot项目</a><br><a href="https://yan624.github.io/（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）.html">（六）搭建springcloud：在idea启动项目出现的问题（hibernate-validator与javax.validation的兼容性问题）</a><br><a href="https://yan624.github.io/（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器.html">（七）搭建springcloud：springboot如何将所依赖的jar包的类放入spring容器</a><br><a href="https://yan624.github.io/（八）搭建springcloud：修改静态文件，如html，js后立即生效.html">（八）搭建springcloud：修改静态文件，如html，js后立即生效</a><br><a href="https://yan624.github.io/（九）搭建springcloud：在linux上运行springboot项目.html">（九）搭建springcloud：在linux上运行springboot项目</a><br><a href="https://yan624.github.io/（十）搭建springcloud：记录项目运行的日志.html">（十）搭建springcloud：记录项目运行的日志</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云安装mysql</title>
    <url>/%E8%85%BE%E8%AE%AF%E4%BA%91%E5%AE%89%E8%A3%85mysql.html</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/itor/p/6339505.html" target="_blank" rel="noopener">linux下mysql-5.6忘记root密码，重置root密码详细过程</a><br><a href="https://www.cnblogs.com/BenWong/p/4322085.html" target="_blank" rel="noopener">Linux 下MySql 重置密码</a><br>这两篇结合着看就大致装完了，但是会有一个问题，就是外网连不上mysql，这是由于mysql默认不允许外网连接，所以需要做一点小变动。</p>
<p>当执行第三行时可能会出现：Duplicate entry ‘%-root’ for key ‘PRIMARY’的错误提示信息，但是不需要管它</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">mysql&gt;use mysql;</span><br><span class="line">mysql&gt;<span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> host = <span class="string">'%'</span> <span class="keyword">where</span> <span class="keyword">user</span> = <span class="string">'root'</span>;</span><br><span class="line">mysql&gt;<span class="keyword">select</span> host, <span class="keyword">user</span> <span class="keyword">from</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure>
<p>最后再执行<code>flush privileges;</code>即可<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>linux下安装redis</title>
    <url>/linux%E4%B8%8B%E5%AE%89%E8%A3%85redis.html</url>
    <content><![CDATA[<h3 id="下载redis"><a href="#下载redis" class="headerlink" title="下载redis"></a>下载redis</h3><p>首先下载redis，由于是linux下安装，所以下载tar.gz压缩包，官网地址<a href="https://redis.io/" target="_blank" rel="noopener">https://redis.io/</a><br>然后使用工具把下载下来的压缩包上传到服务器上（我用的是腾讯云），我放在了/usr/local/redis下</p>
<h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p><code>tar zxvf  redis-4.0.10.tar.gz</code></p>
<h3 id="进入解压后的文件夹"><a href="#进入解压后的文件夹" class="headerlink" title="进入解压后的文件夹"></a>进入解压后的文件夹</h3><p><code>cd redis-4.0.10</code></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight gauss"><table><tr><td class="code"><pre><span class="line"><span class="built_in">make</span></span><br><span class="line">cd src(下一步需要有gcc编译redis源文件，如果没有，运行yum install gcc-c++)</span><br><span class="line"><span class="built_in">make</span> install PREFIX=/usr/<span class="keyword">local</span>/redis</span><br></pre></td></tr></table></figure>
<h3 id="移动配置文件到安装目录"><a href="#移动配置文件到安装目录" class="headerlink" title="移动配置文件到安装目录"></a>移动配置文件到安装目录</h3><figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cd</span> ..</span><br><span class="line"><span class="keyword">mkdir</span> /usr/<span class="keyword">local</span>/redis/etc </span><br><span class="line">mv redis.<span class="keyword">conf</span> /usr/<span class="keyword">local</span>/redis/etc</span><br></pre></td></tr></table></figure>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight jboss-cli"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cd</span> bin</span><br><span class="line"><span class="string">./redis-server</span></span><br></pre></td></tr></table></figure>
<p>这是你会发现你的控制台无法再输入命令了，因为你启动了redis，如果想启动了redis还想干其他事，那么需要将redis设置后后台运行</p>
<h3 id="关闭redis服务并编辑刚才的配置文件"><a href="#关闭redis服务并编辑刚才的配置文件" class="headerlink" title="关闭redis服务并编辑刚才的配置文件"></a>关闭redis服务并编辑刚才的配置文件</h3><figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">ctrl <span class="keyword">c</span></span><br><span class="line"><span class="keyword">cd</span> etc</span><br><span class="line"><span class="keyword">vim</span> redis.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
<p>将daemonize设置为yes。<br>如果找不到daemonize在哪，在命令模式下按”/“输入要查找的字，回车进行查找，”n”继续查找。</p>
<p><strong>注：如果改了之后还是没有反应，那么可以在启动redis时指定配置文件，如：./redis-server ../etc/redis.conf</strong><br>然后可以使用netstat -lntp|grep 6379看看6379端口是不是被监听了。6379是redis默认端口。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>安装与部署</tag>
      </tags>
  </entry>
  <entry>
    <title>thymeleaf在@{}中无法使用其他服务器链接的问题</title>
    <url>/thymeleaf%E5%9C%A8-%E4%B8%AD%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E5%85%B6%E4%BB%96%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%93%BE%E6%8E%A5%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>以下是官网原话</p>
<ul>
<li>Absolute URLs: <a href="http://www.thymeleaf.org" target="_blank" rel="noopener">http://www.thymeleaf.org</a></li>
<li>Relative URLs, which can be:<ul>
<li>Page-relative: user/login.html</li>
<li>Context-relative: /itemdetails?id=3 (context name in server will be added automatically)</li>
<li>Server-relative: ~/billing/processInvoice (allows calling URLs in another context (= application) in the same server.</li>
<li>Protocol-relative URLs: //code.jquery.com/jquery-2.0.3.min.js</li>
</ul>
</li>
</ul>
<p>中文版</p>
<ul>
<li>绝对网址： <a href="http://www.thymeleaf.org" target="_blank" rel="noopener">http://www.thymeleaf.org</a></li>
<li>相对URL，可以是：<ul>
<li>页面相对： user/login.html</li>
<li>与上下文相关:( /itemdetails?id=3服务器中的上下文名称将自动添加）</li>
<li>服务器相对:( ~/billing/processInvoice允许在同一服务器中的另一个上下文（=应用程序）中调用URL。</li>
<li>与协议相关的网址： //code.jquery.com/jquery-2.0.3.min.js</li>
</ul>
</li>
</ul>
<p>它就提供这几种方式，但是我的服务器比如是baidu.com，使用<code>th:href=&quot;@{http://sina.com/index.css}&quot;</code>这样是可以的，但是<code>th:href=&quot;@{${sina_static}/index.css}&quot;</code>这样却不行，也就是说静态资源无法使用变量直接取出来，那么以后改动这些资源将十分麻烦，因为只能写死了。thymeleaf不让动态的获取。<br>thymeleaf提供的几种方式没有一种可以显示不同服务器的地址。<br>后来发现还有另外一种方式，就是投机取巧的使用url参数。<br><code>th:href=&quot;@{(由于博客系统原因删除这句话，包括小括号){variable}/index.css(variable=${value})}&quot;</code>这样即可。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>thymeleaf</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>bootstrap-switch常用操作/属性</title>
    <url>/%E5%89%8D%E7%AB%AF/bootstrap-switch%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C-%E5%B1%9E%E6%80%A7.html</url>
    <content><![CDATA[<h3 id="引入css、js"><a href="#引入css、js" class="headerlink" title="引入css、js"></a>引入css、js</h3><p><a href="http://www.bootcss.com/p/bootstrap-switch/" target="_blank" rel="noopener">bootstrap-switch中文官网</a></p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&lt;link <span class="attribute">rel</span>=<span class="string">"stylesheet"</span> <span class="attribute">href</span>=<span class="string">"<span class="variable">$&#123;CONTEXT &#125;</span>/css/bootstrap-switch.min.css"</span>&gt;</span><br><span class="line">&lt;script <span class="attribute">src</span>=<span class="string">"<span class="variable">$&#123;CONTEXT &#125;</span>/js/bootstrap/bootstrap-switch.min.js"</span>&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure>
<p><strong>注：</strong>貌似还有一个叫做bootstrapSwitch.js的插件，不要引入错了。</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">div</span> <span class="built_in">class</span>=<span class="string">"input-group"</span>&gt;</span><br><span class="line">&lt;label <span class="keyword">for</span>=<span class="string">"open"</span>&gt;操作&amp;nbsp;&lt;/label&gt;</span><br><span class="line">&lt;input <span class="built_in">id</span>=<span class="string">"open"</span> checked type=<span class="string">"checkbox"</span> data-<span class="keyword">on</span>-<span class="built_in">text</span>=<span class="string">"开"</span> data-off-<span class="built_in">text</span>=<span class="string">"关"</span> data-<span class="keyword">on</span>-color=<span class="string">"success"</span> /&gt;</span><br><span class="line">&lt;/<span class="keyword">div</span>&gt;</span><br></pre></td></tr></table></figure>
<h3 id="常用属性"><a href="#常用属性" class="headerlink" title="常用属性"></a>常用属性</h3><ul>
<li>data-on-text=”开” data-off-text=”关”，代表开启/关闭时按钮上的文字。</li>
<li>checked代表默认选中，不填代表不选中。</li>
<li>data-on-color=”success”，代表选中时的颜色。</li>
</ul>
<h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><ul>
<li>$(‘#open’).bootstrapSwitch();，开始自动生成一个按钮。注：#open是input的id。</li>
<li>$(‘#open’).on(‘switchChange.bootstrapSwitch’, function (event,state) { }，switch触发的事件，所以不用自己再写click事件了。注：state代表按钮当前选中状态。</li>
<li>$(‘#toggle-state-switch’).bootstrapSwitch(‘toggleState’);，来回切换。</li>
<li>$(‘#toggle-state-switch’).bootstrapSwitch(‘setState’, false);，设置按钮状态，false代表未选中。</li>
</ul>
<p>另外不要写官网上的那个格式，官网上的格式是误导人的，它那个写法：把input放在div里，然后div加一个switch的class，这样其实是代表按钮自动生成后的dom节点。所以按照我这个写就行了。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>assorted</category>
        <category>front-end</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>基于shiro的单点退出功能</title>
    <url>/%E5%9F%BA%E4%BA%8Eshiro%E7%9A%84%E5%8D%95%E7%82%B9%E9%80%80%E5%87%BA%E5%8A%9F%E8%83%BD.html</url>
    <content><![CDATA[<p><strong>2019.01.24更新：发现其他sso系统的实现思路跟我的不一样，我也不知道这样对不对，但是功能是能实现的。</strong></p>
<hr>
<p>由于非单点登录系统在检测用户是否已经单点退出时有点复杂。<br>分两种情况：</p>
<ol>
<li>在每次访问链接时，重定向到单点登录系统进行验证，这是可以做到的，但是用户体验差（这样做或许也可以）。</li>
<li>发送一个http请求，这样可以在用户不知情的情况下进行验证，但是发起http请求，请求的用于不再是当前用户了，即session 不是同一个了。</li>
</ol>
<p>而shiro需要获取subject才能退出登录（我找了很久没有找到其他的办法），所以需要做一个折中的方法，在单点退出的过滤器那，设置session过期，session失效，然后在authc过滤器，判断当前session是否失效，如果失效则执行退出。</p>
<p>2018-5-4更新：<br>最终实现：其他系统发出退出登录请求，upms接收到直接将该用户注销，同时全局session从redis中自动删除，并且在删除之前写入一个名为logout的属性，设置为true（由于session瞬间被删除了，但是并没有多余，因为以后可能加入某些机制，使得session过一会才被删除，所以有必要提前做好防患）。<br>由于upms系统中退出登录，但是没有办法通知其他子系统，所以只能被迫让其他子系统自己来得到通知。具体实现如下：</p>
<ol>
<li>用户在登录时，会在全局session中添加一个属性名为logout，设置为false。</li>
<li>这样当用户在子系统退出后，当前子系统是知道用户已经退出。</li>
<li>但是其他子系统并不知情，所以用户再次访问当前子系统是直接跳转到登录页面。</li>
<li>而当用户访问其他子系统时，子系统需要先跳转到upms系统进行查看，该用户是否退出登录。</li>
<li>规则很简单，upms只需要判断logout属性是否为空或者是否为false即可。</li>
</ol>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>solr使用自动建议搜索出现No suggester named default was configured</title>
    <url>/bug/solr%E4%BD%BF%E7%94%A8%E8%87%AA%E5%8A%A8%E5%BB%BA%E8%AE%AE%E6%90%9C%E7%B4%A2%E5%87%BA%E7%8E%B0No-suggester-named-default-was-configured.html</url>
    <content><![CDATA[<p>solr的默认配置文件中，有一个路径为/suggest的requestHandler，它里面没有配置suggest.dictionary，这个意思就是建议的字典。而默认值可能就是default，所以出现这个错误，只需要配置一下字典就行了。我碰到的时候我也想不明白，用全文搜索都没搜到default。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/b093167a98b9425ca7318393fb6ed37a.PNG" alt="solr配置" title="solr配置"><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>Solr中文分词以及拼写纠错</title>
    <url>/Solr%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E4%BB%A5%E5%8F%8A%E6%8B%BC%E5%86%99%E7%BA%A0%E9%94%99.html</url>
    <content><![CDATA[<p>由于在更改配置文件后，重启tomcat，solr的web页面可以直接看到变化，所以我都懒得使用dataimport了。<br>我之前一直配置中文分词以及拼写纠错失败，我把配置中默认查询字段就改了一个名字，把text改为spell，每一处都替换掉，居然会产生不一样的结果。我搞了一个多小时，发现怎么换都是这样，前后结果完全不同。要么可以分词要么可以拼写纠错，二者没有同时出现过。最后要睡觉的时候，dataimport了一下。发现居然可以了。<br>我猜测可能在建立索引时把text作为键，于是我更改了text的名字，自然是无法查询到任何东西。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>二、将mysql的数据导入到solr中</title>
    <url>/%E4%BA%8C%E3%80%81%E5%B0%86mysql%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0solr%E4%B8%AD.html</url>
    <content><![CDATA[<h3 id="一、solr部署于tomcat"><a href="#一、solr部署于tomcat" class="headerlink" title="一、solr部署于tomcat"></a>一、solr部署于tomcat</h3><p><a href="https://yan624.github.io/一、solr部署于tomcat">上一篇</a></p>
<h3 id="二、将mysql的数据导入到solr中"><a href="#二、将mysql的数据导入到solr中" class="headerlink" title="二、将mysql的数据导入到solr中"></a>二、将mysql的数据导入到solr中</h3><p>tomcat同级目录中的solr被称为solr_home<br>tomcat的webapps文件夹中的solr是solr的web工程<br>将从apache上下下来的solr<font color="red">根</font>文件夹中的/contrib和/dist，复制到solr_home的lib文件夹中，如果没有这个文件夹可以自己建一个。如下图:<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/1375f60bca34409d952810bc12beb13c.PNG" alt="solr lib" title="solr lib"><br>这两个文件夹中全是jar包。现在只是复制过来了，solr还不能用它们，所以需要配置一下。<br>solr_home中应该有一个collection1文件夹，这是solr的一个示例，如果不放心可以拷贝一份，改一个名字。注意如果拷贝了一份，需要进入你拷贝的这份文件夹，找到core.properties，将里面的name属性改为你的文件夹名字。<br>我复制了一份，取名为order，如下图：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/a193f1c3fea84613b0b048c0791dd1b2.PNG" alt="solr官方示例" title="solr官方示例"><br>进入order文件夹，里面有一个conf文件夹，再进入，找到solrconfig.xml文件，打开编辑。前几十行很多注释，往下大概80行的样子，有几个lib标签，这些标签就是代表导入之前的jar包。但是我们的路径发生了改变，所以这里需要配置一下。</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/dataimporthandler/lib"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-dataimporthandler-\d.*\.jar"</span> /&gt;</span><br><span class="line">   </span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/extraction/lib"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-cell-\d.*\.jar"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/clustering/lib/"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-clustering-\d.*\.jar"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/langid/lib/"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-langid-\d.*\.jar"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/contrib/velocity/lib"</span> <span class="attribute">regex</span>=<span class="string">".*\.jar"</span> /&gt;</span><br><span class="line">&lt;lib <span class="attribute">dir</span>=<span class="string">"../lib/dist/"</span> <span class="attribute">regex</span>=<span class="string">"solr-velocity-\d.*\.jar"</span> /&gt;</span><br></pre></td></tr></table></figure>
<p>lib标签dir属性的根目录位于conf文件夹的同级目录。需要注意的是前两个标签，不出意外的话，这两行你们是没有的，这里需要加上这两行。因为它们关系到导入数据。<br>还是在这个文件中，往下拉，大概找到requestHandler标签，其实不找到也行，但是为将相同的标签放在一起便于管理，所以需要找到并在其中一个位置加上下面的代码：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">requestHandler</span> <span class="attr">name</span>=<span class="string">"/dataimport"</span> <span class="attr">class</span>=<span class="string">"solr.DataImportHandler"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">lst</span> <span class="attr">name</span>=<span class="string">"defaults"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">str</span> <span class="attr">name</span>=<span class="string">"config"</span>&gt;</span>data-config.xml<span class="tag">&lt;/<span class="name">str</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">lst</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">requestHandler</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>这个就是代表了一个请求路径，看英文名代表了数据导入。这里需要注意一点，class是solr.DataImportHandler，而不是其它的，我就是因为懒得自己写，顺手改了一个注释掉的标签，然后忘记改class属性，导致数据一直导入失败。里面的config标签就是代表了数据的配置。接下来需要配置这份文件。<br>与solrconfig.xml同级目录中<font color="red">新建</font>一个（如果不存在这个文件的话）data-config.xml。配置如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dataConfig</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dataSource</span> <span class="attr">type</span>=<span class="string">"JdbcDataSource"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">driver</span>=<span class="string">"com.mysql.jdbc.Driver"</span> <span class="attr">url</span>=<span class="string">"jdbc:mysql://localhost:3306/order"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">user</span>=<span class="string">"root"</span> <span class="attr">password</span>=<span class="string">"password"</span> <span class="attr">batchSize</span>=<span class="string">"-1"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">document</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">entity</span> <span class="attr">name</span>=<span class="string">"tb_businessman"</span> <span class="attr">query</span>=<span class="string">"select * from tb_businessman"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"businessman_id"</span> <span class="attr">name</span>=<span class="string">"businessman_id"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"businessman_name"</span> <span class="attr">nam</span>=<span class="string">"businessman_name"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"introduction"</span> <span class="attr">name</span>=<span class="string">"introduction"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"business_hours"</span> <span class="attr">name</span>=<span class="string">"business_hours"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"start_price"</span> <span class="attr">name</span>=<span class="string">"start_price"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"lunch_box_fee"</span> <span class="attr">name</span>=<span class="string">"lunch_box_fee"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"merchant_store_thumbnail"</span> <span class="attr">name</span>=<span class="string">"merchant_store_thumbnail"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"monthly_sales"</span> <span class="attr">name</span>=<span class="string">"monthly_sales"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"average_score"</span> <span class="attr">name</span>=<span class="string">"average_score"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"is_open"</span> <span class="attr">name</span>=<span class="string">"is_open"</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">field</span> <span class="attr">column</span>=<span class="string">"QR_code"</span> <span class="attr">name</span>=<span class="string">"QR_code"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">entity</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">document</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dataConfig</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>dataSource不做讲解，document中的entity标签的name属性指的是类似于一个id，代表在solr的web应用的网页里面的一个select的一个option（目前我是这么理解的）。query就是查数据。column就是数据库列名，name是接下来配置的。<br>接下来配置最后一步，schema.xml文件，也是在conf目录下。<br>里面solr已经配置很多了，所以看起来很麻烦。找到fields标签，这个标签下面就是配置各个字段，就是上面说的name。其中</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"_version_"</span> <span class="attribute">type</span>=<span class="string">"long"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line"></span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"_root_"</span> <span class="attribute">type</span>=<span class="string">"string"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"false"</span>/&gt;</span><br></pre></td></tr></table></figure>
<p>这两个不要删，然后把fields下的所有标签全删了，改成如下：</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"businessman_id"</span> <span class="attribute">type</span>=<span class="string">"string"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span> <span class="attribute">required</span>=<span class="string">"true"</span> <span class="attribute">multiValued</span>=<span class="string">"false"</span> /&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"businessman_name"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span> <span class="attribute">multiValued</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"introduction"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"business_hours"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"start_price"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"lunch_box_fee"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"merchant_store_thumbnail"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"monthly_sales"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"average_score"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"is_open"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"QR_code"</span> <span class="attribute">type</span>=<span class="string">"string"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span> <span class="attribute">multiValued</span>=<span class="string">"true"</span>/&gt;</span><br><span class="line">&lt;field <span class="attribute">name</span>=<span class="string">"text"</span> <span class="attribute">type</span>=<span class="string">"text_cn"</span> <span class="attribute">indexed</span>=<span class="string">"true"</span> <span class="attribute">stored</span>=<span class="string">"true"</span> <span class="attribute">multiValued</span>=<span class="string">"true"</span>/&gt;</span><br></pre></td></tr></table></figure>
<p>篇幅有限，不做太多解释，接下来，在fields标签下一个标签应该是<uniquekey>，把里面的值，改为你的id，我的是businessman_id。注意上面的text_cn这是某个solr的域类型，现在是初步测试，就不写这个了，可以缓存solr已经配置好的text_general，这样虽然我们的数据是中文的，但是完全可以进行简单的处理了。<br>最后还需要一个mysql的jar包，把它放到WEB-INF的lib目录下，就可以启动tomcat访问了。如下图，进行导入数据，点击菜单栏中Query进行查询。（注：4,5两步中间需要等一会，不要马上点，如果成功了，“显示”那会出现绿色字体。）Query中其他参数可自行学习。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/0820f706eae3497599ffd4d1b6783ab2.PNG" alt="步骤" title="步骤"><br><a id="more"></a></uniquekey></p>
]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>系列</tag>
      </tags>
  </entry>
  <entry>
    <title>一、solr部署于tomcat</title>
    <url>/%E4%B8%80%E3%80%81solr%E9%83%A8%E7%BD%B2%E4%BA%8Etomcat.html</url>
    <content><![CDATA[<h3 id="一、solr部署于tomcat"><a href="#一、solr部署于tomcat" class="headerlink" title="一、solr部署于tomcat"></a>一、solr部署于tomcat</h3><p>solr版本：solr-4.7.1<br>操作时间：2018-4-11<br>tomcat版本：tomcat9</p>
<p>由于《solr实战》是用solr4.*，所以我就下了一个版本4的。但是其实已经出到版本7了。<br>下载地址：<a href="http://archive.apache.org/dist/lucene/solr/4.7.1/" target="_blank" rel="noopener">http://archive.apache.org/dist/lucene/solr/4.7.1/</a></p>
<p>我是先照着《solr实战》中基础部分，在jetty中练习了大概一两个小时再移到tomcat中玩的，如果以前没玩过solr建议先知道哪些文档有哪些作用，再移到tomcat。因为移到tomcat和直接在jetty中玩，文件路径有点不一样，而且配置也有点不一样。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/b0e4d6cd5265431fae04c44c4debf8a1.PNG" alt="solr文件路径" title="solr文件路径"></p>
<ol>
<li>将我上图中选中的solr文件夹移动到tomat<font color="red">同级</font>目录下（或者其他目录，这个路径问题是可以自己配置的，下面会说。），该文件夹被称呼为solr_home。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/22852cec88614c66ad0613cf657d6dda.PNG" alt="solr home" title="solr home"></li>
<li>然后将第一个图中<font color="red">webapps文件夹</font>中的一个名为solr.war的war包，移动到<font color="red">tomcat中的webapps文件夹</font>中，就是部署war包。</li>
<li>启动tomcat，让tomcat解压war包。这时webapps下多了一个名为solr文件夹（与上面的solr_home不一样），它的WEB-INF/lib下有很多jar包，但是还是缺少了一些。还是第一个图，进入lib文件夹，里面会有一个ext文件夹把里面的jar包全部复制到tomcat的solr项目的lib文件夹中。这样就差不多完成了。最后结果如下图。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/0dca7561a59340c2b3b404508487c60e.PNG" alt="tomcat中最后结果" title="tomcat中最后结果"></li>
<li>然后之前说solr_home放在了tomcat同级目录下，而solr这个项目并不能自己发现这个solr_home，所以需要配置一下。进入tomcat中的solr项目更改WEB-INF文件夹中的web.xml文件。用ctrl+f的快捷键搜索一下env-entry这个标签。更改为：</li>
</ol>
<figure class="highlight fortran"><table><tr><td class="code"><pre><span class="line">&lt;env-<span class="built_in">entry</span>&gt;</span><br><span class="line">    &lt;env-<span class="built_in">entry</span>-<span class="keyword">name</span>&gt;solr/home&lt;/env-<span class="built_in">entry</span>-<span class="keyword">name</span>&gt;</span><br><span class="line">    &lt;env-<span class="built_in">entry</span>-<span class="keyword">value</span>&gt;D:\tomcat\solr&lt;/env-<span class="built_in">entry</span>-<span class="keyword">value</span>&gt;</span><br><span class="line">    &lt;env-<span class="built_in">entry</span>-<span class="keyword">type</span>&gt;java.lang.String&lt;/env-<span class="built_in">entry</span>-<span class="keyword">type</span>&gt;</span><br><span class="line">&lt;/env-<span class="built_in">entry</span>&gt;</span><br></pre></td></tr></table></figure>
<p>注意其中env-entry-value是我solr_home的绝对路径。这样solr项目就可以找到solr_home了。<br>这个时候启动tomcat应该就可以访问了。项目地址是<code>http://localhost:8080/solr</code>，到现在只是完成了部署，下篇介绍如何导入mysql中的数据。</p>
<h3 id="二、将mysql的数据导入到solr中"><a href="#二、将mysql的数据导入到solr中" class="headerlink" title="二、将mysql的数据导入到solr中"></a>二、将mysql的数据导入到solr中</h3><p><a href="https://yan624.github.io/二、将mysql的数据导入到solr中">下一篇</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>安装与部署</tag>
        <tag>系列</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>访问browse页面solr出现lazy loading error</title>
    <url>/bug/%E8%AE%BF%E9%97%AEbrowse%E9%A1%B5%E9%9D%A2solr%E5%87%BA%E7%8E%B0lazy-loading-error.html</url>
    <content><![CDATA[<p>如图<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/4494aa77feea482388e892e7e6793b8a.PNG" alt="lazy loading error" title="lazy loading error"><br>如果你将下载下来的solr自己部署到tomcat下，然后启动无异常，但是访问browse路径下时，出现该异常。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/c84bd8f2e8b1496284c705b515bbc522.PNG" alt="solr配置" title="solr配置"><br>看选中的那行，参数名wt它的值时velocity，意识是让solr返回以velocity模版引擎构建的页面。而velocity实际上也是一个配置。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/b1c20a04a208423ba9a5cb73aef2caf2.PNG" alt="solr velocity配置" title="solr velocity配置"><br>同一份配置文件，往最下面拉，看见这个配置，原因是没有这个类，<code>class=&quot;solr.VelocityResponseWriter&quot;</code>，这里有问题。所以只需要导入该包，或者直接在访问browse时跟一个参数，wt=xml或者wt=json等等。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>other-framework</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>thymeleaf遍历列表出现的问题</title>
    <url>/thymeleaf%E9%81%8D%E5%8E%86%E5%88%97%E8%A1%A8%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p>今天做一个功能，找了好久，也看了thymeleaf的官网好久，发现th遍历集合居然不能根据索引获取对象。还要后台自己整理结构，真的麻烦。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>thymeleaf</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>吐槽</tag>
      </tags>
  </entry>
  <entry>
    <title>swagger的初步运用</title>
    <url>/swagger%E7%9A%84%E5%88%9D%E6%AD%A5%E8%BF%90%E7%94%A8.html</url>
    <content><![CDATA[<p>网上的介绍很多，我主要描述网上不常见的：<br><code>@ApiImplicitParam</code><br>这个注解有一个属性：paramType。它的有效值仅有5中，分别为：path，query，form，body，header。<br>但是这几个值，我感觉用起来特别麻烦。</p>
<h3 id="path（配合-PathVariable）"><a href="#path（配合-PathVariable）" class="headerlink" title="path（配合@PathVariable）"></a>path（配合@PathVariable）</h3><p>这是最简单的，如/user/{userId}，如果使用这种形式，就可以使用path；</p>
<h3 id="query（配合-RequestParam）"><a href="#query（配合-RequestParam）" class="headerlink" title="query（配合@RequestParam）"></a>query（配合@RequestParam）</h3><p>这个我认为是有bug的，如果在swagger2的配置中这样写：</p>
<figure class="highlight x86asm"><table><tr><td class="code"><pre><span class="line">new Docket(DocumentationType.SWAGGER_2)</span><br><span class="line"><span class="meta">    .apiInfo</span>(new ApiInfoBuilder()</span><br><span class="line"><span class="meta">        .title</span>(<span class="string">"订餐系统_接口文档"</span>)</span><br><span class="line"><span class="meta">        .description</span>(<span class="string">"描述：用于订餐管理系统其他模块的数据调用"</span>)</span><br><span class="line"><span class="meta">        .version</span>(<span class="string">"版本号:1.0"</span>)</span><br><span class="line"><span class="meta">        .build</span>()</span><br><span class="line">    )</span><br><span class="line"><span class="meta">    .select</span>()</span><br><span class="line"><span class="meta">    .apis</span>(RequestHandlerSelectors.any())</span><br><span class="line"><span class="meta">    .paths</span>(PathSelectors.any())</span><br><span class="line"><span class="meta">    .build</span>()</span><br><span class="line"><span class="meta">    .pathMapping</span>(<span class="string">"/"</span>)</span><br><span class="line"><span class="meta">    .directModelSubstitute</span>(LocalDate.class, String.class)</span><br><span class="line"><span class="meta">    .genericModelSubstitutes</span>(ResponseEntity.class)</span><br><span class="line"><span class="meta">    .useDefaultResponseMessages</span>(false)</span><br><span class="line"><span class="meta">    .enableUrlTemplating</span>(true)</span><br></pre></td></tr></table></figure>
<p>看最后一个方法，这就代表你的文档最后生成是这样的：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/ee28b7c4e8f84f33938d2d2c1deced0a.PNG" alt="swagger post的接口样例截图" title="swagger post的接口样例截图"><br>我为什么要说有bug？这看起来很美好，但是当你在网站上测试时，如下<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/737c0b56dad0405cb8a5b0e330ee8380.PNG" alt="url模板bug" title="url模板bug"><br>swagger提供了，在网站上直接测试的功能，但是请看黄色部分，/modify-password后面跟了什么，看起来那只是说明用的{?newPassword…}，直接加在了链接上，导致报了404，找不到该url。<br>就这玩意搞了我好几个小时，后来还是无意中发现的。但是如果把那个方法删了那么就会变成下面的样子。<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/e98acd4958754ee5b5876623b758a493.PNG" alt="正常的post请求url模板" title="正常的post请求url模板"><br>这样就是正常的，就是普通的get请求，但是这里注意一点，post请求就不要用query了，因为它是直接在url后跟参数的。说是post其实被它改成了get（ps：仅在网页上测试时很坑爹，如果只是给其他开发者使用，那是无所谓的，顶多不要在线测试这项功能）。</p>
<h3 id="form"><a href="#form" class="headerlink" title="form"></a>form</h3><p>这个也很坑爹，我也是后来发现的。常见的表单提交，可以f12看到是Query String Parameters，但是这个使用了坑爹的Request Plyload（并没有说它不好）。哪坑爹呢？后台的controller方法里我试了无数个注解，无数个方法接受不到值，要么是null，要么http错误。<br>我最会想到了，也是因为前段时间正好用过—-》我看到swagger在网页上提示是formData，玩过前端的人应该知道js有一个对象叫做：FormData（感兴趣的自己百度）。坑爹的就在这，FormData是用来提交二进制数据的（我是这么理解的），比如一张图片，一份文件。所以我抱着尝试的心态，在Controller的方法中加入了一个参数MultipartFile对象，尼玛，居然正常接收到了参数。<br>综上，如果用了form，想使用在线测试功能，就加个MultipartFile参数。</p>
<h3 id="body"><a href="#body" class="headerlink" title="body"></a>body</h3><p>这个还行，因为如果使用了这个swagger会提交json数据（但是不知道是字符串还是对象，初步估计应该是对象），所以如果使用body，那么@ApiImplicitParam就不用写很多了，比如说，你想接受用户的账号（account），密码（password），那么不需要这样写：</p>
<figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="variable">@ApiImplicitParams</span>(</span><br><span class="line">    &#123;<span class="variable">@ApiImplicitParam</span>(name=<span class="string">"account"</span>,value=<span class="string">"账号"</span>,dataType=<span class="string">"String"</span>, paramType = <span class="string">"body"</span>), </span><br><span class="line">    <span class="variable">@ApiImplicitParam</span>(name=<span class="string">"password"</span>,value=<span class="string">"密码"</span>,dataType=<span class="string">"String"</span>, paramType = <span class="string">"body"</span>)&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>只需要这样：<code>@ApiImplicitParam(name=&quot;user&quot;,value=&quot;用户信息&quot;,dataType=&quot;User&quot;, paramType = &quot;body&quot;)</code><br>就可以了，User就是实体类，在网页上显示就是这样：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/327d110778174272982d99f219254f7e.PNG" alt="swagger中将参数类型设置body后的结果" title="swagger中将参数类型设置body后的结果"><br>它会让你提交json数据，只有body会让你提交json数据，很好区分。<br>但是我为什么要说它坑爹。因为swagger吧User的所有字段都放进去了，虽然可以一个个删掉，但是每次浏览网页都要做一遍，麻烦死了。所以需要使用如下方法：<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/b9b79737da504931b2a940c1a201330f.PNG" alt="用户实体类" title="用户实体类"><br>设置为只读，这样就不会在网页上显示了，我们可以把除了account（图中的tel）和password的字段全部设置为只读。这里放心我测试过了，只读只是在网页看不见，其他的功能暂时没有受到影响。<br><strong>注：</strong>@RequestBody和@ModelAttribute不能一起用<br><strong>更新：</strong>今天测试的时候发现，如果其他客户端使用代码的方式调用会出现不小的问题。这里把我解决的部分写出：content-type必须设置为“application/json”，传递参数时，不需要写键，只需要传一个json字符串即可，并且json字符串必须使用双引号，而不能是单引号（测试于fastxml jackson）。<br>我列出我的代码（使用RestTemplate）<br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/ebcbfdb2368e43be85252b69a65dce96.PNG" alt="使用body的问题" title="使用body的问题"></p>
<h3 id="header"><a href="#header" class="headerlink" title="header"></a>header</h3><p>没有用过，但是是配合@RequestHeader用的<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>swagger</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>shiro角色和权限的用法</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/shiro%E8%A7%92%E8%89%B2%E5%92%8C%E6%9D%83%E9%99%90%E7%9A%84%E7%94%A8%E6%B3%95.html</url>
    <content><![CDATA[<h3 id="shiro通配符的理解"><a href="#shiro通配符的理解" class="headerlink" title="shiro通配符的理解"></a>shiro通配符的理解</h3><p>shiro提供通配符，例如<code>*:*:*</code>的形式<br>意味着：第一部分代表一个域，第二部分代表操作，第三部分代表实例。<br>举个例子，现在有一个权限管理系统，称为<code>:upms</code>，可以写出权限<code>upms:update:coment</code>,这个权限值代表可以在upms这个域中对coment进行update操作。但是也不要思维定死，“域就是一个系统”。<br>再比如说一个权限值<code>upms:update:user</code>,代表可以更新一个用户的信息，这里的域的确是upms，但是并不是所有的用户都用这个域（事实上也不能用）。比如普通的用户也有权限更新自己的信息，那么可以再创建一个权限值比如“user:update:15900000000”,意味着一个用户可以更新15900000000这个账号的信息。</p>
<h3 id="shiro角色和权限的理解"><a href="#shiro角色和权限的理解" class="headerlink" title="shiro角色和权限的理解"></a>shiro角色和权限的理解</h3><p>之前一直想角色和权限的用法差不多，为什么还要分角色和权限？<br>然后看了<a href="http://shiro.apache.org/permissions.html" target="_blank" rel="noopener">shiro官网的文档</a>貌似懂了一些。<br>官网说shiro一共有三种权限粒度，分为：资源级别，实例级别，属性级别（ <a href="http://shiro.apache.org/java-authorization-guide.html#levels-of-permission-granularity" target="_blank" rel="noopener">这三种粒度的描述页面</a>）<br>角色可以控制对资源的访问，比如对一个页面的访问。如果程序中已经设置了隐式角色，那么直接使用注解的形式可以很快速的控制这个网页的浏览权限。<br>而权限是对角色更细粒度的划分。比如现在有一个角色名为“admin”，他拥有对系统中所有权限的CRUD操作，现在需要临时给另外一个很普通的用户（比如只是一个user）查看权限的操作（R）。如果没有shiro提供的权限控制，<font color="red">那么只能给这个用户一个临时的“admin”角色，但是这样他的权限就太大了，因为“admin”是可以查看权限，但是他同样可以删除权限。</font></p>
<p>上述是个人理解。</p>
<p>又看了一遍才看懂了shiro官网的意思。</p>
<ol>
<li>资源级别指的是一个人可以干什么</li>
<li>实例级别指的是一个人可以对什么干什么</li>
<li>属性级别指的是一个人可以对什么上的什么干什么</li>
</ol>
<p>以我的博客网站为例，1代表一个人可以管理整个博客后台，2代表一个人可以对博客类别进行某些操作，3代表一个人可以对博客类别的某一个属性（比如博客类别的状态）进行某些操作</p>
<p>2018-3-17更新，红色部分有错误，如果只给一个用户赋予权限而部赋予角色，用户还是访问不了该链接，因为shiro会执行角色过滤器和权限过滤器，所以用户必须同时满足角色和权限的需求。<br>所以给一个用户赋予临时权限的思路是错的。</p>
<p>2018-3-17再次更新，突然角色这条思路不是全错，因为没有必要为一个链接同时赋予角色和权限，因为一个角色下囊括了多个权限，为何还要多次一举呢？我觉得如果一个链接需要同时赋予角色以及权限，那么肯定是因为该链接非常重要，不允许非该角色的用户操作。<br>综上，如果一个链接很重要，比如删除整个系统的功能，那么肯定是同时需要角色和权限的，而如果是一个很普通的操作，那么可以只赋予权限，而不赋予角色。</p>
<p>但是权限的细分可以快速的为一个角色增加或删除权限，如给一个角色新增一个权限，那么该角色就可以多访问一个链接。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>linux安装nginx</title>
    <url>/linux%E5%AE%89%E8%A3%85nginx.html</url>
    <content><![CDATA[<p>参照<a href="https://www.linuxidc.com/Linux/2016-08/134110.htm" target="_blank" rel="noopener">Linux中Nginx安装与配置详解</a><br>但是出了点小问题，按照步骤往下直到 配置nginx的时候出现了找不到openssl的错误（我的是阿里云的服务器）。<br>然后自己去下了一个openssl的包去安装死活不行。 最后执行<code>yum -y install openssl openssl-devel</code>就可以了。。<br>其次该博文中提示需要在某个文件中加入一句话，并重启防火墙，但是这样做我之前安装的tomcat也访问不了了，所以只要把防火墙关了就可以暂时解决了。</p>
<p>注：博文中的方法是为了开启防火墙，并且只运行80端口访问。我的服务器的防火墙不是iptables，而是firewall，所以不应该用博文中的方法，应该为firewall添加允许80端口访问<br>详见<a href="http://blog.csdn.net/codepen/article/details/52738906" target="_blank" rel="noopener">centos7 Firewall防火墙开启80端口</a><br><a id="more"></a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>安装与部署</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云部署javaweb项目</title>
    <url>/%E9%98%BF%E9%87%8C%E4%BA%91%E9%83%A8%E7%BD%B2javaweb%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<h3 id="配置java运行环境"><a href="#配置java运行环境" class="headerlink" title="配置java运行环境"></a>配置java运行环境</h3><h4 id="查看服务器系统版本"><a href="#查看服务器系统版本" class="headerlink" title="查看服务器系统版本"></a>查看服务器系统版本</h4><p><code>#getconf LONG_BIT</code></p>
<blockquote>
<p>64</p>
</blockquote>
<p>一般都是64了吧</p>
<h4 id="下载jdk"><a href="#下载jdk" class="headerlink" title="下载jdk"></a>下载jdk</h4><p>下载地址：<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">jdk下载地址</a><br>版本：jdk-8u151-linux-x64.tar.gz（我下的版本）<br>使用WinSCP将jdk移动到linux中（有很多类似的软件，使用起来很简单）<br>在使用WinSCP连接时，输入公网IP，并输入帐号密码即可，不需要改变端口号。<br>至于将jdk放在哪个目录，可以看这篇<a href="http://blog.csdn.net/ubuntu64fan/article/details/8289335" target="_blank" rel="noopener">Linux下JDK到底应该安装在哪儿</a><br>我选择放在/usr/local/java下</p>
<h4 id="查看当前系统是否有JDK"><a href="#查看当前系统是否有JDK" class="headerlink" title="查看当前系统是否有JDK"></a>查看当前系统是否有JDK</h4><p>使用rpm -qa |grep jdk<br>如果有就移除</p>
<h4 id="解压JDK"><a href="#解压JDK" class="headerlink" title="解压JDK"></a>解压JDK</h4><p><code>tar zxvf 文件名</code></p>
<h4 id="配置java环境"><a href="#配置java环境" class="headerlink" title="配置java环境"></a>配置java环境</h4><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#vim /etc/profile       </span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/local/java/jdk1.8.0_91   </span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">CLASSPATH</span>=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar   </span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=<span class="variable">$PATH</span>:$JAVA_HOME/bin   </span><br><span class="line"><span class="builtin-name">export</span> JAVA_HOME CLASSPATH PATH</span><br></pre></td></tr></table></figure>
<p>按esc-&gt;输入“:wq”退出</p>
<h4 id="查看java版本，检验是否成功"><a href="#查看java版本，检验是否成功" class="headerlink" title="查看java版本，检验是否成功"></a>查看java版本，检验是否成功</h4><p>在此之前先运行source /etc/profile命令，或者重启机器<br>重启命令sudo shutdown -r now<br>不出意外就可以了</p>
<hr>
<p><strong>java运行环境到此配置成功，接下来安装、配置tomcat</strong></p>
<hr>
<h3 id="下载tomcat"><a href="#下载tomcat" class="headerlink" title="下载tomcat"></a>下载tomcat</h3><p><a href="http://tomcat.apache.org/" target="_blank" rel="noopener">tomcat官网</a><br>我下载的是<a href="https://tomcat.apache.org/download-90.cgi" target="_blank" rel="noopener">tomcat9</a></p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/d6a2325465794dd88baf548bc89131a0.PNG" alt="tomcat官网截图" title="tomcat官网截图"></p>
<p>选择tar.gz即可，后面括号里的按了貌似没反应（我不知道干嘛用的=.=）<br>还是使用WinSCP，将tomcat放到/usr/local/tomcat下</p>
<h3 id="解压tomcat"><a href="#解压tomcat" class="headerlink" title="解压tomcat"></a>解压tomcat</h3><p><code>tar -zxvf apache-tomcat-9.0.2.tar.gz</code><br>之后就是跟在windows系统下一样部署web项目，具体我也在摸索…..=.=</p>
<h3 id="配置完成后依然无法访问"><a href="#配置完成后依然无法访问" class="headerlink" title="配置完成后依然无法访问"></a>配置完成后依然无法访问</h3><p>如果无法通过8080访问，其他原因暂且不说（如防火墙，8080被占用），有一个阿里云的问题，需要配置安全组</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/4b0bf9c8aa524d1f8bcf85b5506d1433.PNG" alt="阿里云安全组" title="阿里云安全组"></p>
<p>如上图端口并没有配置8080，而tomcat默认端口号是8080，所以无法访问到，点击“添加安全组规则”添加即可。</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/8ae0fd824bc0431499b37a9fd4e7c0f3.PNG" alt="添加安全组" title="添加安全组"></p>
<p>如上图所示，添加成功。<br>手太快了，无法对比，之前无法访问的网页忘记截图了，但是我截了访问成功的图片。这里可以看到可以通过公网ip访问了。</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/898416f96b5643de87e73a3579aba6ec.PNG" alt="成功访问tomcat主页" title="成功访问tomcat主页"></p>
<h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><p><img src="http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/attach/52685/cn_zh/1496913639790/%E6%96%B0%E6%89%8B%E5%85%A5%E9%97%A8_%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%89%88_V2.pdf" alt="阿里云的教程"><br>这篇讲的很详细</p>
<h3 id="使用navicat连接"><a href="#使用navicat连接" class="headerlink" title="使用navicat连接"></a>使用navicat连接</h3><p><a href="https://help.aliyun.com/knowledge_detail/37855.html?spm=5176.11065259.1996646101.searchclickresult.7755a429KB6TbE" target="_blank" rel="noopener">Navicat for SQL Server 连接 RDS For SQL Server 数据库</a><br>这篇用sql server连得，但是也差不多。<br>只要复制mysql的外网连接地址去连就行了。</p>
<p><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/8db46e497396457e970db85ff37b756e.PNG" alt="navicat链接成功" title="navicat链接成功"></p>
<h3 id="在阿里云控制台登录数据库"><a href="#在阿里云控制台登录数据库" class="headerlink" title="在阿里云控制台登录数据库"></a>在阿里云控制台登录数据库</h3><p><a href="https://help.aliyun.com/document_detail/26138.html?spm=5176.7741843.2.4.m1jXSQ" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/26138.html?spm=5176.7741843.2.4.m1jXSQ</a><br>这篇文章讲的很简单。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
        <tag>安装与部署</tag>
      </tags>
  </entry>
  <entry>
    <title>关于shiro的session总结</title>
    <url>/%E5%85%B3%E4%BA%8Eshiro%E7%9A%84session%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<ul>
<li>关闭浏览器，使用shiro记住我功能时，session失效，即重新打开浏览器需要重新登录。其实并没有失效，而是再次打开浏览器时，shiro又重新创建了一个session，所以导致之前的session与现在的session不一致。所以造成了session失效的假象.</li>
<li>使用浏览器，在地址栏输入访问的链接但是不按回车时（意思就是只在地址栏输入一个链接不访问网页），系统会产生一个session，但是马上（一秒不到）自动删掉。</li>
<li>使用RememberMe时，断开会话后会重新生成一个session，并且sessionId以及里面原来保存的值全都没有了，但是不需要重新登录（此适用于user级别的过滤链）。</li>
<li>使用RememberMe时，断开会话后会重新生成一个session，并且sessionId以及里面原来保存的值全都没有了，需要重新登录，但是由于是重新登录，本质上session中的值是没有的，实质上登陆一次后又在session中存入了所有属性。（此适用于authc级别的过滤链）</li>
<li>可以看到断开会话后会重新产生一个session，所以之前的session没用了，但是没有自动删掉。而由于user级别的用户不需要重新登录，session中的值全没了（比如用户个人信息：头像等），所以网页中呈现出来肯定很怪。可以创建一个filter将之前会话的值全部复制到当前session中，然后清除之前的session（由于authc级别是不需要复制的，所以只需要清除之前的session即可，但是因为保不准用户在操作时，session中会存入一些其他的值，所以最好也复制一遍）。<a id="more"></a></li>
</ul>
]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>在shiro的realm中注入service引发的惨案</title>
    <url>/%E5%9C%A8shiro%E7%9A%84realm%E4%B8%AD%E6%B3%A8%E5%85%A5service%E5%BC%95%E5%8F%91%E7%9A%84%E6%83%A8%E6%A1%88.html</url>
    <content><![CDATA[<p><strong><em>2019.01.24更新，由于发现了mybatis-plus插件已经实现了我要实现的功能，我自己写的肯定没别人研发的框架写得好，所以此方法已经废弃。</em></strong></p>
<hr>
<p>环境介绍一下：由于service层的代码重复太多，于是我想直接把一些重复的代码抽出来。<br>比如说这个方法，它可以实现根据条件查询，框架是mybatis。而注入的mapper是有spring提供的方法获取到的，问题就出在这里了。<br><figure class="highlight monkey"><table><tr><td class="code"><pre><span class="line">@SuppressWarnings(<span class="string">"unchecked"</span>)</span><br><span class="line">@Override</span><br><span class="line"><span class="keyword">public</span> List&lt;Record&gt; selectByExample(Example example) &#123;</span><br><span class="line">    <span class="class"><span class="keyword">Class</span>&lt;? <span class="keyword">extends</span> <span class="title">Object</span>&gt; <span class="title">mapperClazz</span> = <span class="title">mapper</span>.<span class="title">getClass</span>();</span></span><br><span class="line">    List&lt;Record&gt; res = <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="function"><span class="keyword">Method</span> <span class="title">method</span> =</span> mapperClazz.getDeclaredMethod(<span class="string">"selectByExample"</span>, example.getClass());</span><br><span class="line">        res = (List&lt;Record&gt;) <span class="function"><span class="keyword">method</span>.<span class="title">invoke</span>(</span>mapper, example);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalArgumentException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NoSuchMethodException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (SecurityException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个方法获取到了mapper的类型，即CLass对象。</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SuppressWarnings(<span class="meta-string">"unchecked"</span>)</span></span><br><span class="line"><span class="keyword">public</span> Class&lt;Mapper&gt; getMapperType() &#123;</span><br><span class="line">    ParameterizedType genericSuperclass = (ParameterizedType) <span class="keyword">this</span>.getClass().getGenericSuperclass();</span><br><span class="line">    <span class="keyword">return</span> (Class&lt;Mapper&gt;) genericSuperclass.getActualTypeArguments()[<span class="number">2</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这里使用了我自定义的SpringContextUtil类获取了mapper这个bean。注意我这里使用了构造器初始化，问题就出在这里。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">BaseServiceImpl</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Class&lt;Mapper&gt; mapperType = getMapperType();</span><br><span class="line">    <span class="keyword">this</span>.mapper = (Mapper) SpringContextUtil.getBean(mapperType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>getBean()方法就不提供了，可以自行百度。主要是这个类SpringContextUtil。<br>要想使用spring提供的以java代码方法获取bean有5种方式（大致差不多，我是用了第四种）：实现ApplicationContextAware接口。只需要在实现类中定义private static ApplicationContext applicationContext;变量，再设置set方法，即可使用这个对象获取到bean。<br>到这里一切都没问题，可以当启动tomcat时，问题出来了。<br>它报了一个异常，必须初始化applicationContext。我纳闷了，我之前在Controller层注入service对象用的好好的，为什么在shiro的realm中注入就出事了？<br>我看了我的applicationContext.xml文件，我是这么写的：</p>
<figure class="highlight xl"><table><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:springContextConfig.xml"</span> /&gt;</span><br><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:spring/upms-dao.xml"</span> /&gt;</span><br><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:spring/upms-service.xml"</span> /&gt;</span><br><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:spring/upms-tx.xml"</span> /&gt;</span><br><span class="line">&lt;<span class="keyword">import</span> resource=<span class="string">"classpath:shiro/shiro.xml"</span> /&gt;</span><br></pre></td></tr></table></figure>
<p>然后看了web.xml文件</p>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">&lt;context-<span class="built_in">param</span>&gt;</span><br><span class="line">	&lt;<span class="built_in">param</span>-name&gt;contextConfigLocation&lt;/<span class="built_in">param</span>-name&gt;</span><br><span class="line">	&lt;<span class="built_in">param</span>-<span class="built_in">value</span>&gt;classpath:applicationContext.xml&lt;/<span class="built_in">param</span>-<span class="built_in">value</span>&gt;</span><br><span class="line">&lt;/context-<span class="built_in">param</span>&gt;</span><br></pre></td></tr></table></figure>
<p>好像稍微有点头绪了<br>我在初始化spring上下文的时候，直接初始化了shiro.xml，这没问题。但是在applicationContext.xml中第一行的<code>&lt;import resource=&quot;classpath:springContextConfig.xml&quot; /&gt;</code>里面注册了SPringContextUtil这个工具类。（这个工具类不会一下子注册进去成为bean，它会在spring上下文初始化完毕之后才行）。<br>所以我需要在realm中注册AdminService——-&gt;AdminService继承了BaseServiceImpl——-&gt;BaseServiceImpl在构造器中使用了applicationContext.getBean()。这个步骤在spring上下文初始化还没完成的时候执行，但是你要想使用applicationContext，spring上下文必须初始化完毕，所以冲突了。<br>那我就想直接把shiro.xml托到DispatcherServlet里面初始化就完事了。因为在DispatcherServlet加载的配置文件中，我扫描了Controller层，而在Controller层使用是没问题的（我之前还不懂的时候就这么做的，没报异常）。然后我这么做了，但是又报异常了——-&gt;No Bean Named “shiroFilter”。这不坑爹吗？<br>我先缕一缕，<code>&lt;context-param&gt;</code>最先执行，其次是<code>&lt;filter&gt;</code>，最后是<code>&lt;servlet&gt;</code>（不一定完全正确，但是可以这么理解）。所以说shiro.xml放在DispatcherServlet中不行！因为filter比servlet先执行，它必须要shiro.xml中的bean。所以怎么办？我必须把shiro.xml放在context-param和filter的中间。这根本不可能（起码我做不到）。<br>后来我想到了懒加载（我印象中有听说过这个概念），我一百度还真有。那我应该把懒加载放在哪个bean上呢？我决定放在AdminService上。但是没用，因为Realm这个类不能懒加载，因为它马上就被初始化了。而realm中注入了AdminService，所以即使是懒加载也没屌用。<br>最后终于解决了。<br>既然你要初始化bean，要用构造器，我就把调用applicationContext的代码不放在构造器里。我写了一个initMapper方法。</p>
<figure class="highlight aspectj"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">initMapper</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Class&lt;Mapper&gt; mapperType = getMapperType();</span><br><span class="line">    <span class="keyword">this</span>.mapper = (Mapper) SpringContextUtil.getBean(mapperType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意@Override注解。<br>那么现在问题就是，怎么调用这个方法。<br>经过度娘之后，终于解决了。贴上类，我就不解释了。附上链接<a href="https://www.cnblogs.com/rollenholt/p/3612440.html" target="_blank" rel="noopener">https://www.cnblogs.com/rollenholt/p/3612440.html</a></p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapperInit</span> <span class="keyword">implements</span> <span class="title">ApplicationListener</span>&lt;<span class="title">ContextRefreshedEvent</span>&gt; &#123;</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> onApplicationEvent(ContextRefreshedEvent event) &#123;</span><br><span class="line">        <span class="comment">//下面的方法执行就行了，这个方法会执行两次，我们不需要这么做</span></span><br><span class="line">    &#125;</span><br><span class="line">       </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 需要执行的逻辑代码，当spring容器初始化完成后就会执行该方法。</span></span><br><span class="line"><span class="comment">     * 由于加了<span class="doctag">@PostConstruct</span>注解所以这个方法跟上面的onApplicationEvent一样。</span></span><br><span class="line"><span class="comment">     * 但是这个方法只会在root application context环境中执行，不会在 projectName-servlet context中执行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</span><br><span class="line">    <span class="meta">@PostConstruct</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> initMapper() &#123;</span><br><span class="line">        Map&lt;String, BaseService&gt; beansOfType = SpringContextUtil.getApplicationContext().getBeansOfType(BaseService.<span class="keyword">class</span>);</span><br><span class="line">        <span class="keyword">for</span>(Map.Entry&lt;String, BaseService&gt; <span class="string">entrySet :</span> beansOfType.entrySet()) &#123;</span><br><span class="line">            BaseService service = entrySet.getValue();</span><br><span class="line">            service.initMapper();</span><br><span class="line">            System.out.println(service);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>vo,dto,do,po的理解</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/vo-dto-do-po%E7%9A%84%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<ul>
<li>po:持久化对象，没啥好说的，数据库里一张表对应一个po。</li>
<li>vo:视图对象，是指某个指定页面的所有数据，注意是<font color="red">所有数据</font>。</li>
<li>dto:跟vo差不多，甚至可以代替vo，但是二者理论上是不一样的。vo用于数据显示，dto用于数据传输(展示层与业务层之间)。如性别在数据库里用01表示，在传输时dto直接定义Integer，但是在展示时总不能展示0和1，并且男女也有不同称谓(帅哥美女，先生女士等等)，所以需要另建一个vo用于处理这些逻辑。但是如果没有特殊需求，其实vo就是dto，dto是将业务层数据传输出去，在视图层展示时将以数字表示的性别转换为文字。如果客户端没有要求，那么不传输出数字，直接写上男女也没问题，如果确是有要求，在业务层就不能传出文字，而只能传出数字了。</li>
<li>do:无法里给<br>以上出自:<a href="https://www.cnblogs.com/qixuejia/p/4390086.html" target="_blank" rel="noopener">博客园博主</a><br><img src="https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/e3592ba9de284f128b9183a2434ae018.jpg" alt="领域模型" title="领域模型"><a id="more"></a></li>
</ul>
]]></content>
      <categories>
        <category>java</category>
        <category>javaee</category>
      </categories>
      <tags>
        <tag>印象笔记上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>js一些常用的属性和函数</title>
    <url>/%E5%89%8D%E7%AB%AF/js%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84%E5%B1%9E%E6%80%A7%E5%92%8C%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<p>由于老是忘记js和jq的函数，这里记一下。</p>
<h3 id="javascript"><a href="#javascript" class="headerlink" title="javascript"></a>javascript</h3><ol>
<li>string.replace(//, “”)<br>第一个参数是正则，表达式在//两个之间写，第二个参数是需要替换成什么。</li>
<li>string.substr()<br>用法和一样</li>
<li>new Date(date).toLocaleString()<br>将毫秒数转成本地的时间，如中国：2017/9/4 下午9:42:41</li>
<li>(a/b).toFixed(2)<br>相除保留两位小数，如700/100等于7.00。要不然就是只等于7（java里整数相除只得到整数部分）</li>
</ol>
<h3 id="Jquery"><a href="#Jquery" class="headerlink" title="Jquery"></a>Jquery</h3><ol>
<li>$(“#id”).attr()<br>为一个节点添加属性，一个属性:attr(“name”, “file”)。多个属性:attr({“name”:”file”, “type”,”button”})</li>
<li>$(“#id”).remove()<br>移除节点</li>
<li>$(“#id”).removeAttr(“”)<br>移除一个节点的属性</li>
<li>$(“#id”).children(“:first”)<br>一个节点的子节点</li>
<li>$(“#id”).offset()<br>一个节点离网页顶部的top值和left值，不是离浏览器的top值和left值（以整个网页的高度做计算）。<br>$(“#id”).offset().top代表节点的top值，left值同理。</li>
<li>$(“body,html”).animate({<br> scrollTop:scroll_offset.top //让body的scrollTop等于pos的top，就实现了滚动<br>},0);<br>滚动监听</li>
<li>window.location.search<br>获得当前url，如：<a href="http://localhost:8080/contextpath/index.html" target="_blank" rel="noopener">http://localhost:8080/contextpath/index.html</a></li>
<li>$.parseJSON(string)<br>将一个字符串转为json对象，但是不要在用ajax的时候，data:$.parseJSON(string)这样用，我搞了2个小时，没查出错，原来是这里错了。<br>string json = ‘{“comment”:”abcdrf”,”name”:”xiaowang”}’;<br>我在用$.ajax()的时候，把data赋值$.parseJSON(json)，出错了！！！data:json。后台接受的很成功。<br>但是后台传来一个json字符串，用这个方法转成json对象，可以直接通过.取值挺方便的。</li>
<li>new FormData().append()<br>使用 FormData对象获取到form中的数据后，可以继续append自己想要添加的数据。<a id="more"></a></li>
</ol>
]]></content>
      <categories>
        <category>assorted</category>
        <category>front-end</category>
      </categories>
      <tags>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>再探String</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%86%8D%E6%8E%A2String.html</url>
    <content><![CDATA[<p>估计是最终版了。</p>
<figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line"><span class="keyword">String</span> s1 = <span class="keyword">new</span> <span class="keyword">String</span>(<span class="string">"张三"</span>);</span><br><span class="line"><span class="keyword">String</span> s2 = <span class="keyword">new</span> <span class="keyword">String</span>(<span class="string">"张三"</span>);</span><br><span class="line"><span class="keyword">String</span> s3 = <span class="string">"张三"</span>;</span><br><span class="line"><span class="keyword">String</span> s4 = <span class="string">"张三"</span>;</span><br></pre></td></tr></table></figure>
<p>我们都知道<code>String s3 = &quot;张三&quot;;</code>这句代码的”张三”字符串位 于常量池中，而s3==s4这种明显就是小儿科，true！<br>我们来解析一下<code>String s3 = &quot;张三&quot;;</code>这句代码。</p>
<ol>
<li><p>首先s3进入栈中，”张三”进入常量池中，”张三”返回一个地址，s3拿到这个地址，所以我们就可以拿着这个地址做一些事情。这应该都很熟悉了，那么问题来了：“String s1 = new String(“张三”);”这句代码是怎么执行的？</p>
</li>
<li><p>首先我们要知道一个定义：成员变量和局部变量。<br>s3是一个局部变量，当然是相对来说，比如说上面的4句代码都位于一个方法中，那么s3是一个局部变量。</p>
</li>
<li><p>接下来我们解析<code>String s1 = new String(&quot;张三&quot;);</code>这句代码。<br>s1还是一个局部变量所以进入栈中，new String会在堆中创建一个String实例对象（OK很简单），那么这个<br>“张三”怎么办？注意了！String对象是如何存储字符串的？实际上就是一个char数组（同时也解释了一个中文可以用一个char来存储），所以”张三”在String中实际上是这样的：value[0]=’张’，value[1]=”三”。<br>这个value是一个char数组，而它优势String的成员变量也就是说，value这个变量不会进入栈，而是跟着String在堆内存中（实际上不是，我先打个比喻）。<br>String对象的构造器还没完！它还会初始化一个hash码，这个是String自己初始化的，不用你来控制，所以hash码这个变量也是跟着String进入堆中。</p>
</li>
</ol>
<p>现在我们来缕一缕———-&gt;String在堆中开辟一个空间，这个空间里有value数组，hash变量（其他的我就不说了，我上面也说过了，实际上value并不在String中）。</p>
<p>知道上面的一切之后，就可以分析了。</p>
<p>是不是”张三”这种形式的写法，字符串就进入了常量池？比如：<code>String s1 = &quot;张三&quot;;String s1 = new String(&quot;张三&quot;);StringBuilder s3 = new StringBuilder(&quot;张三&quot;);StringBuffer s4 = new StringBuffer(&quot;张三&quot;);</code></p>
<p>其实我们有办法做到，观察是否进入了。<br>但是之前先考虑，<code>String s1 = new String(&quot;张三&quot;);</code>和<code>String s3 = &quot;张三&quot;;</code>这两个”张三”是同一个字符串吗？<br>其实看我上面的某些描述，可能也推测出来了，这两个”张三”就是同一个（指同一个地址）。下面讲如何做到观察是否为同一个。</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">String <span class="built_in">s1</span> = new String(<span class="string">"abc"</span>)<span class="comment">;</span></span><br><span class="line">String <span class="built_in">s2</span> = new String(<span class="string">"abc"</span>)<span class="comment">;</span></span><br><span class="line">Class&lt;?&gt; clazz1 = <span class="built_in">s1</span>.getClass()<span class="comment">;</span></span><br><span class="line">Class&lt;?&gt; clazz2 = <span class="built_in">s2</span>.getClass()<span class="comment">;</span></span><br><span class="line">Field declaredField1 = clazz1.getDeclaredField(<span class="string">"value"</span>)<span class="comment">;</span></span><br><span class="line">Field declaredField2 = clazz2.getDeclaredField(<span class="string">"value"</span>)<span class="comment">;</span></span><br><span class="line">declaredField1.setAccessible(true)<span class="comment">;</span></span><br><span class="line">declaredField2.setAccessible(true)<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField1.get(<span class="built_in">s1</span>) == declaredField2.get(<span class="built_in">s2</span>))<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField1.get(<span class="built_in">s1</span>))<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField2.get(<span class="built_in">s2</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>上面的代码结果如何，显然已经解释的很清楚了。</p>
<p>解</p>
<p>首先”张三”实际上是一个char数组，而它是String的成员变量，所以它硬该在堆内存中，但是事实是在常量池。<br>常量池：看名字就知道是存常量的。<br>在网络上的熏陶，我们知道String name = “李四”；”李四”在常量池，所以name是一个常量（无语ing，别被带偏了，我在今天之前也是这么认为的）。<br>被final修饰的才是常量。现在你们大概猜出什么了。去看String的源码吧，看看value这个成员变量的前面是用什么修饰的！！！自己去看才会记忆深刻，我就不贴源码了。</p>
<p>还没完<br>是不是”张三”这种形式的写法，字符串就进入了常量池？<br>我之前问了这个问题。答：不是。<br>StringBuilder name1 = new StringBuilder(“张三”);<br>String name2 = “张三”;<br>这时候又不一样了。<br>这两个”张三”不是同一个（也就是说地址不一样）。话不多说上代码。</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">String <span class="built_in">s1</span> = new String(<span class="string">"abc"</span>)<span class="comment">;</span></span><br><span class="line">StringBuilder <span class="built_in">s2</span> = new StringBuilder(<span class="string">"abc"</span>)<span class="comment">;</span></span><br><span class="line">Class&lt;?&gt; clazz1 = <span class="built_in">s1</span>.getClass()<span class="comment">;</span></span><br><span class="line">Class&lt;?&gt; clazz2 = <span class="built_in">s2</span>.getClass()<span class="comment">;</span></span><br><span class="line">clazz2 = clazz2.getSuperclass()<span class="comment">;</span></span><br><span class="line">Field declaredField1 = clazz1.getDeclaredField(<span class="string">"value"</span>)<span class="comment">;</span></span><br><span class="line">Field declaredField2 = clazz2.getDeclaredField(<span class="string">"value"</span>)<span class="comment">;</span></span><br><span class="line">declaredField1.setAccessible(true)<span class="comment">;</span></span><br><span class="line">declaredField2.setAccessible(true)<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField1.get(<span class="built_in">s1</span>) == declaredField2.get(<span class="built_in">s2</span>))<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField1.get(<span class="built_in">s1</span>))<span class="comment">;</span></span><br><span class="line">System.out.println(declaredField2.get(<span class="built_in">s2</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>上面的结果有没有出乎意料？地址打印出来都不一样。你们也猜到了什么吧。<br>去看StringBuilder的源码吧。StringBuffer同理。<br>注：StringBuilder中没有value，但是它的父类AbstractStringBuilder有，去看看value前面是什么修饰的。</p>
<hr>
<p>2018.9.6更新<br>java1.7之后貌似有很大变化。上面的理论可能不太适用</p>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>解决li标签使用inline-block时产生的4px</title>
    <url>/%E5%89%8D%E7%AB%AF/%E8%A7%A3%E5%86%B3li%E6%A0%87%E7%AD%BE%E4%BD%BF%E7%94%A8inline-block%E6%97%B6%E4%BA%A7%E7%94%9F%E7%9A%844px.html</url>
    <content><![CDATA[<p>完美兼容的方法<br>父元素（比如ul）设置font-size:0;<br>使用letter-spacing:-Npx;<br>li标签重新设置字体大小<br>原文出处：解决inline-block出现的间隙<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>assorted</category>
        <category>front-end</category>
      </categories>
      <tags>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>理解接口和抽象类</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%90%86%E8%A7%A3%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8A%BD%E8%B1%A1%E7%B1%BB.html</url>
    <content><![CDATA[<p>这篇只是自学java基础时的个人理解而已。</p>
<p>初学abstract和interface时会感到很疑惑，明明这两个没有具体实现的方法，为什么还要继承或者实现他们呢？明明可以自己在自己编写的类中写一个方法，而不去继承或实现。现在看来大概是因为一种规范，也是为了让别人更好的看懂，如果自己写方法，自己去用，当然不需要多此一举。如果编写一个Run的接口，让别人去用，别人一看就知道这个接口是用来让某种东西跑起来。</p>
<ul>
<li>抽象类<br>它只有声明，而没有具体的实现。抽象方法必须加上abstract关键字。如果一个类含有抽象方法即这个类是抽象类，但是抽象类也可以不含有抽象方法，不过这样定义一个抽象类就毫无意义。<br>抽象类中也可以拥有成员变量和普通的成员变量。<br>抽象类与普通类主要有三种区别：</li>
</ul>
<ol>
<li>抽象方法必须为public或者protected，缺省情况下为public</li>
<li>抽象类不能被用来创建对象</li>
<li>如果继承一个抽象类则必须实现它的抽象方法，如果不实现则必须将这个方法也加上abstract。</li>
</ol>
<ul>
<li>接口<br>在软件工程中，接口泛指供别人调用的方法或者函数。它是对行为的抽象。<br>接口可以含有变量和方法，但是它们都会被自动加上public static final关键字，如果没有加上，系统会自动添加。<br>一个类可以实现多个接口。<br>接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法；</li>
<li>区别<br>抽象类是“是不是”的关系，定义一个animal类，再定义一个Dog类，animals extends Dog，是一种“是不是”的关系，即狗是不是动物。<br>接口是“有没有”的关系，定义一个interface Eat，实现它，Dog implements Eat，即Dog类实现Eat接口，狗有没有吃这种功能。<br>具体内容引自 <a href="http://www.cnblogs.com/dolphin0520/p/3811437.html" target="_blank" rel="noopener">http://www.cnblogs.com/dolphin0520/p/3811437.html</a><br>这篇只是为了帮助自己理解二者的区别。</li>
</ul>
<a id="more"></a>]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>自动装箱和自动拆箱功能</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%87%AA%E5%8A%A8%E8%A3%85%E7%AE%B1%E5%92%8C%E8%87%AA%E5%8A%A8%E6%8B%86%E7%AE%B1%E5%8A%9F%E8%83%BD.html</url>
    <content><![CDATA[<p><code>Integer a = new Integer(1000);</code><br>在jdk5.0之后走了自动装箱功能<br><code>Integer a = 1000;</code>即与上面的等式相同。<br>在编译阶段，编译器帮助我们改进代码，将<code>Integer a = new Integer(1000);</code><br>自动改为<code>Integer a = 1000;</code></p>
<p><code>int c = new Integer(1500);</code>进行自动拆箱。<br>编译器将上述代码自动改为<code>int c = new Integer(1500).intValue();</code></p>
<p>但是在–128到127之间的数依然会当做基本数据类型处理。</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">Integer <span class="built_in">a1</span>=<span class="number">100</span><span class="comment">;</span></span><br><span class="line">Integer <span class="built_in">a2</span>=<span class="number">100</span><span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>a1==a2返回的照理说应该是false，但是在处理时，它只是基本数据类型，所以返回true。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>StringBuilder和StringBuffer的区别</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/StringBuilder%E5%92%8CStringBuffer%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<p>StringBuilder线程不安全，效率高。<br>DtringBuffer线程安全，效率低。<br>都是可变字符序列。<br><code>StringBuilder s = new StringBuilder()</code>字符数组长度初始化为16<br><code>StringBuilder s = new StringBuilder(32)</code>字符数组长度初始化为32<br>其中的append方法最终返回this，可以实现方法链，<code>s.append(&quot;ab&quot;).append(&quot;cd&quot;)</code></p>
<figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">StringBuilder</span>(String s)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="selector-tag">super</span>(s.length() + <span class="number">16</span>);</span><br><span class="line">	<span class="selector-tag">append</span>(s);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StringBuilder类的其中一个构造器，建立一个初始化内容的长度加上16的数组<br>如果传入的一个数组比初始化的大，则会进行数组扩容。<br>新容量 = 旧容量 * 2+2<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>区分“==”和equals</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%8C%BA%E5%88%86%E7%AD%89%E5%8F%B7%E2%80%9C==%E2%80%9D%E5%92%8Cequals.html</url>
    <content><![CDATA[<p>比如</p>
<figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line"><span class="keyword">String</span> s1=<span class="keyword">new</span> <span class="keyword">String</span>(<span class="string">"abc"</span>);</span><br><span class="line"><span class="keyword">String</span> s2=<span class="keyword">new</span> <span class="keyword">String</span>(<span class="string">"abc"</span>);</span><br></pre></td></tr></table></figure>
<p>s1==s2返回的是faluse，因为s1指向的是String类的value数组，而s2指向的是另一个String类的value数组，对象不同。<br>而s1.equals(s2)返回true。因为eauals比较的是内容，即value数组。</p>
<figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">String </span><span class="built_in">s3</span>=<span class="string">"abc"</span><span class="comment">;</span></span><br><span class="line"><span class="keyword">String </span><span class="built_in">s4</span>=<span class="string">"abc"</span><span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>这个初始化的，s3==s4和s3.equals(s4)返回的都是true，因为s3和s4指向的都是方法区的常量池<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
  <entry>
    <title>new String(abc)与String s=abc的区别</title>
    <url>/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/new-String-abc-%E4%B8%8EString-s-abc%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<p>String类是不可变字符序列。<br>比如<code>String s=&quot;abc&quot;;</code><br>原理是将字符串”abc”存入value数组，而value数组是final，所以value数组是不能改变的，但是String产生的s对象，s这个指针是可变的。<br>局部变量s在栈中产生，指向在堆(方法区的常量池)中的”abc”,然后如果想在字符串后面添加字符，则会诞生一个新的字符串(比如”abcd”)，然后将s指针指向”abcd”。<br>而<code>String s=new String(&quot;abc&quot;);</code><br>同理s产生在栈中，字符串”abc”产生在堆(方法区的常量池)中，但是s不直接指向”abc”。<br>new String在堆中产生一个对象，由常量池中的”abc”对它进行初始化，而s对象指向的是new String这个对象。<br>s指向String，String指向”abc”。</p>
<p>总结:new String()比String多创建了一个对象，浪费内存。<br>用<code>String s=new String()</code>创建了在栈中的s对象以及String类对象。<br>用String s仅仅创建了s对象<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>新浪博客上的博文</tag>
      </tags>
  </entry>
</search>
